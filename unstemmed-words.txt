baiduresearch sunnyvale ca wangyang yangyi zhaoliang wei xu baidu comzhenheny usc edu abstract visedlearning however inthisworkwe motion chairs mpi espe exist introduction ornamelyopticalw isafun withtheaccurate opticalwprediction onecouldestimatethe dstructure ofascene cues andvideoobject detection traditionally networks cnns wherethe ordirectlyoutputthe densewinanend to endmanner onemajor classicalenergy wheremoststate of the artenergy basedmethodsrequire whiledeepnets onlyneedaround ing unfortunately mostlarge andground erallyhardtoannotate un theoverallstrategybe truthw theyusea targetimageandthe inversely volutionalnetworks end to truth wannotations however counterparts estimation learningmethods inthispaper weproposeanewend to issues more targetimage duringreconstruction therewillbeafraction toocclusion itcouldlimit otherpixels forexample infig wewouldliketoesti toframe andrecon structframe bywarpingframe withtheestimatedw http www cvlibs net datasets kitti eval_ scene_flow php benchmark flow arxiv cs cv nov figure inputframe inputframe ground truthopticalw imagewarpedbyground truthopticalw backwardopticalw estimatedbyourmethod opticalwfrom imagewarpedby image itmovesinthedown leftdirection andsomepart whenwewarpframe backtoframe usingtheground truthw fig the resultingimage fig hastwochairsinit thechairon thetop rightistherealchair because theground thechair inframe iscarriedbacktoframe tointheoccluded background therefore frame warpedbytheground truth fromthe otherperspective whichis illustratedinfig larappearance hasonlyonechairinit toaddressthisissue moreconcretely we fig anduseitto fig frame occluded fig canhavetwochairsinit fig without methodsinfouraspects weproposedanewend to ourworkisso learningforopticalw wedevelopedanewwarp motion the asaresult mpi sintel andkitti onkittibenchmark where relatedwork decades duetopagelimitation wewill learningapproaches opticalwestimation thepioneeringworks startingfromthen the and mpi sintel benchmarkdataset mostclassicalopti othertrendsin adesignof andmotionblur whichwewillde however accuracy sofar of the artresults occlusion sinceoc itisin andocclusion wedividethem intothreemajorgroups thelast aboutocclusion ourmodelissimilarto thesecondgroup to end trainablefashion thesuccessofdeep usesdeep formation later basedpatchmatch ingfeatures accuracyandfastspeed meanwhile to demonstratesthatwith fashion later cludingmpi sintel andkitti witharelatively however thesupervisedlearn figure itcontainstwocopies offlownets thefor backwardw inalframe overthenon occludedarea thereisalsoa tothelackofground datasets intro ducesanend to supervisedse later adoptasim markdatasets truth works propagation to endmanner this butthewes inthispaper weshowthat overallstructure ournetworkcontains theup and asinput from to and from figure demonstratinghowthe occlusionmap isgeneratedusing thebackwardopticalw componentopticalw and where denotesmov ingright denotemovingleftand denotesstationary in theocclusionmap denotesoccludedand denotesnon occluded to theforwardw isusedtowarp toreconstruct wecallthis backwardwarping thebackwardw by forwardwarp ing thatis regionin thatdoes photometricterm andasmoothnessterm forthe photometricterm andthe originaltargetimage inthenon occludedregion toob ingmethods appliedto wemodelthe non occludedregionin astherangeof which max max where isthe rangemap valueatlocation and are since iscontinuous onanimagegrid figure neighbors the occlusionmap canbeobtainedbysim atthevalueof andthe min to endwith therestofthenetwork and haveonly pixelseach pixelvalues and themotion from to andcoversit wecreateanimage therefore theoneatthetop top zero thetop rightcorner of isoccludedbypixel whichis the from withforwardopticalw themethodadoptedhere issimilarto space estneighbors proposedposition dientsignals forexampleinfig inthepositionof calw neighbors however at direction thisprob al themotionmightnot moreconcretely towarp backtoreconstruct atagridpoint in theyellow square to in becausethepoint isnotonthegridpoint in normally thevalueat nearestneighbors rightsideoffig neighbor aroundthepoint forinstance ifintheenlarged neighborofpoint is weassignthe valueatthepoint at pointslabeled withredcrossesinfig withrespecttopoint by doingthis lossterm ponents aphotometricloss andasmoothnessloss overthe non agegradient where and theloss trivialsolutions forthesmoothnessloss weadoptanedge awareformu lationsimilarto sincetheoccludedarea theopticalwestima loss byusinganedge theopti weuseboth andsecond figure ofthedecodingstage ontheleft weshowtheoriginal flownetsstructure ontheright weshowour conv andconv arefea image andimage times cordingly where and indexesover partialderivativeon and directions thelossisa flownetworkdetails ourinnerwnetworkis adoptedfromflownets sameasflownets weuse amulti bydown the we similartoflownet bydoingthis eachlayeronly scale by anditmoderately preprocessing ingobjectsinthedown sampledimages wepreprocessthe the we estimationresults methods chairs kitti kitti test traintesttraintest traintesttraintest supervise flownets œœœ flownets ft œœ spynet œœœ spynet ft œœ flownet flownet ft unsupervise dstflow dstflow best backtobasic œœœœ œœ ours ours ft sintel ours kitti table point error epe kitti test fl all apixel pointerroris pxor forallmetrics smallerisbetter thebest setofdata experimentalresults mpi sinteland kitti pervisedmethods epe whichistheaver to endusingadamopti mizer with and thelearning rateissettobe for or dependingonthein putimageresolution aday duringtraining to thehyper parameters aresetto be sinteldatasets and forkitti dataset weonlyusedhorizontal ping dur ingtesting thetotal addinganex tra anopencv cpuimplementation milliseconds table viousstate of the including flownet spynet flownet dstflow andbacktobasic bestﬂhere flyingchairs agesfromflickr weuseittotrainour truthw werandomly splitthedatasetinto trainingand testing welabel ourepeis epe decreasesfrom to mpi sintel sincempi weusethe ft sintelﬂ comparedtoother unsupervisedmethods mance epedecreasesfrom to onsintel figure fromsintelfinal figure fromkitti cleantest here fig sintel kitti world drivingconditions beleddata thetrainingdataweuse hereissimilarto viewexten sions frombothkitti andkitti duringtraining weexcludetwoneigh truthw ineachmulti view sequence ture andabundantdata we kittiﬂintable table improvesepefrom to onkitti test but testtraintrain xx xxx xx xxx xxxx table ablationstudy method cleanfinal our œœ modof œœ table thenumberswe measure thes dmethodis trainedwithground truthocclusionlabels flownets ft byalargemargin tothestate of the fig ourmodelcor theframe seenindstflow see figure intheocclusionarea occlusionestimation estimationonmpi videground frames amongtheliteratures weonlylimitedreports table showstheocclu measureintroducedin onmpi sintel ourmethod neural network basedmethods onkittiweobtain and forkitti andkitti respectively wedid notethats dusedground ablationstudy table showstheover sintel our whichisthesameas occlusionhandling thetoptworowsintable sug network to on flying chairsandfrom to onmpi sintelfinal whichis enlargedsearch showthat addingenlargedsearch theepeimprovesfrom to onflying chairsandfrom to onmpi sintel final flownet asmalltothe flownetalsoimproves assuggestedinthe throwintable byonlyaddinga moreparameters andcomputation theepeimprovesfrom to on flying chairsandfrom to onmpi sintelfinal contrastenhancement bycom paringthe wethe epeimprovesfrom to onflying chairsand to onmpi sintelfinal wealsothatsome butthe effectofdata kittirawvideos samplescomparedto totrainourmodel butwedid notanyimprovement andpwc net and howeverwedid although conclusion wepresentanewend to weshowthatwith ourunsupervisedap datasets references ahmadiandi patras in imageprocessing icip pages ieee alvarez deriche papadopoulo andj anchez sionsdetection ayvaci raptis ands soatto in advances pages ayvaci raptis ands soatto sparseocclusionde tectionwithopticalw vision bai luo kundu andr urtasun exploitingse in pages springer bailer taetz andd stricker flowdensecorre calwestimation in pages bailer varanasi andd stricker cnn basedpatch loss in recognition cvpr baker scharstein lewis roth black and szeliski calw ballester garrido lazcano andv caselles tv pattern recognition pages bao yang andh jin fastedge preservingpatch in proceedings recognition pages blackandp anandan plemotions smoothw bouguet intel corporation brox bruhn papenberg andj weickert highac computervision eccv pages broxandj malik de ieee butler wulff stanley andm black in pages springer chen jin lin cohen andy wu largedis pro patternrecognition pages dosovitskiy fischer ilg hausser hazirbas golkov vandersmagt cremers andt brox flownet in computervision pages finn goodfellow ands levine unsupervisedlearn in ad pages forsythandj ponce computervision amodernap proach uppersaddleriver nj london prenticehall godard macaodha andg brostow unsuper rightconsistency arxivpreprintarxiv uneyanda geiger deepdiscretew in asiancon pages springer hafner demetz andj weickert whyisthecensus in inter odsincomputervision pages springer hornandb schunck determiningopticalw intelligence hu song andy li efpatch in proceedings recognition pages hurands roth in europeanconferenceon computervision pages springer hurands roth arxivpreprint arxiv ilg mayer saikia keuper dosovitskiy and brox flownet withdeepnetworks arxivpreprintarxiv inceandj konrad occlusion awareopticalwestima tion jaderberg simonyan zisserman etal spatial transformernetworks in processingsystems pages janai gney wulff black anda geiger slow exploitinghigh in cvpr jason harley andk derpanis backtobasics in computervisionœeccv workshops pages springer kingmaandj ba adam mization arxivpreprintarxiv krizhevsky sutskever andg hinton imagenet in pages leordeanu andc sminchisescu locally afsparse to timation in enceoncomputervision pages lucas kanade etal mayer ilg hausser fischer cremers dosovitskiy andt brox opticalw andscene westimation in pages menze heipke anda geiger discreteoptimization foropticalw in tion pages springer pathak girshick doll ar darrell andb hari haran arxiv preprintarxiv patraucean handa andr cipolla spatio temporal arxiv preprintarxiv ranjanandm black arxivpreprintarxiv ren yan ni liu yang andh zha unsu in aaai pages revaud weinzaepfel harchaoui andc schmid edge foropticalw in pages sevilla lara sun jampani andm black op in pages sevilla lara sun learned miller andm black in pages springer simonyananda zisserman two streamconvolutional in advances pages stein sustransform in dagm symposium volume pages springer strecha fransens andl vangool aprobabilistic detection in eccvworkshopsmvp pages springer sun liu andh ster in proceedingsofthe tion pages sun roth andm black secretsofopticalw in computervisionandpat ternrecognition cvpr ieeeconferenceon pages ieee sun sudderth andm black layeredimage temporalconsistency and depthordering in ingsystems pages sun yang liu andj kautz pwc net cnnsfor opticalwusingpyramid warping andcostvolume arxiv preprintarxiv sun li kang andh shum symmetricstereo in computervisionand patternrecognition cvpr ieeecomputerso cietyconferenceon volume pages ieee tao bai kohli ands paris non iterative in computer graphicsforum volume pages wileyonline library unger werlberger pock andh bischof joint in computervision cvpr ieeeconferenceon pages ieee vijayanarasimhan ricco schmid sukthankar andk fragkiadaki sfm net tionfromvideo arxivpreprintarxiv vogel roth andk schindler anevaluationofdata costsforopticalw in recognition pages springer weinzaepfel revaud harchaoui andc schmid ing in computervision pages xu ranftl andv koltun accurateoptical arxivpreprint arxiv xu jia andy matsushita ticalwestimation yamaguchi mcallester andr urtasun efjoint segmentation occlusionlabeling stereoandwestimation in pages springer zabihandj wnon in europeanconfer enceoncomputervision pages springer zhu wang dai yuan andy wei flow guided arxivpreprint arxiv 
astudyof re scoringmethodsand yilinyang lianghuang mingboma corvallis or usa yilinyang liang huang sh cosmmb gmail com baiduresearch sunnyvale ca usa abstract chinetranslation it however beam sizeslargerthan we andproposesev fur thermore teriaforthesemethods resultsshowthat ourhyperparameter thewidely usedhyperparameter freeheuristic bleu and chinese to englishtranslation introduction inrecentyears nmt basedorsyntax becomingthenewstate oftheartinmt sutskeveretal bahdanauetal search ranzato etal shenetal and wiseman andrush awidely usedheuristic unlikephrase intheor derof koehnetal mostnmt forexam ple google sgnmt wuetal andface book sconvs gehringetal usebeam sizes and respectively intuitively thelarger thebeamsizeis basedmt systems surprisingly itis not thecasefornmt or tu etal koehnandknowles wecall thisphenomenonthe ﬁbeamsearchcurseﬂ which nmt koehnandknowles however onthisproblem huangetal hintthat butdonotexplainwhy bleu ottetal namelythe copying non literal translations however theproblemisonly seemtobotherpre neuralmtsystems therefore ontheotherhand butwithvari ouslimitations forexample rnnsearch bah danauetal andconvs suse lengthnor malization which wewillshowinsec seems butfarfrom beingperfect meanwhile heetal and huangetal useword reward buttheir re ward isahyper sec and searchcurse sec weshowthatour hyperparameter freemethod length normalization by bleu sec rescoringmethods sec experiments the tionmethodimprovesby bleu preliminaries nmtandbeamsearch wereviewtheencoder decoderarchitec bahdanauetal hiddenstates foreachtimestep thernnde generatedtherefore whendoinggreedy search attimestep thedecoder eos inthe end with eos withmodelscore log quality let denotethebeamsize thenatstep thebeam isan ordered listofsize top fh ijh inthemostnaivecase afterreachingthemaxi mumlength ahardlimit weget possiblecan didatesequences the default strat we beamsearchcurse bleu papinenietal isas bleu bp exp log where bp min lr where lr here arethe gramprecisions and and while bp isthe brevitypenalty penalizingshort translations and lr isthe lengthratio shietal koehnandknowles respectively figure bleu alltermsarecal culatedby multi bleu pl figure erates eos earlier andthird eos decrases which asshowninfig asafunctionofthe lengthratio since bp isakeyfactorinbleu thisexplainswhythe thereasonwhy asbeamsizeincreases themorecandidates itcouldexplore therefore itbecomeseas eos symbol fig showsthatthe eos indices then asshowninfig shortercandidates haveclearadvantages modelscore thelengthratiois not justaboutbleu ifthehypothesis lengthisonly ofreferencelength somethingthatshould badadequacy we pre neuralsmtmodels beingprobabilistic alsofavor shorttranslations andderivations whichisaddressedby word andphrase reward wholeinput eos figure candidatelengthsvs modelscore thisscat candidateswhen hence asbeamsizeincreases thesearchalgo andthen rescoringmethods inparticular weproposeto rnnsearch bahdanauetal intro ducesthe lengthnormalization method whose length norm free gnmt wuetal incorporateslength method adjusttheir perforexactformulas baidunmt heetal borrowsthe word reward methodfrompre neuralmt whichgives areward toeverywordgenerated where isa wr basedontheabove huangetal propose avariantcalled boundedword reward which this tioﬂ gr sourcesequencelength namelytheaveragenum word itgivesreward toeachworduptoa boundedlength min fj gr jg bwr gr from boundedword reward weusea layermlp input gr then wereplacetheedratio gr withit andgetour predictedlength pred gr boundedword reward withpredictedlength thenewpredictedbound andscorewouldbe min fj pred bwr there wordreward sowe boundedadaptive reward wepropose boundedadaptive reward toauto currentbeam withbeamsize therewardfor timestep probability log word adar bp norm bp norm methodasfollows bp log bp bp here length exp bp bp bp exp log stoppingcriteria thestoppingcriteria whentostopbeamsearch isalsoimportant for bothefyandaccuracy bydefault opennmt py kleinetal be highermodelscores however thisisnotthe thescore couldstillincrease usedby rnnsearch bahdanauetal stopsthe candidates havebeenfound forboundedword reward huangetal introducesaprovably wealso norm date weupdateourbestscore then forthe wehave bp min pred where since if we reachoptimality meawhile forboundedadaptive reward we if pred islowerthan wereachoptimality proof thepartof adar in willdecrease aftertimestep samewhen pred figure lr experiments to englishtrans lationtask basedontheopennmt pycodebase wetrainourmodelon msentences andap plybpe sennrichetal onbothsides sizesdownto kand krespectively wethen sourceortarget tokens wevalidateonnist andtestonnist wereport case insensitive referencebleuscores weuse encoder wetrainthemodelfor epochs and set batchsizeis bothwordembeddingand hiddenstatesizes anddropout thetotal parametersizeis above we optimalstopping forboundedword rewardmethodwithand wechoosethebest thelengthnormal https github com opennmt opennmt py figure izationusedby wuetal hastwohyper parameters namely forlengthpenaltyand for coveragepenalty set andchoosethebest figure we formanceonthedevset andcontinuegrowing asbeamsizeincreases wealsoobservethatop bleu in ourexperiments asthe in we thatourmethodskeep andgreatlyimprovethe whicharenoto riouslyhardfornmt shenetal table sets weshowresults whichaver ageover and respectively discussion fromtable boundedword reward onthe otherhand smallbeam devtest bleuratiobleuratio moses default lengthnorm optimalstopping wuetal boundedword withpredictedlength boundedword boundedadaptive reward bp norm largebeam devtest bleuratiobleuratio moses default lengthnorm optimalstopping wuetal boundedword withpredictedlength boundedword boundedadaptive reward bp norm table smallandlargebeams indicatesourmethods bleu whilewithboth ourproposedmethods andgainimprovement overhyperparameter freebaseline lengthnor malization by bleu boundedword rewardhasthereward asanhyper parameter amongthem werecommendthebp norm method andyet conclusions beyondthat toaddressthisproblem free freebaseline lengthnormalization by bleu acknowledgements andnsfiis and iis references dzmitrybahdanau kyunghyuncho andyoshuaben gio arxivpreprint arxiv jonasgehring michaelauli davidgrangier denis yarats andyannndauphin convolutional in proc oficml weihe zhongjunhe huawu andhaifengwang features in pages aaaipress lianghuang kaizhao andmingboma when ation modulobeamsize in emnlp recur in emnlp vol ume page klein kim deng senellart anda rush opennmt open sourcetoolkitfor arxive prints philippkoehn hieuhoang alexandrabirch chris callison burch marcellofederico nicolabertoldi brookecowan wadeshen christinemoran richardzens etal moses opensource in pro ceedingsofthe pages tics six corr abs myleott michaelauli davidgrangier and marc aurelioranzato analyzinguncer arxivpreprint arxiv kishorepapineni salimroukos toddward andwei jingzhu bleu in proceedingsof acl pages philadephia usa marc aurelioranzato sumitchopra michaelauli andwojciechzaremba sequenceleveltrain iclr ricosennrich barryhaddow andalexandrabirch subwordunits arxivpreprintarxiv shiqishen yongcheng zhongjunhe weihe hua wu maosongsun andyangliu minimum in pro ceedingsofacl xingshi kevinknight anddenizyuret why in proceed ingsofthe pages ilyasutskever oriolvinyals andquocvle works in ingsystems pages zhaopengtu yangliu lifengshang xiaohualiu andhangli reconstruction in aaai pages sequence to searchop timization in proceedingsofemnlp yonghuiwu mikeschuster zhifengchen quocv le mohammadnorouzi wolfgangmacherey maximkrikun yuancao qingao klaus macherey etal google sneuralma arxivpreprint arxiv 
proceedingsofthe pages vancouver canada july august https doi org proceedingsofthe pages vancouver canada july august https doi org 
neuralnetwork dingchengli peiniliu minghuang yugu yuezhang xiaodili danieldean xiaoxiliu jingminxu huilei andyaopingruan bigdatalab baiduinc usa ibmresearch china mayoclinic mn usa watsonhealthcloud ibm usa singapore donghuauniversity china abstract however nomatterrule based noneof inthis work mixturefeature mfecnn tocon multi classesasmany connected soft maxlayerbasedonmulti viewembedding experimental thantraditionalstate of the ofonlyusingbag of wordsasinputs ntroduction hospitals pharmaceu ticalfactories andotherhealth composingdocuments word healthorganization hl havemadegreatefforts mostofhealth however meanwhile mostofdocuments texts consequently nlp community sincedecadesago asthere lotsofresearch models rahmetal asschema onlybased instance content basedandcombination approaches schema instance value patternsandranges tacklethe recently bydoanetal machinelearning themethodsacquire thetrainedmodelsina domain bydifferentstandards datamodel inthiswork ingmodel network mfecnn the ourdata hl messages fromhealthproviders involvesemi structureddata which usesanon unitedstates thestandardsofhl allowsomecustom meanwhile withmanynlpfeatures basically task firstly withthird partytoolsasmulti modalinputs includingbag of words bog part of speech pos syntaxandconcepts secondly thirdly these viewbasedcnnmodelto ondatamappingfromhl vectormachine svm likebasiccnn although ii intheinputdata namelybog inaddition hl part of speech pos randomfield crf allthose differently modaland multi beforedelvinginto ourmfecnnapproach modalandmulti viewmodels ngiametal theirdeepnetwork rbm modalfeature learning audioandvideo this multiplemodalities includingwords syntax viewem beddingforcnn accordingtothemodel variable mayhaveatwin viewembedding tv embedding withre gardstoany suchthat where thetv datarepresentation further thelearnttv embeddingcanbe functionas iftherearemultipletv embeddings as whichcanberegarded asmulti viewembedding mv embedding models view viewembed ding concepts andsyntax viewembeddingare usually neuralnetwork rnn andlong short termmemorynetwork lstm however inhumanlanguages theorderand portant atree lstm oflstmstotree inthiswork tree lstmis iii eural etwork rchitecture fig the tion respectively inthelayer athird partytoolctakes was fig tences themulti of words bow modal part of speech pos modal conceptmodal tree lstm withtensoroutputs togetherwithamax andprediction followedbyafully the themulti viewstyleand datamapping andmulti cnn herehasbeende firstly depictedinfigure andthenamulti viewbasedcnnmodel boththecnnmodels areathree max layer viewembedding mfe isthecombinationsof multi viewembeddingasshown infigure giventheinput multi modalfeaturescan mainlyincludeposmode conceptmode andsyntaxmode ininputsentences toroftargets sentencenature humanlanguages tree rolemode all layerforcross tionallayer max modellearning mfe ontheother hand modallearningintoa multi viewembeddingproblem cx inequation view embeddings multi viewembedding forinputfeatures inaddition thosemulti viewembeddings whichallare fromwordlevels formulti equation willbeexpandedinto cx lstm inthetree lstm child sumtree lstmand arytree lstmandbothvariants arytree lstmtakestheorder here weemploy arytree ofthemodel inourwork forexample asentencewithsubject predicate np the vp averb andanothernp object let theconstituent treehasatmost arytree statesofaunit sumtree lstmand thelstm lowerweights workablepipeline weobtained theend to trainingstageand applyingstage inthetrainingstage thecollectionreader includinghl message data intheapplying stage newhl sourcedatainhl language hl xml content intosegments theseparatedhl contents these afterthemodels fig end to iv xperiments inthissection messagetocommonsif we followed evaluation intheevaluation datasets customercda customerhl total tablei classidclassname al dg diagnosisdescription nk contactrole nte comment obx observationvalue pid patientaddress pid patientname pv admitreason remainder txa documenttype tableii mes sagev wehavetotally hl documentswitheach containingonlyonehl message weareusing datasets tocomposethesehl messagedata datasets which sample recordspatients medicaltreatment andsamplehl message medicaltreatmentand labtests alltheofhl elements table formatwithannotated mappings weuseabout ofoveralldatasets fortraining validation andtestrespectively namely inour experiment weuse in total classes seeclassnames listedintables whichrepresent kindsoftargetdata schemaelements experimentalsettings besides weusedsvmmodel whichenablesus of the artmachine learningmodel shownintableiv forthesvmmodel weusedlibsvm bothcnn withthe we precision pre recall rec andf scoreforthemetrics whichareas support correctmappingsfound pre support allmappingsfound rec support allcorrectmappings pre rec pre rec finalresults classidpre rec support all tableiii cnn linearunits windows of withdropoutrate of constraint of andmini batchsizeof feature uniquetokensand asthefeaturemap conceptsinvolve uniqueand asthefeaturemapwhile totalabout dimensions thesevalues set table tables and showthe featuresincluded cnn althoughsvm cnn model scores the which thatcould comparedwith cnn pid patientaddress mefcnn averagef scoreisashighas namely weachieve inaddition recently customerhl comparison mappingaccuracy onclusionand uture ork inthiswork withthisframework amulti labelproblem innovatively weincorpo inordertomakefull modalsemanticspaces we realizedamulti modalandmulti viewapproachtodigest thisisquitedifferent which incontrast wecombined medicalconcepts aswellaswordspace ourexperimental in addition infuturework wemayconsidertouse cknowledgments watsonhealthcloud eferences geller he perl morrey andj xu ﬁrule assignments vol no pp gu chen he halper andl chen medicine vol no pp rahmandp bernstein thevldbjournal vol no pp eisenstein connor smith ande xing classsvmcnnmfecnn idpre rec pre rec pre rec microave macroave tableiv ulti svm cnn and fe cnn all tablev hevaluesarethe numberoftruepositive in proceedingsofthe stroudsburg pa usa yan miller haas andr fagin ﬁdata mappings ﬂin acmsigmodrecord vol no acm pp drumm schmitt do ande rahm ﬁquick mig projects ﬂin ment acm pp doan domingos anda halevy ﬁreconciling amachine learning approach ﬂin acmsigmodrecord vol no acm pp alexe tencate kolaitis andw tan ﬁde in proceedingsofthe acm pp madhavan bernstein doan anda halevy ﬁcorpus basedschemamatching ﬂin dataengineering icde proceedings stinternationalcon ferenceon ieee pp miwaandm bansal ﬁend to arxiv preprintarxiv ngiam khosla kim nam lee and ng ﬂin proceedings ofthe icml pp johnsonandt zhang ﬁsemi bedding ﬂin systems pp tai socher andc manning ﬁim structured longshort termmemorynetworks arxivpreprint arxiv savova masanz ogren zheng sohn kipper schuler andc chute ﬁmayo ctakes architecture cations association vol no pp murphy weber mendis gainer chueh churchill andi kohane ﬁservingtheenter andthebedside vol no pp changandc lin ﬁlibsvm alibraryforsupport vectormachines temsandtechnology tist vol no abadi agarwal barham brevdo chen citro corrado davis dean devin etal ﬁtw large scalemachinelearn arxivpreprint arxiv conneau schwenk barrault andy lecun arxivpreprintarxiv 
deepvoice multi speakerneuraltext to speech sercanö sercanarik baidu com gregorydiamos gregdiamos baidu com andrewgibiansky gibianskyandrew baidu com johnmiller millerjohn baidu com kainanpeng pengkainan baidu com weiping pingwei baidu com jonathanraiman jonathanraiman baidu com yanqizhou zhouyanqi baidu com bordeauxdr sunnyvale ca abstract to speech tts withlow singlemodel asastartingpoint of the speakerneuraltts deepvoice andtacotron weintroducedeepvoice voice weimprovetacotron byintroducingapost anddemonstratea speaker speakertts datasets introduction speechsynthesis commonlyknownastext to speech tts technologyinterfaces accessibility media andentertainment singlespeakervoice modelparameters asaresult inthiswork neuralmulti speakersystems concretely wepresentdeepvoice ariketal weintroduceawavenet based oordetal spectrogram to audioneuralvocoder and useitwithtacotron wangetal listedalphabetically nips longbeach ca usa arxiv cs cl sep usingthesetwosingle wedemonstratemulti speakerneural andtacotron section section presentsdeepvoice andhighlights section section mos evaluationandsection speakerdeepvoice and section relatedwork inorder startingfrom single withregardstosingle ponents zenetal ronanki etal acousticmodeling zenandsak by oordetal mehrietal ourcontributions includingdeepvoice ariketal tacotron wangetal andchar wav soteloetal buildingsingle speakerttssystems speakerttssystem forinstance intraditionalhmm basedttssynthesis yamagishietal speakers data dnn basedsystems yangetal withi similarly fanetal linespectralpairs forfurther context wuetal basedmulti speakermodeling morerecently gans hsuetal speakertts inspeechrecognition abdel hamidandjiang vectors inaddition speaker whichleadstohigher datarequirements weproposeusing salimans etal basedmethods reynoldsetal lietal single speakerdeepvoice inthissection wepresentdeepvoice ariketal ariketal asdepictedinfig the animprovedsingle qualitymulti speaker model anddeepvoice deepvoice figure text secondpredict phonemedurations feed andfrequency voicednessandtime indeepvoice allmodelsaretrained anddeepvoice insection segmentationmodel similartodeepvoice ctc loss gravesetal pairs deepvoice layers deepvoice relu where istheoutputofthe thlayer istheconvolution isthebiasvector and incontrast deepvoice relu bn where bn isbatchnormalization ioffeandszegedy inaddition whichcan processing we durationmodel indeepvoice valuedduration scaledbuckets and wemodelthesequence byaconditionalrandom crf lampleetal duringinference backward algorithm frequencymodel per deepvoice frequency max where isthe audiosignal max isthemaximumvalueof and is one exceedsaed threshold audiosamplingrate milliseconds forexample ifaphonemelasts milliseconds theinput frames ifitlastslessthan milliseconds itis extendtoasingleframe gru layers choetal anafprojection hiddenstates predictions theprediction gru ismade withasingle thesecondprediction conv finally gru conv predictionvia where and are respectively forthespeakerthe modelistrainedon wethatpredicting vocalmodel thedeepvoice oordetal withatwo layer bidirectionalqrnn bradburyetal conditioningnetwork similartodeepvoice however weremovethe in addition insteadofgeneratinga multi low unlikepreviouswork ourapproachdoes notrelyonper speakerweight matrices or layers speaker low weuse rnn initialstates nonlinearitybiases each empirically thevocalmodel speakerembeddings transformthe andanonlinearity embeddings inputaugmentation featuregating while however haveontheactivations figure speaker segmentation duration and frequencymodel next multi speakerdeepvoice thedeepvoice yet theycanbeviewed segmentationmodel inmulti convolutionlayers insteadofeq wemultiplythebatch speakerembedding relu bn where isaspeakerembedding inaddition embedding similarly ratherthanhaving durationmodel themulti tation andanothersi frequencymodel themulti layers withasinglespeaker embedding as describedinsection speakerfrequency modelpredicta normalized frequency byaedlinear transformation forthespeaker malespeakers forinstance tendto haveamuchlowermean embeddings insteadofeq wecomputethe predictionas softsign softsign where isaspeakerembedding and tothe and and figure griforourspeaker vocalmodel themulti suggestedin oordetal networkaswell sounding yet weindeedobservethat multi speakertacotron wealsoextendtacotron wang etal asequence to sequencecharacter to waveformmodel whentrainingmulti speaker tacotronvariants wealso themodelsaremuchless thus wetrimallinitial preprocessing thus likedeepvoice character to spectrogrammodel thetacotroncharacter to bank highway gru cbhg encoder anattentionaldecoder andacbhgpost processingnetwork duetothecomplexity ofthearchitecture outputquality withoutaspeaker dependentcbhgencoder seeappendix forspeaker dependentattention visualizations weuseoneembedding secondembedding weuseone net gruhiddenstate basedattention mechanism model samp freq mos deepvoice khz deepvoice khz tacotron grif khz tacotron wavenet khz table meanopinionscore mos evaluationswith intervalsofdeepvoice deepvoice andtacotron the spectrogram to waveformmodel wangetal usesthegrifn spectrogramstotime we tacotron vocal model buttakeslinear scaledlog as input thecombinedtacotron wavenet results inthissection speakerandmulti single wetraindeepvoice deepvoice approximately hoursofsingle speakerdata anddeepvoice canbefoundintable withinappendix crowdmosframework ribeiroetal table theresults yieldgainsin qualityoverdeepvoice multi hoursofspeech whichcontains utteranceseach ofaudiobooks whichcontains speakerswith minutesofaudioeach foratotalof hours wealso seefig asanexampleand appendix formoredetails framework setbeingevaluated quality however the overall dataset multi speakermodel samp freq mos acc vctk deepvoice layerwavenet khz vctk deepvoice layerwavenet khz vctk deepvoice layerwavenet khz vctk deepvoice layerwavenet khz vctk tacotron grif khz vctk tacotron layerwavenet khz vctk groundtruthdata khz audiobooks deepvoice layerwavenet khz audiobooks tacotron grif khz audiobooks tacotron layerwavenet khz audiobooks groundtruthdata khz table speakermodels toobtainmos weuse discriminativemodels seeappendix fordetails onthesamples figure layervocalmodel and character to seeappendix fordetails amulti highmos toshow butalsogenerate distinguishable voices we accuracy samples seeappendix formoredetails tacotronwithwavenet thewavenet conclusion inthiswork weexplorehowentirely speakertext to speechvialow westartbypresenting deepvoice animprovedsingle speakermodel next speakerdeepvoice andmulti and inconclusion highqualitytext to futureworkmaytest howlittle andwhetherthe references abdel hamidandh jiang in icassp arik chrzanowski coates diamos gibiansky kang li miller raiman sengupta andm shoeybi deepvoice real timeneuraltext to speech in icml bradbury merity xiong andr socher quasi in iclr cho vanmerriënboer gulcehre bahdanau bougares schwenk andy bengio learning arxiv fan qian soong andl he multi basedtts synthesis in ieeeicassp graves fernández gomez andj schmidhuber in icml hsu hwang wu tsao andh wang arxiv ioffeandc szegedy batchnormalization shift arxivpreprintarxiv kingmaandj ba adam arxiv lample ballesteros kawakami subramanian andc dyer recognition in proc naacl hlt li ma jiang li zhang liu cao kannan andz zhu deepspeaker anend to end arxivpreprintarxiv mehri kumar gulrajani kumar jain sotelo courville andy bengio samplernn an unconditionalend to arxiv oord dieleman zen simonyan vinyals graves kalchbrenner senior and kavukcuoglu wavenet arxiv reynolds quatieri andr dunn ribeiro florêncio zhang andm seltzer crowdmos scorestudies in ieeeicassp ronanki watts king andg henter median non parametricapproach arxiv salimans goodfellow zaremba cheung radford andx chen gans in nips sotelo mehri kumar santos kastner courville andy bengio char wav end to end speechsynthesis in iclr workshopsubmission wang skerry ryan stanton wu weiss jaitly yang xiao chen bengio etal tacotron towardsend to endspeechsynthesis in interspeech wu swietojanski veaux renals ands king basedspeech synthesis in interspeech yamagishi nose zen ling toda tokuda king ands renals robustspeaker adaptive hmm basedtext to speechsynthesis speech yang wu andl xie onthetrainingofdnn in signal apsipa asia zenandh sak forlow in ieeeicassp zen agiomyrgiannakis egberts henderson andp szczepaniak fast compact andhighquality lstm arxiv appendices atrainingdeepvoice figure forconvenience infig similartothein ariketal to phonememodel asin ariketal similarto ariketal counterpartsintable models model evaluationmetric deepvoice deepvoice segmentation phonemepairerrorrate duration meanabsoluteerror ms ms frequency meanabsoluteerror hz hz table duration anddeepvoice ariketal to weadded totheoverallloss function was decayedbyafactorof every iterations kingma andba with and convolutional layers and outputchannels ineachlayer thesizeis whereheight isin isintimeframes ariketal similarly any wangetal single speaker vctk audiobooks segmentation numberofmfccs convolutionallayers recurrentlayers bi gru wide wide wide learningrate silencethreshold gaussianwidth ms ms ms batchsize speakerembeddingsize duration fully connected units units units recurrentlayers bi gru wide wide wide outputbuckets learningrate batchsize speakerembeddingsize frequency hiddenlayers bi gru wide wide wide outputdimension convolutionwidths learningrate batchsize speakerembeddingsize vocal layers learningrate batchsize speakerembeddingsize character to spectrogram enc cbhgbanksize enc cbhgchannels enc cbhgrecurrentsize enc cbhghighwaylayers enc cbhgmaxpoolwidth enc cbhgproj sizes enc cbhgproj width decoderlayers attentionsize attentionstatesize decoderprenetsizes post cbhgbanksize post cbhgchannels post cbhgconv widths post cbhgrecurrentsize post cbhghighwaylayers post cbhgmaxpoolwidth reductionfactor ctclosscoef learningrate œn batchsize speakerembeddingsize table speakermodels speakermodelsintable wereferthereadersto ariketal model hardware timeperiteration numberofiterations totaltime segmentation titanxgpu ms hours duration teslak gpu ms hours frequency teslak gpu ms hours vocal layer titanxgpus ms hours vocal layer titanxgpus ms hours vocal layer titanxgpus ms hours vocal layer titanxgpus ms hours character to spectrogram titanxgpu ms hours table speakerdeepvoice inthissection outputs modelactivations speaker figure time andvoiced ness or speaker isa year isa year old ariketal as showninfig forexample female speaker figure speaker isa year old speaker isa year and speaker isa year sixspoonsof fig it figure layervocalmodel shownwith sionalspace fig andfig to wecan besides weobserveapparent inthetwo dimensionalspace especiallygreat https en wikipedia org wiki regional_accents_of_ english figure to spectrogrammodel figure tocomputemulti speakeraccuracy deepspeaker lietal thestate of the weusemel mfccs then weemploytwo poolinglayer we thenmean entropy loss nonlinearity inthissection onlythe resultsforthemodels and arepresentedintable accuracy param intable audioresamplingfreq khz khz khz khz numberofmfccs hoplength convolutionlayers maxpoolwidth stride fullyconnectedsize dropoutprobability learningrate table param intable audioresamplingfreq khz khz khz khz numberofmfccs hoplength convolutionlayers maxpoolwidth stride fullyconnectedsize dropoutprobability learningrate table dataset multi speakermodel intable vctk deepvoice layerwavenet vctk deepvoice layerwavenet vctk deepvoice layerwavenet vctk deepvoice layerwavenet vctk tacotron grif vctk tacotron layerwavenet vctk groundtruthdata table andd the dataset multi speakermodel intable audiobooks deepvoice layerwavenet audiobooks tacotron grif audiobooks tacotron layerwavenet audiobooks groundtruthdata table andd the 
background leishi baidu com inc china shilei baidu om abstract of expcomputing moreover theassu mptionofequalback topicsmaybetoostrong thispap erextendstopropose sam lrb an dobtainssim particularly employingadou blemajorizationbound weapproximatelog boun dwithoutthelog sum expterms implyembodiedby nuclearnormand normregularizers interestingly wendthattheoptimiz tiontaskofsam nrobustpca consequently lrbcanbeefc ientlylearnedus dproximalgradient weextendsam lrbtofavorunsu pervisedandmul tifacetedscenarios andefciencyofsam lrb of the artmodels introduction ichlet however unnecess arilyover sever formulations orinstance recentlyin hdirichlet els namelyinferencecost overparameterization andlac kofsparsity motivatedbythem asparse sage wasproposedin asana el spa successfullyapplying sage effort andpaper detectscommuni however first theli sum of maxgenerati venature ingforlargescales second topics orequivalently ss constrainedtobeequal insomeapplications whenlots ses topics weareproposetousea however og spaceisdifcult weturntoap proximatethedatalog byaquadraticlower boundbasedonthe sothatthecostlylog sum therst problemofsage isavoided sam lrbmodel rizedintofour foldasbelow trainedsettinginsage elbyaquadraticlower bound sothatthecos tlylog sum exponentialcom putationisavoided ngianrelaxations leadingtoa consequently sa lrbcanbeefciently orithmforrobustpca extendsam unsuper facetedmodel tatovalidatesam lrb sameasinsage caldistributioninlog spacecomes ctors particularly wearegivendoc uments documentsover words foreachdocument let represent and wc dw bethetotaltermcount weassumeeachclass hastwovectors space respectively then inadocument withlabel isasoft maxform exp dw dw exp di di given with and thelog likelihoodofdata is log xj log exp ki ki similarly atestingdocument isclassiedintoclass accordingto argmax insage same for issparse althoughintuitive thebackground forinstance toexpressasame similar meaning inthiscase rseadditivepart insteadofasthe background takingfig thelo spacedistribution left isthesum ofthelow rankbackground middle andthesparse right data rankstructure and orthe additivepart wouldbenotsparse tting orunder tting moreover sincethereexistssum of exponentialtermsine the islarge asaresult althoughperforming wellin over figure lowrankbackground spacedistr background and sparse resp rowsindex terms andcolumnsforclasses figure lower bound soptimization lefttoright bound and resp of xp lower boundof and furtherbasedon thislower bound acrossclassesislow rank incontrastto intheliterature of expter minvolvedinsoft max forinstance basedonthec onvexityoflogarithm onecan obtainabound log iexp iexp log forany namelythe lb log cvx bound moreover viaupper on ecanobtainthefol shortlynamedas lb quad loc log exp ix exp iexp log iexp in bo lb quad dm heprevioustwo log exp log exp with and beingauxiliary variational variables and exp exp this djordan employingeq weobtainalower bound lb tothedatalog likelihoodineq lb ak with ki ki ki log exp ki ak diag foreachclass and bound therein abs ki ki ki ki abs partic ularly theleftshowsthelower groundtruth usuallywithin rounds inourexperiences boundswit onecanndthat lb quad dm approximatesbetteror pleases ee formorecomparisons supervisedsam radient likelihoodineq li keinsage weturntooptimizeits lower boundineq gningthelow rankconstrainton and concretely mizationtask max lb with lb speciedineq islowrank issparse sed parse dditive modelwith ow ank ackground orsupervisedsam lrbforshort althoughbotho assign ingappropriatepriors anddelivering wedeterminet fcientalgorithm intheliterature similartoeq ient papers studiedthe sis rpca aimingtodecouplean rix closelyrelatedtorpca ourscenarioineq ulation andtheweightsare in theauthorsp neously and respectively lettingt hepartialderivativew of lb equaltozero themaximumof lb canbeachievedat since ak is theoptimalsolution iswell thelowrankon andthesparsityon onecanrewritteneq min jj jj jj jj with where jjjj jjjj and jj nuclearnormand norm respectively from into and lagrange multipliers and nstraint respectively interestingly eq frpca paper proposedan accelerated proximalgradient apg rpca showingitsadvantages plain proximalgradient wechooseit algorithm in for seekingsolutionstoeq pg ereferredto formoredetails the data termcountsandlabels of docsand classes sparsethres result log spacedistributions low rank andsparse initialization while notconverge do if then iterativelyupdate accordingtoeq for do calculate ak and byeq and apg rpca byalgorithm in with end algorithm supervisedsam lrblearningalgorithm consequently thesupervisedsam lrbalgorithmisspecie dinalgorithm therein onecan iftheyarexed algorithm ence of oursupervisedsam tissaved tonupdatingasinsage moreover addinglaplacian exponentialprioron forsparseness initializations whileincontrastsam lrbdoesnothaveth isrisk additionally sincetheevolution fromsagetosam lrbistwofolded thelowrankbackgro relaxation extensions analogousto oursam facetedclasslabels extension elling followingtopicmodels thatis eachdocumen proportions dirichlet andeachterm label multinomial is exp ineq tdirichletallocation lda with thelower boundtreatmentineq leadingtothefollowi ngunsupervisedlower bound lb ak log ih log wh log wj ih log with ki ki ki log exp ki ak diag whereeach thitemin is kw dw intopic and kw isthetopic thisunsupervisedsam deviation thee stepto updateposteriors and is once ak arecomputedasabove them steptoupdate extension weconsiderhowsam multi combiningper andtopics intheliterature elshavebeenstudiedin andthey orfromadocumentlabel topic manytopics manylabe ls in sageneedsnoswitching facetedmodeling morerecently paper applyingsam westillass parti cularly foreachtopic andsparsedeviation foreachlabel wehave labelbackground andsparsedeviation foreachtopic labelinteractionpair we kj again and sdistributi onsimilarity thenforasingleterm giventhelatenttopic wandtheclasslabel together exp dw dw dw withparameters thelog likelihood slower boundinvolves labelpairs lb kj akj kj kj kj kj log ih log wh log wj ih log with kj kj inthequadraticform thevaluesof akj kj and kj andeq latenttopics duetospacelimit andstandardlda thefollowingsteps and with xed whichapproximates thesumof and put intoalgorithm toupdate and with xed whichapproximates thesumof and put intoalgorithm toupdate and with xed update byproximalgradient experimentalresults inordertotestsam thissectio tasks unsuper visedtopicmodeling andmulti faceted respectively documentclassication wersttestoursam particularly thesupervised sam riorindirichlet multinomialmodel nonparametricj effreysprior isadoptedinsage asaparameter freesparseprior alvariables ig iinthequadratic lower boundofsam lrb gthemareconsidered newsgroups data ingsinto newsgroups of terms wevarythep after theclassicationaccu intermsofbox plots ningdataproportion figure newsgroups data thepro following weusethetraining testingsetsfromhttp people csail mit edu jrennie newsgroups onecanndthat multinomialm trainingdata moreover withrandomandxed variationalvariables thesam if bound theperformanceofsam lrbis substantiallythebest witha table alsoreportsthe lrb wecanseethat yavoidingthelog sum exp calculation sam lrb xed performsmorethan timesfast erthansage whilesam lrb optimized table inm inutes methodsagesam lrb xed sam lrb optimized timecost minutes nips data followingthe wehaveatr ainingsetof documentswith terms andatestingsetof documentswith terms forconsistency sam multinomialmodel variationallda andsage forallthes eunsupervisedmodels thenumber to andthento afterun supervisedtraining theperformance thesmallerthebetter theperf ormancesof againintermsofbox plots figure perplexityresultson nips data asshown eroftopics perhapsmainly whereas sam sam lrb xed mainlyca ifupdatedinstead sam lrb opt imized multifacetedmodeling lrb sameas wechoosea publicly the presidentialelection outofthetotal politicalblogs dthreearefromleft thereare usingfourblogsfor training onthistask ahmedandxingin also suppor tvectormachinepro whilesupervisedlda in sageisrepeated mbers http www cs nyu edu roweis data html http sailing cs cmu edu socialmedia blog html result at usingsam lrb optimized runs foreach interestingly sam lrbpr of the art result whileachievingitat sam table politicalblogs databysam lrb optimized topic accuracy medianoutof runs concludingremarks deling jorizationtechnique weapproximatethelog sum exponen likelihood intoaquadraticlower bound bo und spacedistribut ionofsage intoalow rankcon straint leadingtooursam lrbmodel then aftertheconst intotheformofrpca sobjectivefunction analgorithmbas lrb themodelspecication extensionsofsam lrbtounsupervised experimenta efciencyofsam multinomial andsage first whicharepro data inthiscase nuclearnormand normareexpected beingone possiblechoice second thispaper whilebayesi isanalternati vechoice moreover wemayalsoadopt nferencein lastbutnottheleast mightb insam one additionally ngdirectionmethods couldbealso zation references ahmedande xing stayinginformed supervisedand semi supervisedmulti view in proc emnlp pages beckandm teboulle thre problems bleiandj mcauliffe in advancesinnips pages blei ng andm jordan latentdirichletall ocation jmlr bohning annalsofinst ofstat math bouchard models in hybridsystemsat nips chen lin kim carbonell ande xing smo ding he andl carin mponentanalysis ieeetrans imageprocessing eckstein te chnicalreport rutcorresearch reportrrr eisenstein ahmed ande xing sparseadditive in proc icml eisensteinande xing thecmu politicalblog corpus technicalreport carnegie mellonuniversity machinelea rningdepartment figueiredo sprior in advancesinnips pages gormley dredze vandurme andj eisner sha in proc naacl hlt pages hong ahmed gurumurthy smola andk tsio utsiouliklis discoveringgeo in proc thwww pages jaakkolaandm jordan andtheirextensions in proc aistats jaggiandm sulovsk in proc icml pages jianganda saxena opics factoredtopicsmodels in proc ijcai joulin bach andj ponce efcientoptimization in advancesinnips pages laffertyandm blei advancesinnips pages lin ganesh wright wu chen andy ma fast rankmatrix technica lreport uiuctechnicalreport uilu eng august mei ling wondra su andc zhai topicsen timentmixture modelingfacets in proc www minka tech nicalreport technology paulandr girju atwo dimensionaltopic aspectmo faceted topics in proc aaai richard savalle andn vayatis estimationo matrices in proc icml pages smithandd smith community in yearsofdiscoveries wangandd blei process in advancesinnips pages wangandd blei ugatemodels toappearinjmlr wright ganesh rao peng andy ma robustpri exact ation in advancesinnips pages yangandx yuan math comp zhu ahmed ande xing medlda maximummargins upervisedtopicmodels jmlr 
attribute guided grainedrecognition xiaoliu jiangwang shileiwen erruiding yuanqinglin baiduresearch liuxiao wenshilei dingerrui linyuanqing baidu comwangjiangb gmail com recentattention however theyare inthis work bydesigninga novelrewardstrategy basedﬁne grained recognitionmethods dataset wahetal attributerecognition introduction guishﬁne forexampleinfig ure inorderto buildhuman levelﬁne itis state of the artﬁne zhang etal liu shen andhengel wangetal krauseetal gaoetal zhangetal oremployreinforce transformer basedattentionmod els sermanet frome andreal liuetal zhaoetal however both fullysu consuming error probingman equalcontribution copyright intelligence www aaai org allrightsreserved forexamplein figure themethod liuetal failstolocatethe asidefrom jectcategories partdescriptions suchas redwing also locatingability intheirearlyages childrenlearntorec topartdescriptions anditismuch wecallit partattribute grainedrecog nition signal speciﬁc vironmentstate basedonthisintuition itisreasonableto differentparts intheproposedscheme multiplefullycon each wedesigna attributepredictors partattribute figure moreim portantly frompart grainedrecognition dataset wahetal moreover partattributecanbeac ingtechniques recognition parikhandgrauman akataetal hwangandsigal imageretrieval huangetal facelocalization liuetal andimagegeneration yan etal attentionmodel mnihetal ba mnih and kavukcuoglu tion xuandsaenko xuetal imagerecog nition zhangetal seoetal actionrecog nition wangetal personre identiﬁcation liuet proceedings of the thirty first aaai conference on artificial intelligence aaai al andimageparsing fanetal toourbest knowledge grainedrecognition attribute ure stage inthetrainingstage afully calizationnetwork liuetal islearnedforeach part incontrastwith liuetal thetaskofthe fully fortesting problemformulation given ntrainingimages xn andtheirobject labels ln sifyingeachimage xiasitsground truthlabel li fine grainedobjectshave pparts localizingbydescrib the il xi xi fp xi li wherefp deﬁned iscross theclassiﬁcation in we scriptions ance ofanobjectpart thpartis denotedas yp yp ypn eachpartdescription ypiisabinaryvector ypi ypi ypi kp whereeachelement ypi thimage thpart weaim tation lp il tp fp xi ypi wheretp tp tp kp isamulti labelat thpart andeach tp thattributeof partp isamulti labelcross entropyloss xi ypi ypi klogtp fp xi ypi log tp fp xi wholeobject weuseeq localizerfp thatoptimizeseq trainingoflocalizers wejointlylearna prediction differential we williams labelpredictors for fp imagepatch fp xi andthewholeimage xi thereward gorithmforpart pisjp fp tp iefp rp tp fp xi ypi wherethereward efp rp sp ipfp sp xi sp sp thpartofthe thimage sp pfj sp xi region sp sp sp previousmethods sermanet frome andreal liuetal sp tobe however since dicted therefore egy ifbothofthefollowing criteriaaresatisﬁed lowestamongthe batch batch following liuetal sinceboth aredifferentiable re inforcealgorithm williams isappliedtocompute figure liuetal part attribute guidedattention thered green breast wingandtail respectively bestviewedincolor has pattern plain has color red has color red shaped figure the inthetrainingstage inthetestingstage fpefp rp sp ipfp sp xi fplog pfp sp xj sp sp mmm fplog pfp smp xi sm wheresmi jpfi xj pfi smi xj mlocalregions batch welistthe trainingofclassiﬁers were trained scaledlocalregions whenre trained scaledlo weextractfea thejointrep indetails we grained recognition representation algorithm input trainingimages xn attributedescrip tionsofeachpart yp yp ypn output multi labelattribute predictionfunction for eachpart pdo initializefpandtp repeat randomlysample himages for eachimage xido samplesmp ipfp xi endfor for eachlocalregion sm ido calculatelmp tp smp ypi endfor calculateˆl lmp for eachimage xido sortlmp iinascendingorder for eachlocalregion smp ido iflmp iisinthetop andlmp ˆlthen setreward sm else setreward smp endif endfor endfor fpaccordingto tp fpandtp untilconverge endfor tures prediction fig fp xi forfeatureextraction tion categoryprediction ﬁne however datasets wahetal thedatasetcontains imagesof birdcategories where imagesarefortraining andtherest imagesarefortesting inadditiontothe categorylabel partlocations binaryattributesanda infigure with bb trainingandtesting and withoutbb wheretheobject bounding boxisnotutilized wechoose parts head wing breast and tail thecroppingsizeof amongallthe attributes thenthe thenumberof and respectively weutilizeresnet heetal asthevisualrep in thetrainingstage weutilizetheroi pooledfeaturemaps inthe withbb setting boundingbox tolearnmulti eachpart theoutputofthe res layerofresnet is calizationnetworks pooled featuremaps girshick tentionlocalizers sgd withmomentumof epochnumberof weight decayof andamini batchsizeof onfourk gpus throughonce isaddedafter res andthesizeof fc ischanged from to theparametersbefore res areinitializedbythe model heetal dengetal andparametersoffc arerandomlyini tialized andreduced twicewitharatioof after and epoches thelearn fc is timeslargerthanother layers szegedyetal atraining and and ofthewholeimage anditsaspectratiois and alexnet stylecolor augmentation krizhevsky sutskever andhinton is weﬁnallyresizethe imageastheinput percent correctparts pcp apart timestheground shihetal liuandbelhumeur liu li andbelhumeur ours withoutbb ours withbb table measuredbypcp onthecub dataset of the artpartlocaliza tionmethods shihetal liuandbelhumeur liu li andbelhumeur method shihetal pcp inthe withoutbb scenario ourschemethat highestaveragepcp izing breast whenground used figure results ground cles shihetal thus forexample ourpre dicted head information head positions normallyappearonthe forehead underthecurveofroc auc arereportedintable we arehighlyimbalanced veryfrequently foreachpart itsattributes aucofthepart and averageaucforparts head breast wing and tail respectively thefourpartsis onoverall averageauc resultto tributeprediction fine for withoutbb scenario thebaselineresnet usingthe wholeimageachieves recognitionaccuracy adding featuresoftwoparts head and breast improvesthere sultto attention image attention table measuredbyaverage auc onthecub dataset theresultto for withbb scenario thebaseline achieves accuracy provestheresultto provestheaccuracyto ioffeandszegedy andvggnet simonyanandzis serman achieves and respectively inthe fullimage attribute value experiment weconcatenate binaryattributela inthe fullimage attributefea ture experiment oneistheoriginalfull imagemodel tunedfor astable shows baseline provements and be tributes fourpart weachievethe and forthe withoutbb and withbb scenarios respectively wealsoexplorejointly locations tompsonetal butweﬁndtheaccuracy withoutbb ofdata of the artmethodson zhangetal trainpart basedr cnnmodeltode themethodrelieson ourschemeoutper forms zhangetal andotherpart basedmodels si monandrodner zhangetal byalargemargin figure ground predictedpart noground methodaccwithoutbb accwithbb lin roychowdhury andmaji krauseetal zhangetal liuetal jaderbergetal zhangetal simonandrodner fullimage fullimage parts fullimage parts fullimage attributevalue fullimage attributefeature fullimage parts attributefeature table tion lin roychowdhury andmaji constructhigh andachieve and accuracyforthe withoutbb and withbb scenar ios respectively krauseetal learnandcombine and achieve and accuracyforthe withoutbb and withbb scenarios respectively liuetal uti theaccu racyis withbound ingbox whileouraccuracyis withoutboundingbox and twolocalparts head and breast similarly jader bergetal achieveanaccuracyof by visualizedinfigure fromlefttoright weshowthe heatmapsof st th th th th and thit erationsformulti rewardsand ascanbeseen after theout head positionduring trainingasexpected figure ouralgorithmuses iterationstolocate head themulti themiddlerowshows thelocalizerisencour thebottomrowshows lighterpositionsindi forﬁne theproposedscheme itisalso improvesat tributeprediction improvementonﬁne grainedrecognition inthefuture we references akata perronnin harchaoui andschmid label basedclassiﬁcation proc eccv ba mnih andkavukcuoglu multipleobjectrecog arxiv deng dong socher li li andfei fei imagenet alarge proc cvpr fan mei prokhorov andling hi arxiv gao beijbom zhang anddarrell ompact bilinearpooling arxiv girshick fastr cnn arxivpreprintarxiv he zhang ren andsun deepresiduallearn proc cvpr huang feris chen andyan cross domain awarerankingnetwork proc iccv hwang andsigal proc nips ioffe andszegedy batchnormalization accelerating proc icml jaderberg simonyan zisserman andkavukcuoglu proc nips krause jin yang andfei fei fine grained proc cvpr krause sapp howard zhou toshev duerig philbin andfei fei ofnoisydataforﬁne grainedrecognition arxiv krizhevsky sutskever andhinton ima proc nips lin roychowdhury andmaji bilinearcnnmod elsforﬁne proc iccv liu andbelhumeur exemplar sistency proc iccv liu luo wang andtang deeplearningface attributesinthewild proc iccv liu feng qi jiang andyan end to identiﬁcation liu xia wang andlin fullyconvolutional ﬁne grainedrecognition arxiv liu li andbelhumeur part partlocalization proc eccv liu shen andhengel thetreasurebeneath convolutionallayers cross convolutional layerpoolingforimage classiﬁcation proc cvpr mnih heess graves andkavukcuoglu re proc nips parikh andgrauman relativeattributes proc iccv seo lin cohen shen andhan hierar arxiv sermanet frome andreal attentionforﬁne arxiv shih mallya singh andhoiem part grainedcat egorization proc bmvc simon androdner iccv simonyan andzisserman networksforlarge arxiv szegedy liu jia sermannet reed anguelov erhan vanhoucke andrabinovich going arxiv tompson jain lecun andbregler jointtrain poseestimation proc nips wah branson welinder perona andbelongie thecaltech ucsdbirds dataset wang yang chen andlin object centric samplingforﬁne arxiv wang wang tang hare chang andli videos arxiv williams followingalgo machinelearn ing xu andsaenko ask attendandanswer explor ingquestion arxiv xu ba kiros cho courville salakhutdinov zemel andbengio show attendandtell neural arxiv yan yang sohn andlee at tribute image tributes arxiv zhang donahue girshick anddarrell part basedr cnnsforﬁne proc eccv zhang shenwei wu cai lu nguyen and do weaklysupervisedﬁne grainedimagecatego rization arxiv zhang xu elhoseiny huang zhang elgamnal andmetaxas spda cnn unifyingsemanticpart grainedrecognition proc cvpr zhang lin brandt shen andsclaroff top arxiv zhao wu feng peng andyan diversi arxiv 
dazhongshen hengshuzhu chenzhu tongxu chaoma huixiong baiduresearch china sdz mail ustc edu cn zhuhengshu zhuchen machao baidu com tongxu ustc edu cn xionghui gmail com abstract whichformsa whilesubstantial viewprocess to thisend inthispaper thelarge scalereal worldinterviewdata cally oint earning odelon nterview ssessment jlmia candi jlmiacan therefore ava abled suchasperson jobandinterviewques tionrecommendation ductedonreal tivenessofjlmia introduction management businesssuccess us bersin inparticular job interview whilesubstantialef process thissitua correspondingauthor recently theintelligence ai trendhasmadeits ma linowski etal paparrizos etal zhang etal talentmapping xu etal andmarkettrend analysis zhu etal however fewereffortshavebeen view ment intuitively topic abilityofapplicants forexample acandidateforﬁsoft wareengineerﬂ might ﬁprogrammingﬂ butalsoﬁresearchﬂ meanwhile compared tothisend inthispaper scalereal worldinterviewdata wedevelop oint earning odelon nterview ssessment jlmia tion jlmiacan recordsinhistory also the technical and comprehensive interviews whicharehosted spectively furthermore based onjlmia namelyperson tion worlddata whichcanlead problemstatement formally of uniquejobs where thjoband figure md md contains interviews where md is thinterview and md isthecor tions resumes we usebag of wordstorepresentthem mj similarto md and md ajobdescription andaresume md meanwhile the andaccord asweknown duringtheinterview interviewerstendto however jobdescriptionisusu thuswethink resumes resumes inaddition generally as task task howtomodelthe task interviewcategories tosolvetheabovetasks model namelyjlmia inthissection wewillformallyintro modelformulation resume ics representedby and inallofthem and first lationbetweenresume md md bution md overtopics second algorithm foreachtopic draw dir draw et and ec dir dir md md md md forthe thword mdr inresume md drawtopicassignment mdr multi md ii drawword mdr multi mdr forthe thword mde md drawtopicassignment mde multi md ii drawword mde multi et mde mde ti iii drawword mde multi ec mde mde ci wegenerate md from thelogistic andtheto picnumbersof and aresetas ck inotherwords foreachtopicin there are topicsin relatedtoit third weusealabel ti ci echnical nterviewor omprehensive nterview assessment et ec tosim plifyourmodel wefollowtheideain wangandmccallum since lda blei etal herewe sessment showedinalgorithm where inline logvec torsof log kmodk and inline and isthelogistic transformation md exp md ck exp md duetothenon nomial thuswe here thebasic sothatthe liebler kl divergenceto trueposterior inourmodel let andallhyper parametersby thejoint md md md md md md md mdr md md mde md ck et ec mj then wepositthe wherethe ck et ec mj ck md md mdr md mde accordingto blei etal minimizingthekldiver is viewrecords elbo logp logp logp logp wheretheexpectation and denotestheen elbo isthenon whichleads viewrecords similarto wangandblei weintro elbo herewetake the logp mdr the logp mde logp mdr md md mdr log ck exp md md mdr md ck exp md log md formaximizingthe elbo wedevelopanem styleal meters application here person person jobfit person therightjob formally givenajobdescription andare sume and respectively however thusweconstructa forre sumes jobdescriptions empty eachword and arelearned wecancomputethe document topicdistributionby gn gn ck then and we sume actually thesimilarity leibler divergence notethat sincethedimensionof and may bedifferent herewehave where isgeneratedfrom inparticular thevectors and rankse thus of words randomforest is jobfit duringtheinterview toevaluatecandidates however sources espe jobfit thus inthispaper we the ofeachquestion interviewassessment let ofquestion as on theonehand ontheotherhand wehope weselectquestions function rel rel div div sim rel dis div where rel and div diversityabove sim ischosenas cosine while dis issetas cosine and rel and div arenor malizationfactors blevaluesof rel and div respectively ingeneral iscompu forallsubsets fortunately since inthispaperissubmod ule tang etal achievea experimentalresults inthissection basedonareal experimentalsetup techcompanyinchina which containstotal tobespe wemanually afterthat the tereddatasetcontains latedto jobpositions injlmia weempiricallyseted parameters note that and jlmia viewrecords inparticular wesettheparameters and table containing should similarly thecorresponding tainfront end relatedkeywords ﬁhttpﬂ ﬁwebsiteﬂ ﬁele mentﬂ ﬁjsﬂandﬁcssﬂ ofcandidates resume moreinterestingly wecanthat topic andtopic ofresume fromtopic ofjobdescription table topicexampleofjlmia jobdescription resume tech interview com interview topic topic topic topic experience function foundation technology foundation management knowledge communication technology backstage code study php http element knowledge web moudle development development topic topic topic engineer website js job interest webpage methods pressure development system css solution webpage web elements like maintenance framework events workovertime meanwhile compared therearemorekeywords likeﬁcommunicationﬂ whicharerelatedtothe performanceofperson jobfit here person jobfit sume then wetrainclassic thecandidate besides nessofourmodel jobfit benchmarkmethods lda andbag of words bow forlda andforbag of words wherethe quencyofthe ititselfisakindof representation results leibler wedidnot datapreparation onlyonetype ofsamples positivesamples isrequired the wedonotknowtheexact forexample somefailedap therefore wemanuallygener alongthisline theexperi after that werandomlyselected theother datafortest rocaucs praucs figure theperson table theperson rocauc prauc cosinesimilarity jlmia lda kullback leibler divergence jlmia lda randomforest estimators jlmia lda bow gbdt estimators max depth jlmia lda bow performanceanalysis wetrainedjlmiaby varyingtheparameter from to andtheparameter from to theperson infigure and atingcharacteristic roc aucsandprecision recall pr andreachthehighest with and therefore wechosethebestpara meters and similarly we meters andchose forotherexperiments table showstheperson andbaselines fromtheresults wethatourmodelcon itindicatesthat moreinterestingly theperfor thanmostofbaselines mendationofjlmia wecollected interviewques ments andthen comparedjlmia and withbm inouralgorithm theparameters areempiricallysetas rel div and werandomlyselected foreachquery werecommend questionsbyjlmiaand bm then weasked table bm with questionsrecommended relevance diversity personalquality jlmia ti jlmia ci bm table givenexperience item iamfamiliarwith andhavesome webdevelopment experience questions recommendedby jlmiafor technicalinterview whatare ajax and interactivemodel between howtosolve cross domainissues whatarethemeaningsof gracefuldegradation and progres siveenhancement cssprogramming whatistheroleofthe httpstatuscode questions recommendedby jlmiafor comprehensive interview talkabout osi tcp ip and five layersnetworkmodel html and xhtml scrollbar and jscrollpane questions recommendedby bm whatare web applications html please program aread web standardsandw theywere query rele vance measure then tions whichis diversity measure andhowmanyquestions areabout personalquality ti ci wecan proachbm meanwhile jlmiaalsocanrec espe cially whichdis furthermore obviously ment andt isaboutﬁ andc is aboutﬁ webdevelopment also jlmiarecommendsques andc whichisuseful second forthecomprehen siveinterview suchasc which ability last since the ﬁhttpﬂandﬁwebﬂ thus bm relatedwork recruitmentanalysis re tions xu etal zhu etal asearlyas malinowski etal ma linowski etal in paparrizos etal exploitedall ofemployees paparrizos etal recently besidesthe acz etal researchersare perspective zhu etal lin etal li etal talentcircles xu etal andpopularitymea sureofjobskills xu etal althoughtheabovestud market tothisend inthispaper large scalereal worldinterviewdata paltopicmodels lda blei etal hasalotofextensions zhu etal mimno etal pyo etal etc amongthem andhavea forexample mimno et al mimno etal pyo etal pyo etal searchefforts candidateresume concludingremarks inthispaper scalereal world interviewdata candi jlmiacaneffectively furthermore worldappli cations namelyperson mendation world whichcan acknowledgments grant no references bersin joshbersin https www forbes com sites joshbersin corporate recruitment transformed new breed of service providers blei etal davidm blei andrewy ng and michaeli jordan mach learn res march blei etal davidmblei alpkucukelbir andjond mcauliffe variationalinference areviewforstatisti cians just accepted li etal huayuli yongge hengshuzhu hui xiong andhongkezhao opmentoftalents in pro ceedingsofthe halifax ns canada august pages lin etal haolin hengshuzhu yuanzuo chen zhu junjiewu andhuixiong collaborativecompany sperspective in pro ceedingsofthethirty intelligence february sanfrancisco califor nia usa pages malinowski etal jochenmalinowski tobiaskeim oliverwendt andtimweitzel in systemsciences hicss proceedingsofthe thannualhawaii volume pages cœ ieee mimno etal davidmimno hannamwallach ja sonnaradowsky davidasmith andandrewmccallum in proceedingsofthe con cessing volume volume pages association paparrizos etal ioannispaparrizos bbarlacam bazoglu andaristidesgionis machinelearnedjobrec ommendation in onrecommendersystems pages acm pyo etal shinjeepyo eunhuikim etal lda based netics acz etal aborr acz attilasali andklausdi eterschewe cruitment tang etal fangshuangtang qiliu hengshuzhu enhongchen andfeidazhu divsocial maximization in andmining asonam ieee acminternational conferenceon pages ieee wangandblei chongwanganddavidm blei articles in proceedingsofthe thacmsigkddinter mining kdd pages newyork ny usa acm wangandmccallum callum topicsovertime anon markovcontinuous time modeloftopicaltrends in proceedingsofthe thacm eryanddatamining pages acm xu etal huangxu zhiwenyu jingyuanyang hui xiong andhengshuzhu transitionnetworks in proceedingsofthe ndacm eryanddatamining pages acm xu etal tongxu hengshuzhu chenzhu panli andhuixiong recruitmentmarket amulti criteriaapproach in proceed ingsofthethirty intelligence february neworleans louisiana usa zhang etal yingyazhang chengyang andzhixi angniu oncollaborativein anddesign iscid siumon volume pages ieee zhu etal chenzhu hengshuzhu yongge en hongchen andqiliu cialemotions atime in icdm shenzhen china december pages zhu etal chenzhu hengshuzhu huixiong pengliangding andfangxie in pro ceedingsofthe pages acm appendix stylealgo firstofall tobethe is thevariationaldistri butionof md the md md thevariationaldis tributionof mj mdr and mde arebyfreemulti nomialwithparameters mj mdr ck and mde ck respectively et and ec et and ec where and arethe resumeandinter viewassessment respectively actually weeachtermof elbo injlmiais similartosomepartsof elbo inldamodel blei et al orctmmodel wangandblei except logp md whichcanbecomputedby logp md logn md ck log log ck md log md log md whereweassumethat and kmodk ity andthe then wedescribeourem stylealgorithm ine step we parameters first weoptimizethe md inequation md ck exp md second weoptimize mj mdr ck and mde ck foreachcoordinate assumethat mj mdr and mde mde ti mj exp mdr exp md mde exp et et md third weoptimize weuse newton delbo ck md wherefunction onlyif otherwise fourth weoptimize md ck again delbo md md md md md md md md md exp md last weoptimize et and ec theircalculation processaresimilar token and et asexamples mj mj et md mde mde mde ti inthem step wemaximizethe elbo withrespectto parameter similartolda parametersin asedparameters 
weihe zhongjunhe huawu andhaifengwang baiduinc no shangdi thstreet beijing china hewei hezhongjun wu hua wanghaifeng baidu nmt conductsend to end guagedecoder however themethodhassome limitations consumingtrainingand decoding of vocabularyproblem furthermore translations in smt features suchasatranslation modelandan gramlanguagemodel withthenmtmodel underthelog linearframework qualityofthestate of the to to introduction sutskever vinyals andle andbahdanau cho andben gio to endtranslation nmt typically anen anda intoconsideration however annmtsystemusually usesthetop whichcauses aseriousout of vocabulary oov problem whenoov thetransla correspondingauthor zhongjunhehezhongjun baidu com copyright intelligence www aaai org allrightsreserved shorttranslations sourcesentence monolingualcorpus therefore itisdifﬁcultforannmt whichisproventobeuse translation smt luongetal oovwordsinapost processingstep gulcehreetal rnn how ever problems intuitively ofthesmtcomponents the gramlanguagemodel nevertheless thecurrentnmt quality inthispaper linear framework weincorporate smtfeatures includingthe translationmodel gramlanguagemodel based smtapproach koehn och andmarcu andem tion andthe tences ourmethodhasthefol lowingadvantages thelog ilyextended linear fromtheword proceedings of the thirtieth aaai conference on artificial intelligence aaai dajia doubei chuanran releihengliu unkeos decoderencoder henc dec figure decoder problem onibmmodels brownetal thetranslationta tem thetranslation usedinapost processstep luongetal longtranslation weaddan linear framework scalemono the whileintheshallow fusion gulcehreetal rescoretop devlinetal aulietal choetal li liu andsun zhaietal themaindifference smtframework enhanced sourcenmt systemgroundhog bahdanau cho andbengio https github com lisa groundhog groundhog to endtrans lation we contain ingabout millionsentencepairs collectedfromthe web experimentsonchinese to of the artnmtsystem background decoder are network andthelog linearmodels thedominantframework rnnencoder decoder figure encoder decoder bahdanau cho andbengio for chinese to englishtranslation givenasourcesentence fintoasequence ofvectors previouslygenerated schusterandpaliwal withahiddenlayer attheencodingstep theencoder fintowordvectors xirkx where ofthesourcelanguage hienc genc xi hi enc where the tanhfunction hienc hienc hienc sentence atthedecodingstep quenceiscomputedas jj yj yj yj jj gdec sjdec yj cj where sj whichiscom putedby sj dec gdec sj dec yj cj gdecandgdecarenon thecon textvector statesoftheencoder cj txi jihienc where theweight yjistranslatedfroma sourceword xi bahdanau cho andbengio used afeed modeltoestimate basicrnnencoder ﬁxedlength followingchoetal bahdanau cho andben gio reset gatesandupdategates the reset which the updategatescontrol short term memory lstm sutskever vinyals andle butmuchsimpler thernnencoder andperformsanend to endtranslation however underthe currentarchitecture log linearmodels thewidelyusedlog ducedbyochandney exp mi ihi eexp ihi where hi iistheweight thestrengthofthelog easilyaddedintoit astandardphrase basedsmt koehn och andmarcu typicallycontains features thebi andp the xicjsjyj yj yj gramlangugemodel wordtranslatio jijip yj xi figure illustrationofthelog linearnmt topredict thetargetword yj suchasthe the gramlanguagemodel bi plex andplex the languagemodel thereorderingmodel thewordpenalty and thephrasepenalty recently toimprovelanguagemod eling devlinetal translationmodeling aulietal choetal andphrasereordering li liu and sun zhaietal thenmtmodelviathelog linearframework makingthe nmtmodelextendable log linearnmt weusefig ure toillustrateouridea getword yj rnn gramlan guagemodel thetranslationtable estimatingfromword translatethelow knownwords weusea log featuredeﬁnition thernnencoder decoderfeature thisfeatureisthe hrnn jj log yj sj cj thebi ateach stepofdecoding sourcewords htp jj ii jilog yj xi htp jj ii jilog xi yj where targetword estimated bythernnencode decoder section rnnencoder decoder andp och andney andthe grow diag ﬁnal koehn och andmarcu method lows xn yn wheren istheco words xandy thestandard gramlanguagemodel hlm jj log yj yj pus thewordrewardfeature hwp jj weadd thetranslationcanbe asmentioned thenmtencoder riousoovproblem thepost processingmethod luong etal duringdecoding seefigure forillustration weﬁrstlyﬁnditscor respondingsourceword the unk symbol hengliucrossﬂow hengliuover translationtable figure translationtable bythernnencoder decoder hengliu thenweobtaintrans theﬁnal linearmodel bah danau cho andbengio ˆy argmax yp givenasourcesentence initialstate ateachtimestep thedecoderselectstop states nisthebeamsize of sentence eos symbolisproduced theﬁnaltranslation highestscore lationunderthelog linearframework ateachdecoding state equation toselecttop cabulary inourdecoder foreachwordintar getvocabulary the weightsofthelog dardminimum error rate training mert och al gorithm tospeedupthedecoder weuseapriorityqueue huetal to englishtransla tion theweb containingabout billionenglishwords sofarasweknow thisisthelargest systemdevtest groundhog tm wr lm pbsmt table tm translationmodel wr wordreward lm language model pbsmt phrasebasedsmt weusednistmt oursystemonnistmt insensitivebleu papinenietal thefeature dardminimum error rate training mert och to maximizethesystems weusetheopen sourcenmtsystem groundhog bah danau cho andbengio baselinesystem wesetbeamsizeto fordecoding as acomparison basedsmt pbsmt system whichisare implementation ofthestate of the artphrase basedsystem moses koehn etal andthe forthesmt system wesetthestack limitto andthetranslation option limitto training totrainthe groundhog system welimitthevocabulary to getlanguages bol unk abackwardrnn andeachhas hiddenunits the decoderhas hiddenunits thewordembeddingsare dimensional amini sgd togetherwithadadelta zeiler areusedto trainthenetworks eachmini batchofsgdcontains sentencepairs ofparameters and weranboththe card nvidiateslak forthe pbsmtsystem thegiza ochandney andthe grow diag ﬁnal koehn och andmarcu method wetraineda gram languagemodel stolcke withkn discountonthe thewordtranslationta integratedwiththe groundhog system resultstable weobservedthat moreover oursys largetrainingcorpus speciﬁcally table wardfeatures mentsoverthebaseline tm wr provements firstly sec ondly tionsofunknownwords thirdly thewordrewardfeature theaverage groundhogare and respectively thisindicates lem thenextsection weaddeda gramlanguage tothe groundhog system row lm asthe gramlanguage improvetheﬂuency comparedwiththe groundhog system oursystem groundhog tm wr lm mentof pointsinbleuscore whichisstatistically signiﬁcantatp level riezlerandmaxwell method asanexample the chuanshu transmission infact ourmethodproduces groundhog sourceandtargetwords inthisexample thetranslationta chuanshu transmission with highprobabilities we employaconventional gram languagemodeltoim provethelocalﬂuency forexample thereisanotheren try chuanshu transfer forthechineseword chuanshu the because plm transmission seriesofhighspeed isgreater thanplm transfer seriesofhighspeed sourcer pbsmtyes likether laptop wirelesstransmission groundhogyes likether laptop ourmethod yes likether laptop aseriesofhighspeed transmissionofthe source crossﬂowoftears andtearsunk ourmethod andtears crossﬂow table translationexamples systemoovpercentage devtest pbsmt groundhog ourmethod table thepbsmt table pbsmt groundhog andoursystemsonnist andnist test sets alignmenterror however forthe groundhog asshownin thesourceword hengliu isnottranslatedby groundhog linearframework thiswordwas crossﬂow moreover inthepbsmtsystem asweknow pbsmtsystemex tractsword alignedbilingual corpus however ment phrasetable ideally thernnencoder decoderislim rnnencoder andpbsmt table thetranslationtable therow ourmethod sharesthesame settingswithsection experiment inthesesettings thetrans butalso systemdevtest ourmethod oov table ourmethod groundhog tm wr lm figure therow oov meansthat decoder butnotgenerate itisobserved provementof figure sizeinthedecoder wecanseethatthe sizeincreasesafter ontheotherhand theperformanceof size thereasonisthat withthebeamsize increasing inthispaper smtcomponents thetranslationtable thelanguage model underthelog linearframework whichmakesthe thetranslationtable istrainedonword dardphrase basedsmtmethod andthelanguagemodel theproposed chitecture andthelanguage lingualcorpus experimentsonchinese to englishtrans asanewapproach provement currentrnnencoder decoderisactuallya word inthefuture weplantoim localwordreordering idiomtranslation etc acknowledgements programofchina programno cb we references auli galley quirk andzweig joint works in proceedingsofthe bahdanau cho andbengio neuralma in arxiv cs cl brown pietra pietra andmer cer translation parameterestimation computationallinguis tics cho vanmerrienboer gulcehre bahdanau bougares schwenk andbengio learning decoderforstatis in proceedingsofthe con cessing emnlp devlin zbib huang lamar schwartz and makhoul in proceedingsofthe linguistics gulcehre firat xu cho barrault lin bougares schwenk andbengio on in arxiv cs cl hu li lan wu andwang op in mtsummitxv koehn hoang birch callison burch fed erico bertoldi cowan shen moran zens dyer bojar constantin andherbst moses translation in acl demonstrationsession koehn och andmarcu statisticalphrase basedtranslation in proceedingsofhlt naacl li liu andsun foritg basedtranslation in ing luong sutskever le vinyals and zaremba in proceedingsofthe rdan ticsandthe languageprocessing volume longpapers och andney inproceedingsofthe och andney och in proceedingsofthe stannual riezler andmaxwell onsomepitfallsin in pro orsum marization schuster andpaliwal bidirectionalrecur rentneuralnetworks signalprocessing ieeetransactions on stolcke srilm toolkit in volume sutskever vinyals andle sequenceto in nips zeiler adadelta method in arxiv cs lg zhai zhang zhou andzong rnn based in proceedingsofthe linguistics 
to endspeechrecognition ericbattenberg jitongchen rewonchild adamcoates yasheshgaur yili hairongliu sanjeevsatheesh davidseetapun anuroopsriram zhenyaozhu abstract inthiswork ctc rnn transducer andattention basedseq seqmodels forend to endspeechrecognition weshowthat without anylanguagemodel seq seqandrnn transducermod with languagemodel onthepopularhub benchmark on thesetrendscontinue rnn theseresults we whenallencoderlayers areforwardonly introduction inrecentyears of the asr tasks acousticfeatures hmm butalso whichresultsinend to endneu ralasrsystems andboth lengthsarevariable asaresult severalneural network inthiswork wefocus mechanisms models ctc rnn transducer andsequence to sequence seq seq withattention fortheasrtask thesemodelsdiffer ferenttimesteps givenaudio thisisnotareason ctcmakesthisas sumption butrnn donot tonic task tion ctcandrnn tion butattentionmodels donot hardvssoftalignments ctcandrnn transducer everyinputstep task turesatscale inthiswork inordertoper formafaircomparison put viz rnn transducers attention areabletolearnan wethat evenoutperform ingctc guagemodel amongthem rnn parameterstotune models mentations then insection hub benchmark whichuses hoursoftrainingdata of hours insection we onlylay ers ineachmodel section turework encoder anda decoder recently arxiv cs cl jul ctc rnn transducer attention fig andlabelledasﬁcatﬂ thenodeat horizontalaxis verticalaxis notallowedfor ctc forctc orpredictingnothing forrnn transducer forctcandrnn transducer andsoftones forattention asnoticed inctc andrnn transducer whileinattention allinputframes charactersorwords alsodifferent lengths so mentbetweenthem moreformally oflength andtheoutputsequence of length witheach beinga dimensionalone hotvec tor tion withtime scaledownsampling theencodercan bebuiltwithfeed dnns re rnns orconvolutionneural networks cnns andthemappingfrom to ctc ctc givenalignedinputs anextra blank label whichcanbein terpretedasnolabel isintroducedtomap and tothesame length analignment path blanksinto amapping isbetween and inglettersin ctc can backwarddynamic programmingalgorithm asdetailedin notethatthe alignments ctc softmax the step tive anddecodingis log ctc log lm wordcount search the toaddressthis modelscould butthe predictions rnn transducer rnn transducer alignments likectcdoes softmax exp exp strictlyspeaking not butwe timesteps more theprediction of attimestep but rt where timestep mine andthe thesummationofthe andthe softmax weuse asin likeinctc are localandmonotonic decod butwedonotuselength sincewedonotit necessary attentionmodel attentionmodel likernn transducer attention unlikectcandrnn transducerhowever itcom putes attn where whichis weightedby known asattention exp exp where tocompute weused alocation ments attn attentionrnn attn computeattention attn decoderrnn attn andthusthealign mentscanbenon localandnon monotonic however this ingfortheasrtask therefore thedecoding tasktheargmaxof log attn cov log lm where the encodertimesteps overthesametimesteps performanceatscale inthissection thepromiseofend to systems end to ingprocess decoding withmas sivelanguagemodels sinceattentionand rnn speechtrainingcorpus does table whenan simplyrescor ingthebeam typicallysmall between and table thedecod languagemodels wherestate of artnmtsystems hub results benchmark ispresentedintable in domain data usethestandard exceptforthe rowsmarkedﬁnolmﬂ architecture swbd ch wer wer published iterated ctc blstm lfmmi lace lfmmi dilatedconvolutions ctc gram ctc blstm featurefusion ours ctc rnn transducer beamsearch nolm beamsearch lm attention beamsearch nolm beamsearch lm table onfisher switchboardhub benchmarkusing in domain data allthe we don though andarehighly sincethelm rescoringwiththe transducermodels inequa tion however show thernn pre atten tionandrnn model wer subs ins dels ctc rnn transducer attention table deepspeechcorpus hoursof suchasfarwith backgroundnoise accentsetc additionally thetrainand don distribution model devtest ctc greedydecoding beamsearch lm beam rnn transducer greedydecoding beamsearch beam lmrescoring attention greedydecoding beamsearch beam length normweight coveragecost lmrescoring table model prediction groundtruth silence ctc silence rnn transducer silence attention groundtruth ctc greedy ladingtoblack irpen songs beamsearch lm leadingtoblack european songs rnn transducer greedy playtheblack eyepiece songs beamsearch playtheblack eyepiece songs lmrescore attention greedy playtheblack eyedpea songs beamsearch playtheblack eyedpea songs lmrescore table wethat theexam the stagesofdecoding onattentionandrnn transducers onthedevelopmentset notethatrnn transducermodel andcompletelycloses candi dates surprisingly butthe ctcmodels spellings butthe inthesecases theattention codertimesteps thegreedyde shownintable themonotonicleft to rightdecodingof ctcandrnn fur ther experimentaldetails data throughoutthepaper allaudiodatais sampledat log linearorlog melspectrograms thetypeoffeatur izationisahyper parameterwetuneover areextractedwith ahopsizeof msandwindowsizeof ms andthenglob meanandunitvariance inanyofourmodels everyepoch oftheutterancesare allmodelsintable fisher weuseaportionof thert corpus forhyper parametertuning the gramlm the isbuiltfroma model allmodelsintables and are weperformarandom amountofpooling minibatchsize choiceofoptimizer learningandannealing rates further in wallclocktime orothers anduses sortagrad andallmodelsusebi directionalrelugru encoderswithbatch andmay end inshorthand conv gru representsastackof layersof bidirectionalrelu gru weightnoise inmost cases only largeun normalization by inshorthand thebestctc modelis conv gru thebestrnn transducer sencoderis conv gru and decoderis fwd gru end theencoder is gru andthedecoderis fwd gru mparameters allmod on gpus usingsynchronoussgd inthissection areevaluating transducer however particularly theamountof themodel encoderswithforward onlylayersalsoallow forstreamingdecoding we layers of withweight noise coder end dataaugmen tation parameters we kiterationsoftrain ing thissearchoverhyper theattentionmodel intable hasawerof dev set in similarly thanreportedin forward onlyencoders models only recurrentlayers andrnn attentionmodels puttingthecharacter butnone iterations awerof wsjdev set fullattentionmodels nevertheless webelieveacomparison inourexperiment wereplace everylayerof withalayerof forward onlylstmcells model bidirectional forward only decoding greedy beam beam nolm lm lm ctc rnn transducer attention table set on smallerdatasets rnn rnn transducersandatten as shownintables and fromtable morestable onlysetting also betterthanrnn transducermodels thefullattentionover fig rateonwer steps previousresultshave stepspersecondof audio at bestatabout sogiventhe tionmodelwith lesser thisisimportant sincernn transducers sameencodertimestep weexpectrnn transducerstobe poolingintheencoder whilefigure showsthattheyare wethatatten inaddition we lay ersofpooling encodersteps ctcandrnn andeveryinputstep inaddition rnn transducerandatten herein fig ure vset truth text insteadofpredictions ablealignments axisintheleftsub toalignwithinputs multipleattending producingcharacters alongthe sameinput thesamecolumn canbefoundinrnn transducer middle andattention right models transducer aremoreconcentrated orpeaky comparedtothatof attention inaddition relatedwork segmentalrnns modeltheasrtask segmentalrnnsmodel using azeroth ordercrf withthoseofctcandrnn fig left rnn transducer middle andattention right truthtext axis axis notethat scaledownsampling whichresultsin shortersequences xaxis papers mizationmethodology butonlypresent timit inintroducing rnn transducermodels withtheseconstraints theing butthey inother words beamsearchdecoding coveragepenalty fortheend to endasrtaskatscale andthatinthe bidirectionalsetting same however notably end to endmodels rnn tive betweenthesetwo rnn parameterstuningfor decoding transducers to endspeechmodels in attempttotrainrnn constraint we transducers acknowledgements ofthebaiduspeech ingimprovethedraft references sequencetose arxivpreprintarxiv darioamodei rishitaanubhai ericbattenberg carl case jaredcasper bryancatanzaro jingdongchen mikechrzanowski adamcoates gregdiamos etal deepspeech end to glishandmandarin arxivpreprintarxiv dzmitrybahdanau janchorowski dmitriyserdyuk philemonbrakel andyoshuabengio end to end attention abs http arxiv org abs ericbattenberg rewonchild adamcoates christo pherfougner yasheshgaur jiajihuang heewoojun ajaykannan markuskliegl atulkumar etal reduc arxivpreprint arxiv williamchan navdeepjaitly quocle andoriol vinyals listen attend andspell abs http arxiv org abs williamchan navdeepjaitly quocvle andoriol vinyals listen attendandspell arxivpreprint arxiv chung chengchiu dieterichlawson yupingluo georgetucker kevinswersky ilyasutskever and navdeepjaitly anonlinesequence to sequence arxivpreprint arxiv janchorowski dzmitrybahdanau dmitryserdyuk kyunghyuncho andyoshuabengio attention based abs http arxiv org abs towardsbetterde sequencemodels arxivpreprintarxiv ronancollobert christianpuhrsch andgabrielsyn naeve wav letter anend to endconvnet basedspeech recognitionsystem arxivpreprintarxiv alexgraves ralnetworks arxivpreprintarxiv alexgraves santiagofern andez faustinogomez and urgenschmidhuber tion rentneuralnetworks in proceedingsofthe rdinter pages acm towardsend to end in proceedingsofthe machinelearning icml pages alexgraves abdel rahmanmohamed andgeoffrey hinton networks in icassp awniy hannun andrewl maas danieljurafsky and andrewy ng first abs http arxiv org abs hinton deng yu dahl mo hamed jaitly senior vanhoucke nguyen sainath andb kingsbury ieeesignal processingmagazine november hairongliu zhenyaozhu xiangangli andsan jeevsatheesh gram ctc corr abs lianglu lingpengkong chrisdyer noaha smith andsteverenals forend to endspeechrecognition in interspeech yajiemiao mohammadgowayyed andflorianmetze eesen end to modelsandwfst baseddecoding in automaticspeech asru ieee workshopon pages ieee povey ghoshal boulianne burget glembek vesel goel hannemann motlicek qian schwarz silovsky and stemmer in asru danielpovey vijayadityapeddinti danielgalvez pe gahghahremani vimalmanohar xingyuna yiming wang andsanjeevkhudanpur purelysequence trained freemmi in in terspeech pages colinraffel thangluong peterjliu ronjweiss anddouglaseck onlineandlinear timeattention arxivpreprint arxiv georgesaon gakutokurata tomsercu kartikau dhkhasi samuelthomas dimitriosdimitriadis xi aodongcui bhuvanaramabhadran michaelpicheny lynn lilim etal arxiv preprintarxiv andreww senior hasimsak felixdechau montquitry taran sainath andkanishkarao ctc smbrlstmrnns in asru densepredictionon sequenceswithtime recognition arxivpreprintarxiv jasonrsmith hervesaint amand magdalenapla mada philippkoehn chriscallison burch andadam lopez dirtcheapweb moncrawl in acl pages yonghuiwu mikeschuster zhifengchen quocv le mohammadnorouzi wolfgangmacherey maxim krikun yuancao qingao klausmacherey etal google bridging arxiv preprintarxiv waynexiong jashadroppo xuedonghuang frank seide mikeseltzer andreasstolcke dongyu andge offreyzweig speechrecognition arxivpreprintarxiv geofferyzweig ghengzhuyu jashadroppo andan dreasstolcke advancesinall neuralspeechrecogni tion arxivpreprintarxiv 
gangniu niugang baidu com tokyo japan baiduinc beijing china bodai bodai gatech edu atlanta ga usa christo sg cs titech ac jp masashisugiyama sugi cs titech ac jp tokyo japan abstract the largevolumeprin ciple esisspace the volumeapproximation hashith problems inthispaper weproposeanovelgen allowingapplica ingproblemssuchas multi class multi label and serendipitous anon theglobally beobtainedusing time noveltheoretical ablywiththeone vs restextension introduction thehistoryofthe largevolumeprinciple lvp goesback vap nik butit wasproposedin el yanivetal toimplementlvp fromthenon ithas fully el yanivetal binaryclustering niuetal andoutlierde tection li ng proceedingsofthe st learning beijing china jmlr cpvolume copy right bytheauthor figure thelarge volumeprincipleand itsapproximation lvpisalearning as hypothesisselecting fromacertain hypothesisspace canalwaysbeparti observeddataset whereeach equivalenceclass isasetof data lvp asoneofthelearning seethe illustrationinfigure and itispartitionedinto intersectedwith lvpclaimsthat and and since and havelargervolumethan and inpractice thehypothesisspace cannotbeassimpleas infigure dimensional volumeestimation ischallenging therefore el yanivetal proposed volumeapproximation itdi sincelearningisre it viaan ellipsoid cipalaxesof thismanneris reasonable lieinlarge volumeregions infigure and aretwohypotheses and isthelong shortprincipalaxis lvpadvocates that ismorepreferablethan as iscloseto and iscloseto nevertheless el yanivetal potentialadvantage multi classlearning zhouetal multi labellearn ing kongetal and zhang etal el yanivetal for dataand labels ahypothesisspaceisde in nc suchthat beaccordingly multi classapproximate volumeregularization mavr itinvolvesanon convex optimizationproblem timefollow ing forsythe golub moreover wetheoretically and theone vs restextensionof el yanivetal insection insection thebinary classvol atlastthe vapnik supposethat xˆ where isa naturalnumber aedset of points from isobserved andthelabels asubset ofsize andthen is revealedif wecall the labeleddataand theunlabeleddata using and thegoalistopredict of whileany unobserved xn transductivelearning tl blum chawla szummer jaakkola joachims zhouetal el yanivetal slightlydiffersfrom semi supervisedlearning ssl bennett demiriz zhuetal grandvalet bengio belkinetal lietal li zhou niuetal xn more istic issampledfrom and issampledfrom stochastic moreover buttlmight dealwithitdirectly tosumup innature asanextensionof el yanivetal thevolumeap the settingis multi classlearning insteadof wehave where and isanatural number eachofthe thesecondsettingis multi labellearning with where isalabelset or with where isalabelvector cf kongetal thethirdsettingis whichisamulti classsettingwith missingclassesin thatis someofthe labelshaveno labeleddata cf zhangetal itisnon trivialtosee covariateshift yamadaetal or class priorchange duplessis sugiyama fromsemi whereasitisunneces inprinciple class problemswiththeone vs restextension butthismaynot convex optimizationslike el yanivetal furthermore the vs restextension the settingsinamanner multi processingas kongetal volumeapproximations andproposeourmulti cepts el yanivetal the andthepower wangetal givenasetof data where softresponsevector isan dimensionalvector sothat forbinaryproblems suggeststhat isfromthepositive classif and hypothesisspace thevol matrix about thesetof signvectors sign containsallof and canbepartitionedin toanumberof equivalenceclasses such thatfored allhypothesesin willgeneratethesame labelingof then the power ofanequiv alenceclass pothesesinit vapnik where over thehypothesesin vapnik isunknown uniformdistribution where isthe volume of thatis init asaresult isproportionalto andthe largerthevalue is themoreweareofthe hypotheseschosenfrom however letaloneall convexbodies so el yanivetal introducedan efapproximation let betheeigen valuesof and eigenvectors actually thehypothesisspace ineq centeredellipsoidin with and thprincipal axis in to some withasmall largeindex along shortprin cipalaxis impliesthat islarge small cf figure we where and wesubsequentlyexpect tobesmallwhen liesinalarge andconverselyto belargewhen liesinasmall multi themulti manner inordertoextendtheeq weneed tobeginwith foreachofthe labels thevalue isasoftorlabelof con cerningthe shouldpossessthe thlabel if shouldnotpossessthe thlabel if if formulti ispredicted by argmax formulti labelproblems weneed athreshold and canbe predictedby orwecanemploythe kongetal then softresponsematrix by matrixby anda asanequivalenthypoth esisisan nc dimensionalvectorby vec nc where vec isthevectorizationof formedbystacking asymmet ricpositivmatrix whichcontainsthe isprovided andweassume isavail able tr qhp let nc nc bethe kroneckerproduct of and dueto and the kroneckerproduct and in vec vec and comesfromthe factthat tr qhp vec vec follow ingthewell knownidentity see theorem of laub vec vec qhp asaconsequence and centeredellipsoidin nc thesetofsignvectors sign spreadsover allthe nc quadrantsof nc andthusthesetofsign matrices sign containsallof possi bledichotomiesof inotherwords canbepartitionedinto equivalenceclasses suchthatfored will andsoisthe ofthevolume and is thquadrantof nc and we tr qhp fro where vec and fro of wesubsequentlyexpect tobesmallwhen liesinalarge andconverselyto belargewhen liesinasmall notethat and ingproblems when wemayconstrain where meanstheall zerovectorin let where then whichcoincideswith ineq similarlyto and fromthe sameequivalenceclass and maynotneces sarilybethesamevalue inaddition thedomainof couldbeextendedto thoughtheof isoriginallynullfor outside multi regularization we actlyandef model firstofall or wecanset if islabeledtohavethe thlabeland other wise if islabeledto havethe thlabel if islabeledtonothave the thlabel and otherwise let between and themulti min tr qhp fro where thedenomina tor fro in el yanivetal or niuetal we asascaleparameter constrain tobeofnorm replace thefeasibleregion with anditbecomes min tr qhp fro theregulariza tionisrelativeto since tr qhp isaweighted vec andtheprincipal axesof undertheconstraint fro subsequently wedenoteby and the and tobe inoptimization zhou etal el yanivetal and niuetal actually is zerofor in since and areconstantsand isalsoaconstant thesecondoneisun thus we instantiate fro andoptimization becomes min fro tr qhp fro as multi mavr anun min fro tr qhp algorithm optimization isnon convex vec as min nc where vec isthevectorizationof inthisrepre sentation degreepolynomialand centeredsphere andfortunately forsythe golub tothisend see theorems and of laub theorem let betheeigenvalues and vectorsof and be thoseof andtheeigen decompositionsof and be and then theeigenvalues of are vectors for andthe eigen decompositionof is pq pq pq where pq and pq and intheobjec tiveofoptimization where nc hence where is notaneigenvalueof wehave ˆi nc pq pq ˆi nc pq pq ˆi nc vec yv algorithm mavr input and output and eigen decompose and constructthefunction recover using andreshape to basedoneq andtheorem next wesearchforthe feasible for and optimal let vec yv thenplugging into givesus pq ˆi nc intoa non descendingsequence pq pq nc rearrange nc accordingly andthesmallest which asaresult eq impliesthat nc pq foranystationary bytheorem of forsythe golub thesmallestrootof determinesaunique so that globally here theonly byeq fora valueof iswhen isaneigenvalueof this however thetheorem theproofis intheappendix theorem thefunction ineq hasex pq andnorootin theinterval where pq it iseasytoseethat inalgorithm insteadof thesmallestrootof suftosolveoptimiza tion moreover foraspecialcase where is anystationary issimply ˆi ˆi let where meanstheall onevectorin and then thesmallestrootof givesusthefeasible is more eigen decomposing inthestepof algorithm costs putationtime eigen decomposing justneeds and withoutloss ofgenerality inthesecondstep itrequires nc log nc and forcom puting basedonabi log inthethirdstep inthefourthstep recovering puting andcosts timecomplexityofmavr givened and butdif ferent and ifwereusetheeigen decompositionsof and andthe sortedeigenvaluesof itis tobela finally the forsolvingmavrex evenif isedinoptimization thestationarycon ditioneq isa whichcon sumes forsolvingit sima theoreticalanalyses undercertainassump tions twooptimal and dicatormatrices and thedifferenceof fromthegroundtruth theorem guaranteesthat pq infact withhigh itholdsthat we didnotmeet inourexperiments forthisreason fix and andallow titionof intodifferent and thereis whichjustdependson and trainedwithdifferent pq theremustbe since pq and wecanprovethethe boundof intheorem theorem stabilityofmavr assumetheexistenceof let and and respectively then fro fro min fk fro fro consequently wehave fro fro fro fro wehave fro fro weassumethereisa ground withtwoproperties firstly thevalueof shouldbebounded namely tr qh fro where isasmallnumber thisensuresthat lies inalarge volumeregion closeto secondly tionabout mavrmakesuseof and onlyand themeaningsof and areedalready somavrmay onlythrough tomake and correlated weassumethat where and allentriesof andthe varianceofthemis or wecould expectthat suchthattheentriesof inlabeled but theentriesof noticethat evenif is since mayhave havetotallydiffer entmeanings wecanprove thetheorembelow theorem accuracyofmavr assumetheexistenceof from and let and positionsin andassumethat fro wherethe foreach possible let withit then fro pq fro max pq pq fro and fro fro theproofsoftheorems and areintheappendix con andtheerror boundsintheorem thatbeingsaid bounds arejustbounds experiments inthissection thebase vs restextensionofthe bina data data data data data data data data data data data data figure bavr aswellasa multi learningwithlocal andglobalconsistency lgc zhouetal hartigan wong ngetal sugiyamaetal orestimating theclass priorchange duplessis sugiyama the thereare datasetscome from zelnik manor perona thematrix was asthe see von luxburg nor wd where is thedegreematrixof thematrix wasby fordatasets and exp withthekernelwidth andfordatasets to we appliedthelocal scalingsimilarity zelnik manor per ona exp data data data data data figure withthenearest neighbornumber whereeach and isthe thnearestneighborof in weset and furthermore aclass balance to whichtries tominimize tr classes eterwassetto wecanseethatinfigure mavrsuccessfully more over asshownin and offigure ontheotherhand andlgcwiththeclass to infigure multi classlearning we experiments actually althoughlgc itcan min fro tr nor therefore lgcif and nor nowwespecify and nor lgc thedataset circles isgeneratedasfollows we let betheground truthlabelof then isgeneratedby cos sin where isanangledrawni tion and and arenoisesdrawni from inourexperiments we thedefault default small large small large figure circles varying varying varying varying varying varying figure bavrandmavron circles valuesoffactorswere and exp exp exp exp sincewesam es figure circlesgiven figure wherethemean areplotted cationofthosefactors on randomsamplings wecanseefromfigure that asmavr have though isadap tiveinbavr wewouldhave different valuessinceit isbasedontheone vs restextension conclusions weproposedamulti multi class multi there convex butcanhoweverbe solvedexactlyandef acknowledgments pro gramno cb enhi andaoard references belkin niyogi andsindhwani manifoldreg ularization journalofmachine learningresearch bennett anddemiriz semi supervisedsupportvec tormachines in nips blum andchawla in icml duplessis andsugiyama semi supervisedlearn tionmatching in icml el yaniv pechyony andvapnik largemar ginvs machine learning forsythe andgolub second journal grandvalet andbengio semi supervisedlearningby entropyminimization in nips hartigan andwong meansclustering algorithm appliedstatistics joachims titioning in icml kong ng andzhou transductivemulti ieeetransac laub li andng international li andzhou neverhurt in icml li kwok andzhou semi supervisedlearn ingusinglabelmean in icml ng jordan andweiss onspectralclustering in nips niu dai shang andsugiyama maxi mumvolumeclustering ingapproach niu jitkrittum dai hachiya andsugiya ma squared tion anovelinformation supervisedlearning in icml sima algorithmsforlinear marceldekker sugiyama niu yamada kimura and hachiya information onsquared neuralcomputa tion szummer andjaakkola partiallylabeled in nips vapnik icaldata springerverlag vapnik johnwiley sons vonluxburg statistics andcomputing wang jebara andchang semi supervised cut journalofmachine learningresearch yamada sugiyama andmatsui semi signalprocessing zelnik manor andperona self tuningspectralclus tering in nips zhang liu andsi in kdd zhou bousquet navinlal weston and sch olkopf tency in nips zhu ghahramani andlafferty semi supervised in icml 
subspaceaddition leishi baidu com inc shilei baidu com abstract subspacead dition pcsa bothrowsandcolumns dimensional features wiseandcolumn spectively inconsequence tricately andisabletohandlenon by furthermore pcsais and of artmethods experi sparse pcsaon modelingmatrix tensor dataandmissingvalues introduction usinggaussian process gp xu etal outputsfromgps lmc in matrixfactorization pmf wiseandcolumn wise pma describes regression pmfandlmc however subspaceaddition pcsa model which asitsname assumesthatall wiseandcolumn wisehiddensubspaces seesection pcsa thenon ticdensity subspaces mensionalitiesofpcsa furthermore threeextensions first second withajeffreysprior finally nd order tensor topcsa wecompare sparse pcsawithseveralstate of artmodels approaches includingpma the labeldata user itemratingdatafor pcsaiscom paredwiththe sa method although probabilisticco subspaceaddition pcsa letting bean observedmatrixwith for and with and where denotesa denotesa identitymatrix twomatricesofhidden factors and arecolumn wise respectively and eachentry ij isindependentgiven and by ij ij where isthe throwof each ij ij independentfrom thusis get for get for get ij ij for get ay bz given and subspaces givenparameters and is ij pcsaowns althougheachentry ij isindepen dentgiven and columnsinthejoint particularly assuming is thesamplesize thesamples columnvectors in sa when isconstrainedas ppca assumption non ifwestillconsider asthedatadimensional ityand asthesamplesize asanextremeexample ifallcolumnsof setof vectors with components whose thatis thegroupppca also ifmarginalizing weare co wiseandcolumn wisecovariances pma large kernelmatrices withsizes and respectively applications incontrast pcsahas matrices andrecoverspmawhen and moreover pcsaisableto and simultaneously otherwise wecantranspose given wecanestimatepcsa sparameters however eralizationability byintroducinghyper parameters and fora hierarchical normal gammaprior on and respectively wehavetheprior as where eter eachcolumn ofthemappingmatrix priori withaprecisionscalar ard typeprior each precision where sincemcmc lihood log log fortractability maximizing theabove for wedenote andsimilarlyfor and during maximizing thesolutionsof and where isexpectationand norstably and then isobtainedbysolving zl with as as toupdate given and updating issimilar theremaindersof ineq areupdatedas tr diag tr diag thechoiceofcomputing forlearningefy tr tr jj jj diag diag where tr standsfortrace diag inter and jjjj isthefrobeniusnorm inimplementation allgammapriorsineq aresettobevagueas during learning redundantcolumnsof and whichactuallymakes and extensions fillingmissingvalues thegoalhere arerandomlywedenote ij ismissing valuestherein ineachiteration weﬁpretendﬂthat by eqs thengiven themissingentries ij mg ij ij with argmax moreover missingvaluesinpma gibbssamplingormap incontrast pcsadirectlyemploys bayesiansparsepcsa asdiscussedabove and inthe co subspacesthrough and respectively and theparameters and probably isonepopularly inthispart weextendtoprovidea lasso regularizer whichisequivalenttoa laplacianprior in jeffreys nj prior paper ppca inthispaper differentfromeq eachcolumnof and jeffreysprior with with and theprioron remainsthesameasineq wenowlet and eq inconsequence weoptimize log and where log asabove and arereplacedwith and respectively thengiven thevariances and areupdatedvia diag and diag modelinghigh ordertensordata uptillnow whenitisamatrix tensors arehigher st ordertensors andmatrices nd ordertensors mode andthe order ofitsmodes faceuppercaseletters incomparison formatrices th ordertensor canbedenoted by respectively anelementanda st mode vectorof aredenotedby and respectively where foreach moreover the st mode transformof denotedby st mode vectorsof viceversa tensorization ofamatrix isas sothat an th mode shift transformisas the th modein becomesthe st modein th ordertensordata foreachmode areassumedi drawnfrom ii draweachelement isactually generatedbyamode shiftco subspaceaddition whereeach andthematrix maps to shortlynamedas pcsa andparameters withlatent scales when pcsa onmatrixdata also it sameaseq eachcolumnof gammaprior andthebayesian inferenceinsection model pleaseseethedetails remembering and sylvesterequations wecandecoupleneither nor forthegeneral instead sequentiallyforeach weupdateonly or andkeeptheremaining or ed experimentalresults on emotions and cal data lmcandpmfin following oddsmatrixinmulti labelfor samplesand classes berepresentedasan binarymatrix atruncatedlog oddsmatrix isconstructedwith ij if ij and ij if ij where isnonzeroconstant inexperiments certain entries ij ij byanalgorithm andtheperformanceis sign ij twomulti namely emotions and cal alreadyusedin the emotions contains sampleswith numericat tributesin classes to the constructed for emotions isthus the cal contains sampleswith numeric attributesin classes and respectively theconstructed for cal isthus itssizeislargerand emotions from to with asastepsize after fig report oddsmatrices bybayesian pcsa emotions data pcsaout whilesparsepcsatakes sparsity onthemorebalanced cal data pcsa moreover table wheresparsepcsashows sincetheydonotneed seesection figure errorratesof independentrunsfor emotions left and cal right data dataset emotionscal pcsa sparsepcsa pma table averagetimecost inseconds indepen on movielens and jesterjoke data collaborative thesizeofthematrix weightdatasets the movielens data andthe jesterjoke data particularly the movielens datasetcontains kratingsof userson movies whichareordinal valuesonthescale the jesterjoke userswhohaverated between and piecesofthetotal jokes recentlyin rbmf student entriesonboth movielens and jesterjoke data following ineachrunweran domlychoose fortesting andthepredictions theperformanceiseval uatedbasedonthe rmse rmse and the meanabsoluteerror mae mae after independentruns sparse pcsaare reportedintable rbmf rr collectedfrom itisnotconsideredto comparablewithrbmf table averagermseandmaeon movielens left and jesterjoke right model rmsemae pcsa sparsepcsa rbmf rr model rmsemae pcsa sparsepcsa rbmf rr downloadedfromwww grouplens org node namely frey and orl frey has imagesofsize takenfromoneperson andthedata isthusa matrix orl has imagesofsize takenfrom persons imagesperperson andthedata isthusa matrix in latentvariablemodel gplvm recentlya bayesiangplvm partiallyobserved frey faces thusineach run werandomlypick sameas bayesiangplvmusesthe imagesfor incontrast sparse matrix the for frey to andfor orl to correlation coef corr on frey and orl datarespectively fig andfig andpcsagives thebestmatching also sparse observedsamplesize all imagesarepartially observed asexbyfig obviously furthermore frey dataforasingle person tothe orl sparsepcsaperforms figure resultsof runson frey faces figure resultsof runson orl faces figure frey left and orl right observed respectively pie database andtotallyhas faceimagesfrom individ uals eachperson sfaceexhibits differentpose and illumination combinations greyscalepixels identity andpose illumination respec tively figure thepcsa model with onthe weusethecodeinhttp staffwww dcs shef ac uk people lawrence vargplvm rd ordertensor insection subspacestructures correlations amongpixels identities andposes in an sa onthetensordata figure pie database true true figure originalimages intheoddrows are andpcsa table averagecorr left andmae right of runs bypcsa and sa onthecmu pie data missingproportion pcsa sa missingproportion pcsa sa here but images comparedwith thetruemissingimages mae isconsideredas and respectively after the and sa arecompared intable duringimplementing sa as accordingtofig in asshownintable pcsa allcases fordemonstration fig is concludingremarks subspaceaddition pcsa model whichsimultaneously tensors vari andtheposteriors butalsopcsa that modelsarbitrary th ordertensordata ularapplication matrix tensor dataandmissingvalues subspaces acknowledgments references agovic banerjee ands chatterjee in proc icml pages bishop neuralcomputation bishop in proc icann volume pages figueiredo in advancesinnips volume pages mitpress cambridge ma gelfandands banerjee ina gelfand diggle guttorp andm fuentes editors crcpress geng smith miles zhou andl wang ieeetrans syst man cybern cybern ghahramaniandg hinton technicalreport crg tr universityoftoronto toronto canada goldberg roeder gupta andc perkins eigentaset algorithm informationretrieval guanandj dy in proc aistats jmlr cp volume pages huandl reichel krylov linearalgebraandits applications jordan editor mitpress cambridgema lakshimanarayan bouchard andc archambeau in proc aistats jmlrw cp volume pages lawrence in advancesinnips volume pages mitpress cambridge ma neal springer verlag newyork roweis saul andg hinton in advancesinnips volume pages mitpress cambridge ma salakhutdinovanda mnih in advancesinnips volume pages mitpress cambridge ma samariaanda harter proc pages sim baker andm bsat thecmupose illumination ieeetrans patten anal mach intell tibshirani stat soc tippingandc bishop neural computation titsiasandn lawrence in proc aistats jmlrw cp volume pages trohidis tsoumakas kalliris andi vlahavas in proc intl conf ismir pages turnbull barrington torres andg lanckriet soundeffects ieeetrans audio speechandlang process virtanen klami khan ands kaski in proc aistats jmlrw cp volume pages xu kersting andv tresp multi in proc ijcai pages 
inrecruitmentmarket amulti criteriaapproach tongxu hengshuzhu chenzhu panli andhuixiong tongxu ustc edu cn lp mail ustc edu cn baiduinc zhuhengshu zhuchen baidu com rutgersuniversity hxiong rutgers edu abstract indeed itisanontrivialtask skills tothatend inthispaper weproposeadatadriven theanalysisoflarge scalerecruitmentdata ally we jobpostings then wedevelopanovel kill opularitybased opic odel sptm network inparticular ofjobs salarylevels companysize aswellasthela facetedpopularity exten skills suchasthepop paidemployment introduction thecompeti indeed targetedskillswill suchashigh however therestillex istsaﬁskillgapﬂ skillsyouneed betweenemployers andjobseekers ontheonehand companies skilledtalents especiallywhen ontheotherhand tocompete nelpool suchsituation china where millionfresh marketeveryyear daily therefore itis correspondingauthor copyright intelligence www aaai org allrightsreserved indeed popularityisnotnovel currently manyorganizations likelinkedin linkedin andcomputerworld world how ever line first inpractice someapplicantspursue highsalary town evenwithlowerincome second jobskillsarenotiso lated ﬁphpﬂandﬁnode jsﬂusually appearedtogether front enddevelopmentﬂ therefore inthispaper we proposeadata scalere cruitmentdata skill net cruitmentmarket then wedevelopanovel kill opularity based opic odel sptm jobskills particularly riaofjobs salarylevel companysize skillcategories therefore withthe helpofsptm theirmulti finally weevaluatethe siveexperiments criteriaperspective which large scalerecruitmentdata worldrecruit theexperimental and arxiv cs cy dec table designoriented datadescription inthispaper drivenapproach spe cially todealwiththistask wecollectedtworeal worldre cruitmentdatasets includingalarge scalejobpostingdata trainingmodel togeneratetheﬁskill netﬂ jobpostingdataset zhuetal rang ingfrom to whichcontains jobpostings intotal amongthem descriptions ispresentedintable riaofjobs besides wecon clude salary dividedinto levels veryhigh high medium low verylow krespectively location company salary postdate figure frequencyofskills frequencyofskills log figure inwhichy companyscale dividedinto categories verybig big medium small verysmall correspondingtothe numberofemployees respectively location dividedinto categories hugecities big cities normalcities catedcities financinground dividedinto categories angel listed unknown statusofround worktype dividedinto categories fulltime part time intern skilllistandﬁskill netﬂ atthesametime weextracted china csdn the skillcategories andthe detailedskills whichbelongsto onecategory totally skillswithin categorieswere extracted wehavere andthenbuiltthe standardﬁ dictionary thefre asthebase areshowninfigure whichindicatesthe skillssuffera longtaileffect onlyafewjob atthesametime categoryofskills skill net asmentionedabove butmutu which un fortunately figure thesnapshotofﬁskill netﬂ thus inthispaper wepickupthe andthenbuildthe ﬁskill xuetal par ticularly occurinthesame jobposting figure shows asnapshotoftheﬁskill netﬂ ourreal netﬂareprepared inthissection tofa overviewofsptmmodel generally derdifferentcriteria salarylevel companysize etc therefore viaestimating specially insptm wetreateachskill named central skill inourpaper asaﬁ document ﬂinﬁskill netﬂ also named skilltokens areregardedas correspondingﬁ wordtokens ﬂwithin whichissimilarwith figure showsanexample inwhicheach besides aswe posting labelsofthejob veryhighsalary verybigcompany etc bythisassumption wegenerateasetof criteriala bels foreachcentralskill justasinlda wealsouse and torepresentthe table symboldescription topicpresence absencevectorof thcentralskill thcentralskill topiclabelof thskilltokenof thcentralskill skillcategoryof thskilltokenof thcentralskill skillcategoryof thskill thskilltokenof thcentralskill thtopicand thcategory numberoftopics criteria numberofskills thcentralskill algorithm for eachtopic do generate dir for eachcentralskill do for eachtopic do generate bernoulli generate dir generate dir for eachskilltoken do generate multi generate multi generate multi butionoverskillsfor thtopics besides tomodelthecon wefollow theideasin ramageetal withacriterialabel specially wehave topresentsthe criterialabelsof thcentralskill where isabinary topicpresence absencevectorandeach then wegeneratethe thcentral skill by dir besides thepopular andcriteria which frommacroperspective wefollowtheideasin huaietal categoryofskill chinelearningﬂ thustheskilltoken isgener atedby multi algorithm marizedintable modelinference then inference specially training whichin figure figure dicatesthetopicof thskilltokenin thcentralskill is givenby where isallofskilltokensof thcentralskillex cept thoneand issimilar isthetopiclabelsof here anditscategory given and in thcentralskill regardlessof thskilltoken where iscountof thskillin thskillcategory and and iscountof thtopicappearingin thcentralskillexpect isthecountof ingin thcentralskillexpect ofgibbssampling probabilityofaskill wecouldnow specially in salary location com panyscale roundorworktype consideringthat wecanes category viabayes formulaby experiments inthissection posedmodel specially lines afterthat andacasestudywill experimentalsetup firstly thissubsection includingdatapre processing parameter datapre processing wecon toen we outthosejob whichcomposeof oftheoriginaldata correspond ingly description parametersetting inoursptmmodel threesetsofpa first ofsptm with second as correspond ingtothe uniquecriteriawithin differentcategories mentionedin datadescription finally forthemodeltrain ingprocess and baselinemethods frequency ularityofjobskills specially andthen llda labeled lda llda ramageetal isa whichhoweverdoes withinskills lda experimentalresults twokindsof skill orientedtopicevalua tion skill elingperformance wepickedupthetop skillsofeach andthenasked se indetail twofac torswillbeconsidered namelythe validitymeasure vm valid at least skillsamong cruitment andthe coherencemeasure cm whichcounts skillsinthesame topic vm of valid topics oftopics cm of relevant skills ofskills theaverageresultsof in bothvmandcmmetrics moreover asweonlyselected thetop skillsofeachtopic then weturntoevaluatethe itisintuitive tion ployersandjobseekers specially fortheevaluation by which contains andthenuse thelog mance whichindicates table averagevm cmcomparison modelvmcm sptm labeled lda frequency table likelihood modellog likelihood sptm labeled lda frequency inthissub section coveriesasfollows specially welistthetop skillsranked and indeed thebusinessrange companies forinstance asshowninfigure bigcom whichresultinthe massivedata thus agedatabase orevenﬁhadoopﬂ onthecontrary the thusﬁhtmlﬂforfront enddesigning orﬁandroidﬂand verybigcompany verysmallcompany figure top popularjobskills companyscale veryhighsalary verylowsalary figure top popularjobskills salary table changesoftop popularjobskills sql software test database class hardware io framework dataanalysis framework mathematics communication datastructure android io python management relation searchengine dataanalysis network software algorithm ui network datastructure framework whichare shownin thus bedifferent whataretheﬁsalary orientedﬂskills andwhy asshowninfigure likeﬁandroidﬂ ﬁiosﬂandﬁweb which innovation meanwhile skills onthecontrary clearly differentyears intable criteria veryhighsalary verybigcompany huge city listedandfulltime skills likedevelopmentandc ble driven onthecontrary thenetwork relatedskillsdecline andwecouldwitnessthe orientedjobskills tosoftware orientedjobskills casestudy hiredeasily finally resultswithareal recruitmentdataset companyinchina whichcontains resumesthat specially partmentasfollows score score score score histograminfigure of sumptions tobe and whichis score skill where and isthe besides consideringthesta tusofthiscompany thecriterialabels aresetas very highsalary hugecity bigcompay listed tomeasurethecor scores hypothesistest besides in ourmodel specially withmean as andvarianceas where equalstothenum berofsamples thus weconductedthe teston obtained bysptm andthe mulabelow value insummary weprove also figure table spearman kendall relatedwork inthissection threeaspects namelyskillranking recruitmentmarket skillranking estinthelastdecade previousstudyin prabhakar litecky andarnett dencyof wherewebprogramming unix java sqlprogramming andoracledatabase recently someonlinere hottestskillslike miller and skomorochetal forexample in linkedin linkedinpredictedtop basedontherank also in world hottesttechskillsfor judgementsofexperts however instead theypro butalsolackofinter especiallyforde datingbackfrom adamsmithandhis thewealth forlabordistribution ety entprospective macroprospective encyclopedias andothersstudy rateoflabor expenditures etc microprospective wherestudiesareall statingthat work and recently includ malherbe cataldi andballatore lietal market trendanalysis zhuetal linetal andtal entcircles xuetal however fewofthemstudied cruitmentmarket nottomentionthemulti facetedpopular ityranking topicmodels liuet al emotionrecognition zhuetal biomet rics wangetal genetics bleiandlafferty socialmedia xuetal andmobiledatamining far rahiandgatica perez zhuetal inthepastyears lda blei ng andjor dan ofproblems super visedldamodels mcauliffeandblei haveshown ment tothisend labeled lda ramageetal has asingledocument models however therefore some suchashier archicallda grifandtenenbaum andldac baoetal inthispaper thepro and further theﬁdocumentsﬂhere inﬁskill netﬂ conclusion inthispaper weproposedadata drivenapproachformea large scalerecruitmentdata webuiltajob inrecruitmentmarket then sptm formodelingthegener ationofskillnetwork therefore withthehelpof sptm multi andalso orientedrules whichprovedthe inthefuture torsforcompanies rangeofjobskills tweenthem also worldapplica be sides such asthejob andthepopular acknowledgments grantno and thean grantno qf grantno wk references baoetal bao cao chen andtian in ondatamining bleiandlafferty blei andlafferty correction statistics blei ng andjordan blei ng andjor dan journalof jan csdn csdn skillknowledgebase http lib csdn net bases daily daily op ed chinasfresh ers http en people cn html accessed encyclopedias encyclopedias ency farrahiandgatica perez farrahi andgatica perez thedistantn gramtopicmodel in onwearablecomputers grifandtenenbaum grifd andtenen baum processingsystems huaietal huai chen zhu xiong bao liu andtian towardpersonal asemisupervised bayesianhmmapproach discoveryfromdata tkdd lietal li ge zhu xiong andzhao in proceedingsofthe acm linetal lin zhu zuo zhu wu andxiong sightsfromanemployee sperspective in proceedingsof thethirty february sanfrancisco california usa linkedin linkedin linkedinun https blog linkedin com top skills week of learning linkedin accessed liuetal liu fu xu xiong andchen in datamining icdm ieee malherbe cataldi andballatore malherbe cataldi andballatore bringingorder tothejobmarket recruitment in mcauliffeandblei mcauliffe andblei in advancesinneuralin miller miller skill rankingmethodand prabhakar litecky andarnett prabhakar litecky andarnett itskillsinatoughjob market ramageetal ramage hall nallapati andmanning labeledlda asupervised labeledcorpora in proceedingsofthe volume volume skillsyouneed skillsyouneed theskillsgap https www skillsyouneed com general skills gap html skomorochetal skomoroch hayes gupta andpatil skillrankingsystem wangetal wang ding tang dong he qiu andwild findingcomplex lda plosone world world needin http www computerworld com article it careers it skills that employers need in html accessed xuetal xu zhu chen huai xiong andtian interactionanalytics xuetal xu yu yang xiong and zhu networks in proceedingsofthe ndacmsigkddin mining sanfrancisco ca usa august xuetal xu zhu zhao liu zhong chen andxiong taxidrivingbehav to vehiclenetworks asocial perspective in proceedingsofthe ndacm eryanddatamining sanfrancisco ca usa august zhuetal zhu liu ge xiong and chen asequentialapproach zhuetal zhu zhu ge chen liu xu andxiong knowledgeandinfor mationsystems zhuetal zhu zhu xiong ding and xie in theacmsigkddinter nationalconference 
shiqishen yongcheng zhongjunhe weihe huawu maosongsun yangliu tsinghuauniversity beijing china tsinghuauniversity beijing china baiduinc beijing china vicapple chengyong gmail com hezhongjun hewei wu hua baidu com sms liuyang tsinghua edu cn abstract end to estimation metrics entiable state of the ourapproach introduction recently end to tion nmt sutskeveretal bahdanauetal munity translation large languagesentence wordalignment phrasesegmentation phrasere ordering andscfgderivation thatarevitalin smt brownetal koehnetal chiang encoder decoder framework choetal sutskever etal asource from languagesen tence correspondingauthor yangliu ed lengthvector bahdanauetal advo erated theystill themodelsareop translationquality ranzatoetal indicate twodrawbacksof mle fornmt first themodelsareonlyex predictions second thelossfunctionisat inthiswork weintroduce minimumrisktrain ing mrt the loss risk onthetrainingdata mrthasthe ationmetrics our levelloss functions tiable mrtdoesnot canbeappliedtoanyend to endnmtsys tems tionalsmt och smithandeisner heanddeng gaoetal arxiv cs cl jun intoend to endnmt languagepairs chinese english english french andenglish german of the artnmtsystem bahdanauetal background givenasourcesentence andatargetsentence end to probability where predictingthe exp where isthe side thtar getword and isanon linearfunction cur and and pleasereferto sutskeveret al bahdanauetal formoredetails fh ig istomaximizethelog data mle argmax where log log weuse thtarget sentence parameter iscalculatedas ranzatoetal pointoutthatmle forend to first which canbeerroneous attesttime thisisreferredto as exposurebias ranzatoetal second entropylossfocus ingonword whichmighthardly levelandsentence level papinenietal andter snoveretal asaresult to endnmttoinclude ationmetrics machinetranslation minimumrisktraining mrt whichaimsto och smithandeisner heanddeng gaoetal let bethe thsentencepairinthe trainingdataand beamodelprediction weuse lossfunction tomeasurethediscrep andthegold standardtranslation suchalossfunction levelevalua papinenietal nist doddington ter snoveretal ormeteor lavieanddenkowski evaluation inmrt the risk isastheexpectedloss where translationsfor table isitscorresponding gold standardtranslation and and aremodelpredictions forsimplicity wesupposethatthe thelossfunction standard thelastcolumn mrt argmin intuitively forexample intable con and ac thegold standardtranslation itisclearthat isthebestcandidate isthesecondbest and istheworst therighthalfof table showsfourmodels asmodel column paredwiththegold standard achieving standardthan model bypredicting model column reducestheriskto asmodel column derwiththegold standard theriskgoesdownto column asaresult data wellwiththegold standard inmrt modelparameter isgivenby sinceeq ferentiate inaddition ourap to endnmtmodels mrtfacesamajor challenge theexpectationsineq areusu searchspaceof thenon decomposability ofthelossfunction andthecontext sensitivenessofnmt weproposetoonly trainingobjective where ˆy isasampledsubsetof thefullsearchspace and isadis notethat isahyper sharpnessofthe distribution och algorithm showshowtobuild by thesampledsub standardtranslation line then lines catecandidates input the the thesetof modelparameters space output sampledspace thegold while do while do samplethe thtargetword if eos then break end end end algorithm thesubspace distinctcandi datesbecausehigh sampledrepeatedly inpractice wetakeadvantageofgpu sparallel giventhesampledspace thepartialderivative of is givenby since js jy theexpectations ineq in ourexperiments fullspacewith samples works seesection tobuildthesubset putingtop translations ingtop translationsforefy samplingismoreef cientandeasy to bestlists es experiments setup tiontasks chinese english english french and english german papinenietal ascalculatedbythe multi bleu perl script forchinese english of mchinese wordsand menglishwords respectively we usedthenist hyper tion andthenist and datasetsastestsets forenglish french to endnmt sutskeveretal bahdanauetal jeanetal luongetal weused trainingcor pusthatcontains msentencepairswith englishwordsand mfrenchwords thecon catenationofnews test andnews test test as thetestset forenglish german tocomparewiththe jeanetal luongetal weusedthesamesub setofthewmt msentencepairswith menglishwordsand mgermanwords test andnews test isusedasthevalida tionsetandnews test asthetestset figure effectof onthechinese englishvali dationset of the artsmtandnmtsystems oses koehnandhoang aphrase training och rnn search bahdanauetal an attention oses phrase parttotraina srilmtoolkit stolcke thelog linear ratetraining mert algorithm och that toevaluationmetrics rnn search anattention ontopofrnn search ourapproachreplaces mlewithmrt rnnsearch model bahdanauetal we kforchinese english andenglish frenchand kforenglish german thedefault level bleu effectof thehyper parameter the distribution seeeq asshownin bothm oses andrnn search gulcehreetal sen nrichetal figure englishvalidationset criterion loss bleu ter nist mle sbleu mrt ster snist table englishvalidationset figure wethat hasacriticaleffecton englishvalidation set while deceasesbleuscores dramatically improvestranslation reducing furtherto however resultsinlower bleuscores therefore weset in effectofsamplesize forefy wesample tobuildan section figure on thechinese englishvalidationset itisclearthat however oryrequirement therefore weset inthe followingexperiments effectoflossfunction weinvestigatethe ter figure chinese englishvalidationset englishvalida tionset asshownintable negativesmoothed sentence levelbleu sbleu leadstosta thisdiscrepancymight as metrics ourexperiments weusedaclusterwith telsak gpustotrain thenmtmodel formle ittakesthecluster aboutonehourtotrain mini batches each ofwhichcontains sentences thetrainingtime mini batches figure formle thebleu hoursand initializing withthebestmlemodel mrtincreasesbleu hours af terwards resultsonchinese englishtranslation table english datasets forrnn search wefollowluong model figure english figure onthechinese englishtestset etal tohandlerarewords we mentsoverm oses andrnn search withmle upto and bleupoints respectively acrossalltestsets all table english datasets sbleu mentsoverm oses andrnn search withmle upto and terpoints respectively acrossalltestsets all figure generatedbym oses rnn search withmle system training mt mt mt mt mt mt oses mert rnn search mle mrt table case englishtranslation system training mt mt mt mt mt mt oses mert rnn search mle mrt table case englishtranslation mlevs mrt evaluator evaluator table onchinese englishtranslation andrnn search withmrtonthechinese lengths mleforalllengths words onereasonisthatrnn search tendstopro as showninfigure bothmleandmregen oses rnn search forefyreasons asentence words this words subjectiveevaluation two andmrttranslationsof sourcesentencesran generated table tion ments around thanmre areequal and arebetter exampletranslations table we thatm oses yi guowuyuanguanyuan ﬂthatrequireslong distance whichisanotorious in contrast rnn search mleseemstoovercome distancede pendencies however asmleusesalossfunc itstranslation lackssentence levelconsistency ﬁchineseﬂoc byopti sentence levelbleu rnn search mrtseems resultsonenglish frenchtranslation table frenchtrans lation welistexistingend to endnmtsystems allthesesys train rion cabularysizes ourrnn search mlesystem etal rnn search mrtachievesthe andsutskeveretal notethatourap resultsonenglish germantranslation table german translation ourapproachstillout source zhuanjia reference oses achineseexpert council rnn search mle rnn search mrt table examplechinese englishtranslations ﬁrefer enceﬂisagold standardtranslation ﬁm oses ﬂandﬁrnn search systems ﬁrnn search mrtﬂisoursystem system architecture training vocab bleu existingend to endnmtsystems bahdanauetal gatedrnnwithsearch mle jeanetal gatedrnnwithsearch jeanetal gatedrnnwithsearch posunk luongetal lstmwith layers luongetal lstmwith layers posunk luongetal lstmwith layers luongetal lstmwith layers posunk sutskeveretal lstmwith layers ourend to endnmtsystems thiswork gatedrnnwithsearch mle gatedrnnwithsearch mrt gatedrnnwithsearch posunk mrt table frenchtranslation thebleuscoresarecase sensitive system architecture training bleu existingend to endnmtsystems jeanetal gatedrnnwithsearch mle jeanetal gatedrnnwithsearch posunk jeanetal gatedrnnwithsearch lv posunk luongetal lstmwith layers dropout localatt posunk ourend to endnmtsystems thiswork gatedrnnwithsearch mle gatedrnnwithsearch mrt gatedrnnwithsearch posunk mrt table germantranslation thebleuscoresarecase sensitive withstate of the etal architectureeasily the marginsonenglish germanandenglish french english sons first thechinese bothenglish frenchandenglish germandatasets second chineseand relatedwork translation och smithandeisner heanddeng och describesa ents eter function linear our best smithandeis ner traininglog besthypothe sis heetal gao etal translation tooptimizeasub to endneural translationmodels entropyrein force mixer algorithm ranzatoetal buildingon reinforceandcross entropy themajordif leveragerein mumrisktraining inaddition mixeronlysam figure indicatesthat sca ourex perimentsranzatoetal sentence leveltextgeneration morerecently ayanaetal lin ourapproach conclusion inthispaper to endneuralma chinetranslation onthetrainingdata improveefy lation relatedlanguages inthefuture to endneural mtsystems lowingsmithandeisner asourapproach to end acknowledgments maosongsunand program cb cb yangliuis dationofchina no andno andthe program aa thisre searchcentre references ayanaetal ayana shiqishen zhiyuanliu andmaosongsun neuralheadlinegenera arxiv bahdanauetal dzmitrybahdanau kyunghyun cho andyoshuabengio neuralmachine in proceedingsoficlr brownetal peterf brown stephena dellapietra vincentj dellapietra androbertl mercer parameterestimation chiang davidchiang ahierarchical phrase tion in proceedingsofacl choetal kyunghyuncho bartvanmerrien boer caglargulcehre dzmitrybahdanau fethi bougares holgerschwenk andyoshuabengio encoder in proceedingsofemnlp doddington georgedoddington auto ingn gramco occurrencestatistics in proceedings ofhlt gaoetal jianfenggao xiaodonghe wentao yih andlideng learningcontinuous in proceedingsofacl gulcehreetal caglargulcehre orhanfirat kelvinxu kyunghyuncho loicbarrault huei chilin fethibougares holgerschwenk and yoshuabengio onusingmono arxiv heanddeng xiaodongheandlideng in proceedingsofacl jeanetal sebastienjean kyunghyuncho rolandmemisevic andyoshuabengio on chinetranslation in proceedingsofacl nalkalchbrenner andphilblunsom recurrentcontinuous translationmodels in proceedingsofemnlp koehnandhoang philippkoehnandhieu hoang in proceedingsofemnlp koehnetal philippkoehn franzj och and danielmarcu statisticalphrase basedtrans lation in proceedingsofhlt naacl lavieanddenkowski alonlavieandmichael denkowski machinetrans lation lin chin yewlin rouge apackagefor in proceedings ofacl luongetal minh thangluong hieupham effectiveap proachestoattention tion in proceedingsofemnlp luongetal minh thangluong ilya sutskever quocv le oriolvinyals andwo jciechzaremba addressingtherare in proceedingsofacl och franzj och minimumerrorrate in pro ceedingsofacl papinenietal kishorepapineni salimroukos toddward andwei jingzhu bleu lation in proceedingsofacl ranzatoetal marc aurelioranzato sumit chopra michaelauli andwojciechzaremba networks arxiv sennrichetal ricosennrich barryhaddow andalexandrabirch improvingneural arxiv smithandeisner davida smithandjasoneis ner linearmodels in proceedingsofacl snoveretal matthewsnover bonniedorr richardschwartz linneamicciulla andjohn makhoul in proceedingsof amta stolcke andreasstolcke srilm amex in proceedings oficslp sutskeveretal ilyasutskever oriolvinyals andquocv le in proceedingsofnips willams ronaldj willams simplesta tisticalgradient machinelearning 
gram ctc labelling hairongliu zhenyaozhu xiangangli sanjeevsatheesh abstract thesemethodssuffer thesetofbasic unitsised suchasthesetofwords charac and optimal inthispa per andproposeanew lossfunctioncalled gram ctc whilepreserv gram ctcautomat grams as getsequences unlikectc gram ctcallows tersateachtimestep whichenablesthemodel thecomputationalefy wedemonstrate thattheproposedgram ctcimprovesctcin tiplescalesofdata andthatwithgram ctcwe of the artonastandard speechbenchmark introduction inrecentyears ctc loss gravesetal andsequence to sequence seq seq models choetal sutskever etal plications asr chanetal hannunetal bahdanauetal equalcontribution bordeauxdr sunnyvale ca usa correspondenceto hairongliu liuhairong baidu com proceedingsofthe th learning sydney australia pmlr copyright bytheauthor machinetranslation sébastienetal and parsing vinyalsetal suchas words sutskeveretal phonemes chorowskietal orcharacters chanetal and aed andpre thesebasicunits simplifytheproblems whichusu ctcmodelsareespe because violatedinpractice itis reallyhard ifnotimpossible forexample inenglishasr wewillneedtodealwith thelargevocabulary sizedsoftmax aswellasrarewords ontheotherhand ifweuse for example the oh ingways oa oe ow ough eau oo ew commonlyco itisimpossibleto systemsmodelphonemes sub xiongetal similarly state of the temsusepre segmentedwordpieces wuetal inreality formanytasks fortheasrtask wordscanbedecom sound suchas tion and eaux forthemachinetrans lationtask explicitlybetween paternal and paternity sincethis itis perhaps at thesametime howtode arxiv cs cl aug gram ctc thisiscoupled thus recently thereare seq framework forexample chanetal chanetal inthiswork wepropose gram ctc astrictlymoregen eralversionofctc called grams and ofgrams aedalignment gram seq ingnetworks datavalidatethatgram andthatusinggram ctc of the artsonstandardspeech benchmarks relatedwork predictiontasks hand writingrecognition machinetranslation andimagecap tioning hand basedones hand craftedbasicunits graphemes gravesetal amodeietal word pieces wuetal collobertetal zweigetal words soltauetal sébastien etal andphonemes leeandhon sercuand goel xiongetal havebeenwidelyusedas butallofthemhavedraw backs textsequencesaprior timalforend to endlearning word wer however these millionin soltauetal out of vocabulary words soltauetal sébastienetal and datasparsityproblems soltauetal butitre word acterstowords gravesetal chanetal word pieceslieatthemiddle groundofwordsandchar acters providingagoodtrade offbetweenvocabulary sizeandcontextsize piece fortheasrtask leeandhon withphonemicmodels xiongetal sercuand goel however doss etal accentedspeechdata learning basedbasicunits morerecently attempts luongandmanning proposedahybridword chanetal chanetal proposedthelatentse cartwrightandbrent goldwateretal predictingthefullcor puswithprobability atthestart edythis gram ctc ctc ctc gravesetal seq whichisusually expensive ifnotimpossible toobtain ctcmarginalizes thatis ittriestomaximize where isinput and representa validalignment forexample ifthesizeofinputis and theoutputis hi whoselengthis alignments hi and hi where represents blank forthedetails graves etal fromctctogram ctc inctc thebasicunitsareed someapplications called gram asawhole which gram ctc figure cat herewelet bethesetofalluni gramsand bi forthelabel cat arelistedtotheleft ctcareinblack ctcareinorange ingeneral any isavalidtransition forexample wecantransitioninto cat from cat ca ca and ca butnotfrom cat or cat let beasetofn ofthe targetsequence and agram units thatis and oneadditionalsymbol blank tosimplifytheproblem we alsoassume foraninputsequence oflength let be anddenoteby asthe probabilityofthe thgramattime where istheindex ofgramsin blank thenwehave justasinthecaseofctc of aspaths anddenotethemby whichrepresents thedif itmay forex ample theword hello sequence forctc assumeuni gram ctchere he ll if he and ll arein foreach removes removesall blanks sincegram ctcwill training stepsinfigure thisisamany to onemappingandwe denoteitby ontheserules foratargetsequence rep then wehave be training gram butalso theset has thisisbecause thereare and statesin theprevioustimestep aspecialcaseofgram ctcwhen and rithmisnontrivial where themselves additionally toanyarbitrary grams uptolength theforward backwardalgorithm toefcompute programmingalgorithm gram ctc inourcase the suchthatthe backto forgram ctc therefore the stateisatuple path and usedformakingthe isvalidandmeansthat blank wasused wedenotethegram by andthestate as forreadability wewillfur thershorten to and to forastate its andthepositionsof aredenotedby and respectively forastatesequence denotedby figure cat herewesuppose gramsandbi grams thus for eachcharacterin cat sociatedwithit thecurrentcharacter thebi gram and the blank aftercurrent character thereisalsoone blank atbeginning intotalwe have states is we scan togettheset ofallpossiblestates suchthatforall itscorresponding jg and foratargetsequence theforward variable forany attime def initialization otherwise andrecursion when when and when and where and istheprobabilityof blank attime isthenex similarly as def wehave otherwise and when when and when and where backpropagation similartoctc is lab where lab isthesetofstatesin whosecorrespond inggramis output ln lab where def gram ctc trainingcurvesbefore blue andafter or ange ofgrams blue andwith orange joint training joint trainingarchitecture figure figure blue andafter orange ofgrams theylookverysimilar gramsparsity figure training figure typicaljoint training modelarchitecture ctcloss methodology ctctoworkefaswell aseffectively althoughgram grams theto forexample inenglish wehave characters thenthetotalnumberof bi gramsis thetotalnumberoftri gramsare becomesintractable however itisunnecessarytocon sidermanygrams suchas aaaa whichareobviouslyuse less inourexperiments thatis wecount then wetrainamodelwith gram byapplying de coding weget susage ultimately wechoose gramsasour nalgramset table selectiononwsj withoutlm figure showsitscorre fordetails pleaserefertosection gram menttasks ctc toovercomethis dif thegram ctc similar tojoint in saketal works kimetal kimandrush atypicaljoint andthe the effectofjoint andtable in theexperiments experiments wetestthegram ctclossontheasrtask whileboth foralloftheexperi ments thesameasin amodeietal themodelisarecur rentneuralnetwork rnn with two dimensionalconvo lutionalinputlayers followedbykforward fwd orbidi rectional bidi gatedrecurrentlayers ncellseach and inshort hand dconv kxngru to endwiththectc gram thiscombination inallexperiments khz lin ms andwindowsizeof ms the from khzwith featuresper msframe ateach epoch gram ctc loss wer ctc uni gram ctc bi gram gram ctc handpick gram ctc alluni grams bi grams gram ctc table dataset backgroundnoiseto batch size learning rate mo mentum andetc outset typical andmomentumof dataandsetup wallstreetjournal wsj andcon tainsabout hours speechdata weusedthestandard oftrainsi datasetfortraining dev for validationandeval fortesting thisisarelatively clean miaoetal bahdanauetal zhangetal chanetal fisher switchboard cts corpora which contains hours ctsdata followingtheprevious works zweigetal poveyetal xiongetal sercuandgoel thenist ctstestset board swb andcallhome ch subsets kspeechdataset hours this fardifferentac cents andsoon itisa verychallengingtask worldapplication gramselection ctc sinceitisa vhowever becauseitissmall wecannotuse thus the datasets but gramsandbi grams loss wer epochtime mins stride ctc uni gram ctc bi gram gram ctc table dataset uni gramsand bi grams intotal grams top bi gramstogetherwithall uni grams auto grams forcomparison pickedgrams grams allthe first gramsandallbi grams simi lartoword segmentedmodels thegramset grams thusmorero bust grams notinclud inghigher ordergrams second theperformanceofbest this isdesirable itistime es ordergrams themethod butusuallybet ter third theperformanceofgram ctcon grams this isbecausegram since wsj ctc since tion however thelargeststridethat the uni gram small stride puteverycharacter differentlengths igh eye thelargerthe gramsare ds amodeietal employednon overlappingbi thisimposesan notonlythe spellingofeachword grams forexample part issplitas pa rt buttheword gram ctc figure max decodingresults withoutcollapsing ofctcandgram thepredicted characters byctc orgrams bygram ctc asthegram the represents blank apart ap ar gram changeenablesgram gramctc asintable intable and forgram ctc tion asexpected usingstride almostcutsthetrain comparedtostride from stride tostride theperformanceofuni gramctc dropsquickly asforgram ctc fromstride tostride whilein gram ctcconstantly worksbetterinstride wsjistoosmallforgram incontrast theperformanceofbi gramctcisnotasgood asthatofgram ctcineitherstride decodingexamples figure illustratesthemax andgram ctconnineutterances herethelabelsetfor grams andsomehigh frequencyhigh ordergrams heregram ctcusesstride whilectcusesstride fromfigure wecanthat gram ctcdoesauto such as the ng and are nunciation composition natural sincegram ters agram eachtime dependent oneexampleis theword will since the stillend to end decomposition insummary gram ctccombinesthead wsj dataset themodelusedhereis dconv bidigru withactcorgram ctcloss theresultsareshown intable languagemodel intermofwer gram gram ctc architecture wer phonemectc trigramlm miaoetal graphemectc trigramlm miaoetal attention trigramlm bahdanauetal deepconvlas nolm zhangetal deepconvlas lsd nolm chanetal temporalls cov lm chorowskiandnavdeep vanillactc nolm ours vanillactc lm ours gram ctc nolm ours gram ctc lm ours table to isher witchboard convolutionsandsixbi di mension gramlanguagemodels portionofthenist dataset rt hyper parameters set resultsareintable in domain data out of domain and togetherthesetech niquesallow xiongetal toreachawerof ontheswbdset ks peech ataset finally recognition lvcsr systems suchasfar backgroundnoises accents inallcases themodel is dconv fwdgru laconv withonlya laconv referstoalook amodeietal onlyrnnsfordeploy mentpurpose aswiththefisher switchboarddataset theoptimalstride is forgram ctcand thus inbothexperiments bothgram ctcandvanilla architecture swbd ch wer wer iterated ctc zweigetal blstm lfmmi poveyetal lace lfmmi xiongetal dilatedconvolutions sercuandgoel vanillactc ours gram ctc ours vanillactc gram ctc ours table switchboardbenchmark respectively using in domain data architecture wer nolm wer withlm vanillactc gram ctc vanillactc gram ctc table lvcsrresultson kspeechdataset ctc gram ctcitself gram ctc afterjoint trainingwithvanilla itsper whichvjoint training helpstraining infact oftime to withoutlanguagemodel inthispaper wehaveproposedthe gram ctc lossto learnedgrams gram ourextensiveexper ctcenablesthe by usinglargerstride sequencelabelling ctcobtainsstate of the withoutin lengthﬂconstraint onlyshort upto inourexperiments aswellashigh optimizationofgram ctcloss ofgram gram ctc references alexgraves santiagofernández faustinogomez and jürgenschmidhuber tion rentneuralnetworks in proceedingsofthe rdinterna pages acm kyunghyuncho bartvanmerriënboer caglargulcehre dzmitrybahdanau fethibougares holgerschwenk andyoshuabengio ingrnnencoder tion arxivpreprintarxiv ilyasutskever oriolvinyals andquocvle sequence in advances pages williamchan navdeepjaitly quocle andoriolvinyals listen attendandspell in speech andsignalprocessing icassp pages ieee awniy hannun carlcase jaredcasper bryancatan zaro gregdiamos erichelsen ryanprenger san jeevsatheesh shubhosengupta adamcoates andan drewy ng deepspeech scalingupend to endspeech recognition corr abs dzmitrybahdanau janchorowski dmitriyserdyuk yoshuabengio etal end to endattention basedlarge in ieeeinterna speechandsignalpro cessing icassp pages ieee jeansébastien kyunghyuncho rolandmemisevic and yoshuabengio oriolvinyals kaiser terrykoo slavpetrov ilya sutskever andgeoffreyhinton grammarasaforeign language in systems pages jankchorowski dzmitrybahdanau dmitriyserdyuk kyunghyuncho andyoshuabengio attention based in advancesinneural pages wxiong jdroppo xhuang fseide mseltzer astol cke dyu andgzweig themicrosoft con arxivpreprint arxiv yonghuiwu mikeschuster zhifengchen quocv le mohammadnorouzi wolfgangmacherey maxim krikun yuancao qingao klausmacherey jeff klingner apurvashah melvinjohnson xiaobing liu lukaszkaiser stephangouws yoshikiyokato takukudo hidetokazawa keithstevens george kurian nishantpatil weiwang cliffyoung jason smith jasonriesa alexrudnick oriolvinyals gre gorys corrado macduffhughes andjeffreydean google bridging corr abs williamchan yuzhang quocle andnavdeepjaitly in arxiv darioamodei rishitaanubhai ericbattenberg carl case jaredcasper bryancatanzaro jingdongchen mikechrzanowski adamcoates gregdiamos etal deepspeech end to glishandmandarin arxivpreprintarxiv yonghuiwu mikeschuster zhifengchen quocv le mohammadnorouzi wolfgangmacherey maxim krikun yuancao qingao klausmacherey etal google bridging arxiv preprintarxiv ronancollobert christianpuhrsch andgabrielsynnaeve wav letter anend to endconvnet basedspeechrecog nitionsystem arxivpreprintarxiv geoffreyzweig chengzhuyu jashadroppo andan dreasstolcke advancesinall neuralspeechrecogni tion arxivpreprintarxiv hagensoltau hankliao andhasimsak neural speechrecognizer acoustic to wordlstmmodelfor arxivpreprint arxiv fleeandh whon large vocabularyspeaker in acoustics speech andsignalprocessing icassp pages ieee densepredictionon sequenceswithtime recognition arxivpreprintarxiv waynexiong jashadroppo xuedonghuang frank seide mikeseltzer andreasstolcke dongyu andge offreyzweig speechrecognition arxivpreprintarxiv gram ctc williamchan navdeepjaitly quocvle andoriol vinyals listen attendandspell arxivpreprint arxiv mathewmagimaidoss toddastephenson hervé bourlard andsamybengio phoneme graphemebased in automaticspeechrecog asru ieee workshopon pages ieee minh achiev withhybridword charactermodels arxivpreprint arxiv seg therolesof arxivpreprintcmp lg sharongoldwater tation in proceedingsofthe thannual tics pages guistics hasimsak andreww senior kanishkarao and franã goisebeaufays corr abs suyounkim takaakihori andshinjiwatanabe joint ctc attentionbasedend to multi tasklearning arxivpreprintarxiv sequence levelknowl edgedistillation arxivpreprintarxiv yajiemiao mohammadgowayyed andflorianmetze eesen end to modelsandwfst baseddecoding in automaticspeech asru ieee workshopon pages ieee yuzhang williamchan andnavdeepjaitly verydeep to endspeechrecogni tion arxivpreprintarxiv geofferyzweig ghengzhuyu jashadroppo andan dreasstolcke advancesinall neuralspeechrecogni tion arxivpreprintarxiv danielpovey vijayadityapeddinti danielgalvez pegah ghahrmani vimalmanohar xingyuna yimingwang andsanjeevkhudanpur purelysequence trainedneural freemmi submittedto interspeech towardsbetterdecod quencemodels arxivpreprintarxiv 
wordsup hanhu chengquanzhang yuxuanluo yuzhuowang junyuhan erruiding idl baiduresearch hanhu microsoft comf zhangchengquan luoyuxuan wangyuzhuo hanjunyu dingerruig baidu characters words textlines andtextblocks amongtheseelements characteristhe chi nese japanese itisnatu however trainingchar acters actually theexisting toremedythisdilemma eitherintight forcharac terdetectortraining scalerealscene textdatasets icdar andcoco text the textdetectionengine itachievesthestate of the artperfor marks byvariousscenarios introduction foralongtime scannedenglishdocu ments there the itisnoteasy firstly languages foranexample english equalcontribution baidu research figure differentscenarios inhierarchy character while chinesenot foranotherexample regularhumanlanguage structural secondly neverthe less asillustratedinfig itis wecanthendetermine upmanner theatomicity oriented deformed see however annotatingcharac because blurry actually asillustratedin table inthispaper dataset im word real synth anno icdar realcharicdar krealword svt realword coco text krealword iiit word realword char msynth charvggsynthtext synth chartable nearlyallmedian terlevelannotations asillustrated infig weareabletotrain icdar andcoco text tectionpipeline it achievesthestate of the marks icdar icdar andcoco text relatedworks tion theap characterbased asmentionedearlier characterisanat nearlyall fortraining becauseoflackingchar however syntheticdatacan limitingthemodel ingrealscenetexts actually benchmark arebasedon characterdetection recently technologies imageslookmoreﬁrealﬂ nevertheless realtextimagesare butbyincorpo weareabletoex ourpipeline weachievethestate of the artperformanceon the update mask update network detection network figure foracharactermodel giving thecurrentmodel red andgreenpoints supervisethe textlinebased matelinemodels ofdocumentanalysis videsstrongpriors documentscenarios wordbased suchasfasterrcnn and ssd yet componentbased basedmethods extractcan mser andswt thesemethodsonceled ic dar however iossuchasicdar tally inaddition thesubsequentsteps recently learning els orsegmentboxes thesemethods inaddition textcompo whichmayalso nevertheless ourmethod pects character figure ourpipeline therearetwomodules characterdetector tureanalysismodule second ment whilecomponentnot matchexpression third boundingboxannota tionsinthecoco textdataset thisisbecauseour theirnoisylabels ourapproach pipeline givenanimage this its instead we textdatasets andsection respectively analysismodule wehan weproposeamethodto lines mathexpression sequential symbols tion which ssd anddensebox nevertheless tobeapplied forcharacters first deconv conv deconv conv loss input patch conv conv conv conv conv features predictions kx labels kx figure thevgg networkmodel pixelsin an mpixelimage second ferentscenarios streetscenes weusefea sponses itis oftheoriginalimage otherthan or tion we adoptthemethodinfpn forsuchpurpose which usesan resolutiondifference othermethods forproducingthesame numberoffeaturemaps seefig asanillustration networkmodel conv featuresare up mergedwith conv featuresbyan eltsumlayer the elt sumedconv conv conv featuresinthesameway bothtext non sion with fortext non text andk weuse anchors pixels pixelsand pixels onthe inputpatch respectively of againsttheanchor sareregardedpositive weadopta two ing allposi fornegatives onlytop secondis hardpatchmining duringtraining wetestalltrainingim agesevery usingthe batchsamplingpro cedure training batch ofthe lation scaleperturbation theother arerandomly after iterations westart fornegative patches inference weconductmulti scaletestforanimage the usedscalesare respectively since onlydown thecomputation overheadisafforded atabout times comparedto single scaletest iscon itshouldbenoted thatmulti tector scales explor overview asillustratedintable mostrealtextimage icdar and coco text icdar oraboundingbox coco text which tightlysurroundsit inthispa per available given cedure whichsuccessfully it isillustratedasfig givenacharactermodel givenacharacter mask thesetwostepsare duringthetraining proved section duringforwardandback wardofeachmini batch tations asillustratedinfig bottom first wemakefor maximizingscore area chars area anno whereb acters area and arethelargest wethelearn anditissetas bydefault thetermofeq favorslargercoverage whilethesecondone onastraightline toapproximately maximizeeq firstly amaximumspanningtree whichisbuilt bythe non textscores ij exp whered nnnodes non textscoreofcandidate itis equalstopartitioning foreachpartitioning we andrun score doesnotrise forthe former ineq byareara tioofquadrangles forthelatter addingatermtoeq groundtruth training wealoss contributemore as wherel tectionframeworks fig ingtraining duringtraining allyimproved word annotation initial chars initialepoch epoch epoch epoch chars at epoch figure datasets the the th datasets forillustration indexedby colormapinmatlab and narios textlines words textblocks mathexpressions andetc fig cations deformedlines thena withthemodel werectifythetextline optionally wesep aratelinesintowords thisisnotnecessary butenables traction andnon tion inthefollowing extractingtextlines periments charactergrouping weadaptthemethodin to givencharacters buildsa unary tering andthetextcategory characterscores thepairwise spatial andscaledistances agreedymin costwalgorithmis see for details themethodin only weintroduceahigher tweenthreecharacters graph nodes scaledistances thenthe angledistance newgraph thesameasin wethenconductagreedy min actergrouping lines ordercosts foreach charactergroup ingcomplexity firstis ordermodel textlinesareei secondis ordermodel text lastisa piecewiselinear model textline complexity werectifythetext tps method wherethever wordpartition cessonlywordinputs anlstm textline experiments inthissection both evaluated finally weshow vggsynthtext part thevggsynthtext datasets consistof images generatedbya theimageshavede tailedcharacter levelannotations forexperimentef ciency werandomlyselect imagesfortraining and imagesforvalidation thissubsetisreferred toas vggsynthtext part icdar theicdar datasets arefrom theicdar with fortesting levelbound ingboxes cused icdar theicdar datasets arefromthe icdar with fortesting the sprior intention level quadrangles coco text thecoco text isalargescale datasetwith imagesfortrainingand fortesting cocodataset thevggsynthtext experiments bothcharacter levelandword levelevalua terion intersection over tion icdar andcoco text foricdar andicdar videdwiththedatasets forcoco text theprotocolpro thevgg locdataset precision recallfigure tionnetwork synthtext partdatasets sented source strategy partdatasets second istrainedon kicdar trainingimagesplus kvgg synthtext part aresampledfromicdar andthe part thethird part withmini batchalsosampledhalf these synthﬂ ﬁvgg synth icdarﬂandﬁvgg synth cocoﬂ respectively weusesgdwithamini batchsizeof on gpus pergpu atotalof models fortheﬁvgg synthﬂmodel kareatalearn ingrateof andtheother kat forother models partcharacter thelearn ingrateis atthisstage then kand kiterations learningratesof and respectively theweight decayissetas andthemomentumas icdar andcoco text introducedinsection izations markdatasets forfaircomparison wetunehyperparame trainingimages thevggsynthtext partdatasetsareused wet section withthestate of the artalgorithmsinthe fasterrcnn andssd forfasterrcnnandssd providedbytheauthors fig recallsofourbasicnet work re spectively workwiththestate of the vs the two ategain asshowninfig threemod elsaretrained thesecondis kimages agesand thetrainingap fromfig supervised kcharacters icdar ic dar andcoco text thesebenchmarksare allbasedonword levelevaluation hence thetextline inthe weonlyuse order and order mation table and entmethodsontheicdar icdar andcoco text datasets of the onicdar weachieve measure whichis ctpn datasets imagesare ex tremeillumination andetc measureof measuremclab fcn yaoetal guptaetal zhuetal ctpn our vgg synth our vgg synth icdar table using thedetevalcriterion measuremclab fcn ctpn yaoetal scut dmpnet rrpn our vgg synth our vgg synth icdar table bestmethod vs comparingourap vgg synth icdar synthmodel vs vgg synth icdaronlyadds ktrainingimage comparedtothevgg synthmodel ktrainingimages real data other thanmoredata oncoco ourbestmodelachieves and inrecall precisionandf measure respectively it takesoveryao smethodby intotalf measure vgg synth synth model fig dar icdar andcoco texttestimages byex blurry perspectivedis tortion handwritten artfonts computationaltime fora image thecharac ternetworktakesabout gpu msusinga ghzcpu mathexpressions figure synthmodel top bottom onthe respectively figure indexedby colormapinmatlab measurea yaoetal our vgg synth our vgg synth coco table text tionofmethoda bandc thesemethods yet consistingof character line levelannotatedimages linesareinvolved section section fig top row bottomrow resultsonsome representativeimages itisalso classes character levelannotatedimages however weempirically notations tion mathexpressionsare non sequential niqueisnotapplicable wecan recognizeeachofthem conclusion levelanno ened references aho hopcroft andj ullman datastructures andalgorithms addison wesley bookstein principalwarps thin platesplinesandthe ieeetpami chen tsai schroth chen grzeszczuk andb girod edge in icip pages dai he andj sun boxsup exploitingbounding mentation in pages epshtein ofek andy wexler in cvpr pages gers schmidhuber andf cummins learningto forget neuralcomputation gupta vedaldi anda zisserman syntheticdatafor in cvpr he huang qiao andj yao accuratetextlo network corr abs he huang qiao andj yao text attentionalcon ieeetip he luo yin hu han ding andc liu context anend to in icpr he luo yin hu han ding andc liu context anend to in pattern recognition icpr huang yang deng andy yu densebox unify corr abs huang lin yang andj wang textlocalization variancedescriptors in iccv pages huang qiao andx tang robustscenetextdetec in eccv pages jaderberg simonyan vedaldi anda zisserman textrecognition arxive prints jaderberg simonyan vedaldi anda zisserman ijcv jaderberg vedaldi anda zisserman deepfeatures fortextspotting in eccv pages kang li andd doermann in cvpr pages karatzas gomez bigorda nicolaou ghosh bagdanov iwamura matas neumann chandrasekhar lu etal icdar competitiononro bustreading in icdar pages ieee karatzas shafait uchida iwamura ibig orda mestre mas mota almazan and delasheras icdar in icdar pages ieee kong yao chen andf sun hypernet towardsac incvpr pages li jia shen anda vandenhengel characterness ieeetip lin doll ar girshick he hariharan and belongie incvpr liu anguelov erhan szegedy reed fu anda berg ssd ineccv pages liuandl jin to wardtightermulti arxivpreprint arxiv long shelhamer andt darrell fullyconvolutional in cvpr pages ma shao ye wang wang zheng and xue arbitrary proposals in corr abs mao rosenfeld andt kanungo documentstructure analysisalgorithms aliteraturesurvey in electronicimag ing pages andphotonics meng huang song xiang andc pan extrac in iccv pages mishra alahari andc jawahar scenetextrecogni in bmvc bmva ren he girshick andj sun fasterr cnn towardsreal works in nips pages ronneberger fischer andt brox net convolu in mic cai pages springer russakovsky deng su krause satheesh ma huang karpathy khosla bernstein berg andf li challenge ijcv schantz historyofocr tion shi bai ands belongie arxivpreprint arxiv simonyananda zisserman verydeepconvolu corr abs tian pan huang lu yu andc limtan textw images in iccv pages tian huang he he andy qiao detectingtext in eccv pages veit matera neumann matas ands be longie coco text in arxivpreprint arxiv wang babenko ands belongie end to endscene textrecognition in iccv pages wangands belongie in eccv pages wang wu coates anda ng end to endtext in icpr pages yao bai liu ma andz tu detectingtexts in cvpr pages yao bai sang zhou zhou andz cao multi channelprediction corr abs yinandc liu pattern recognition yin pei zhang andh hao multi ieeetpami yin yin huang andh hao robusttext ieeetpami zhang shen yao andx bai symmetry basedtext in cvpr pages zhang zhang shen yao liu andx bai multi works in cvpr zhou yao wen wang zhou he and liang east arxivpreprintarxiv zhuandr zanibbi in cvpr pages 
deepvoting zhishuaizhang cihangxie jianyuwang lingxixie alanl yuille baiduresearchusa zhshuai zhang cihangxie wjyouch xc alan yuille gmail com abstract inthispaper partsofanobject awheelofacar underpartialocclu sion inthissce nario theproposal baseddeepnetworks likercnn series becauseboththepro toaddressthis proposeda clusions however thismethodismanually designed thus to endmanner inthispaper wepresentdeepvoting whichincorporates therobustnessshownby intoadeepnetwork sothat it network thepool layerofvggnet thestlayer andthesecond we bylearning inexperiments includingfaster rcnn forse inaddition deep introduction rangeofvisiontasks recently familyofproposal basedapproaches which followed byatopredictobjects scoreforeachproposal however despiteitsimportance semanticpart isafractionof described suchasa wheel ofa car ora chimney ofa train whichenables intherealworld quentlyoccluded in thispaper thevehicleseman sometyp note that dataset withtwo named occluders only numberofoccluders cases however ofdifferentnumbers toover comethisdif trainingdetectors only onocclusion freeimages butal betweensemanticparts etc thismotivatesusto relatedworkis based andinsteadpro arxiv cs cv mar figure somesemanticparts wheel whilesomeothers chimney onlyappearinoneclass train thespatialre part however thismanually andthusitisdiftoop timizeitinanend to endmanner thismotivatesustosee to endtrainingnaturally tothisend wepropose deepvoting anend to end sion the neuralresponsesatthe pool layerofvggnet the visualconcepts whichwere this whichsetsa andadropout layer afterthat thespatial geometric izedas spatialheatmaps wefollow totrain jectboundingboxes andwecall thisimprovedversion deepvoting inour experiments theversion inwhichcontextsareex cluded outperforms withthesameset ting to endtrainingmannerpro thesec ondversion rated of the rcnn deep andtheadvantage ingrobusttoocclusion modelcomplexity faster and ii relatedwork lems seman ticsegmentation objectdetection etc forobjectdetection oneofthemostpop ularpipeline andthen bounding boxregressionandnon maximumsuppression wereattachedforpost processing thisframeworksignif basedmodel jectdetection blepartsintoanobject also partscanbeusedfor beappliedasauxiliary cuestounderstand orbetrainedforac tionandattribute besides investi combined targetsemanticparts different from andat to endtraining thisyields motivation derocclusion firstofall wearguethat only occlusion free thisisbe arbitrary occlusion of the artob jectdetectionmethods suchasfaster rcnn tode tectsemanticparts theadaptedmethods but we even withcorrectproposals themaystillgowrong totallydifferent rcnninsection thevotingmechanism egy whichaccumulatesmid high levelsemanticparts thesemid levelcuesarecalled visualconcepts asemanticpart itisstillpos however itinvolvestoo manyhyper inthispaper intoadeepnetwork following termediatelayer the pool layer hasaedrecep tivesize therefore ingstages inthetrainingstage weusedtheground truth and computetheobject to work scalenet forscaleprediction seesection fordetails inthetestingstage andthenweresizethe formulation let follow ing wefeedthisimageintoa layervggnet andextractthe pool raloutputs denotetheoutputofthe pool layeras ora cube where and arethedown sampled scalesof and and is forvggnet thesefea high dimensionalvec tors region denoteeach where isanindexatthe grid thesefeature vectorsare normalizedsothat in asetofvisualconcepts areob tainedviak meansclustering level the atthe pool featurevec tor ismeasuredbythe distance wenotethat hasunitlength andso as sowehave where isthedotproduct operator thenthelog buttheabsenceofa differentfrom thekernelsizeofthis each isconsid eredindividually thereluactivation followstoset vidingnegativecues with adropratio this consequently improves ofsize jvj where we set jvj performance we showinsection wecansomevisualcon cepts in likelihoodratiotests these ed figure bestviewedincolor car imagewithtwo wheels markedbyredframes oneof themisoccluded isfedintovggnet layer andobtainalow basedonthismap avisualconceptcan atleastweakly suggesttheexistence ofasemanticpart forexample asshowninfigure ina car image headlight impliesthatthereisa wheel nearby headlight to the wheel differentfrom namedthevotinglayer in tobelarge occluded training wereferto heatmap thesemantic partmap as itisa jsj cubewhere isthe setofsemanticparts wesetananchorbox sized andthenlearn box from thetrainingdata theanchorsize istheaverage trainingandtesting freeimagecorpus truth boundingbox tobeconsistentwith then werescale pix els whichismotivatedby ceptsataedscale theimageisfedintothe layer vggnet pool layer we withtheground truthannotation predictionandground truth togeneratetheground truth grid down andothersas truth annotation truthannota tion thelabelcube isalsoofsize jsj the similaritybetween and isas jsj jsj lossfunction onthetestingstage weusescalenet seesec tion then werescalethe pixels knowitslocation then votinglayers towardsmoreac curatelocalization astandardnon maximumsuppression with moreover to figure freeimage thesecond thirdandforthimage inrow majororder thereare and occluders computedbypixels is and respectively verifythis nameddeepvot ing wealsoresize pixels inthetrainingstage deepvoting achieves tion scale first asshownin thesamescale boxcontains pixels second asemanticpart asanexample car the headlight of wheel andthe spatialoffseton and axesareabout and pixels and atthe pool grid respectively suchinformationis notscale invariant work namelythescalenet ineachimage toa the fc layer isreplacedbya dimensionaloutput andthelabelisthe ground truthobjectsize so pixels itisplacedatthe centerofan duringthetraining andaskthedeep imagelongedge pixels inthetestphase animage thedesiredsize itsshortedgecontains pixels we showinsection suchasfaster rcnn lemsetting theaccuracy however occludedvisualcues we inaverylow resolutionsetting down sampledfromthe wealsoborrowthe ideafromsegmentation astheseman the experiments datasetandbaseline cleocclusiondataset forevaluation thevehiclese trainingimagesand airplane bicycle bus car motorbike and train intotal se semanticpartdataset somerandomly positionedoccluders object objectisconstrained figure wetrainsixmodels allthe freedataset butevalu atedoneithernon occludedimages ortheimageswithdif inthelatercase wevary object criterion map ade iouratewithaground each andthemapofeach noocclusions category kvc dvc vt fr dv dv vt fr dv dv vt fr dv dv vt fr dv dv airplane bicycle bus car motorbike train mean table left columns meanap of kvc dvc vt fr dv and dv withoutocclusion right columns meanap of vt fr dv and dv levels notethat dv denotedby dv and dv respectively section kvc of pool featuresusing means thescalenet detailedinsection dvc voting tionlayer thescalenet detailedinsection is vt vialog likelihoodratiotests fr foreachcategory wetrainamodelwith jsj classes corresponding to jsj different fromotherbaselines faster rcnnhereistrainedon fullimages this enablesfaster asatask occludedobjects thisisalsoa table ferentmethods regardlesswhetherthe meansclusteringor deepvoting figure actualscale mean while to endtrainingmanner oraclescalere sultsin in objects whichindicatesthe ontheotherhand rcnn weargue thatfaster boundingboxes aswecansee deepvoting faster method meanwhile tionaloverheads itruns faster whichisessen foreachtestingimage we tualscale recall atdifferentlevels map addt lprop map bydeepvoting category airplane bicycle bus car motorbike train mean table left columns therecallrates middle andright columns detection maps offaster rcnn ground anddeepvoting cases exceed actually enoughfordeepvoting evenifground truthscaleispro thedetection to sincethebaselines kvc and dvc inthelevel weplace occludersoneachob ject andtheoccludedratio oftheobject computedbypix els satisfying for and wehave and occluders and and respec tively seefigure forexamples theoriginalocclusion summarizedintable rcnnin thesecases forthefaster rcnn theaccuracygainin suggestingtheadvan asasideevidence ofspatialheatmap atthe to to distancevoting whenthekernelsize isincreasedto therefore for rcnnsuffers the andtheaccuracyofthe resultsaresummarized intable first thus thesecondstage hastostartwitharela tivelylow inthesecondpart we addtheground the images evenwithsuchfaster rcnnstillpro forexample inde bicycle atthehighestocclu sionlevel ground accuracyfrom to thanthenumber producedbydeepvoting thisim infigure thevisual which tional thematchingtemplate tracesbacktothe originalimagelattice totheneuronatthe pool layer heatmaps weseethatthe callymeaningful finally infig ure inwhichthetargetse occluded respectively semanticparts supporting todigintoerrors figure bestviewedincolor foreachvisualconcept weshow patcheswith thehighestresponses inwhichyellow positive zeroandnegativecues respectively forexample vc windshield oftenappearsabove sp licenseplate and vc carsidebottom oftenappearsbelow sp sidewindow figure thethirdcolumn thetargetsemantic part the licenceplate ona car isfullyoccludedbya bird bluedots especiallythe rdvc markedinred vc withthehighestscores scanbemuchlarger conclusions inthispaper network nameddeepvoting tations named visualconcepts the deepvotingiseval tofaster rcnninthenon occlusionscenario andsuperior ifcontextisutilized deepvoting votingandfaster moreover plainable ingvisualconcepts inthefuture semanticpartsofnon mals also leveldetectionunder acknowledgements minds and machines cbmm wethankchenxiliu zhuotunzhu junzhu siyuanqiao cussions references alexe deselaers andv ferrari measuringtheobject nessofimagewindows chen papandreou kokkinos murphy and yuille deeplab atrousconvolution andfullycon nectedcrfs chineintelligence chen mottaghi liu fidler urtasun and yuille detectwhatyoucan in proceed recognition dalalandb triggs humandetection in tion ieee everingham vangool williams winn and zisserman voc chal lenge felzenszwalb girshick mcallester andd ra manan basedmodels machineintelligence girshick fastr cnn in putervision ieee gkioxari girshick andj malik actionsandat in oncomputervision ieee he zhang ren andj sun deepresiduallearn in recognition ieee huang xu tao andy zhang part stackedcnnfor visualcategorization in computervisionand patternrecognition ieee krizhevsky sutskever andg hinton imagenet in kuo hariharan andj malik deepbox learningob in internationalcon ieee liu anguelov erhan szegedy reed fu anda berg ssd in springer long shelhamer andt darrell fullyconvolutional in computervision ieee milletari navab ands ahmadi net fully agesegmentation in dvision ieee nairandg hinton linearunitsimprovere in onmachinelearning novotn larlus anda vedaldi ihaveseenenough in britishmachinevi sionconference qiao shen qiu liu anda yuille scalenet yond arxivpreprintarxiv redmon divvala girshick anda farhadi you onlylookonce real timeobjectdetection in com ieee ren he girshick andj sun fasterr cnn towards real in simonyananda zisserman networksforlarge in international srivastava hinton krizhevsky sutskever and salakhutdinov dropout ralnetworksfromov research szegedy liu jia sermanet reed anguelov erhan vanhoucke anda rabinovich in computervisionand patternrecognition ieee uijlings vandesande gevers anda smeulders interna wang xie zhang zhu xie anda yuille in wang zhang xie zhou premachandran zhu lingxi anda yuille visualconceptsandcom positionalvoting plications zhang xu elhoseiny huang zhang el gammal andd metaxas spda cnn unifyingsemanticpart in ieee zhang donahue girshick andt darrell part basedr in euro springer zhang qiao xie shen wang anda yuille single arxivpreprintarxiv zheng jayasumana romera paredes vineet su du huang andp torr conditionalran in international ieee zhu chen anda yuille deepm adeeppart tion arxivpreprintarxiv 
aguidelinetotrain largermodelsfaster newshaardalani ardalaninewsha baidu com joelhestness hestness baidu com gregorydiamos gregdiamos baidu com abstract understandingthe rameters todesignnext dl hardware toaidsuch understanding onoverallperformance anddebunksalong heldbeliefthatlarger time step weobservethat haltingpoint therefore time asmodelsizescales ifsuccessful introduction energy efcient gpus andasics learning withtrainingdata therefore datasetsize inthispaper figure withbatchsizeof and as depicted whiletraining hardware nvidiamaxwell model note modelarchitecture if numberofsteps smallerones numberofsteps timeperstep overalltime figure step asmodelsizegrows modelparallelism andkernelparallelism whichrequirenon recently dataparallelism asdataset sizegrows comparably however therearehardupper boundsonscalabiltyof dataparallelism further weshowthat wepredictan todatasetsize steps data size model size meanwhile the without changingparallelism finally futurehardware systemdesign methodology wetime to within weuseearlystopping whilekeep samelearningrate samebatchsize etc dlmodels character levellanguagemodel lm word levellm andspeechrecognition characterlm wordlm speech figure stepsvs tersinlog scale percentage offulldataset forcharacterlm to ofthe bdataset forwordlm wevarydatasetsize from to of bdataset forspeech to ofaninternal hourdataset characterlm wordlm speech figure modelsize scale weuseearlystopping at tocontrolov lowerisbetter wordlm weimplementlstm basedwordlmsasde scribedin the networksare layerlstms withsequencelengthof the characterlm weimplementchar lmusingrecurrent highwaynetworks rhns wetraina layer depth rhn sequencelength speechrecognition ds resultsandanalysis inthissection westudythetrade accuracyandmodelsize hurtingaccuracy figure wordandspeechmodel axisandy axisareinlogarithmic scale layer percentageof afulldatasetsize asshown ingeneral where ismodelsize and and parameters respectively accuracy figure asshown loss sensitivityanalysis lawrelationship weobservethat to wealsoobservethat usingadamoptimizer improves itshiftsdownthepower lawcurve system hardwareimplications movingforward therefore system throughputperdevice references carebrase https www cerebras net online accessed nov graphcore https www graphcore ai online accessed nov nvidiateslavolta everyindustry https www nvidia com en us data center volta gpu architecture online accessed nov shun ichiamari naotakefujita andshigerushinomoto fourtypesof learningcurves neuralcomputation darioamodei rishitaanubhai ericbattenberg carlcase jaredcasper bryan catanzaro jingdongchen mikechrzanowski adamcoates gregdiamos etal deepspeech end to in icml ciprianchelba tomasmikolov mikeschuster qige thorstenbrants phillipp koehn andtonyrobinson arxivpreprintarxiv tianshichen zidongdu ninghuisun jiawang chengyongwu yunjichen andoliviertemam diannao asmall footprinthigh forubiquitousmachine learning in acmsigplannotices vol acm yunjichen taoluo shaoliliu shijinzhang liqianghe jiawang lingli tianshichen zhiweixu ninghuisun etal dadiannao amachine in proceedingsofthe thannualieee acminterna ieeecomputersociety yu hsinchen tusharkrishna joelsemer andviviennesze eyeriss an energy ieeejournalofsolid statecircuits trishulmchilimbi yutakasuzue johnsonapacible projectadam system in osdi vol zidongdu robertfasthuber tianshichen paoloienne lingli taoluo xiaobingfeng yunjichen andoliviertemam shidiannao shifting in news vol acm iangoodfellow yoshuabengio andaaroncourville deeplearning mit press priyagoyal piotrdollàr rossgirshick pieternoordhuis lukaszwesolowski aapokyrola andrewtulloch yangqingjia andkaiminghe accurate largeminibatchsgd trainingimagenetin hour facebookairesearch publications joelhestness sharannarang newshaardalani gregorydiamos heewoojun hassankianinejad mdpatwary mostofaali yangyang andyanqizhou empirically arxivpreprint arxiv normanpjouppi cliffyoung nishantpatil davidpatterson gauravagrawal raminderbajwa sarahbates sureshbhatia nanboden alborchers etal in in proceedings ofthe acm rafaljozefowicz oriolvinyals mikeschuster noamshazeer andyonghui wu arxivpreprint arxiv nitishshirishkeskar dheevatsamudigere jorgenocedal mikhailsmelyanskiy andpingtakpetertang onlarge arxivpreprintarxiv noamshazeer azaliamirhoseini krzysztofmaziarz andydavis quocle geoffreyhinton andjeffdean the sparsely gatedmixture of expertslayer arxivpreprintarxiv cho zhaozhang ima arxivpreprintarxiv yangyou igorgitman andborisginsburg kforimagenettraining arxivpreprintarxiv juliangeorgzilly jankoutnik andjurgenschmid huber in icml 
haoyuangao junhuamao jiezhou zhihenghuang leiwang weixu baiduresearch losangeles gaohaoyuan baidu com mjhustc ucla edu zhoujie huangzhiheng wanglei wei xu baidu com abstract inthispaper wepresentthe mqa model thecontentofanimage aphraseorasingleword alongshort termmemory lstm to cnn to answer agequestionanswering fm iqa itcontainsover imagesand answer turingtest thehuman theywillalsoprovide ascore thelargerthebetter theex perimentsshowthatin ofcases modelfromhumans theaveragescoreis forhuman thedetails ofthiswork includingthefm iqadataset http idl baidu com fm iqa html introduction recently andvision inparticular cnn rnn orlong short termmemory lstm thelarge therearestill inparticular butinmanycases orobjectofanimage theuser inthispaper inthistask themethodneeds weproposethemqa thismodelhasfour components seefigure network thiscomponentwaspre trainedonimagenet task denserepresentations likelihoodloss figure fm iqa dataset function totrainourmethod weconstructalarge dataset fm iqa seedetailsinsection thecurrent imageswith chinesequestion answerpairsand suchasaction recognition objectrecognition ﬁwhatistherein yellow ﬁwhereisthekitty and seelast columnoffigure answerpairs usinghumanjudges wemixthequestion inaddition wrong partiallycorrect or correct ofthis test andtheaveragescoreis inthediscussion weanalyze rnn model ourmodelcan relatedwork forcomputervision network cnn achievethestate of the suchasobjectclas detection andsegmentation fornaturallanguage therecurrent neuralnetwork rnn andthelongshort termmemorynetwork lstm arealso andspeechrecognition rnnmodel image weextend andgenerateanswers intheexperiments rnnmodeland however mostof template inaddition ourfm thereareonly and imagesfor and respectively http idl baidu com fm iqa html whichcontains imagesand question answerpairs figure image ﬁwhatisthecatdoing tothemodel thequestion thetwolstms areshared inaddition asin this inatransposedmanner bestviewedincolor proposealarge dataset comparedtothem humanjudges whichcanbeusefulfor othertasks ourdatasetandthe andlead both and andtheanswer for differentfromthem grammar ofquestionsand answers embeddings forthedataset adoptthedataset proposedin iqadataset object number color andlocation complementarytoours themultimodalqa mqa model longshort termmemory lstm ii cnn iii anlstm and iv thedetails theanswer hotvectors binary andhaveonlyonenon weadda boa signanda eoa sign astwospatialwordsin theworddictionary theywillbe inthetestingstage generatetheanswer boa candidates inpractice finetuningthe boa it containsa memorycells the space lstm vanishingproblem aquestion theinput gate areoutofdate differentfrom thiscomponent themodel thequestionmark willbetreatedasthe ii cnn thatgeneratestherep resentationofanimage inthispaper weusethegooglenet suchasalexnet andvggnet weremove iii thestructureis as theanswer in anduseasinglelstm becauseof grammar ofquestionandanswer inthispaper weusetwoseparate answeraslstm andlstm theweightmatrixin lstm inthecomponents word iv finally forthe th whereﬁ ﬂdenoteselement wiseaddition memory and denotes th wordintheanswer respectively and isan element wisenon linearfunction afterthefusinglayer in seedetailsinsection similarto relu asthenon thenon tangentfunction tanh asmentionedinsection however themeaningof therefore wesharetheweight matrixbetweentheword inaddition intuitively matrixintheword representation wordrepresentation embedding trainingdetails thecnnweusedispre thiscomponentised duringtheqatraining weadoptalog andwedecreaseit byafactorof cross validation these fm iqa dataset insection ofannotations thelatest http idl baidu com fm iqa html thedatacollection westartwiththe training validationand sonlinecrowdsourc ingserver anytypeofquestions theques ontheonehand ontheotherhand werandomlysampled imagesas they donotknowthisisatest about question answerpairs perannotator individuals whoseannotationsare satisfactory images answerpairsfromthe wealsoprovide currentlythereare imageswith chinesequestion translations theaveragelengths http test baidu com figure sampleimagesinthefm iqadataset thisdatasetcontains chinesequestion and some werandomlysampled question answerpairsandtheir toanswerthem the actionsofobjects theobjectclass ﬁisthereany personintheimage ﬁisthecomputer ﬁwhatisthecolor ofthefrisbee inaddition levelreasoning withcluesfromvision forexample doesthebusparkthere thesequestions dataset astheanswer ﬁyellowﬂ or ﬁthefrisbeeisyellowﬂ experiments underthissetting such aswu wups however the formostofthecases thereare meteor cider critical bleuandmeteor ordifferentweights accordingtothetf idffrequencyterm cider hencecannotfully sameproblem toavoidtheseproblems weconductareal visualturingtest inaddition withascore thelargerthebetter insection method insection passfailpassrate avg score human blind qa mqa table iqadataset thevisualturingtest basedontheanswer passthetest oramachine failthetest inpractice iqadataset weuseour mqa wedenoteitasblind qa theblind qamodeland thisleadsto correspondingimages humanjudges itshowsthat theblind but choice questions weconduct thestandard and forhuman theblind mqamodeland mqamodelrespectively wealsoconducta orﬁ ﬂandﬁ categoriesarewrong judges test ﬂifthe will possiblymakemistakes ﬂinfigure scoresarenot we andourmqamodelare and respectively inaddition for and ofthe cases we forthevariant ﬁmqa avg questionﬂ we figure ﬂgivenby thehumanjudges figure worderrorloss mqa avg question mqa same lstms mqa notws mqa complete table differentmqavariants tionusingword vec andextractor forthesecondvariant ﬁmqa same lstmsﬂ weusetwoshared weightslstmsto itisusedtoshowtheef thelstm andthelstm inourmodel forthe thirdvariant ﬁmqa notwsﬂ tws strategy mqa complete areshownintable discussion inthispaper wepresentthemqamodel weconstruct fm iqa question answerpairs it showsthat human thefm where inasentence this andprovidean answertothisquestion fortheimageinthe column imagesinthesecondand fourthcolumn fromeverydaylife ourmodeloutputsa oov sign infuturework mation figure iqadataset references antol agrawal lu mitchell batra zitnick andd parikh vqa arxivpreprint arxiv bigham jayant ji little miller miller miller tatarowicz white white etal vizwiz nearly real in pages chen papandreou kokkinos murphy anda yuille fullyconnectedcrfs iclr chenandc zitnick in cvpr cho vanmerrienboer gulcehre bougares schwenk andy bengio arxivpreprintarxiv donahue hendricks guadarrama rohrbach venugopalan saenko andt darrell long termrecurrentconvolu in cvpr elman cognitivescience fang gupta iandola srivastava deng doll ar gao he mitchell platt etal fromcaptionstovisual conceptsandback in cvpr geman geman hallonquist andl younes pnas girshick donahue darrell andj malik in cvpr grubinger clough uller andt deselaers theiaprtc benchmark systems in pages hochreiterandj schmidhuber longshort termmemory neuralcomputation kalchbrennerandp blunsom in emnlp pages karpathyandl fei fei deepvisual in cvpr kiros salakhutdinov andr zemel unifyingvisual tacl klein lev sadeh andl wolf arxivpreprintarxiv krizhevsky sutskever andg hinton in nips lavieanda agarwal meteor in pages lebret pinheiro andr collobert basedapproach arxivpreprint arxiv lecun bottou orr andk uller efbackprop in neuralnetworks tricksofthetrade pages lin maire belongie hays perona ramanan doll ar andc zitnick microsoftcoco commonobjectsin context arxivpreprintarxiv malinowskiandm fritz amulti in pages malinowski rohrbach andm fritz askyourneurons aneural arxiv preprintarxiv mao xu yang wang huang anda yuille rnn in iclr mao xu yang wang huang anda yuille learninglikeachild descriptionsofimages arxivpreprintarxiv mao xu yang wang anda yuille nipsdeeplearning workshop mikolov joulin chopra mathieu andm ranzato arxivpreprint arxiv mikolov at burget cernock ands khudanpur in interspeech pages mikolov sutskever chen corrado andj dean ality in nips pages nairandg hinton in icml pages papineni roukos ward andw zhu bleu in acl pages ren kiros andr zemel arxivpreprint arxiv russakovsky deng su krause satheesh ma huang karpathy khosla bernstein berg and fei fei simonyananda zisserman in iclr sutskever vinyals andq le in nips pages szegedy liu jia sermanet reed anguelov erhan vanhoucke anda rabinovich goingdeeperwith convolutions arxivpreprintarxiv tu meng lee choe ands zhu multimedia ieee turing mind pages vedantam zitnick andd parikh cider consensus in cvpr vinyals toshev bengio andd erhan showandtell in cvpr wuandm palmer in acl pages xu ba kiros cho courville salakhutdinov zemel andy bengio show attendandtell neuralimagecaption arxivpreprintarxiv young lai hodosh andj hockenmaier in acl pages zhu mao anda yuille svm algorithm in nips pages 
proceedingsofcoling technicalpapers pages coling mumbai december 
nteractive rounded anguage cquisition and eneralizationina dw orld haonanyu haichaozhang weixu baiduresearch sunnyvale ca usa haonanyu zhanghaichao wei xu baidu com bstract dmaze likeworld the and slanguagefrom sentence tionanswering thelanguage andtheactioncontrol oflanguageprediction lationofover objectwords color words spatial relationwords and grammaticalwords theproposedmodel shotsen tences inaddition wedemonstratehuman ntroduction tomasello skinner bruner dvirtualworld where interactive grounded harnad woods whichpositsthatwords asabstractprocedures we asablackbox asaresult ﬁin ontheotherhand chomsky bridgethisgap landauer dumais ourproblemsetting aftertraining zero shot sentences oftwotypes arxiv cs cl feb training testing nav ﬁmovetonorthof avocado ﬂﬁgoto east ofrabbit ﬁgoto east of avocado ﬂﬁcanyoureach watermelon qa ﬁwhatisinnorthwest ﬁwhatis east of avocado ﬂﬁwhatisthecolorof watermelon answer watermelon answer ﬁnothingﬂ answer ﬁredﬂ figure anillustrationof xworld and amixedtraining ofnavandqa testingzs ﬁeastﬂandﬁav ocadoﬂ testingzs sentencescontaina newword ﬁwatermelonﬂ answer inpracticeitmight themapsare onlypartiallyshown interpolation or extrapolation inthefollowing wewillcallthetypezs sentences note thatsofarthezero shotproblems hermannetal chaplot etal incontrast areliablein terpretationofzs sentences whichisessentiallya transferlearning pan yang problem wecreateda dmaze likeworldcalled xworld figure inthisworld navigation nav andquestionanswering qa fornav forqa generatesingle squestions whentheagentisnavi gating once thecurrent session appendixb thezs ormoreprecisely noveltyofourmodel itisalsoableto because thestatespaceishuge evenfora mapwith wallblocksand objectsselectedfrom distinctclasses tomentiontheintra seefigure intheappendix fortwo different moreover the inparticular thetarget https github com paddlepaddle xworld words theagenthas words ithastoﬁplugﬂthe therecentwork hermannetal chaplotetal addresseszs butnotzs sentences whichisa wedescribeanend to ingsystems shrdlu winograd anda bigail siskind thathard codedthe sues oneiswiththeteacher languageandrewards themodeltakes asinputrgbimages sentences andrewards oftheworld thelanguage andtheactioncontrol xworld onapopulationofover consistingof objectwords colorwords spatial relationwords and grammaticalwords detailedanalysis appendixa wespeciallytestthe shotsentences are forzs and forzs whenthezero shotportionishalf comparabletotherate of forzs and forzs whenthezero shotportionishalf inanormal languagesetting odel fortheformer wefollowthestandard rl paradigm itemploystheactor critic ac algorithm sutton barto ap pendixe forthelatter antoletal otivation imagesandsentences groundingproblem thatis theagentmustlink languageconcepts visualcontext withanrnnandencode theperceivedimage withacnn theactionmodulebe be nav rnn cnn qq qa rnn cnn qq hermannetal misraetal chaplotetal in theirimplementations wiseproduct foranyparticular wordinthesentence alltheway totheend figure anoverviewofthemodel weprocess zeropadding sentations and thenavigationaction gradient respectively prediction loc and fornavigation appendixc therefore becauseofthis we implicit thusa pproach disentangle inthemodel bothgroundingand whereeachword embedding istreatedasa detector xplicitgrounding ofanylength givenasentence andanimage representation cnn wesaythat is grounded in as if consistsof and ii pt witheachentry thentityof thus furthermore is explicit if iii some individualwordsof compositionality iv all language forourproblem nav cnn qq qa cnn qq wherethesolelanguage visionfusion noticeinthe asaﬁbottleneck ﬂallowsonly thatis and all relyongrounded resultsbut not bydoingso weexpect tosummarizeallthe fold first con ceptualabstraction garneloetal thatmapshigh thisimprovesthe given thisguar evengivendifferent arethesame pathplanning actionmaking andfeature independentroutines second because is explicit thisis incontrasttoeq whichwewillperform let originallyin nowthe dspatialdomain collapsedinto where isthenumber and loc qpt feat qpt and cube qp respectively intherestofthepaper weremove and from loc feat and cube weassumethat cube isa low cube loc feat loc pr feat pr and cube pr theattentionmap loc isresponsiblefor thechannelmask feat and namely feat feat intuitively canbe modulatedby feat finally anism devriesetal violateiiiandiv somework andreasetal lu etal ceptron mlp afterimageattention andersonetal onceptdetection thisrelationisa common conceptdetection function asaconcept is asinsection spatial into assumeaprecomputed feat feat þñ where embedding modulatedby feat figure cube loc feat where loc attendstoimage regionsand feat selectsfeaturemaps inthisexample loc ﬂinorder color objectname feat hasto color useforthescoring intuitively eachscoreon inthatlocation weimplementitas feat feat where istheelement wiseproduct todoso wehavewordembedding where isequal forprediction andanimage supposethat loc and feat outputsascore vector overtheentirelexicon loc feat loc where isthe theabovesuggeststhat istheresult by loc thlexicalentry topredictananswer argmax softmax notethattheroleof feat tion whethertopredict acolororanobjectname byusing feat imageattributes moreanalysisof feat isperformedin appendixa tocompute cube wecompute loc and feat separately wewant loc maps loc supposethat consistsof words with beingsomeword inthedictionary let beasequenceofindices where thissequencefunction organizedinwhatorder we loc as loc figure dcon andcrosscorrelation theattentionmapof ﬂnotethatinpractice theatten ifany toperform thisconvolution where pt isavectorofones ofindividualwords as such toeq duringtesttime wecouldset and neuralmodulenetworks nmns andreasetal huetal however and basedonwordattention bahdanauetal tobypassthe oflength toabidirectionalrnn schuster paliwal emb andasequenceof wordcontextvectors each called interpreter inaniterativemanner forthe thinterpretationstep wordattentionas wordattention exp cos attendedcontext attendedword interpreterstate gru where cos choetal hereweuse viasoftwordattention weset tothecompactsentence embedding emb afterthis theattendedword theinterpreter by detection softmax maptransform loc mapupdategate wp mapupdate loc where denotesa dconvolution issigmoid and isascalar and areparameterstobe learned finally loc as loc where isthemaximumstep dconvolution thisoperationenables loc isegocentric when itcan dconvolutionaloffset figure forthisreason weset asaone hotmapwherethemap centerisone klein foravoxel grid drepresentation byassumption thechannelmask feat namely which thusitiscomputedas feat mlp rnn activation elatedwork our xworld issimilartothe dblockworldmasebase sukhbaataretal however we whiletheydonot therehavebeenseveral kempkaetal deepmind lab beattieetal andmalmo johnsonetal still withlessornolanguage liketheirs poten wehavemultiple perceptualmodalities kielaetal severalend to misraetal dplane hermannetal chaplotetal learnto navigatein dunderinstructions andbothevaluatezs generalization shotsentences generalization pan yang problem otherrecentwork andreasetal ohetal onzero languagetokensas parsed inthesepapers thezero shotsettingsare ofsiskind ourlanguage videos yu siskind gaoetal rohrbachetal chen mooney tellexetal barrett etal thequestion vqa antol etal gaoetal renetal luetal yangetal andersonetal devriesetal mnihetal xperiments ability xworld shotconditions finally dworld foreq thischoiceofimplemen computing feat unlike loc step tocompute feat figure eneralsetup foralltheexperiments and sgrammar thereare navcommand qaquestion andqaanswer whichareillustrated infigure intotal thereare knavcommands mqaquestions and qaanswers theenvironmentare map conformingtosomehigh level fornav nav obj navigatetoanobject nav col obj nav nr obj nav bw obj forqa rec intable wereferthereader omparisonmethods contextualattention ca attentionmodel loc theercoversthe stackedattentionnet san whichwasorig inallyproposedforvqa theattentionmap loc wetrainacnnfrom xworld vis lstm vl posedforvqa we thenitisappended totheinputsentence astheword figure appendixd concatembed ce whichwasoriginally itinstantiates ding then forbothnavandqa figure appendixd byhermannetal misraetal navalone nava and andis figure thebasicevaluation trainingrewardcurves averagedoverevery ktrainingtimesteps theshadedareaofeach navigation nava isexcludedbecauseit doesnottrainqa oneablation andourmodel andabatchsizeof foramaximumof kminibatches aftertraining wetest eachapproachon ksessions fornav forqa we asicevaluation inthisexperiment setting vl and ce havequitelow loc andthus aratioof itispossiblethatthis zero shot duetosampling allthecontent thus comparedtothezero shotsettinginsection both ca and san thissuggeststhatin however especiallyonthezs sentences areusuallyvery weak theablation nava itsperformance san oritcan beaslowasthatof ce seefigure thissuggeststhatqa iseasier whichisabsentinqa shotsentences forcomparison weuse ca and san forazero shotsetting newwordcombination zs qaquestions object spatialrelation object color and object object werandomlyholdout ofthewordpairs duringthetraining newword zs tions werandomlyholdout training wevarythevalueof or outdata forevaluation forthezero outwordpairsorwords theresultsareshownin figure first thezs thezs sentences thisis hermannetal chaplotetal second thezs sentencesare diffor ca and san evenwithaheld outportionassmallas theirnavigation and respectively comparedtothoseinthe incontrast impressively evenwhen qaquestions respectively third in zs ratecurveswith asshowninfigures eand betteronqathanonnav xworld additiontoqa interestingly wenoticethat nav bw obj ca ismuchless in zs nav bw obj ca learnsto kotturetal tosumup andzs sentences the althoughweusea particular figure outportionof where aœc zs dœf zs foreither zs or zs fromtoptobottom the respectively theplots of nav obj in and sentenceofthistypeby owdoesitadaptto world beattieetal johnsonetal buthere generallyspeaking dworldwillposea relatedcomputations cube cube feat and theattentionmap loc regardlessoftheworld sdimensionality eq amore sophisticatedof denvironment additionally the dconvolution eq thisassumption dspatialrelationson dimages lastly dagent aworkingmemory module we onclusion wehavepresentedanend to dworld afterlearning xworld dworld cknowledgments wethankthe otherteammembers yuanpengli liangzhao yiyang zihangdai qingsun jianyuwang and xiaochenlian finally eferences peteranderson xiaodonghe chrisbuehler damienteney markjohnson stephengould and leizhang bottom upandtop arxivpreprint arxiv jacobandreas marcusrohrbach trevordarrell anddanklein neuralmodulenetworks in cvpr jacobandreas marcusrohrbach trevordarrell anddanklein in acl jacobandreas danklein andsergeylevine policysketches in icml stanislawantol aishwaryaagrawal jiasenlu margaretmitchell dhruvbatra clawrencezit nick anddeviparikh vqa in iccv dzmitrybahdanau kyunghyuncho andyoshuabengio in iclr danielpaulbarrett scottalanbronikowski haonanyu drivingunder the oflanguage charlesbeattie joelz leibo denisteplyashin tomward marcuswainwright heinrichk uttler andrewlefrancq simongreen vald es amirsadik julianschrittwieser keithander son sarahyork maxcant adamcain adrianbolton stephengaffney helenking demis hassabis shanelegg andstigpetersen deepmindlab arxivpreprintarxiv yoshuabengio er omelouradour ronancollobert andjasonweston curriculumlearning in icml jeromebruner child stalk devendrasinghchaplot ramakumarpasumarthi dheerajra jagopal gated orientedlanguage grounding in aaai in aaai kyunghyuncho bartvanmerrienboer aglarg ulc ehre fethibougares holgerschwenk and yoshuabengio chinetranslation in emnlp noamchomsky problemsandmysteries in thechomskyan turn harmdevries florianstrub er emiemary hugolarochelle olivierpietquin andaaronc courville in nips johnduchi eladhazan andyoramsinger jmlr jul haoyuangao junhuamao jiezhou zhihenghuang leiwang andweixu areyoutalkingtoa machine in nips qiaozigao malcolmdoering shaohuayang andjoyceychai in acl martagarnelo kaiarulkumaran andmurrayshanahan learning arxivpreprintarxiv stevanharnad physicad karlmoritzhermann felixhill simongreen fuminwang ryanfaulkner hubertsoyer david szepesvari maxjaderberg denisteplyashin marcuswainwright chrisapps demishassabis andphilblunsom world in nipsworkshop ronghanghu jacobandreas marcusrohrbach trevordarrell andkatesaenko learningto reason end to in iccv johnson hofmann hutton andd bignell experimentation in ijcai jolliffe springerverlag kempka marekwydmuch grzegorzrunc jakubtoczek andwojciechja skowski viz doom adoom in ieeeconfer douwekiela luanabulat anital vero andstephenclark virtualembodiment ascalable long in nipsworkshop whereismisty inspace in emnlp satwikkottur jos em moura stefanlee anddhruvbatra naturally inmulti agentdialog in emnlp asolutiontoplato sproblem theoryofacquisition induction psychologicalreview jiasenlu jianweiyang dhruvbatra anddeviparikh hierarchicalquestion imageco attention in nips junhuamao xuwei yiyang jiangwang zhihenghuang andalanlyuille learninglikea child in iccv tomasmikolov armandjoulin andmarcobaroni arxiv preprintarxiv dipendramisra johnlangford andyoavartzi in emnlp volodymyrmnih koraykavukcuoglu davidsilver andreiarusu joelveness marcgbelle mare alexgraves martinriedmiller andreaskfidjeland georgostrovski etal human level nature junhyukoh satinderp singh honglaklee andpushmeetkohli zero withmulti in icml ieeetrans onknowl anddata eng october mengyeren ryankiros andrichardzemel answering in nips annarohrbach marcusrohrbach ronghanghu trevordarrell andberntschiele groundingof in eccv tomschaul johnquan ioannisantonoglou anddavidsilver in iclr schusterandk paliwal ieeetransactionson signalprocessing jeffreymarksiskind intelligencereview jeffreymarksiskind to meaningmappings cognition jeffreymarksiskind verbalbehavior sainbayarsukhbaatar arthurszlam gabrielsynnaeve soumithchintala androbfergus maze base arxivpreprintarxiv anintroduction volume mit presscambridge stefanietellex thomaskollar stevendickerson matthewrwalter ashisgopalbanerjee seth teller andnicholasroy mobilemanipulation in aaai michaeltomasello ausage dmitryulyanov multicore tsne https github com dmitryulyanov multicore tsne pvandermaatenandg hinton visualizinghigh sne journalof terrywinograd cognitivepsychology williama woods meaningandlinks aimagazine kelvinxu jimmyba ryankiros kyunghyuncho aaroncourville ruslansalakhudinov rich zemel andyoshuabengio show attendandtell attention in icml zichaoyang xiaodonghe jianfenggao lideng andalexsmola in cvpr tences in acl figure byawordlabel eachrow column theanswer color object and spatialrelation zoom ppendices av inthissection channelmask feat feat werandomlysample feat wedivide the kquestionsinto groups then whereentry the feat ofaquestionfromthe thgroup figure itisclearthat than objects colors figure wordcontextvectors the wordattentions eachattentionvector interpretationsteps therearethreetopics object color andspatialrelation inthematrix thedistancescomputed channelmask topics italsoimpliesthat thefeaturecube wordcontext attention wevisualizethe wordcontextvectors ineq foratotalof kwordlocations kfromqaquestionsand theother kfromnavcommands dimensionsusingpca jolliffe afterwhichweuset sne vandermaaten hinton ulyanov thet andalearningrateof andrunsfor kiterations kpointsisshownin figure recallthatineq mapsvia basedonthecontexts inblue green andred inblack aremostlyseparated figure interpreter attentionmap loc finally loc ineachofthe followingsixexamples loc andwordattentions ineq of map loc thelastattentionmap loc istheoutputof generatetheoutput asaresult donotcontributeto loc terr inappendixc forabetter visualization figure theexampleshowinghow loc iscomputed figure loc iscomputed figure loc iscomputed figure loc iscomputed figure theexampleshowinghow loc iscomputed figure loc iscomputed figure allthe second to last andthewall last xworldsetup xworld iswith gridmaps oneachmap rangingfrom to appendixf butnotfortesting wepadthemapwithwall left right up and down ineachsession thatis theagentonlyhas stepstoreachatarget ii to iii to iv targetobjectsare and respectively vi apple armadillo artichoke avocado banana bat between blue and bathtub beans bear bed bee beet east brown block by can beetle bird blueberry bookshelf broccoli bull north gray color could destination cabbage cactus camel carpet carrot northeast green direction does cat centipede chair cherry circle clock northwest orange go goal grid coconut corn cow crab crocodile cucumber south purple have identify in deer desk dinosaur dog donkey dragon southeast red is locate located duck eggplant elephant fan southwest yellowlocation me move fox frog garlic giraffe westname navigate near glove goat grape greenonion greenpepper hedgehog nothing object of horse kangaroo knife koala ladybug lemon on one please light lion lizard microwave mirror monitor property reach say monkey monster mushroom octopus onion orange side target tell ostrich owl panda peacock penguin pepper the thing three pig pineapple plunger potato pumpkin rabbit to two what racoon rat rhinoceros rooster seahorse seashell where which will seaurchin shrimp snail snake sofa spider you your square squirrel stairs star strawberry tiger toilet tomato triangle turtle vacuum wardrobe washingmachine watermelon whale wheat zebra table theteacher slexicon thereare spatialrelations colors distinctobject classes and grammaticalwords differentinstances allobject therearein total including typesofqaquestions recursivetemplates andcorresponds intotalthereare distinct sentenceswith fornavand forqa and theobject spatial relation these xworld note an location eightcolors werandomlysampleone ci mplementationdetails theenvironmentimage isa egocentricrgbimage thecnnin hasfourconvo lutionallayers where representsalayer urationof kernelsofsize appliedatstridewidth to relationwords ﬁnorthﬂ allthegated rnns in have units allthelayersin unlessotherwise stated nav obj ﬁpleasegototheapple beginningofasession nameintheenvironment nav col obj differentcolors orhavedifferent namesbutthesamecolor nav nr obj nav bw obj bothreferenceobjects bananaplease rec col obj ﬁwhatistheredobject thecolor rec obj col rec loc obj object rec obj loc rec loc col rec col loc rec loc obj obj referenceobject rec loc obj col rec col obj loc ﬁwhereistheredapple rec bw obj obj separatedbyablock rec bw obj loc objects rec bw obj col andbanana table fornav loc however otherobjects in theenvironment towardthisend terr hf where issigmoid weexpectthat terr detects notethat terr itsolely afterstacking loc and terr together feedsthemtoanother cnnfollowedbyanmlp and bothwith paddingsof itisfollowedbyathree unitsandisrelu activated theactionmodule containsatwo reluactivated weuseadagrad duchietal withalearningrateof sgd of foreachlayer where db aselinedetails aredescribedbelow ca itsrnnhas units figure vl lastmlps figure ce lastmlps vl and this isfollowedbyafully connectedlayerofsize embeddingspace thernnhas units foreitherqaornav thernn slaststategoes throughathree units figure ce ithasthesamelayer sizewith vl figure san itsrnnhas units yangetal weusetwo attentionlayers ee xplorationand xperience eplay decreaseslinearly from to ateachtimestep pt left right up down witha probabilityof where isthecurrentpolicy and and respectively er mnihetal theenvironmentinputs rewards ktimesteps duringtraining eachtimeaminibatch issampled fromthebuffer usingtherank basedsampler schauletal log where and isthevaluefunction delay and isthediscountfactor thetemporal difference td error notethatbecauseofer ouracmethodisoff policy toavoid however weignoredthemin fc urriculumlearning bengioetal min where ii iii iv thegradualchanges intheexperiments weset not dif 
pages nagoya japan october 
de biasingcovariance haoyixiong weicheng yanjiefu wenqinghu jiangbian zhishanguo baiduinc beijing china beijing china mo unitedstates nj unitedstates abstract fisher fld isa well theem in verse covariancematrix toimprovetheaccuracy hdlss settings covariance regularizedfld crld anceestimators suchasgraphicallasso tostrike though inspiredby biasedlasso wepropose anovelfld dbld whichimproves de biasing dbld possesses wecon ityof dbld overclassicalfld crldandother introduction fisher fld duda etal isawell dimensionreduction kulisandothers ithasbeen peckandvanness imageretrieval etc anintrinsic onthe well estimated and non singular covariancematrices formanyapplications suchasthemicro arraydataanalysis singular or ill posed sincethedatais hdlss cai etal under thehdlsssettings the usedinfldisusually ill tion truecovariancematrix cai etal forexample ples mar cenkoandpastur suchinconsistencybe zol lanvarianddougherty for example krzanowskietal krzanowski etal sug gestedtousepseudo variancematrix gular however theprecisionofpseudo inversefldisusu thetwo stagealgorithmpca fld ye etal fld basedonkernels zhangandothers and orothernon parametricstatistics kaskiandpeltonen toover in torsseparately caiandliu proposedtoestimate morepopu larly krzanowski etal wittenandtibshirani lem durrantandkab an bickel etal theill covariance regularizedfld wittenandtibshirani thatreplaces suchasgraphicallasso friedman etal soasto achievea ﬁsuperiorpredictionﬂ intuitively throughreplacing mation theill canbewelladdressed totheinverseoftrue thesampleestimators cai etal withtheasymptotic properties however further withinducedsparsity matorbecomesbiased zhangandzhang theperfor recently researcherstriedto de zhangandzhang through adjustingthe soas inspiredbythis lineofresearch de biasing inthispaper ourcontributions sues inthispaper inspiredbyde biasedlasso javanmardandmonta nari biasingthe covariance regularizedfld crld whichhasbeen widely forper formanceimprovement biasingcrld dbld iased fisher inear dbld leveragesyet anotherde biasedestimatorfor linear problem tore balancethevariances throughde sparsifyingthe undercertainmildas sumptions dbld sharpasymptoticrate theresultsvalidate notations thispaper givena dimensionalvector wede notethe vector normas is anon negativeinteger andthe vector normas max fj jg givenamatrix wedenote the matrix normas jj jj max fj av notethatthesymbol ofthedatawhile the operator referstothebig preliminaries inthissection usingfld graphicallasso fldforbinary tousethefisher fld given the labeleddatapairs we usingthepooled classes duda etal matrixas further and areestimatedasthe inthe trainingsamples respectively and and astheresult of argmax where log where and refertothe foreknown frequenciesof respectively covariance lasso thisalgorithm regularized fld crld viagraphicallasso wasderivedfromthe scout family offldintroducedby wittenetal in wittenandtib shirani thisbaseline functionineq as argmax where argmin tr logdet jk notethat asalinear thecrlddecisionrule introducedineq canbere suchas sign sign where sign functionreturns iftheinputisnon negative and thevector andthescalar log obviously inthispaper statisti anddi mensions ofcrldin theorem theproposedalgorithm inthissection dbld iasedfisher inear iscriminantanalysis viagraph icallasso dbld thede biasedestimationfor covariance regularizedfld giventhe labeleddatapairs drawn asshowninalgo rithm thealgorithm then ii lever further dbld iii proposesade biasedestimator denoted as debias function tode bias vector finally usingtheestimated algorithm dbld estimationalgorithm procedure dbld positivesampleset negativesampleset ii crldestimator toobtain graphicallasso iii dbld estimator toobtain matrix matrix isan matrix everycolumnis isa matrix everyrowis return biasedestimator denotedas debiasing functioninalgo rithm toobtain tiesof thede biasedestimator inspiredbythede biasedlasso javanmardandmonta nari crldthroughde biasing given labeledtraining data withbalancedlabels the weproposeanovelde biasedestimatorof that takestheformas wherewedenote asan matrixwhere andthe th columnis asan matrix vector whose th rowis isa matrixwhereeach columnis asline inalgorithm and isan matrixwhereeachrowis asline inalgorithm the dbld giventhede biasedestimator the dbld the inputvector sign log of dbld complexityanalysisof dbld inthissection thethreestepsof algorithm thestep estimatesthesam whichconsumesat most operations thestep ii performsgraphical friedman etal isupper boundedby thestep iii de biasingisimplemented asanexactformulawith complexity remark allthreestepsof algorithm arescalableon andthenumberoftrain ingsamples is underthehdlsssetting the dbld isupper boundedby ontheotherhand dbld isboundedby obviously theproposedde biasing estimator step iii withcomplexity wouldnot boundtheperformance dbld dbld we given samples drawni from and theopti and and assumption rothman etal having min max exists theoperators min and max denotethesmallest inthisway thereexists and assumption the having thus therehas and theorem log ineq the vector normconvergence rateofcrld is log where max jf gj referstothemaxi maldegreeofthegraph matrix proof here aswas thenwehave and ax jj jj wehave jj jj accordingto rothman etal when log normconvergencerate log theasymptoticrate ofsamplemeanvector dasgupta is and withtheincreas andnumberofsamples further therehas assumption and inthisway there and having log thus wecon log theorem ineq the vector dbld is log proof here con sidertheofthede biasedfldestimator intro ducedineq wehave xl ul isa whilethe restareall referstoa matrix whereeachcolumn isasampleofdatae aswas xl as isamatrixinwhicheach and isavector withhalfelementsas andhalfelementsas thus ul ul aseachcolumnof refers thus estimator withallaboveinmind wehave where referstoa identitymatrix notethat thatde biases thus ofsamplemeanvector dasgupta is and wehave accordingto jankova etal of thespectral estimator undermildconditions shouldbe log thenthereexists log withthevaryingnumber ofdimensions andnumberofsamples inthisway with highprobability log remark comparedtocrld sprojectionvector ourmethod dbld log log inamildcondition thus itwouldben tosomeapplications andfeatureselection that dbld accuracy remark sub optimal solution whencomparedto caiandliu ourworkin throughapproximating and separately while caiandliu approximates experiments inthissection dbld onthesynthesizeddata then theperformanceof dbld usingseveralreal worlddatasets dbld asynthesizeddataset importedfrom caiandliu randomsimulation the dbld vs crld tuning figure accuracyof dbld vs crldonpseudo tributions and withequalpriors the settingsof and areasfollows isa symmet ricandpositivmatrix whereeachelement and and areboth dimensionalvectors where the elementsareall whiletherest elements are and inourexperiment weset to wetraincrldand dbld with to withequalpriors ran foreachsettings werepeatthe experimentsfor ina cross validationmanner inthisexperiment wecompare dbld crldandfld withpseudoinverse here dbld figure presentsthe comparisonbetween dbld andcrld intermsofaccuracy figure withtrainingset sizeas whenvarying from to fromfigure it isobviousthat dbld the showsthat given asmall bothcrldand dbld cannotperformwell asthe and insuchcase wittenandtibshirani when dbld whiletheadvantageof dbld however dbld stilloutperforms crld infigure onunbalanceddatasets using isillustrated varyingfrom to trainingsampleis weproposetheoptimal fld whichisallbasedon wecomparethe and estimatedby dbld crldandfld withpseudo inverse to figure and itisobviousthat ismore closeto than and the theorem and to crld dbld andfld andtheaccuracyof isaround itisreasonableto concludethat dbld outperformscrld because is morecloseto infigure wecompare dbld andotherfldalgo rithms inverse sparsefldvia graphicallasso crld andye fldderivedfrom ye et al onthewebdatasets lin tosimulatethe hdlsssettings from to whileusing samplesfortesting thenum bersofdimensions is foreachalgorithm reported resultisaveragedover training crldand dbld are withthebest that dbld differentsettings thenon dbld decisiontree randomforest svm and thecomparisonresults arelistedinfigure allalgorithmsarewith dbld inhandlingthe realproblems weevaluate dbld onthereal worldelec tronichealthrecords ehr asymptoticity figure dbld vs flds dbld vs downstream figure and tree decisiontree forest randomforest svm kernelsvm andl svm linearsvm trainingsetsize algorithm dbld fld ye fld decisiontree linearsvm kernelsvm adaboost crld randomforest table dbld andbaselines eases zhang etal inthisapplication eachpatient dimensionalvector diagnosed trainingset algorithm dbld fld ye fld decisiontree linearsvm kernelsvm adaboost crld randomforest table dbld andotherbaselines datasets features samples leukemia colon table ativeﬂ indicatingwhetherhe sion anxietydisorders datasets whethera new patientisat pression records zhang etal weevaluate dbld andothercompetitors includinglin kernel decisiontree adaboost randomforestandother fldbaselines from to table tosimplify thecomparison withparameter fold cross validation itisobviousthat dbld andcrldoutper while dbld theadvantageof dbld over otheralgorithms suchassvm issmall withtheincreasing samplesize thoughthemarginsof dbld overtherestofal gorithmsdecrease dbld wealsomeasuredthef scoreofallalgorithms dbld still pleaserefer totable fordetails weevaluate dbld randomforestandsvm using derivedfrom lin tibshirani etal underhdlsssettings and vs table lin tibshirani etal cancerdataset tibshirani etal thatincludes featuresandtotally samples in thisdatasets samplesareasﬁamlﬂ ontheotherhand ﬁcolonﬂ lin thatarewith featuresandtotally samples where samples arenegativeand samplesareaspositive both weusecross validation validation werandomlydrawn sampleswithequal andrandomlydrawn setasthetestingset there weusethe or and foreach experiment werepeatthecross validationfor rounds allalgorithms dbld aretunedto havethebestaccuracy table allresultsshowthat dbld improves crld highestaccuracyandf score baselinesinour in tibshirani etal weevaluate dbld trainingi or underhdlsssetting when thesample estimationoflda inthiscase exist biasingprocedures arenotmandatory colonleukemia algorithm accuracyf scoreaccuracyf score dbld crld decisiontree randomforest svm table accuracyandf dbld and inthissection research alineof research shao etal caiandliu proposedto isnotinvertible ontheother hand peckandvanness bickelandlevina wittenandtibshirani tors tors inverse covariancematrixesti mation jectionvectorwith sub zol lanvarianddougherty moreover theperformanceof durrantandkab an bickel etal inourpaper regularized fld wittenandtibshirani throughde biasingthe witten etal reasons de biasingthesparsefld zhangandzhang biasedlinear regressionmodels we proposedanovelde biasedestimator usingadifferentfor mulateineq forthe covariance regularizedsparselin wittenandtibshirani witten etal weanalyzedthede biasedestimator wevalidateour worlddatasets wheretheevaluation in theorem and vs thecurveshowninfig inthisresearch wecom pare dbld withcrld andcommonfld samplefld pseudo inversefld ye fld caiandliu aswe focusonthecovariance regularization infuturework we plantostudythede conclusion inthispaper formanceofcovariance regularizedfld crld through re estimation inspiredbythede ja vanmardandmontanari weproposed dbld œa novelde throughde biasingthe ouranalysisshowsthat dbld comparedto crld under hdlsssettings real dbld outperformedallbase linealgorithms further references bickelandlevina ina pages bickel etal peterjbickel elizavetalevina etal naive bayes bernoulli caiandliu tonycaiandweidongliu adirect ysis cai etal ttonycai zhaoren harrisonhzhou etal andprecisionmatrices tion dasgupta anirbandasgupta asymptotictheoryof springerscience business media duda etal richardo duda petere hart and davidg stork pattern nded wiley durrantandkab an an learningalineardis ma chinelearning friedman etal jeromefriedman trevorhastie and roberttibshirani biostatistics jankova etal janajankova saravandegeer etal intervalsforhigh anceestimation adeljavanmardandan dreamontanari ingforhigh journalofmachine learningresearch kaskiandpeltonen nen in icml pages krzanowski etal wjkrzanowski philipjonathan wvmccarthy andmrthomas discriminantanalysis methodsandapplica appliedstatistics pages kulisandothers briankulisetal metriclearning asurvey lin chih jenlin libsvmdata binaryclass https www csie ntu edu tw cjlin libsvmtools datasets binary html mar cenkoandpastur vladimiramar cenkoand leonidapastur ofrandommatrices mathematicsoftheussr sbornik peckandvanness ysis intelligence rothman etal adamjrothman elizavetalevina andjizhu anceestimation statistics shao etal junshao yazhenwang xinweideng sijianwang etal theannalsof statistics tibshirani etal roberttibshirani trevorhastie andgilbertchu diagnosis pression wittenandtibshirani tibshirani covariance journalofthe seriesb statisticalmethodol ogy witten etal danielamwitten jeromehfriedman andnoahsimon forthegraphicallasso graphicalstatistics ye etal jiepingye ravijanardan andqili two in nips pages cambridge ma usa zhangandothers zhihuazhangetal learningmet in icml vol ume pages zhangandzhang cun zhang journalofthe seriesb statisticalmethodol ogy zhang etal jinghezhang haoyixiong yuhuang haowu kevinleach andlaurae barnes mseq early in bigdata ieee aminzollanvarianded wardrdougherty in systemsandcomputers 
tdss liweichen yansongfeng anddongyanzhao pekinguniversity beijing china baiduinc beijing china chenliwei baidu com fengyansong pku edu cn zhaodongyan pku edu cn abstract trieval ir wordnet andmaynot queryrewriting in thispaper lyintwoviews keywords wordsenseinduction graphclustering queryrewriting introduction thereare ir better suchquery differencesofwords forexample queryrewriting isticsearchengines onesinthequery inpractice inordertoavoid take thequeryﬁ ﬂasan example wewouldliketouseﬁ breakfast ﬂtosubstituteﬁ breakfast ﬂasa rewritingofthequery however andweneedto twoviewsofawordsense theexplanationview and thecontextview unfortunately most suchaswordnet itonlyuses furthermore update ontheotherhand wsi hasattractedmoreand moreattention includingunsuper visedclustering ortopicmodels words whichmakesthe model inthispaper two tdss torepresent awordsense servingasadescrip tionofthissense forconvenience explanationwords we largescalequerylogs queryparaphrases reasonablewordsenses relatedwork inthissection orientedwsi methods oriented generativeapproaches incluster basedapproaches thementions contexts means thegraph where communi tions forexample lda tosolvethe wsitask insteadoflda bayesianmodel hdp thoseapproaches mostapproach esusethecontextsand intdss view taketheword see look ﬂasanexample wordssuchasﬁ read ﬂandﬁ browse forthecontextview see look itscontexts suchasﬁ book newspaper etc formally givenaword with senses its where and we pop generallyspeaking morepopular sense scontexts pop bow model it tdsssenseextraction weex paraphrasepairs explanationwords finally foragivenword giventhequerylogs weadopttheapproachin togen wordalignment wealignthewordsinone weadoptarule based synonyms them windowsizeissetto arethesameorsynonyms generallyspeaking twoalignedwords and co hyponymsorhypernyms etc thus we canuseaword foreach sense setto empirically givenaword anditssubstitution theprobability of issubstitutedby canbecalculatedas freq freq where freq isthetimes issubstitutedby intotal isallwordswhichcan substitute substitutedbythe explanationword giventhecontextword formally canbecalculated as freq freq where freq isthetimesthat issubstitutedby giventhecontextword underthecontextword inourapproach foragivenword graphpruning we prunethegraph thestrategyissimple forthisstrategy nodesandedges stableandreliable supposewe haveanode whichmeans canbesubstitutedby ifwecananothernode inthegraphsatisfying canbereplacedby thenwe reserveboth and otherwise figure illustratesa triangle where goonatrip istheword travel is theword and takeatrip istheword fig edgeweighting andtwonodes and inthisgraph theedge ab weight ab sub sim sub and averagevalueof and sim cosine similarity between and issubstituted by and respectively thecontextword it foreachtargetword weobtainasense figure themeaningsof in chineseincludes visit cure and observe etc fig thesakeofbrevity graphclustering differentsenses suchaskmeans hac and density sinceitisdifforusto allnodesofthegraph ineachiteration nodesare inaniteration theprocedurewillstop sensegeneration aftertheclustering ofonesense isthesubstitution where foreachsense weaggregateallits sensewords contextstogether the vectorofthissense isthe thus iscomputedas where and iscalculatedas givenaword whichhas senses thepopularityof canbecomputedas pop freq freq where freq of wecollect thesenseswiththetop table beat table word sensewordcluster andcontextcluster beat sense injection injection context whitening hepatitisb anesthetic sense towar strategy passgamelevel attack context bladesoul equipment plug in skills sense mark written keyboardinput show context wps parentheses square document sense beatcruelly bebeaten slap beatup context wakeup baby husband guard experiments ofourmodel nesequerylogs words includingnouns verbs adjectives adverbs and table eachword lybythreevolunteers thisdatasetwill thecontextviewswill table ring giveup etc whole etc weak thin etc fresh novel etc yellow adult etc letter trust etc sill threshold etc wrap bag etc fuelcharging cheerup etc straight directly etc leave remove etc put release etc hot popular etc represent representative etc light smooth etc good like etc through completely etc cheap interest etc hook linkupwith etc reckoning getevenwith etc humidity exaggeration etc comrade gay etc look read etc mouth opening etc wide relieve etc solid strong etc hit from etc branch divide etc rushfor catchupwith etc wer spend etc breakfast abitearlier etc pimple knot etc negative burden etc wecollectthe queriesfromtheyear and producingabout paraphrasepairs thus weonlyconsiderthe annotations apre cisionof andarecallof aregenerated werandomlyselect thetop proportionare intable table reasons percentage thetop mappings asthequery forexample thequeryisﬁ qq howtoseethepictures intheqzone qq thiswillmakeﬁ see bealignedto upload second taskofwordalignments forin stance intheparaphrasesﬁ andﬁ thecorrectsegmen somuch miss butthesecond misssomuch miss itmakesﬁ so much ﬂbealignedtoﬁ misssomuch finally areof tennotwellnormalized characters underthese circumstances taketheparaphrasesﬁ net andﬁ net asanexample the wordﬁ complete play play inthiscontext becauseﬁ complete andﬁ play wordbymistake complete toﬁ play largerthan reasons weusetheb cubedcriterion clustering theresultsofthetop table fmeasure clustering wordsense thecor forexample onesense ofthewordﬁ see treatadisease cure andﬁ heal recall accurate further more casestudy queryrewriting proaches cbowandskip gram werandomlycollect queriescontainingthe chinesewords forcbowandskip grambaselines weselectedthetop similarwords containthe grammodel forourtdsswordsenses weuse recalloftheresults table approach precision cbow skip gram tdss precision expla nationsandcontexts intotwoviews cbowand skip gram typesofinformation skip tdss experience senses conclusions inthispaper whichcaptures andfurtherpropose withoutre retrievaltasks suchasqueryrewriting sired forfurtherwork acknowledgement wensonghe shuaixiangdai xiaozhaozhao qiannanlv dprogramofchina grant no aa aa grant no foranycorrespondence pleasecontactliwei chen references bagga baldwin entity basedcross model volumepp brody lapata in proceedingsofthe thconfer pp chen xu he wang in proceedingsofthe rdannualmeet volume shortpapers pp association beijing china july http www aclweb org anthology chen liu sun tion in proceedingsofthe processing emnlp pp doha qatar october http www aclweb org anthology dorow widdows in proceedingsofthe volume pp guo che wang liu bilingualresources in proceedingsofcoling the technicalpapers pp dublin ireland august http www aclweb org anthology huang socher manning ng in proceedingsofthe volume longpapers pp association jejuisland korea july http www aclweb org anthology jurgens in graph pp associationforcom mikolov chen corrado dean sinvectorspace corrabs http dblp uni trier de db journals corr corr html abs miller wordnet miller beckwith fellbaum gross miller anon linelexicaldatabase neelakantan shankar passos mccallum efnon parametricestimation in proceedingsofthe conference emnlp pp associa doha qatar october http www aclweb org anthology niu ji tan chineseword sensedisambiguation in proceedingsofthe thin pp linguistics purandare pedersen similarityspaces in learning vol boston sch utze tian dai bian gao zhang chen liu aprobabilisticmod elforlearningmulti in proceedingsofcoling the technicalpapers pp dublin ireland august http www aclweb org anthology yao vandurme in proceedingsof textgraphs graph pp associa zhao wang liu in proceedingsof the pp associa 
directionallstms zeyingxie guangling baiduinc beijing china xiezeying lingguang baidu com abstract inthispaper directionallstms challenge dbdc thetaskofdbdc istodetectinappro systems loguesystems ourmodeliscontext awareandcanbetrained to endmanner tureengineering tems weevaluateourmodelon bythedbdc datasets andoutperformsthecrf indexterms bi lstm end to end dialoguesystems introduction devices spokendialoguesys machineinterface tantshaveemerged includingsiri googleassistant amazon echo cortanaandxiaoice onethat inganytasks orientedorgoal andthelaterchat stillunsatisfactory oftheusers ally anddetectionofthem thispaperfocuseson chat wenowintroducechat chat orienteddi mimic theobjectiveofchat inglytousers utterances recentchat orienteddialoguesys andaretrainedinanend to endmanner evolvedfromlabor intensiverule basedsystems todata drivenapproaches systems bothselectionbased andgenerationbased methods orienteddia loguesystems fromsatisfactory intentionoftheuser turnconversa tion user sponses dersuchcircumstances downs priateresponses reformulatehis herquestions multi aspects inchat takegenera responses inaddition inspokendi aloguesystem whichmayproduceer roneoussentences tembecausetheafore inthispaper lstmbased to endman ner insection task theemperical insection taskdescription tion dbdc duetothesubjectivena sation threelabels notabreakdown nb easily possiblebreakdown pb itisdiftocontinuethe conversationsmoothly breakdown tion table tktk iris cic yi train test train test train test train test no ofsessions no ofannotators nb notabreakdown pb possiblebreakdown breakdown datasets dbdc distributedmulti turnhuman eight areavailable arejapanesedatasets inthispaper weonlyfocusonenglish datasets tktk iris cicandyi or systemresponses ta ble evaluationmetrics dbdc relatedmetrics elatedmetrics themetricsusedbydbdc areasfollows łaccuracy łprecision recall measure theprecision recall andf łprecision recall measure pb theprecision re call andf blabels thatis distribution relatedmetrics distribution themetricsusedby dbdc areasfollows łjsdivergence nb pb shannondivergence łjsdivergence nb pb łjsdivergence nb pb łmeansquarederror nb pb distancebetweenthe łmeansquarederror nb pb meansquarederror figure hierarchicalbi ture łmeansquarederror nb pb meansquarederror however theclassication proposedmodel inthissection hierarchicalbi directionallstms bi lstm neuralnet workindetail bi lstmmodel isshowninfigure motivation awell of the artinmanynlptasks weuseabi lstmtoencodea dialoguesysteminter figure longshort latestturn thusweuseanotherbi lstm the dialogue besides our to no bi directionallstm rnn isapowerfulmodelthat unfortunately inprac exploding numerous amongwhichlstm worldproblems lstmunit basically tiplicativegates inputgate which memory figure formally aregivenby tanh tanh where denoteselement wise product ishiddenstatevector attime bi lstm astandardlstmonly bi directionallstm bi lstm tothisproblem thebasicideais tion respectively toformtheoutput theoutputattime ofbi lstmis calculatedasfollows lstm lstm where lstm lstmdenote respectively modelarchitecture bi leveltohigh level word embedding context connectedlayerand breakdownlabels wordrepresentation length duetothe weusethepre trainedglove embeddings weuseabi givenanutterance withnwords weembedthewords through togetwordembeddings word lstmen isregardedasthecor uttr userutteranceembed similarly weuseanotherbi coder mturns uttr uttr uttr weapplytheencoder breakdowndetection thecontextembedding canbeusedashigh levelfeatures arethecorre softmax table testdatasets english accu jsd nb jsd nb jsd nb mse nb mse nb mse nb model racy pb pb pb pb pb pb pb crfbaseline majoritybaseline plecorun rsl bdrun ncdsrun kthrun sam run bi lstm oursrun bi lstm oftest loss withre asthetrainingloss log experiment inthissection datapreparing preparations andwordsareconverted glovewordembeddings out of vocabularywordsare unk utteranceswith morethan wordsaretruncated whichwillbere loss data areusedforvalidation modeltraining weusethesamehyper forwordembed dings dimensionalem beddingstrainedon text thedimensionofbothbi lstmhiddenstatesare tomitigateovertting weapplydropout layersonword embeddings spectivelywith dropoutprobability wechooseadamopti of andbatchsizeof themodelistrained epochsfor asmentionedabove inourexperi ments tion labelissetto to usedinh bi lstm counts usedinh bi lstm forexample withnb pb insetting insetting bi and arenamedh bi lstm andh bi lstm respectively bi lstm af we usedh bi lstm baselines wecomparedh bi dbdc oneisacrf basedmethod thedetectorlabelsut thefea terances resultsanddiscussion weonlysub denotedash bi lstm bi lstm achievesahigh and tothecrfbaseline however bi lstm isnotperform especiallyinturns off relatedmetrics bi lstm we outtheperformanceofh bi lstm isbadlyhurt thus wesetupextraof experiments bi lstm bi lstm bi lstm bya hugemargin metrics besides jsd nb pb andmse nb pb ofh bi lstm asfromthetable teamswith aswellindistribution whileh bi lstm however thereisstillsome ei therbyhyper conclusion inthispaper tectorfordbdc bi extraofexperiments inthefuture work attentionmechanism canbeintegratedintoh bi webelievethis acknowledgement china programno cb references williamsands young computerspeech language vol no pp vinyalsandq le model corr vol abs online avail able http arxiv org abs zhou dong wu zhao yu tian liu and yan ﬁmulti computercon versation ﬂin proceedingsofthe emnlp austin texas usa november pp online available http aclweb org anthology pdf martinovskyandd traum ﬁtheerroristheclue breakdown inhuman machineinteraction technologies tech rep higashinaka funakoshi kobayashi andm in aba task description datasets andevaluationmetrics ﬂin proceed portoro slovenia may online available http www lrec conf org proceedings lrec summaries html bahdanau cho andy bengio ﬁneuralma late corr vol abs online available http arxiv org abs luong pham andc manning ﬁeffectiveapproaches toattention ﬂin proceedings ofthe languageprocessing emnlp lisbon portugal septem ber pp online available http aclweb org anthology pdf cassell pelachaud badler steedman achorn becket douville prevost andm stone ﬁanimated conversation rule gesture in proceedingsofthe siggraph orlando fl usa july pp online available http doi acm org henderson dialoguesystems ﬂph dissertation wu wu xing zhou andz li ﬁsequential matchingnetwork turnresponse selectioninretrieval basedchatbots ﬂin proceedingsofthe linguistics acl vancouver canada july august volume longpapers pp online available https doi org li galley brockett gao andb dolan ﬁapersona corr vol abs online available http arxiv org abs higashinaka funakoshi inaba tsunomori taka hashi andn kaji challenge ﬂin lenge dstc workshop higashinaka funakoshi kobayashi andm inaba taskdescription datasets andevaluationmetrics ﬂin lrec bengio simard andp frasconi ﬁlearninglong termde ieeetransactions onneuralnetworks vol no pp hochreiter bengio frasconi schmidhuber etal ﬁgra thedifoflearninglong term dependencies hochreiterandj schmidhuber ﬁlongshort termmemory neuralcomputation vol no pp schusterandk paliwal ralnetworks vol no pp pennington socher andc manning ﬁglove global ﬂin languageprocessing emnlp pp online available http www aclweb org anthology manning surdeanu bauer finkel bethard andd mcclosky processingtoolkit ﬂin acl systemdemonstrations pp online available http www aclweb org anthology srivastava hinton krizhevsky sutskever and salakhutdinov ﬁdropout worksfromovﬂ vol no pp bahdanau cho andy bengio ﬁneuralmachinetrans arxivpreprint arxiv 
usingachunk xianchaowu baidu japan inc roppongi hillsmori tower roppongiminato ku tokyo wuxianchao baidu com introduction suchastwitter twit teristheworld userstosend readtext basedmessagesofupto characters knownas tweets duetothefacts asfastaspossible tweetsare fullofhand suchas news blogs consequently traditionalhand made corpora indomainssuchasnews fornaturallan guageprocessing part of speech pos tagging parsing needtobe do mainadapted that is ifonejapanesenew compound wordisnotsuc ime orwelltrans inthispaper wepro wordsfromsingle astate of the artchunk cabocha kudoandmatsumoto which makesuseofmecab withipadictionary postagging and http twitter com http code google com cabocha http mecab googlecode com svn trunk mecab doc index html http code google com mecab downloads detail name mecab ipadic tar gz inthispaper weuse chunk followed andcorre spondstowordssuchas chunk phrase clause inenglish pliedton chenetal wedescribe experi tion andsection compoundwordmining miningsinglebensetsu compoundwordsare left hand sideandright hand sideofthecentral word specially centralwords suchas wo in yasai vegetables itameru cooking willnotbetrimmed tendstosplitoneout of vocabulary oov word foroneword yet forcabocha ittendstoin cludethesesingle kanji bensetsu thus wecanre combinethewrongly compound word this http ime baidu jp type source pstop ˆe figure theminingprocess poundwords suchaspersonalnames inmecab individualwords incabocha quentlyre thus we canre personalname forex ample whentwowords kabushiki stock and kaisya com pany arecombinedtogether theresultpronunci ationis kabushiki gaisya where ka ischangedinto ga another categoryisthat more forexample when ichi one and niti day arecombinedtogether there ichiniti one day or tuitati dayofeverymonth weonlyextractcom relations thatis node node inthe dependencytree notmatter beingtheleft hand sideorright hand sidebensetsu wecaneasilyobtain suchasdeter object verb sov language thus thedirectob forex ample yasai woitameru for yasai vegetableswo particleitameru cooking stir friedvegeta bles and atamawoitameru atama headwo particleitameru pain gotaheadache even itameru ing theﬁrst ent thepre imetyping filteringthelexicons doublebenset formedcompound words acter word poslist suchas wordssuchas andposssuchas acter word poslist suchas wordssuchas andposssuchas suchas youtube akb theminingprocess figure here weusethetwitter jpackage ajavalibraryforthe twitterapi http twitter org ja index html https dev twitter com docs streaming apis figure chunkintweets todownloadtweets here weuse ifat thenitislegal then weusecabochawhichin level thesingle double compoundwords duringthemining generatingof ﬁnallexicons experiments usingthetwitter jpackage wedownloaded japanesesentences wecallthiscorpus tweet hereafter therearetotally averagely wordsfor eachsentence figure tences fromtheﬁgure wecanobservethat chunksthat ofthetotalchunks wordsminedfromtweets wealsoapplythesimi larsingle other gdata wecallthiscorpus here after japaneseweb otherthanthosetweets tabel minedfromtweets single doublebensetsus inor werespec cut cut cut single double single tweets ﬁltered ﬁltered double tweets ﬁltered ﬁltered table single doublebensetsus ofthe tweets dataandthe webdata usingathresholdof and here ﬁltered ﬁlteringstrate gies gweb data table singlebensetsu tivelyused and asthefrequencythresh fromthetable wecan observethat oftheentries therearestill to oftheﬁlteredentries thatdonotappearinthe slexicons wecanaveragelymine single double thesenumbers containedintweets table wecanobserve table top ofcompound doublebensetsus cut cut single double tweets ﬁltered ﬁltered table tions also onecategoryincludes service suchas rt theother suchas girlspowerful face stylecharacters easyto saythatthese everyday wefurtherliststhetop sortedbyfrequency doublebenset susintable tolowercases wordsof rt and rt aretaken oneinterestingthing inthistableisthat tainbothkana such as rt rt sj wealsouseanexisting theex http ime baidu jp type lp girlspower kaomoji top top top baselineime single doublebensetsus table thetop theminedsingle imesystem istingtwitterlexicon contains entries table evenweremovednearly the nearly to gle oldof finally double bensetsulexicons cut tothebaidujapanese imesystem chenetal bytakingthe en triesasatestset thetop precisionchangesare listedintable theprecisionofthetop candi to throughthesenumbers wecansaythat nlpapplications conclusion doublebensetsus ex large one morethantwo ornon contiguousbensetsus such as references longchen xianchaowu andjingzhouhe us ingcollocationsandk then in proceedings ods wtim coling post conferencework shops japanesedepen in conll proceedingsofthe thconferenceonnatu rallanguagelearning coling post conferenceworkshops pages netyougo com 
marthinusc duplessis theuniversityoftokyo tokyo japan christo ms tokyo ac jp gangniu baiduinc beijing china niugang baidu com masashisugiyama theuniversityoftokyo tokyo japan sugi tokyo ac jp abstract isanimportantclassof inthis paper sensitivelearning toanintrinsicbias convexlossfunc isestimatedfromdata this isnaturallyininlier finally showthat thegeneraliza throughexperiments introduction pu tion puisconceivable cover wherepositivesamples built upurban areas canbeeasilyobtained butnegativesamples ruralareas outlier inthispaper weexplainthat pu cost sensitive data thus inprinciple sensitive hingeloss maypotentiallylead completelyseparable theuseofnon convex ramploss isessential theerror effectiveclassprior positivedata thiswouldbeusefulin inlier topu timesthefully supervisedcase finally puascost sensitive inthissection sensitive cation ordinary thatminimizesthe expectedrate aclasspriorof ˇr where and denotetheexpected falsepositiverate andexpected falsenegativerate and and and leadingtopractical cost sensitive acost in ˇc where and aretheper classcosts ˇc and pu positiveclass classprior ˇp pusetup not toinclude more let givesthe positivelabelover ˇp thentherisk canbewrittenas ˇr ˇr ˇr let comparedto where and respectively therisk canthenbeexpressedas where and bycost and some suchas libsvm toclasses inpractice theunknownclassprior necessityofnon inthissection andtheuseofanon suchthat sign islearned the where istheexpectationover and isthezero oneloss sincethezero itmaybe replacedwitha ramploss max min toavoidthenon the hingeloss max givinganobjectiveof convexramplossisthat separability min implies everywhere andforallvaluesof forwhich wehave therefore boundaryasthenon overlapping max max min lossfunctions resultingpenalties figure denotesthe ramploss and denotesthe hingeloss isconstant but ramplossfunctioninpu hingeloss asweshowbelow inputheriskisgivenby anditsramp lossversionisgivenby pu ˇr where comesfrom and inthesenseof yields pu fornon see ontheotherhand forpulearninggives pu ordinaryerrorterm cf penalty butitalsohasapenalty term seealsofigure selected indeed evenif itmaynotminimize pu duetothe penalty andthereforenon convex alternatively it notethat anidentical illustration and where andvariance thehinge lossobjective functionforpu pu wx theex theoptimaldecision class theproblem usingthehingeloss theratefor figure figure causingadifference intherates figure threshold fromthis wenotethatthehinge prior whentheclass priorislargeenough thelargehinge positivelylabeled insuchacase therefore priorestimation thetrue classprior isneeded however itneedstobeestimated priorestimationonthe performance inthissection affectsthe theriskfora isgiven asfollows pp and ˇr istherefore min notethat is concave this supposewehavea argmin totheconcavefunction at ˇr thefunction issuboptimalat classprior risk selectingatominimize andapply leadstoanexcess riskof vs theestimatedclass prior figure thedifference andthetrueclassprior causesanexcessrisk the effectiveclassprior wewishtoselectathat minimizestheriskin inpractice however therefore ˇr gives ˇr thus and thisresult wethe effectiveclassprior sothat and figure fordifferent thegraphshowsthat islarge tendstobearound tobelarge aroughclass prior ontheotherhand ifthetrueclass priorissmall ated inthissection clearly more ofform where let testdataaccordingto and thentheconstants sup sup andthefunctionclass let bea surrogateloss forthezero oneloss if if if forany islowerboundedby andapproaches as approacheszero moreover let yf yf and yf yf ourkeyideaisto yf yf where theorem fix then forany overtherepeated samplingof and yf ln theorem fix then forany overtherepeated samplingof and every yf ln inboththeorems thisorderis datafrom anotherdistribution dataarealli wouldbeoforder however samples althoughtheorders andthelosses differslightly forpuisno worsethan times forfullysupervised assuming and are equal boundsforpu handsideof andtheempirical handsideof table rate inpercent thebest and equivalentby test isindicatedinbold vs vs vs vs vs vs vs vs vs lossfunctions classprioris classprioris classprioris figure ﬂvs ﬂdigits obtainedbypulearning latent classlabelsaregiven when negativesamplesare experiments inthissection with weusedtheuspsdataset fromthe resultsintable hingeloss penaltytermin thiscanbeexplainedby wegetaneffective illustratedinfigure when islargeenough the givingaconstant theratebecomes sinceitis ﬂvs ﬂaregiveninfigure whenthe class priorislow figure andfigure higher forlargeclass priors figure positive inspectionshowedthat and conclusion weshowed however thatanon inpractice thetrueclassprior finally acknowledgments gnwassupportedbythe programno cb references elkanandk noto in proceedingsofthe kdd pages li guo andc elkan classof remote sensingdata hido tsuboi kashima sugiyama andt kanamori inlier inf giannotti gunopulos turini zaniolo ramakrishnan and wu editors icdm pages pisa italy dec scottandg blanchard noveltydetection unlabeleddatahelp in proceedingsofthe aistats pages clearwaterbeach floridausa apr elkan thefoundationsofcost sensitivelearning in ijcai pages changandc lin libsvm vantrees detection estimation andmodulationtheory parti detection estimation and modulationtheory johnwileyandsons newyork ny usa duda hart andd stork pattern johnwiley sons ndedition vapnik springer blanchard lee andc scott semi research duplessisandm sugiyama ieice collobert sinz weston andl bottou in proceedingsofthe icml pages suzumura ogawa sugiyama andi takeuchi outlierpath svm in proceedingsof icml pages beijing china jun ghosh manwani andp sastry corr abs mohri rostamizadeh anda talwalkar mitpress 
dsd ense parse ense rainingfor eep eural etworks songhan huizimao enhaogong shijiantang williamj dally stanforduniversity songhan huizi enhaog sjtang dally stanford edu jeffpool johntran bryancatanzaro nvidia jpool johntran bcatanzaro nvidia com sharannarang erichelsen baiduresearch sharan baidu com petervajda manoharpaluri facebook vajdap mano fb com bstract makingthem veryhardtotrain weproposedsd adense sparse densetrainingw for inthed dense step andimportance inthes sparse step sparsityconstraint inthed re dense step re rnnsandlstmsonthe on imagenet dsdimprovedthetop vgg by resnet by andresnet by respectively onthewsj dataset werby and ontheflickr kdataset attrainingtime hyper parameter attestingtime dsddoesn theconsistent whiledsdeffectively dsd songhan github io dsd ntroduction deepneuralnetworks dnns heetal luongetal andspeechrecognition amodeietal output ratherthan theintendedpattern inthetrainingdataset leadingto overandahighvariance alsoatnvidia nowatgooglebrain eriche google com arxiv cs cv feb figure dense sparse densetrainingflow andthe red algorithm wwofdsdtraining initialization with output šššššššššššššššœ initialdensephase šššššššššššššššœ while notconverged do end ššššššššššššššššš sparsephase ššššššššššššššššš kweights sort mask while notconverged do mask end šššššššššššššššš finaldensephase ššššššššššššššššœ while notconverged do end goto sparsephase foriterativedsd incontrast causingamachine leadingto underandahighbias tosolvethisproblem weproposeadense sparse densetrainingw dsd prunedweights attestingtime anddsdtrainingdoesn mainstreamcnn rnn speechrecognition dsdt raining low stepprocess dense sparse re dense figure andalgorithm initialdensetraining however thegoalofthisdstep figure prunedgooglenet afterretraining thesparsity constrainedgooglenet weights sparsetraining thesstepprunesthelow we appliedthe same thusthere the sparsity the foreachlayer with parameters wesortedthe parameters pickedthek thlargestone asthethresholdwhere sparsity and areshowninequation loss whenconductingahard isactually loss and loss loss hasapowerof weuse asthemetricofpruning smaller loss loss loss loss the sparsity isthesameforallthe and generally finaldensetraining again thesepreviously with byrestoringthepruned connections the layer theun soin in densetrainingstep finally in pruned weights inthisstep parameters weightdecay learningrate etc prunedweights comparingfigure and theun prunedweights zero thisisagood phenomenon choosingthe smallest table imp rel imp vgg visionimagenetcnn resnet visionimagenetcnn resnet visionimagenetcnn klstm deepspeechspeechwsj rnn deepspeech speechwsj rnn top error vgg resnetfromfacebook higherthebetter worderrorrate deepspeech elated ork dsd dropout srivastavaetal anddropconnnect wanetal dropconnectusea random deterministic ourexperimentsonvgg distillation modeldistillation hintonetal thisis changes modelcompression bothmodelcompression hanetal anddsdtraininguse networkpruning lecunetal hassibietal considerablemargins sparse canworkwell however thetruncation langford etal yuan zhang wangetal assameperiodbut independentwork candes romberg xperiments wefoundthat all rnnandlstms andcaptiongeneration fornetworks trainedforimagenet wefocusongooglenet vggandresnet andproduction for sinceithasonly thesparsityisthe same foralltheotherlayers including connectedlayers parameters and theepochsare westopthetraining oog et szegedyetal modelzoo jia ithas millionparametersand convolutionallayers weprunedeach layer excepttheto sparsity asshownintable googlenet top and top overthebaseline wecompareddsdv samenumberofepochs rateupon convergence andcontinuingtolearn lowerthelearning rate re llrcan table googlenet top errtop err sparsity epochs lr baseline sparse dsd llr improve abs improve rel vggn et simonyan zisserman whichiswidelyusedin detection zoo jia similartogooglenet eachlayerisprunedto sparsity dsdtraininggreatly reducedtheerrorby top and top detailedintable dsdalsowinsoverthe table dsdresultsonvgg vgg top errtop err sparsity epochs lr baseline sparse dsd llr improve abs improve rel es et deepresidualnetworks resnets heetal imagenet challenge thebaselineresnet andresnet we pruneto sparsityuniformly errorby resnet and resnet shownintable improvetheaccuracy asafaircomparison butcan asshowninthellrrow table dsdresultsonresnet andresnet resnet resnet top errtop err top errtop err sparsity epochs lr baseline sparse dsd llr improve abs improve rel figure table neuraltalk bleu bleu bleu bleu sparsity epochs lr baseline sparse dsd improve abs improve rel eural alk karpathy fei fei itusesacnnasanimage weed inthepruningstep to sparse paper shownintable retrainingthesparse overbaseline captionsystem intheimage boyandthegirl shairwitharockwall sagirl andthedsdmodel inthethesecondimage sin examples eep peech ds anddeep speech ds networks hannunetal amodeietal theds modelisa layernetworkwith asdescribedintable wsj whichcontains hoursof table deepspeech architecture layerid type conv fc fc fc ctccost params table worderrorrate wer deepspeech wsj wsj sparsity epochs lr denseiter sparseiter denseiter sparseiter denseiter baseline improve abs improve rel speech hourofspeech andwsj andcontain hourofspeechcombined theworderrorrate wer duetotwofactors first indeepspeech hoursof multi speakerspeechdata secondly deepspeech andouttheotherparts epochs inthesparsephase weights epochs inthe densephase epochs for re training validationset same numberof epochs the rowsoftable epochs andthe epochs thesame epochsas dsd wsj and wsj giventhe samenumber ofepochs oftheweights thesparsemodeland epochs re trainingstep baseline wsj and wsj arelative improvementof wsj and wsj so dsdsd eep peech ds network describedintable thisnetworkhas millionparameters around timeslargerthantheds model trainingsetisused hoursofspeech thevalidationsetis comprisedof hoursofspeech andwsj whichcontain hour ofspeechcombined table forthesparsere training similartods table deepspeech architecture layerid type dconv dconv br br fc ctccost params table wer deepspeech wsj wsj sparsity epochs lr denseiter sparseiter denseiter sparseiter denseiter baseline improve abs improve rel layersarepruned dsdtraining epochs withoneiterationof dsdtraining werimprovesby wsj and wsj baseline gain forthesecond sparseiteration arepruned wsj and wsj ontheds architecture ds network iscussion dense sparse densing weconjecture escapesaddlepoint dauphinetal advancedoptimization proach densingframework fromsaddlepoints hwang whichcanescapesub optimal betterminima we test shownintheappendix optimizationtoalower robusttonoise shownintheappendix robustre initialization mishkin matas second ormore solution were forprunedweights breaksymmetry thuspronetoco adaptationintraining indsd onclusion weintroducedsd adense sparse finally thepruned ofparameters andthusmodelcapacity fromthesparsermodel googlenet vggnet andresnetonimagenet neuraltalkonflickr anddeepspeech onthewsjdataset rnns andlstmscanbe fromdsdtraining eferences darioamodei rishitaanubhai ericbattenberg carlcase jaredcasper bryancatanzaro jingdongchen mikechrzanowski adamcoates gregdiamos etal deepspeech end to englishandmandarin arxivpreprintarxiv inverseproblems yannndauphin razvanpascanu caglargulcehre kyunghyuncho suryaganguli andyoshuabengio dimensionalnon convexoptimization in advances pp facebook facebook resnet torch https github com facebook fb resnet torch songhan jeffpool johntran andwilliamdally network in pp songhan huizimao andwilliamjdally deepcompression awnihannun carlcase jaredcasper bryancatanzaro gregdiamos erichelsen ryanprenger sanjeev satheesh shubhosengupta adamcoates andandrewng deepspeech scalingupend to endspeech recognition arxiv preprintarxiv babakhassibi davidgstork etal optimalbrainsurgeon pp kaiminghe xiangyuzhang shaoqingren andjiansun arxiv preprintarxiv geoffreyhinton oriolvinyals andjeffdean arxivpreprint arxiv chii rueyhwang simulatedannealing yangqingjia bvlccaffemodelzoo http caffe berkeleyvision org model_zoo xiaojiejin xiaotongyuan jiashifeng andshuichengyan arxivpreprintarxiv fei deepvisual in johnlangford lihongli andtongzhang in advancesinneural pp yannlecun johns denker andsaraa solla optimalbraindamage in processingsystems pp morgankaufmann minh thangluong hieupham basedneural machinetranslation arxivpreprintarxiv arxivpreprintarxiv jmoody shanson anderskrogh andjohnahertz arxivpreprintarxiv nitishsrivastava geoffreyhinton alexkrizhevsky ilyasutskever dropout jmlr christianszegedy weiliu yangqingjia pierresermanet scottreed dragomiranguelov dumitruerhan vincentvanhoucke andandrewrabinovich in proceedingsoftheieee pp liwan matthewzeiler sixinzhang yannlcun androbfergus dropconnect in icml pp zhaoranwang quanquangu yangning andhanliu arxivpreprintarxiv xiao tongyuanandtongzhang thejournalof ppendix ignificanceof dsd improvements densingthenetwork weights dsdtraining retrainingonbaseline andconventional multiple times datasetusingresnet ifar using es et cifar fortesting trainingoncifar epochs andinitiallrof facebook after epochs weobtained themodelwitha top initializedfromthis baselinemodel werepeated timesofre thedsdusedsparsityof and epochs forsparsetrainingand forre densingtraining asafaircomparison same baselinemodelwiththe same hyper epochs lrof and lrof oncifar andusingresnet architecture achievedtop testingerrorof whichisa absoluteimprovement relativeimprovement conventional table datausingresnet resnet avg top errsd top err sparsity epochs lr baseline directfinetune firsthalf directfinetune secondhalf dsd fisthalf sparse dsd secondhalf dense improvefrombaseline abs improvefrombaseline rel weusedt test unpaired tocomparethetop methods model figure ppendix ore xamplesof dsdt raining mprovesthe aptions eneratedby eural alk magesfrom lickr kt est et 
deepvoice real timeneuraltext to speech sercan sercanarik baidu com mikechrzanowski mikechrzanowski baidu com adamcoates adamcoates baidu com gregorydiamos gregdiamos baidu com andrewgibiansky gibianskyandrew baidu com yongguokang kangyongguo baidu com xianli lixian baidu com johnmiller millerjohn baidu com andrewng andrewng baidu com jonathanraiman jonathanraiman baidu com shubhosengupta ssengupta baidu com mohammadshoeybi mohammad baidu com bordeauxdr sunnyvale ca abstract wepresentdeepvoice aproduction quality text to deepneuralnetworks deepvoicelaysthe to endneuralspeech synthesis jorbuildingblocks agrapheme to aphonemeduration predictionmodel dictionmodel weproposeanovel temporalon ctc loss fortheau diosynthesismodel weimplementavariant byusinganeu oursystemis to speechsystems mainexpertise finally weshowthatinference submittedfebruary icml copyright by theauthor introduction commonly knownastext to speech tts enableddevices navi gationsystems impaired fundamentally itallowshuman modernttssystems arebasedoncomplex multi heuristics duetothiscomplexity developingnewtts to speech whilereplacingall tures speech taylor unlikepriorwork whichuses hand spec tralparameters aperiodicparameters etc ouronlyfea phonemedu rations thischoiceof datasets voices wedemonstrate inaconventional ing arxiv cs cl mar deepvoice real timeneuraltts real qualityttssystem withoutit awavenet vandenoordetal cangeneratecloseto human levelspeech however frequency andithasbeenhitherto tionsystem demonstrateeffaster than real timewavenetinfer quality khzaudioand realizea implementations paineetal relatedwork includinggrapheme to raoetal yao zweig zen sak pascual bonafonte ronankietal and audiosynthesismodels vandenoordetal mehri etal unlikedeepvoice however noneofthese usespecializedhand icallyfortheirdomain mostrecently ricaudiosynthesis notablywavenet samplernn and char wav vandenoordetal mehrietal soteloetal sam char based pre dictionmodel basedvocoder first traininganewdeep existingttssystem incontrast existingttssystem be second engineered features itusesone tophonemeconversion one stresses andnormal waveformsusinganyf estimationalgorithm allofthese imaleffort incontrast existingtts system zenetal ofsyllablesinaword anddynamic parameters char wav moriseetal forpre ulewhichincludef spectralenvelope andaperiodicpa rameters finally readysystem which requires timeforinfer ence second offbetweensynthesis speedandaudioquality incontrast previousresultswith onesecondofaudio marksforsamplernn butthe tierarchitectureasde wavenetmodels timemay provechallenging ttssystemcomponents asshowninfig buildingblocks the grapheme to phonememodel convertsfrom writtentext englishcharacters tophonemes en the segmentationmodel locatesphonemebound givenanaudioanda phoneme by the phonemebeginsandends the phonemedurationmodel predictsthetemporal an utterance the predictswhether aphonemeisvoiced ifitis themodelpre throughoutthe phoneme sduration the audiosynthesismodel combinestheoutputs ofthegrapheme to phoneme phonemeduration and correspondingto deepvoice real timeneuraltts figure trainingprocedureand inferenceprocedure inoursystem loss thegrapheme to suchascmudict dottedlinesdenotenon learnedcomponents thedesiredtext duringinference to phonemes next predictionmodelto con tour finally thephonemes phonemedurations andf synthesismodel unliketheothermodels usedduringinference instead itisusedtoannotate the whichcanbeused theaudio anno fundamentalfrequency sismodel blocksindetail grapheme to phonememodel ourgrapheme to yao zweig however weuseamulti agatedrecurrentunit gru chungetal the weuse bidirectional layerswith unidirec searchwithawidthof candidates duringtraining we fortraining abatchsizeof learningrateof andanannealingrateof applied every iterations kingma ba segmentationmodel phonemes inthatdo main ctc loss gravesetal of the artspeechrecogni tionsystem amodeietal forphonemeboundary detection phoneme phonemestotheaudio deepvoice real timeneuraltts phonemeboundaries toovercomethis wetraintopredict sequencesofphoneme pairs apair considerthestring ﬁhello labels usingapro to phonememodel finally sil hh hh eh eh ow ow sil mel frequency cepstralcoef mfccs ontopoftheinputlayer ers threebidirec andasoftmaxoutput layer heightnine infrequencybins andwidthve intime grucells foreachdi rection isapplied tocom putethephoneme pairerrorrate pper wedecodeusing beamsearch weperform abeamsearchwithwidth pair fortraining abatchsizeof learningrateof andanannealingrateof applied every iterations kingma ba model rationandtime thein hot vector erswith currentlayerswith connectedoutputlayer is recurrentlayer phoneme thephonemeduration phonemeisvoiced and time dependentf values fundamentalfrequency error thephonemeisvoiced theabsolutechangeoff smoothness fortraining abatchsizeof learningrateof andanannealingrateof appliedevery iterations kingma ba audiosynthesismodel whichup and itydistribution thenum everylayer thedimen layer work followedby residual tanh nonlinearities webreak with prev and cur residualconnections concatenatedtoan vectorandprojectedto skipchan nelswith skip convolu tions withweights relu and out with relu nonlineari ties conditioning train faster rnn qrnn layers bradburyetal andthenperform ourhighest qualitymodeluses layers residualchannels and skipchannels fortrain ing abatchsizeof alearning rateof andanannealingrateof appliedevery iterations kingma ba deepvoice real timeneuraltts results hoursofspeechdataseg mentedinto utterances inaddition wepresent oftheblizzard data prahalladetal both framework abadietal segmentationresults wetrainon titanxmaxwellgpus splittingeachbatch reducetoav witheach milliseconds after approximately iterations themodelconvergestoa wealsothatphoneme andrandomlyshift millisecondsmakesno certainpoint grapheme to phonemeresults wetrainagrapheme to fromcmudict weide containnumbers orhavemultiple pronunciations whichleaves outoftheoriginal grapheme phonemesequencepairs milliseconds afterap proximately iterations themodelconvergestoa phonemeerrorrateof andaworderrorrateof yao zweig unlikepriorwork wedonotusealanguage results milliseconds afterapprox imately iterations absoluteerrorof milliseconds forphonemeduration and hz chunk lenceatthebeginning totalchunks including and wethat modelsbelow the and ognizablespeech butthe thanthe layermodels qualityover earheadphones indeed the layer layermod els however whenrunat khz modelswith layers haveonly ofthe receptivesize wetrainon gpu ondifferentgpus milliseconds iterations wethatasingle schunkissuf asiscommonwithhigh theisetal whilemod models inaddition changesinmodel architecture canhave fectonaudioquality ourttspipeline mos ratings highervalues beingbetter ribeiroetal inorderto thewavenet modelquality thewavenetinputs durationandf areextractedfrom els wepurposefully tweenmodels deepvoice real timeneuraltts arepresented only firstofall khzto khz es lawcompandingandquan tization likelybecausea score andalowqualitynoisy whenusedwithground truthdurationsandf ourmodelsscorehighly withthe however usingsynthesized weconcludethat andour theartinthatregard finally slowerthanreal time seetable sowedemonstratethat xand xfasterthanreal time butfound blizzardresults weretrained blizzard dataset prahalladetal forourex periments weuseda mentedinto utterances whichencour groundtruth ontheheldoutset khzcompandedand while optimizinginference terances frequency au networks whengeneratingaudio asinglesamplemust for khzaudio or for khzaudio forour layermodels this forcomparison inorderto time processorcache andopti thesesame pixelcnn oordetal image layer point operations flops vioustimestep andonelayeratatime flops parameters which equatetoabout seeappendix oncpu single flopsandanl to gb timestep gb even ifthemodelweretoinl cache theimplementation wouldneedtoutilize timeonasinglecore but nized anaiveimplementation untenable rnntechnique diamosetal maybeabletotake weimplementhigh enceatfaster than real table els inbothcases toregressive high not our cpukernelsrunatreal timeorfaster than real timefora assumingtwo andanl to bandwidthof bytespercycle deepvoice real timeneuraltts type modelsize mos ci groundtruth khz none groundtruth none groundtruth compandedandexpanded none synthesized synthesized khz synthesized synthesizedf synthesized synthesized xreal timeinference synthesized xreal timeinference table meanopinionscores mos and intervals cis forutterances khzrecording whichmakesallour modelsused phonemedurationsandf models aswellasa hzaudiosamplingrate model platform datatype numberofthreads speed upoverreal time cpu float cpu float cpu int cpu float cpu float cpu int cpu float cpu float gpu float gpu float gpu float gpu float table subsetofmodels thisperformance cpuimplementation weachievereal computation doingcache par nization avoidingcache andus ingcustomhardware thefollowingsteps sampleembedding timestep thatis emb prev emb cur embed deepvoice real timeneuraltts mainthreads auxiliarythreads syncpoints cur cur cur cur prev prev prev prev cur cur cur cur timestep timestep figure computationofthe skip left prev layerinference foreverylayer from to withdilationwidth twodilated vectormultiply prev prev cur cur giventhecondi tioningvector prev cur tanh where denotesthe elementsofthevec tor and denotesthenext elements then vectormultiply res res channel accumulating overalllayers with skip skip output computethetwooutput convolutions relu relu relu relu softmax out out finally sample pictedinfigure cur and and agroupofauxiliary threadscomputes prev and withthe prev being threadscompute and ifthereare multiplethreads matrix vectormultiply binaryoperation orunaryopera tion splitting intotheprocessorl cache ordisablinghyper threading mately dependingonmodelsize thenonlinearities tanh sigmoid and softmax canalsotakeafrac tionofinferencetime high whicharedetailedinap pendix approximationsis for tanh for sigmoid and for tizedto int forlargermodels quantization but finally dukhan spe float nei deepvoice real timeneuraltts int operations gpuimplementation manyneuralmodels whichcanhaveamuch sinceour itmayseem butitturns usually invocations beingitsownkernel however thelatencyforacuda kernellaunch whichmaybeupto combinedwith an xslowerthanreal time togetclosetoreal timeonagpu weinsteadbuildaker diamosetal singlekernellaunch duetothemismatchbe kernels intensivetowrite al time table timewavenetin conclusion inthiswork qualitytext to tem than real timespeeds ateaudioinreal oursystem dramatically ration and int quantizationoncpu aswellasexperiment an durationprediction afullsequence to sequencemodel creatingasingleend to inlieuof fusingthemodels deepvoice real timeneuraltts references abadi mart agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado gregs davis andy dean jeffrey devin matthieu ghe mawat sanjay goodfellow ian harp andrew irv ing geoffrey isard michael jia yangqing jozefowicz rafal kaiser lukasz kudlur manjunath levenberg josh man dan monga rajat moore sherry murray derek olah chris schuster mike shlens jonathon steiner benoit sutskever ilya talwar kunal tucker paul vanhoucke vincent vasudevan vijay vi egas fernanda vinyals oriol warden pete wattenberg martin wicke martin yu yuan andzheng xiaoqiang tensorflow large neoussystems url http tensorflow org org amodei dario anubhai rishita battenberg eric case carl casper jared catanzaro bryan chen jingdong chrzanowski mike coates adam diamos greg etal deepspeech end to andmandarin arxivpreprintarxiv boersma praat asystem glotinternational bradbury james merity stephen xiong caiming and socher richard quasi arxiv preprintarxiv chung junyoung gulcehre caglar cho kyunghyun andbengio yoshua arxiv preprintarxiv diamos greg sengupta shubho catanzaro bryan chrzanowski mike coates adam elsen erich engel jesse hannun awni andsatheesh sanjeev persistent rnns chip in proceed ingsofthe learning pp dukhan marat peachpymeetsopcodes directmachine in proceedingsofthe th performanceandscien computing pp acm graves alex fern andez santiago gomez faustino and schmidhuber urgen tion rentneuralnetworks in proceedingsofthe rdinter icml pp newyork ny usa acm kingma andba adam amethodforstochastic optimization arxivpreprintarxiv mehri soroush kumar kundan gulrajani ishaan ku mar rithesh jain shubham sotelo jose courville aaron andbengio yoshua samplernn anuncondi tionalend to arxiv preprintarxiv morise masanori yokomori fumiya andozawa kenji world avocoder basedhigh systemforreal timeapplications ieicetransac oord aaronvanden kalchbrenner nal and kavukcuoglu koray arxivpreprintarxiv paine tomle khorrami pooya chang shiyu zhang yang ramachandran prajit hasegawa johnson marka andhuang thomass fastwavenetgen erationalgorithm arxivpreprintarxiv pascual santiagoandbonafonte antonio multi output rnn interpolationmodel way prahallad kishore vadapalli anandaswarup elluru naresh etal theblizzardchallenge indianlan guagetask in rao kanishka peng fuchun sak has im andbeau fays franc oise grapheme to phonemeconversionus inglongshort in acoustics icassp pp ieee ribeiro fl avio flor encio dinei zhang cha andseltzer michael crowdmos in acoustics speechand signalprocessing icassp ieeeinternational conferenceon pp ieee ronanki srikanth henter gustaveje wu zhizheng and king simon atemplate interspeech pp sotelo jose mehri soroush kumar kundan santos joaofelipe kastner kyle courville aaron andben gio yoshua char wav end to endspeechsynthesis in iclr workshopsubmission url https openreview net forum id vwyyskx stephenson ian productionrendering designandim plementation springer deepvoice real timeneuraltts taylor paul text to speechsynthesis cambridgeuniver sitypress newyork ny usa stedition isbn theis lucas oord aronvanden andbethge matthias arxiv preprintarxiv vandenoord aron dieleman sander zen heiga si monyan karen vinyals oriol graves alex kalch brenner nal senior andrew andkavukcuoglu ko ray wavenet corr abs weide yao kaishengandzweig geoffrey sequence to to phoneme conversion arxivpreprintarxiv zen heigaandsak has im term layerforlow in acoustics icassp ieeein pp ieee zen heiga senior andrew andschuster mike statistical in acoustics speech andsignalprocessing icassp pp deepvoice real timeneuraltts appendices whichconvertslow tothe nativeaudiofrequency andanauto regressiveprocess andacontextof audiosamples isthereceptivesize andisaproperty thenetworkdetails figure tealinputs greenconvolutionsand qrnns pinkbinaryoperations andindigoreshapes transposes andslices auto regressivewavenet audioisquantizedto valuesusing lawcompanding asdescribedinsection ofwavenet theone hot stack embed embed deepvoice real timeneuraltts where istheone sincetheinputaudio isaone hotvector thisconvolutioncanbe andthen due addstoitsinput togenerateitsoutput tanh where the convolutions onefor andonefor inputand outputchannels duringinference prev and cur therighthalf fora timestep asfollows prev cur tanh where and and and thehiddenstate fromeachofthelayers through skip downtothe numberofskipchannels relu skip skip whererelu max relu relu relu softmax out out conditioningnetwork likeﬁbabblingsoundsﬂ astheylacksuf long generatedfromalower stress soan foreachwavenetlayer instead wepassourinput rnnlayers bradburyetal with fo poolingand convolutions fo tanh deepvoice real timeneuraltts afterbothqrnnlayers weinterleave thechannels sothatthe tanh andthe sigmoid backwardqrnn converge inputfeaturization bit hzfrom bitdual channel pcmaudioat hz itisconditionedona hzphonemesignal dimen sions andthus hasanf frequency tominimumobservedf to and dimensionalone with forsilence anda dimensionalone no stress primarystress secondarystress tertiarystress andquaternarystress inourexperiments iscrucialfor basedupsampling although pauses etc wesamplethephonemes atregularintervals withcontextandf asa result sec ms weusepraat boersmaetal withaminimumf of andamaximumf of isavailableat https github com baidu research deep voice blob master scripts script praat andcanberunwith praat runf script praat ateverytimestep distribution directsampling samplerandomlyfrom temperaturesampling where mean mode argmax nunciations deepvoice real timeneuraltts top samples if th otherwise where temperaturesam butforconverged modelsisworse distribution training asexpected noise throughouttraining inaddition forexample stabilizes phonememodelloss thelossforthe th phonemeis ce where and truthdurationsofthe th phoneme and th phonemeisvoiced ceisthecross entropyfunction and th phonemeattime timesamples duringinference accuracyrationalap proximations inthisappendix denoting asanapproximationto tanh and tanh sign wechooseaforth tanh and byitself tanh and and deepvoice real timeneuraltts approximation stephenson function directly weapproximate andusetheidentity ln let tobetheof then where since ifweusea bittorepresent then and therefore asa bitsinteger representedby wehave andusing resultsto over in equation asa for persistentgpukernels sms cache there isalsoacoherentl theentireaudio thisraises sms numberedsm sm ofatitanxgpu wedonotusesm sm tosm thismeanssm storeslayers and sm storeslayers and andsoonandsoforth prev prev cur cur thatare thussm and andanoutput skip thus skip is partitionedacross sms onlysm needstostore skip sm stores relu and relu finally out issplitacross twosmsšsm andsm stores out to sm andbackagainfromsm thread oneforeachsm cachedinl firstsm and itthenunlocksthelock thatsm thensm doesthesameforsm continuesforeachsm finallysm fortimestep andunlockssm sothatentire fortimestep justlikelocks wepassdatabetweensms cache since cache deepvoice real timeneuraltts and shownintable this anychangeinthose firstly cache cache assemblycode however performancemodel inourmodela takes flopsš multiplicationsand additions thismeansthata matrix an matrixand vectortakes flops thuscalculating uses cost flops and flopsrespectively thismeans tanh and takes flops thuscalculating takes flops finallycalculating foreachlayertakes flops cost layer flops underthesamemodel calculating takes flops whereweassumethat relu takes flop similarly calculating takes flopsand out out takes flops softmax takesone max onesubtract oneexponentiation elementofavector hencecalculating takes flops addingitallup cost sample flops ifwelet and andassumethat and hz wehaveapproximately 
yincui fengzhou jiangwang xiaoliu yuanqinglin sergebelongie cornelluniversity cornelltech baiduresearch googleresearch ycui sjb cs cornell eduwww zhou com wangjiangb gmail com liuxiao linyuanqing baidu com abstract cnns withbilinear pooling representations includingainedvi sualcategorization facerecog nition thekeytotheir nd order featureinteractions inthiswork wepropose wedemon aparameter freemanner combinedwithcnns thecom to propagation theproposed ex of the artperfor introduction recently actions implicitly map svm thestorage numberoftrainingdata datasets inaddition figure forafeaturevec tor in thecaseofacnn weusecountsketch togenerateacompact th order ing schemeisend to the onlyusethe nd order includingstochas ticgradientdescent sgd inthetrainingofcnns the otherwayisto explicitly monomials ifwewantupto th orderinteractionsona the which orfeaturemaps neuralnetworks cnns onvisualdata low levelhand craftedfeatures sift hog figure end to forthe theredorbluebar onthefeaturemap locations andcoef propagation gist combinedwithmid poolingmethods bag of visual words spatialpyra midmatching sparsecoding fishervector traction kernelmethodssuchas morediscriminative recently effortsincombining cnnswith nd eitherbyreplac inghand orjointly trainedinanend to endfashion representative synthesis facerecognition andstyletransfer notably bothgao etal andfukui etal usedtensorsketch linearvectorby sameperformance inthiswork wegeneralizethe strategyusedin interactions forafeaturevector ofdimension we generateits th order withcountsketch inprac tice fft andinversefast fouriertransform ifft ithasbeenproven boththeo asillustrated infig element wisemul tiplication fftandifftunits ofacnn then fortwofeaturevectors and and der asfollows seesec formoredetails ˇk linearop erateonhigh discriminative acnn asshowninfig to endviaback propagationoferrors the asdeterminedbycoef orlearnedfromdata tosumup firstly usingthelinearclas thekerneltrick secondly optimization relatedwork explicitfeaturemaps rahimi etal isoneofthe kernels later nelssuchasmaji etal kernelandvedaldi etal for kernel onthecompact recentproposedran dommaclaurinbykar etal tensorsketchbypham etal etal are thereisalsoalineof wedifferfromthese cnns inanend to endfashion withthejointoptimiza tion the shelffullycon rectlyfromdata sincethedimensionof th exponentiallywith theuseof inrealworldap inthecaseof the intro bilinearmod pliedonbothhand craftedfeatures andlearnedfea tures recently fueledbycompact nd or andnew nd orderpolynomial kernelto th theend to endtrainingwithacnn seesec kernelpooling turevector works cnns tab typicallypeopleuse relu asinthecaseofalexnet andvgg fully andresiduallearning this figure the leveltensor product of isa dimensionalvector whicharebelieved the bilinearmodels dimen sionalfeaturemapfor nd which usingtensor sketch we wetayler pactlyapproximated is withheight width andnumberof channels wedenotethe spatiallocationon as ofakernelfunction and astheinner nels gaussianrbfkernel exp kernel etc tion gaussianrbf taylorserieskernel first wethe leveltensorproduct outer product xx of as alexnet vgg inception resnet bilinear compactbilinear ours strategy hw ij hw ij ij hw ts ij hw ij dimension time hwcd hwc hwc hw log hwp log space hwcd pc parameters hwcd or table where and istheheight and ts and denotesthereluunit tensorsketch and respectively similarly the isas ptimes wealsohave and fig illustratesthe andits leveland leveltensor product and ithasbeenshownin thatthe th asfollows taylor sincethenon akernel polynomialkernels taylor eveninthecaseof and isstill largerthan therefore acompact compactapproximation thereareseveralre featureprojections webuildourapproxima becauseitconsumes anditiseasierto implementcomparedto forsimplicity of taylor and taylor algorithm input output where ˇk initialization for to do generate and theoutputsof and areuniformlydrawn from and respectively as where fft if then concatenate fft return taylorserieskernel leveltensorproduct wethecountsketch of as where thecountsketch isa latedusing hashfunctions and theiroutputs and re spectively the leveltensorproduct canthenbeap proximatedas fft fft fft where inde pendenthashfunctions and denotestheelement wisemultiplication fftandfft is combiningeqn andeqn thefeaturemapofatay asde scribedinalg tor theorder mated wewanttouse figure kernelwithvariant weignorethe constant whenplotting theinnerproductvalue and basedon using gaussianrbf forestimating anditsassociatedcoef com wereduce more from to where ithasbeenprovedthat ineqn isanunbiasedfea turemapestimatorfor th the sin equality seelemma in forthedetailedproof sim ilarly canbebounded as min where min min and if otherwise cos and in ourexperience large min kernelswithlarger in troducelargererror figure to endtrainingwith acnn propagationoftheloss gaussianrbfkernel can beexpressedas rbf exp exp exp where exp isaconstantand exp if and are normalized compared bytermuptoorder bysetting as otherker sioninthesimilarform fig value ingeneral the soweneedtochoose carefullybasedon using of relies and fftandfft combinedwithacnn thecompo asillustrated infig tributionofhigh dimensionalfeatures therefore aker validation to endfashion wear dataweareworkingon nitionaccuracy sec baselinemethods then insec werunacomprehen tures kernelorder and sec inwhichwepresent car aircraft andfood state of the experimentsetup for cnnarchitectures weusevgg andresnet of the artperformance onimagenet vgg has convolutionwithrelu layersand resnet consistsof bothvggandresnet of duringtheconvolution inthecaseofbilinear wekeepthefullyconvo thefeaturevector in alg oflastlayer sfeaturemap wechoosevgg andresnet asrepresentatives re spectively theperformanceofvgg andresnet is reportedbyne pre trainedweights poolingmethods vgg thisisthe originalvgg networkproposedin thearchitec tureofvgg breaking alexnet inalexnet spatialresolution invgg however moreconvolutionlay ers to which achievedstate of the lenge withrelulayers due layers foreachofthedataset trainedweights resnet al it andresidualmodule the capableoftakingin free however itfailstocap wechoosea bilinearpooling bp topoftheconv featuremapfromvgg whichissame asthebest performedb cnn in thefeature wedon resnethas the cbp weusetensorsketch thefeaturemapofvgg andresnet whereasthe originalpaper onlyusedvgg typically compact linearpoolingwith forafaircomparison kp weevaluatethepro cbp fortheactivation featuremap weapplyalg map sameasbpandcbp thefeaturevector thecom viaback propagation feature dimension and willbediscussedinsec implementation in and we wiseimagemean andweresize whilekeeping itsaspectratio thenwecropa squareimagefrom theoriginalimage duringtraining arandomsquareimage iscropped duringinference the centerimageiscropped theaverageof wefollowthepost processingstepsin tothe featurevector beforethelinear becausethe tionperformance weapplyelement wisesignedsquare root sign followedby normalization onthecompactfeature vector mance weusepre andtheinitial ofpre trainedcnnfeatures westartthewith xsmallerlearningrate forvggand for resnet anddivideitby afterevery epochs weuse amomentumof andaweightdecayof forvgg and forresnet around epochs entssometimes therefore gradientclipping isapplied and weusetw models gpu thefor andresnet msona image and msona image around mswith and oncnnfeatures using vgg trainedonimagenet weextractconv feature within putsizeof featuremap thefeatureisa dimensionalvec tor weusethesamefeature poolingdimension for therefore fig showsthe scale order and figure vector and isgivenby rbf rbf from to withthestepof eachdatapointisthe averagederroron fromfig higher general ingorder in fig orderaswellasaproper wecanachievecloseto rel ativeerror inlightofthis weuse and theoutputvectorhasa dimensionof forvgg and forresnet thehyper parameter surethat kernelapproximation visualrecognition weusecub dataset forthistask thedatasetconsistsof imagesfrom birdspecies imagesfor carmake model year thestanford cardataset isusedforthistask ithas images of classeswithcarmake modelandyear aircraft theaircraftdataset challenge which contains images dataset cnn original bp cbp kp others cub vgg resnet stanfordcar vgg resnet aircraft vgg resnet food vgg resnet table weusetheinputsizeof forcub vgg markedbyanasterisk forfood weusetheinputsizeof for allthebaselines figure fromlefttoright stanfordcar aircraft andfood foodrecognition forthistaskweusefood dataset thisisalarge scaledatasetwith imagesand imagesper eachcategory perfor of the art byalargemargin around onallthedatasets discussion inthissubsection wefoundthat especially nd and rd or der in resnet orders networkarchitectures thenon linearfeature connectedlayers soremovingthefully connectedlayersde gradetheoriginal st orderfeature sinceresnetonlyusea togetherwith tecture inourexperiments we andresnet isutilized usinghigh conclusion inthispaper sualrecognition orderandnon plicitfeaturemapping to endmanner ex methodachievesstate of the recognitiontasks acknowledgements searchaward microsoft references abadi agarwal barham brevdo chen citro corrado davis dean devin etal tw large distributedsystems arxivpreprintarxiv avron nguyen andd woodruff subspaceembed in nips blondel ishihata fujino andn ueda polyno newinsightsand eftrainingalgorithms in icml bossard guillaumin andl vangool food in eccv carreira caseiro batista andc sminchisescu se orderpooling in eccv charikar chen andm farach colton findingfre in automata languages andprogramming cimpoi maji anda vedaldi deepbanksfor in cvpr dalalandb triggs humandetection in cvpr deng dong socher li li andl fei fei imagenet alarge in cvpr fukui park yang rohrbach darrell andm rohrbach arxiv preprintarxiv gao beijbom zhang andt darrell compact bilinearpooling in cvpr gatys ecker andm bethge aneuralalgorithm ofartisticstyle arxivpreprintarxiv gatys ecker andm bethge texturesynthesis in nips gosselin murray egou andf perronnin re visitingthevectorfor pat he zhang ren andj sun deepresiduallearning forimagerecognition in cvpr jaderberg simonyan zisserman etal spatial transformernetworks in nips karandh karnick kernels in aistats krause jin yang andl fei fei fine grained in proceedingsofthe tion pages krause stark deng andl fei fei dobjectrep in iccvwork shop krizhevsky sutskever andg hinton imagenet in nips lazebnik schmid andj ponce beyondbagsof features scenecategories in cvpr linands maji in cvpr lin roychowdhury ands maji bilinearcnnmod in iccv livni shalev shwartz ando shamir onthecom in nips lowe invariant keypoints ijcv majianda berg max marginadditivefor detection in iccv maji rahtu kannala blaschko anda vedaldi fine arxivpreprint arxiv olivaanda torralba ijcv pascanu mikolov andy bengio onthedifof in icml perronnin anchez andt mensink improvingthe kernelforlarge scaleimagein eccv phamandr pagh in kdd poggioandf girosi learning proceedingsoftheieee rahimiandb recht scale kernelmachines in nips rendle acmtrans tist roychowdhury lin maji ande learned miller facewithbilinearcnns arxivpreprint arxiv sch olkopfanda smola learningwithkernels sup portvectormachines regularization optimization andbe yond mitpress scholkopf achlioptas andm bernhard sampling nips simonyananda zisserman networksforlarge arxivpreprint arxiv szegedy liu jia sermanet reed anguelov erhan vanhoucke anda rabinovich in cvpr tenenbaumandw freeman separatingstyleand neuralcomputation vapnik springer science businessmedia vedaldianda zisserman efadditivekernelsvia explicitfeaturemaps pami wah branson welinder perona ands belongie thecaltech ucsdbirds dataset californiainsti tuteoftechnology williamsandm seeger usingthenystr ommethodto in nips yang yu gong andt huang linearspatialpyra in cvpr 
xinyuhuang xinjingcheng qichuangeng binbincao dingfuzhou pengwang yuanqinglin andruigangyang baiduresearch beijing china china huangxinyu chengxinjing gengqichuan caobinbin baidu com zhoudingfu wangpeng linyuanqing yangruigang baidu com abstract semantic label ysisofanimage pixel however providingalarge ingalgorithms hasbeen diftheper pensive inthispaper wepresentalarge scaleopendataset apolloscape that dpoint clouds ourdatasethas thestisitsscale our kimagesœeachwithits per pixelsemanticmask upto misscheduled thesecond isitscomplexity the hundred figure andthethirdisthe dattribute each ativeaccuracy thehigh quality dpointcloud moreover ourdatasetalso andstyles cludebutnotlimitedto dsceneunderstanding local ization transferlearning anddrivingsimulation introduction semanticsegmentation orsceneparsing ofurbanstreet tonomousdriving aimingtoincrease figure top dsemanticlabel middle bottom the cambridge camvid whichcontains semanticclasses thekittivisionbench marksuite opticalw forinstance training and dand dbound tationestimation carsand however pixel levelannota arxiv cs cv mar controls asaresult notprovideddirectly thecityscapesdataset focuses on classes and available onlyoneimage pet isannotated collects currently thiscould bethelargestdataset however asmentionedbyauthors itisnotpossibletoman therefore onlytwoseman ticclasses areprovided inthispaper wepresentanon goingprojectaimedto provideanopenlarge banstreetviews notation survey gradedense mentation night vision sensors ouron rangeofenvironment weather andtrafconditions com characteristics thesubset notations hasbeenreleased wedivideourdataset intoeasy moderate andhardsubsets thedif complexity dpoint clouds ourdatasethassurvey gradedense dpointcloudfor staticobjects eachimage creatingthepixel annotatedrgb ourdataset with classes aninteractiveandef djoint labelling onaverageit saves labelingtime basedonourlabelling pipeline allthe aboveannotations therefore ourdatasetisthe dannotations theinstance frames tracking andbehav figure upto sixvideocameras andacombinedimu gnsssystem at http apolloscape auto moredatawillbe addedperiodically acquisition rieglvmx ha halaserscanners fov rangefrom mupto mwithtargetre vitylargerthan vmx cs camerasystem two and gnss positionaccuracy mm roll pitchaccuracy andheading accuracy broom cameras comparingwithcommon usedvelodynehdl preci sion mm mm mid sizesuv figure km however dataset currently thatcontains pixel in thereleaseddataset instance levelannotationsfor whichcouldbepar andpredication table view datasets table cityscapes andourdataset instance level theletters and indicateeasy moderate instance total person vehicle averageperimageemh person vehicle car motorcycle bicycle rider truck bus tricycle easy moderate similarto thecityscapes suchaspersonandvehi cles table inthetable wealso jects datasets moreimportantly forinstance darkandbright ap ofanoverpass levelsegmentational gorithms trafconditions and weathers weannotate table theidsshownin thevalue in theoftheclasses forinstance thisclasscov ersallkindsofthree motorizedandhuman powered wealsoannotate theanno ingcolor whiteandyellow andtype solidand broken table markings classes thiskindofmarking labelingprocess figure dla belingand dlabeling objects thebasicideaofour whilesome forin stance ent therefore wetake scan alignthesepoint clouds tency inordertospeedupthe dlabelingprocess we over then welabelthese we alsore trainthepointnet model topre segmentthe mance wetheresultsby the dlabelingtool the fur whichincludes dro tation inverse selectionbypolygons matchingbetween andsoon oncethe theannotations ofstaticbackground objectsforallthe dimageframesare dprojections thesplat pointsorstrong tospeedupthe dlabelingprocess wetrainacnn andpre segmentthe images dimagesisdeveloped figure center table viewdatasets physicalworld per pixelsemanticlabel dlabels dvideolabels dlanelabels camvid kitti pp sparse cityscapes selectedframes toronto pp building roadselectedpixels synthia pp ours pp dense pp again thewrong couldbecausedbymerge lightingconditions our andadjusted figure presentsanexampleof dannotatedimage traf light in otherdatasets belingefforts benchmarksuite given dannotations dpixelandinstance levelanno tations backgrounddepthmaps we mainlyfocusonthe dimageparsingtask wewouldlike figure our figure dlabelingtool figure anexampleof imageparsingmetric andset ofpredictedlabels iou metric foraclass iscomputedas iou tp tp fp tn tp fp tn iou perbasedevaluation therefore weusepre frame basedevaluation however age weconsiderper frameevaluation weproposetheper andpredictedlabel letthe each predictedlabel willcontainper pixelprediction mean iou orj table movablecar objectmotorcycle bicycle person rider personon motorcycle bicycleor tricycle truck bus tricycle three wheeled vehicles motorized or human powered surfaceroad sidewalk movableand cone shaped markers bollard edwithmany differentshapes fence traflight pole trafsign wall trashcan billboard building bridge tunnel overpass naturevegetation voidvoid otherunlabeled objects and meanslabel truthlabel for threshold uation suchascargroupand bicyclegroup predicted table yellow white typecoloruseid solidwdividing solidydividing doublesolidwdividing nopass doublesolidydividing nopass solid brokenydividing one waypass solid brokenwdividing one waypass brokenwguiding brokenyguiding doublebrokenyguiding doublebrokenwguiding doublebrokenwstop doublesolidwstop solidwchevron solidychevron solidwparking crosswalkwparallel crosswalkwzebra arrowwrightturn arrowwleftturn arrowwthru rightturn arrowwthru leftturn arrowwthru arrowwu turn arrowwleft rightturn symbolwrestricted bumpn aspeedreduction visibleoldy wn marking ap as theapiscomputedfor the meanap map alltheclasses network thereleasedmodelis cropsize uniformsampling timesdataaug mentation and epochs table network using ktrainingimages iou movablecar objectmotorcycle bicycle person rider truck bus miou surfaceroad sidewalk infrastructurefence traflight pole trafsign wall building naturevegetation withonesinglescale andwithoutanypost processing steps theresnet network trainingimagesand test ingimages around ktrainingim agesand testimages table noticethat themiouformovable cityscapes inthiswork wepresentalarge scalecomprehensive datasetofstreetviews thisdatasetcontains higherscene dannotations andposeinformation levelannotations inthefuture rain andfoggyenvironments second panoramicimages inthecurrentrelease thedepthinfor wewould references brostow fauqueur andr cipolla semanticobject classesinvideo agroundtruthdatabase cordts omran ramos rehfeld enzweiler benenson franke roth andb schiele the in proc patternrecognition cvpr everingham eslami vangool williams winn anda zisserman challenge aretrospective putervision geiger lenz stiller andr urtasun visionmeets robotics thekittidataset research ijrr hariharan arbel aez girshick andj malik simul in europeanconfer enceoncomputervision pages springer he zhang ren andj sun deepresiduallearn in pages qi yi su andl guibas pointnet deephi in pages richter hayder andv koltun playingforbench marks in iccv riegl vmx ha http www riegl com online accessed march ros sellart materzynska vazquez anda lopez thesynthiadataset in pro patternrecognition pages velodynelidar hdl http velodynelidar com online accessed march wang bai mattyus chu luo yang liang cheverie fidler andr urtasun torontocity in proceedingsofthe tion pages wu shen anda hengel widerordeeper revis arxivpreprint arxiv xie kiefel sun anda geiger semanticin dto dlabeltransfer in pages 
proceedingsofthe andthe pages beijing china july 
information theoreticsemi gangniu gang sg cs titech ac jp tokyo japan bodai bohr dai gmail com purdueuniversity westlafayette in usa makotoyamada yamada sg cs titech ac jp masashisugiyama sugi cs titech ac jp tokyo japan abstract theoreticap proachcalleds eraph semi supervisedmetric sparsity formet assumption wemaximizetheen entropyreg ularization meaningfulway furthermore eraph isregu rankprojectionin ducedfromthemetric theoptimizationofs er aph stepandcon vexm step er aph known introduction labels sugiyama supervisedtyperequir ingweaklabels similarity weinbergeretal davisetal nolabelinformation belkin niyogi types and worldapplica appearingin proceedingsofthe th onmachinelearning edinburgh scotland uk copyright bytheauthor owner basedonthebeliefthat semi tothebestofour knowledge allsemi off the shelf techniquesintype analysis yangetal sugiyamaetal orman ifoldembedding hoietal baghshah shouraki liuetal pervisedmanner however forms riemannianmanifold differentgoals fisher laplacian eigenmaps belkin niyogi simplyputtingthem inthispaper weproposeasemi ingapproachs eraph semi paradigmwithhyper sparsity asan information theoretic alternative tothemanifold basedmethods ourideaisto following entropyregular ization grandvalet bengio wemaximizetheen andminimizeit information theoreticsemi onunlabeleddata grac aetal thelowun certainty furthermore weemploy mixed normregularization yingetal the bringsustothe hyper sparsity thankstothisproperty the metriclearnedbys eraph first we timation dud schapire second weproposea semi grandvalet bengio basedextension whichmeansthats eraph couldadoptan proposedapproach inthissection eraph and thendeveloptheem notations thatcontains pointseachwith features letthesetsof and aresimilar and aredissimilar wereferto asthe labeleddataand dg astheunlabeleddata aweaklabel isassignedto or to weabbrevi ate and as and metricfor oftheform where isasym the probability oflabeling with whenap plying to itisabbreviatedas inthispaper basicmodel tobeginwith given of sampleability the jaynes suggests datamoments let ln and the max where isaslackvariableand isaregularization parameter generalized dud schapire see appendix theorem theprimalsolution isgivenintermsof thedualsolution by exp where exp and min ln theregularizedlog data onobservedweaklabels as ln then theregularizedmax imumlog throughoutthispaper weadoptthat ln information theoreticsemi whenconsidering wepropose where separatethesets and underthetargetmetric becomes exp wehopefor sotheremustbe althoughweuseeq asourfeaturefunction otherop tionsareavailable pleaseseeappendix fordetails regularization inthissubsection weextend byentropyregular izationtosemi supervisedlearning moreover weregular normregularization jectedtrainingdata wefollowthe grandvalet bengio andhence roughlyspeaking low densityseparation subsequently accordingto grandvalet bengio max ln ln where inaddition itybyencouragingalow dimensionalsubspace itis amatrix max ln ln tr where tr isthetraceof and isaregularization parameter optimization isthemodelofs eraph andwesay sparsitywhenboth and arepositive eraph ifoldextensions formoreinformation pleaserefertoap pendix and algorithm andderivea practicalalgorithm first weeliminate from thanks in theorem max ln ln tr exp let and and respectively then ters and suchthat isequivalentto withrespect to andtheresulting parameterized by and param eterizedby and remark afterthe isdropped and remains thesame optimization projectionmethod polyak eventhoughitisnon convex nevertheless initialsolution and justtonameafewofthe theem inthebeginning and thenthem stepandthee atthe the step similarlyto grac aetal wehave foreachpair that min kl jj ln where kl isthekullback leiblerdivergence and is foundatthelastm step optimization within information theoreticsemi theorem thesolutionto isgivenby exp ln exp ln ontheotherhand atthe thm step wenewmetric whichisgener atedinthelaste stepandonlyfor max ln ln tr rf givenby rf yq where sincetheconvexityof thefeaturefunction withrespectto implies formlybounded themag nitudeof tr theorem theobjective lip withrespectto thefrobeniusnorm kk lip diam where diam max isthediameter of and theinitialsolutionis asdissimilarpairs ityis stepandthewholeem dis discussions inthissection namely we canobtainthe posteriorsparsity grac aetal byen projectionsparsity yingetal bytrace normregularization bya sparse wemeanthattheun certainty theentropyorvariance islow seefigure asanexample dataset euclidean non sparse non sparse sparse sparse figure sparsevs non sixweak theleftthree learnedwith therightthreepan elsexhibitone nearest thisresultsinthe theverticalfeature however tant and therefore we andthentheriskofov wecanrewrite pr objective grac aetal functionbe ln thenmaximiz ing isequivalentto max ontheotherhand of grac etal max min kl jj information theoreticsemi dataset euclidean non sparse non sparse sparse sparse figure sparsevs non sparseprojections thesettingsandthe where areslackvariables since isunconstrained we and itiseasyto seethat shouldbe restrictedon sothekltermis whichimplies and wealsohopeforthe projectionsparsity seefigure asanex normregularization argyriouetal denotethe normofasym metricmatrix as similarlyto yingetal let beapro jection and let the thcolumnof and be and if isiden ticallyzero the thcomponentof hasnocontributionto px sincethecolumn wisesparsityof and are equivalent wecanpenalize toreachthecolumn wisesparsityof nevertheless sionalityreduction extratransformation torotate beforetheprojec tionwhere andadd consequently wepenalize project to pvx andsince pv pv wv wearriveat max wv and isguaranteed bylemma of yingetal moreover infor gomesetal please seeappendix fordetails relatedworks xingetal wisesimilarity metriclearning dm including ca goldbergeretal mnn weinbergeretal andinformation theoreticmetric learning tml davisetal bothi tml ands eraph areinformation theoretic butthe tml agen erativemodel exp where comparedwithg dm tml regularizesthekl divergence between and log detregularization byspecifying itbe thus er aph butthe probabilisticmodel isdiscriminative aprobabilisticg dm yangetal itisaspecial infact eraph ismuchmore general pleaserefertosection fordetails subsequently dm yang etal isthepioneerofsemi ing aretheprin hoietal com maxprincipleof dm basedon belkin niyogi and baghshah shouraki showsthat roweis saul is alsousefulforsemi liuetal bringstheelement wisesparsityto hoietal canbe whereasourunsu information theoreticsemi table ofbenchmarkdatasets classes features training test classlabels iris wine ionosphere balance breastcancer diabetes usps usps usps usps mnist mnist how ever supervisedex tensions mnn can duetolimitedspace instead werecommend huang etal and huangetal forthelatestreviews experiments setup wecompareds eraph four supervised dm xingetal ysis ca goldbergeretal largemarginnearest neighbor mnn weinbergeretal information tml davisetal dm yangetal fda baghshah shouraki table ourexperiments thetopsixdatasets iris wine iono sphere balance breastcancer anddiabetes comefrom the whilethe usps and mnist gray to and and thesym fda http archive ics uci edu ml http cs nyu edu ÿroweis data html bolusps means classes usps means trainingdatafrom eachofall classes mnist meansdigits versus andsoforth thedimension inourexperiments randomsamplings classlabels andthesets and thesizesof and datasets nearest foursettingsofs eraph eraph none standsfor eraph post for and eraph proj for and ands eraph hyper for and weed forsimplicity therewasnocross uponthe pervisedinformation ods thenumberof nearestneighbors nents results figures and datasetsrespectively and inboth information theoreticsemi table neighborrate in onuci for eachdataset testatthelevel uclidean dm ca mnn tml dm fda eraph none eraph post eraph proj eraph hyper usps usps usps usps mnist mnist uclidean dm ca mnn tml dm fda eraph none eraph post eraph proj eraph hyper wereobtainedbyg dm while and weregen eratedbys eraph with infigure and infigure wecanseefromfigures and thats eraph nearest neighborclas arereportedintable dm wassometimesvery eraph isfairly promising sparsity and andoftenstatis cepti tml onusps ands eraph hyper outperformed boths eraph post ands eraph proj moreover itimproved posedmnisttasks thoughthe inaword eraph andhenceoursparsity invividcontrastwiths eraph izationcapability problems especiallyn ca one out the powerfull mnn sinceitwas tml wasthesecondbest but ontheotherhand weobservedthatl dm ofthetargetmatrix andm fda performedthelocalone finally wereportinfigure algorithmoneachtask excludingg dm generallyspeak ing eraph andthefastest fda gleeigen decomposition information theoreticsemi thetaskid iris wine ionosphere balance breastcancer diabetes usps usps usps usps mnist mnist figure computationtime perrun conclusions inthispaper theoreticsemi eraph asanalterna tivetothemanifold basedmethods thegeneralizedmaxi ourfoundation thenasemi ularization moreover weenforcedatrace normregular theresulting likeschemewithsev eraph of the artfully semi supervised formation helpfulforhigh dimensionaldata ifandonlyif theywere sparsity anin futurework acknowledgments helpfulcomments arshipno program references argyriou evgeniou andpontil multi taskfeature learning in nips baghshah andshouraki semi in ijcai belkin andniyogi in nips bellare druck andmccallum in uai davis kulis jain sra anddhillon information in icml dud andschapire in colt fisher problems annalsofeugenics goldberger roweis hinton andsalakhutdinov in nips gomes krause andperona in nips grac ganchev taskar andpereira posteriorvs in nips grandvalet andbengio semi tropyminimization in nips hoi liu andchang semi in cvpr huang ying andcampbell gsml aframe in icdm huang jin xu andliu smoothoptimization in uai jaynes phys icalreview liu ma tao liu andliu semi supervised tion in kdd polyak in russian roweis andsaul science sugiyama journalofmachine learningresearch sugiyama id nakajima andsese semi supervised machinelearning weinberger blitzer andsaul nips xing ng jordan andrussell distancemetric information in nips yang jin sukthankar andliu anefalgo in aaai ying huang andcampbell smoothoptimization in nips 
systemrecommendation lilizhao sinnojialinpan evanweixiang erhengzhong zhongqilu qiangyang hongkong singapore baiduinc china huaweinoahõsarklab shatin hongkong lzhaoae ezhong zluab qyang cse ust hk àjspan star edu sg evan xiang gmail com ones sparsityissue where recently able however elsassumethatentity whichmeansthatforany entity auseroranitem inatargetsystem its this worldsce narioswhereentity usuallyunknown beexpensive forexample identifywhetherauser inthispaper we speciþcally transfer wethenplug þlteringmodel weperformexten introduction cf technologies especiallymatrix factorizationmethods cfaimstogeneraterec however cfperforms whichisa ommendersystems copyright intelligence www aaai org allrightsreserved recently transferlearning panandyang hasbeen recommendersystems similarusers twitter facebook etc orprovidesim ilarproductitems amazon ebay etc thus asource li yang and xue system previoustransfer systemcf cfmethods withcross and those withoutcross systementity correspondence intheformercategory mehtaandhofmann andpan etal proposed toembedthecross systementity correspondencesascon althoughtheseap theyrequiretheex suchasusercor acrossdifferentsys tems mostreal worldscenarios forexample timesoverlapping asisthecase withamazonandebay inaddition eventhoughtheremay tems consumingtoberec oranitemmaybe pre existingcross systemmappings researchershavefo for example li etal proposedacodebook based transfer cbt methodforcross domaincf whereentity themain havesimilarly therefore ofcluster leveluser main thecodebookcan system li etal modelforcross vationwithcbt however theformercategory whichmakeuseofcross systementity inthispaper weassumethatthecross systementity butthatthese inparticular we withpartialentity systemcf components and anextendedtransfer partialentity domain cf notations preliminaries denoteby datargetcftask md nd ndisthenumber ofitems eachentry uvofx correspondstouser uõspreferenceonitem if uv itmeansforuser thepreferenceonitem visobserved otherwiseunobserved pairsof the tems incross systemcf besidesd swhichis ms ns where nsisthe numberofitems similarly let isbethesetofallob served pairsof furthermore weassumethatthe cross systementity butcan beidentiþedwithcost ourgoalisto activelyconstruct entity withbudget and fromthesourcetask stothetargettask inthesequel we denoteby andsuper script andusethewords maximum maximum mmmf srebro rennie andjaakkola servedmatrix encematrix speciþcally the minimizej ih yuváxuv pairsof max isthehingeloss de notesthetracenorm and isatrade offparameter yuv denotesthat userulikestheitem while yuv denotesdislike theobjective tions andsolvedefþciently rennieandsrebro sup posexuv onecanuse thresholds torelatethereal valued yuvtothediscrete valued xuvbyrequiring xuv yuv xuv where and furthermore suppose ycanbedecomposedas where mandv min ir truv ur uv wheretruv forr xuv while truv forr uv and thethresholds ur udandvdfromthe data ur õshereareuser speciþc systemcf overallframework inthissection algorithm tobeginwith afterthat weitera tivelyselect system detail respectively denoteby candv cthefactorsub matricesof andv respectively similarly denoteby candv cthefactorsub matrices crespectively here canbeei therusersoritems algorithm systemcf input and output and initialize apply on togenerate andv for to dostep setc activelearn wherec ties eitherusersoritems and step queryc correspondingindices step applymmmf tl toupdate and endfor return correspondences tocross min ir truv ur ut uv cr candv candv andv respectively theassociated isatrade off intuitively matrices candv matrices candv wherew cv andw cv this factorization cmf singhandgordon andmaynot holdinpractice inthesequel asabaseline wedenoteby mmmfcmf into alternatively the similartoeachother therefore tr cl cw tc wheretr and uisknownasthe laplacianmatrix where cu cisthesimilarity whoseindicesarein theset and uii ja uij thedeþnitionof vonitemsissimilar by liandyeung however theirworkisfocusedon domaincf and tiesinsteadofentity inthe sequel wedenotebymmmf tltheproposedmmmfex tensionbyplugging into inthissection wedescribeamargin basedmethodforac acommonmo tivationbehindmargin thatgivenamargin basedmodel themarginofanexam the thelowerthecertainty isforitsprediction marginsonuser itempairs supposethatmmmf or mmmftl targetsystem thengivenauser foreachthreshold wherek themarginofauser itempair canbedeþnedas uv ify uv ify wherey vifx visobserved otherwise uv foreachuser itempair wehave margins amongthem the marginstotheleft lower andright upper boundariesof whichwe denoteby and respectively similarto othermargin weassumethat fortheunobserveduser itempairs õs thepredictions itempairsaswell in tuitively forapair when the wedeþneanor malizedmarginofauser itempair asfollows notethat marginsonentities itempairin entity eitherauseroranitem forsimplicity intherestof thesection auser deþnition aswell withmdusersand nditems auser ucanberepresented ndpairsintotal item pairs õs furthermore foreachuser ratingson someitemsareobserved idu whoseitemindicesare denotedby idu asfollows idu idu idu idu ratings thetradeoffparameter istobalancethe in thispaper wesimplyset wepro posetwouser mgmin ineachiteration õs and selectthetop kuserstoconstruct cforquery currentcfmodel however duetothe long tailproblemincf parkandtuzhilin manyitemsorusersin thus themostun furthermore temsbesimilar targetsystem tail thisimpliesthatthe factorsub matrices candv ctobetransferredfrom resultinginlimited thus weproposeanother user mghybrid ineachiteration weþrstapplymg mintose lectk users denotedby where afterthat fortherestusers ui õs þnedin andselect userstoconstruct finally weset ui cu uj cusim ui uj ui uj cusim ui uj wheresim ui uj idui iduj max idui iduj isthemeasureof ingbehaviors informative withlargevaluesof ui õs andthussupposedtobe ofstrongcorrelation tothepre withlarge valuesof uj cusim ui uj andthussupposedtobe sicassumptionincf net ßix anddouban userson around movieswithratingsin douban itcontains booksandmusicwith ratingscale fornetßixdataset whilesharingthewhole setofmovies userswith ratingdensity domain domainwith ratingdensity fordouban wecollecta datasetconsistingof usersand itemswith onlymoviesandbooks userswithlessthan ratingsare discarded thereremain ratingson books and ratingson movies givenby users and respectively in thistask further more sincethereareabout moviessharedbynetßix anddouban andobtain ratingsgivenby and ratingsgivenby usersfrom netßixwithdensity main intotal systemcftasks and denoteby netßix netßix doubanmovie doubanbookandnetßix doubanmovie respectively intheexperiments atrainingsetof preferenceentries ran domtimes thenumber oflatentfactors taretuned onsomehand outdataof netßix netßix andþxedtoall experiments here and forevaluationcri terion rmse deþnedas rmse xuv öxuv tively and thesmalleris thevalue intheþrstexperiment http www netßix com http www douban com supposetotalbudgetis table tasks methodsnotransf ocorr notransf corr ocorr corr corr netßix netßix movie book douban netßix doubanmovie cross toapplystate of the inthispaper forstate of the artcfmodels weuselow mf koren bell andvolin sky andmmmf to domainentity correspondences userma trix finally weapplystate of the artcfmodelsonthe cbt toapplythecodebook based transfer cbt mendations cbtdoes notrequireanyentity toapplythepro posedmmmf datawithfullentity tions domain tasksareshownintable gies weusemg hybrid asproposedin ascanbeob apply ingstate of the usermatrixandthen performingstate of the butmayevenhurt sparse fromthe however tlwithonly entity thisveri systementity finally byconsideringthe performanceofmmmf tlwithfullentity correspondences astheknowledge transferupperbound andtheperformance ofmmmfasthebaseline knowledgetrans entity experimentsondiff here weuse domaincf besidesthetwo strategies mg minandmg hybrid troductionsection rand many sourcedomain few sourcedomain mgmax õsasde þnedin figure tlwithdifferenten fromtheþgure wecan observethatthemargin basedapproaches mg min mgmax andmg hybrid inaddition compared withmg minandmg max ourproposedmg hybrid notonly experimentsondiff cross domainregularizers theregu larizationterm in forcross entforms or learningapproaches mmmf cmf ormmmf tlaccord ingly therefore inthethirdexperiment weusemg hybrid proportion of labeled correspondences unit rmse netßix netßix proportion of labeled correspondences unit rmse movie book douban proportion of labeled correspondences unit rmse netßix doubanmovie figure proportion of labeled correspondences unit rmse mmmfcmfmmmftl netßix netßix proportion of labeled correspondences uint rmse mmmfcmfmmmftl movie book douban proportion of labeled correspondences unit rmse mmmfcmfmmmftl netßix doubanmovie figure andcomparetheperfor manceofmmmf cmf andmmmf tlintermsofrmse theproposedmmmf tloutperformsmmmf cmf correspondences relatedwork thereare forcf pan etal tif tointegratethe cao etal andzhang etal domain cfproblemsinamulti ingtocf shi zhao andtang mello aufaure and zimbrao rishandtesauro jinandsi boutilier zemel andmarlin whichassumedthatthe tem however itemsofthesystem them alternatively most siþcationtasks sahaetal raietal shi fan andren chanandng inthispaper ourstudy sparsityproblemincf theexistingframe inthispaper ferlearningforcross inthepro posedframework we correspondingman ner and recommendersystems fullentity correspondences achieve knowledge transferratio whileonlyrequires forfuture work otherapplications suchascross socialnetworks acknowledgement and references boutilier zemel andmarlin active in uai cao liu andyang transferlearningfor inicml chan andng domainadaptationwith in acl jin andsi in uai koren bell andvolinsky matrixfac computer li andyeung trixfactorization in ijcai li yang andxue canmoviesandbooks collaborate cross sityreduction in ijcai li yang andxue transferlearningfor inicml mehta andhofmann crosssystemper alignments in ki mello aufaure andzimbrao active in recsys pan andyang ing ing pan xiang liu andyang trans inaaai pan xiang andyang transferlearning in twenty park andtuzhilin thelongtailofrecom in recsys rai saha daum iii in naaclhlt languageprocessing rennie andsrebro fastmaximum in icml rish andtesauro in isaim saha rai iii venkatasubramanian and duvall inecml pkdd shi fan andren knowledge in ecml pkdd shi zhao andtang batchmodeactive systemsandtechnology singh andgordon relationallearning in kdd srebro rennie andjaakkola maximum in nips tang yan ji zhang guo liu wang andchen backs in aaai zhang cao andyeung multi domain in uai 
ijcai ijcai ijcai ijcai ijcai ijcai ijcai ijcai 
cnn rnn aframeworkformulti labelimage jiangwang yiyang junhuamao zhihenghuang changhuang weixu baiduresearch facebookspeech horizonrobotics cnns have labelimage tainmultiplelabels objects scenes tradi labelimagelearn thesetech niques althoughworkingwell inthispaper weutilize rnns toaddressthisproblem combinedwithcnns theproposedcnn rnnframework learnsajointimage labelrel evance anditcanbetrainedend to endfromscratchto exper thanthestate of the artmulti labelmodels introduction everyreal labels manticinformation suchasobjects parts scenes actions modelingtherichse imageunderstanding asaresult multi in networksin single years end to endframeworks workfor multi labelimage label label problems ingloss orthecross entropyloss however when sky grass runwaydog person roomperson hat nike figure dataset foreachimage thereisonlyonela bel airplane greatpyrenees archery annotatedintheim agenetdataset however multiplelabels previousworks haveshownthatmulti labelproblemsexhibit stronglabelco forinstance sky while waterandcars almostneverco occur mostexistingworksare amongwhichacommon occurrenceprob toinferthe however whendealingwitha largesetoflabels meanings moreover modelhigher ordercorrelations orcompu tionships inthispaper rnns to capturehigher wethatrnnsig improvesaccuracy forthecnnpart sameimagefeatures however whenusingthesame objectsthatare ognizeindependently inthiswork wedesignthernns implicitlyinthecnn rnnstructure theideabehindit forexample when ourmodel runway person hat airplane greatpyrenees archery recognizebyitself contexts finally forexample it image labelembedding metriclearning orlearn ingtorankmethods thejointimage labelembedding low and thejointembed callysimilarlabels however thelabelco occurrencede inthispaper weproposeacnn rnnframe workformulti occurrence dependencyinanend to endway theframeworkofthe themulti labelrnn modellearnsajointlow dimensionalimage labelembed labels vector thehigh orderlabelco thislow whichmaintainsthein duringprediction themulti theproposed cnn labelembedding vgg convnet recurrent neurons joint embedding space shipseaendfigure rnnframeworkformulti labeldependency ageembeddings respectively therecurrent ateachtimestep theprobabilityofa bestviewedincolor andlabelco occurrencemodels end to endway comparedwithstate of the artmulti labelimageclassi methods advantages to endmodeltouti occurrence dependency tivemulti label erfulmodelofhigh orderlabelco occurrencedepen task rnnframeworkwith labelbenchmark datasetsinlcudingnus wide microsoftcoco and pascalvoc of the artmulti label methods works interestingly whichisverysimilar tohumans multi labelprocess relatedwork creationoflarge scalehand genet ralnetworks labelachieves goodresults optimizesa top rankingobjective hypotheses cnn pooling thesemethods lationsbetweenlabels multi ingajointimage labelembedding multiviewcanonical correlationanalysis isathree waycanonicalanaly sisthatmapstheimage label samelatentspace wasabi anddevise learn withwarploss metriclearning learnsadiscrimi labelsimilarity matrix completion andbloom canalsobeemployed aslabelencodings thelabelco occurrencedependency belco labelimageclassi thelabel co calmodels depen dencynetwork andco occurrencematrix label augmentmodel labelcombinations thelow dimensional orderla belcorrelation term ithasbeensuccess machinetransla tion speechrecognition languagemodeling wedemonstratethat pendency input gate forget gate output gate update term xtitftotr figure eachlstm neuronhasaninputgate aforgetgate andanoutputgate method orderlabelcorre lation lstm neu rons whichhasbeendemon termdependency lstm rnn itcanbeconsidered temporaldependencies aforgetgate tocontrol aninputgate toindi anoutputgate tocontrol learnlong andmakeitis easiertooptimize statesr lstmalsoef explodingissues ww ww ww where istheproductwith gatevalue andvarious ters inourimplementation weemploylinearunits relu model weproposeanovelcnn rnnframeworkformulti labelproblem itcontainstwoparts ages beldependency wedecomposeamulti predictionpath forexample ﬁzebraﬂ ﬁelephantﬂ or ﬁele phantﬂ ﬁzebraﬂ theimage label and textrelationshipas ittakestheembed productofthea alabel isrepresentedasaone hotvector whichis atthe thlocation and elsewhere plyingtheone hotvectorwitha labelembeddingmatrix thek throwof thedimensionof beroflabels andmodelstheco occurrencede recurrentstates bylearningnon linearfunctions wherer ando respectively isthe labelembeddingofthe and arethenon linearrnnfunctions whichwill dimensionalspaceas thelabelembedding whereu andu respectively the current labellabel embeddingek recurrent layer wk projection layer image convnet iprediction layer predicted label probability ulultfigure theoutputlabel numberofcolumnsof andu arethesameasthela belembeddingmatrix wewillshowinsec thatthe finally thetransposeof andx inference apredictionpath isasequenceoflabels bell thernn argmax kp argmax kp sincetheprobability doesnothave markovproperty wecanem whichpredictslabel argmax attimestep andthela belprediction atlaterpredictions however thegreedy iswrong cat dog person endcar cat dog person endcar cat dog person endcar cat dog person endcar cat dog person endcar cat dog person endcar figure sizen paths becorrectlypredicted thus algorithmtothetop rankedpredictionpath infigure blelabel intermediatepaths ateach timestep fp attimestep weadd intermediatepath pi togetatotalof paths the pathsconstitutethe intermediatepaths fortimestep thecandidatepath setc it greaterprobability training learningcnn ingthecross ofscoresoftmax andemployingback propagation throughtimealgorithm ishing explodingissues algorithm wekeepthe tationforsimplicity labelcnn rnn intheex perimentsofthispaper trainingdata lessfrequentones difobjects semblesasproposedin order mance ordersineachmini batch diftoconverge experiments inourexperiments thecnnmoduleusesthe lay ersvggnetwork pretrainedonimagenet clas challengedataset framework andoflstmrnnlayerare and respectively we momentumrate and dropout rate multi labeldatasets nus wide microsoft coco andvocpascal datasets theevaluation performancetostate of the artmethods wealsoquali bel evaluationmetric foreachimage wegenerate atedlabels truthlabels wealsocomputetheper pando andrecallscores rando wheretheav re spectively thef ando wealsocompute map nmeasure nus widedataset tains imagesand tagsfromflickr therearea totalof these conceptsby forrnnmodel labelsarepredicted figure wide left andms coco right datasets theground model spredictions methodc pp rc po ro map metriclearning multi edgegraph knn softmax warp jointembedding cnn rnn table comparisonsonnus widedataseton conceptsfor humanannotators fig thequalityof whilethe infig suchas ﬁcloudﬂandﬁcloudsﬂ suchas ﬁphotographerﬂ meanings suchasﬁbeautifulﬂ conceptslabels withstate of the search softmaxprediction warpmethod metric learning andjointembedding intable since thereislessnoisein conceptslabels allmethodsachieve althoughwedonot the of the art methods inparticular wethecnn rnnframework achieves higherprecision dslr taglabel setintable odsareverylow verynoisy thebaselinemethods methodc pp rc po ro map softmax dlsr warp cnn rnn table comparisonsonnus widedataseton tagsfor methodc pp rc po ro map softmax warp binarycross entropy nornn cnn rnn table comparisonsonms cocodatasetfor distinguishgender tressﬂ onimagenet task moremulti microsoftcoco microsoftcoco ms coco dataset isanimage recognition segmentation andcaptioningdataset itcon tains thousandimagesof objectstypeswithper amongthoseimages and imagesare asthelabels ofthems an andthese forexample co appeartogether wecomparethesoftmax multi labelbinarycrossen tropy andwarp modelswiththecnn rnnmodelin table wedonotsettheminimum itcan it hasaslightlylowerper lessthan occurrencede wealsoreplacethere wethatre themulti label theper figure theper cocodataset mapinria cnn svm ft hcp cnn rnn table results apin dataset workisshowninfig suchasﬁpersonﬂ ﬁze braﬂ andﬁstopsignﬂ suchasﬁsportsbarﬂ andﬁbaseballgloveﬂ suchas wecan islarger wholeimage pascalvoc voc datasets abelvoc datasetcontains im agesdividedinto train valandtestsubsets weconduct ourexperimentson trainval test splits images ap andmeanofap figure map thecomparisontostate of the artmethodsisshownin table inria extraction coding poolingpipeline cnn svm di trained onimagenet ft multi labelhcp employsregion glacierarctic norway volcano tundra lakes sky nature blue clouds landscape bravo sunsetsun landscape light bravo yellow railrailway track locomotive tracks steam catdog bear bird hairdrier toaster cow horse sheep bear zebra elephant table klabelsof nus wideandms cocodatasets hawk eagle fauna wind bird glacier volcano cli arctic lakes portraits costume female asian hat portraits people street hospital woman landscape mountain nature mountains bravo bird hwak nature bravo birds images knnclassi cation figure labelseton nus widedataset trainedonimagenet dataset andachivesbetterper mation theproposedcnn thanhcp cmethod labelembedding the cnn image embedding beddingspacefornus wideandms coco areshown intable relatedtoitsnearest neighborlabels fig nus wide embeddingw andimageembedding inthejointem beddingspace semanticallyrelevant moreover wethatcomparedto thetop the example figure label framework truthlabels ﬁelephantﬂ andﬁzebraﬂ thebottom satten tioninthebeginning andthebottom rnnframe work labelmodel infig givenan inputimage tionalnetwork ﬁelephantﬂandﬁzebraﬂ atthebe ginning af thevisualization weproposeacnn rnnframeworkformulti labelembedding andlabelco rnntomodelthelabelco jointimage labelembeddingspace of the artmethods however predictingsmallob itisaninterestingdi butalsopredictthe tentionmodel references cabral torre costeira anda bernardino labelimagein pages chua tang hong li luo andy zheng nus wide areal in page acm cisse usunier artieres andp gallinari ro in pages dahl sainath andg hinton improving anddropout in acoustics icassp pages ieee deng dong socher li li andl fei fei imagenet alarge cvpr ieeeconferenceon pages ieee everingham vangool williams winn and zisserman voc chal lenge frome corrado shlens bengio dean mikolov etal devise adeepvisual semanticembed dingmodel in systems pages ghamrawianda mccallum collectivemulti labelclas in proceedingsofthe pages acm gong jia leung toshev ands ioffe deep arxivpreprintarxiv gong ke isard ands lazebnik amulti view tags and theirsemantics graves mohamed andg hinton speechrecog in acoustics icassp ieeeinter nationalconferenceon pages ieee guillaumin mensink verbeek andc schmid tagprop annotation in computervision ieee pages ieee guoands gu multi labelusingcon in ijcaiproceedings vol ume page harzallah jurie andc schmid combiningef computervision ieee pages ieee hochreiterandj schmidhuber longshort termmemory neuralcomputation jia shelhamer donahue karayev long gir shick guadarrama andt darrell caffe convolu arxivpreprint arxiv krizhevsky sutskever andg hinton imagenet in pages leandw zuidema arxivpreprint arxiv li lin rui rui andd tao adistributedap neu ieeetransactionson li zhao andy guo multi labelimagec uai lin maire belongie hays perona ra manan doll ar andc zitnick microsoftcoco com monobjectsincontext in computervisionœeccv pages springer lin ding hu lin ands ge imagetagcom pletionviadual com liu yan rui andh zhang taganalysis withmulti edgegraph in pages acm makadia pavlovic ands kumar anewbaselinefor imageannotation in computervisionœeccv pages springer mao xu yang wang anda yuille deepcap rnn iniclr pascanu mikolov andy bengio onthedif arxivpreprint arxiv razavian azizpour sullivan ands carlsson cnnfeaturesoff the shelf nition in shops cvprw ieeeconferenceon pages ieee read pfahringer holmes ande frank classi chainsformulti label machinelearning simonyananda zisserman networksforlarge arxivpreprint arxiv sundermeyer schl uter andh ney lstmneuralnet in interspeech sutskever vinyals andq le sequencetosequence in pages szegedy liu jia sermanet reed anguelov erhan vanhoucke anda rabi novich arxivpreprint arxiv tielemanandg hinton lecture rmsprop divide coursera turpinandf scholer in proceedingsof the pages acm vinyals toshev bengio andd erhan show andtell arxivpreprint arxiv wei xia huang ni dong zhao and yan cnn single labeltomulti label arxivpreprint arxiv weston bengio andn usunier wsabie scalingup in ijcai volume pages wu jiang li tang lu zhang and zhuang cross representation imageprocessing ieeetransactionson xue zhang zhang wu fan andy lu correlativemulti labelmulti in computervision iccv enceon pages ieee zeiler krishnan taylor andr fergus in computervisionandpat ternrecognition cvpr ieeeconferenceon pages ieee 
stackgan texttophoto hanzhang taoxu hongshengli shaotingzhang xiaogangwang xiaoleihuang dimitrismetaxas rutgersuniversity lehighuniversity baiduresearch han zhang dnm cs rutgers edu tax xih lehigh edu hsli xgwang ee cuhk edu hk zhangshaoting baidu com to givendescriptions andvividobjectparts inthispaper weproposestacked stackgan togenerate photo scriptions ageablesub reþnementprocess thestage yield ingstage ilow resolutionimages thestage iigantakes stage andgener ateshigh realisticdetails it iresultsandaddcom toimprovethe oftheconditional gan comparisonswithstate of the realisticimagescondi introduction generatingphoto includ ingphoto editing computer aideddesign etc recently gan have worldim ages conditional bcdbc aa efbdefb efbdefb figure one images giventext descriptions stage siccolorsofobjects yieldinglow resolutionimages stage ii ofstackgantakesstage andgenerateshigh realisticdetails resultsbyavanilla of the artgan int cls itisun resolution gans however high resolutionphoto tions of the resolution seefigure the resolutionimagesby sionalpixelspace reed etal onlysuc imagesconditioned ontextdescriptions vividobjectparts beaksandeyesofbirds more over ofobjects wedecompose twomoretractablesub adversarialnetworks stackgan low resolutionimages igan seefigure on thetopofourstage igan westackstage iigantogen eraterealistichigh resolution imagescon ditionedonstage seefig ure iresultandthe textagain stage igananddrawsmorede tailsfortheobject bution iiganis resolutionimages inaddition forthetext to imagegenerationtask the thus weproposea itallows photo realistic imagesfrom textdescriptions ofgeneratinghigh theart agesof resolutionwithphoto realisticdetails fromtextdescriptions eratedsamples nents ourcodeisavailableat https github com hanzhanggit stackgan relatedwork computervision niques vae for lihood autoregressivemodels pixelrnn that syntheticimages recently works gan resolution images severaltechniques anenergy basedgan conditionalimage mostmethodsutilized bels agestogenerateimages do maintransfer andsuper resolution how ever super resolutionmethods canonlyaddlimited detailstolow recently several structuredtext mansimov etal builtanaligndraw thegeneratingcanvas reed etal usedconditionalpix nguyen etal usedanap conditionedontext however with conditionalgan reed etal plausible descriptions theirfollow upwork wasabletogener ate objectpartlocations there isalsowork agegeneration wang etal gan incontrast thesecond dentonetal cianpyramidframework concurrenttoour work huang etal multi model however images generate imageswithphoto realisticdetailsand sixty fourtimesmorepixels togeneratehigh realistic details itdecomposesthetext to image seefigure stage igan description randomnoisevector yieldingalow resolutionimage stage iigan resolution imagefromstage producingahigh resolutionphoto realisticimage preliminaries gan arecom petewitheachother thegenerator gisoptimizedtore pdatabygeneratingim dtodifferentiate fromrealimages meanwhile overall playermin max min gmax dv ex pdata log ez pz log pdata pz conditionalgan ditioningvariables yielding andd this formulationallows variables asshowninfigure thetextdescription tisþrsten codedbyanencoder in previousworks inputofthegenerator however dimen sions itusuallycausesdis whichisnotdesirable we öc incontrasttothe cin werandomly distribution wherethemean embedding textpairs tofurtherenforce overþtting dkl whichisthekullback leiblerdivergence kldivergence stage igan resolutionimage wesimplifythetaskto þrstgeneratealow igan orsfortheobject let trainedencoder inthispa per öc fortextembed dingaresampledfrom tocapturethe meaningof twithvariations conditionedon öc andran domvariable stage andthegenerator ld ineq andminimizing lg ineq ld pdata log ez pz pdata log öc lg ez pz pdata log öc dkl wheretherealimage tarefrom pdata pz gaussiandistribution inthispaper thetwotermsineq weset forallourex periments in both and restofthenetwork modelarchitecture forthegenerator toobtain öc thetextembedding tisþrst figure thestage igeneratordrawsalow conditionedonstage iresults the stage iresults resolutionimage and forthegaussiandistri bution öc gaussiandistribution our vector öc iscomputedby öc where istheelement wisemultiplication then öc isconcatenatedwitha atea imagebyaseriesofup samplingblocks forthediscriminator thetextembedding tisþrst compressedto connectedlayer md md ndtensor meanwhile down md mdspatialdimen sion then theresultingten sorisfurtherfedtoa finally afully sionscore stage iigan low iganusu tions þrststage realisticim ages ourstage iganresults togeneratehigh resolutionimages itisconditionedon low iresults thestage iigancom photo realisticdetails conditioningonthelow resolutionresult öc öc thediscriminator dandgenerator ginstage nativelymaximizing ldineq andminimizing lgineq ld pdata log es pg pdata log öc lg es pg pdata log öc dkl therandom gaus öcusedinthisstageand öc usedinstage igansharethesamepre trainedtextencoder however stage iandstage standarddeviations inthisway stage iiganlearnsto omittedbystage igan modelarchitecture wedesignstage iigeneratoras anencoder sim thetextembedding tisused togeneratethe öc mg mg ngtensor meanwhile thestage iresult generatedbystage igan isfedintoseveraldown samplingblocks encoder un mg mg theimagefeatures mension whicharede signedtolearnmulti andtextfeatures finally aseriesofup samplinglayers decoder areusedtogeneratea hhigh resolutionimage high resolutionimage forthediscriminator stage samplingblocks toexplicitlyen theconditioningtext nator weadoptthematching byreed etal forbothstages duringtraining the whereasnegativesam theup neighbor stride convolution batch normalization theresidualblocks consistof stride convolutions batchnormalization andrelu stack models the down stride convolutions exceptthattheþrst bydefault ng nz mg md nd andw fortrain ing andg ofstage igan for epochsbyþxingstage iigan thenweiteratively traindandgofstage iiganforanother epochsby þxingstage igan solverwithbatchsize ofitsprevious valueevery epochs experiments tovalidateourmethod twostate of the artmeth odsontext to imagesynthesis gan int cls and gawwn arecompared thors inaddition ourproposedstackgan fortheþrstbaseline wedirectly trainstage iganforgenerating and im thenwe and cub contains birdspecieswith images since imagesize ratiosoflessthan asapre processingstep we greater than object imagesizeratios oxford contains imagesofßowersfrom differentcat egories proach mscoco isalso kimages each imageincocohas descriptions while descriptions areprovidedby datasets bycoco into class evaluationmetrics gan wechoosea tionscoreó exp exdkl and yisthelabel theintuitionbehind meaningfulimages therefore tionp shouldbelarge inourexperiments wedirectly usethepre for þne graineddatasets cubandoxford weþne tune assuggestedin foreachmodel it therefore wealso werandomlyselect textde testsets forcocodataset foreachsentence im giventhesametextde scriptions users are theaverage paredmethods of the arttext to imagemethods oncub oxford andcoco aa ba aa bb aa aa ba ba aa gbbf aa gbbf aa ba af aa figure gawwn andgan int cls ba ed fbb figure int cls testset leftmost fourcolumns andcocovalidationset rightmostfourcolumns metricdatasetgan int cls gawwn ourstackgan inception scorecub oxford coco human rankcub oxford coco table gan gawwn andgan int cls oncub oxford andms cocodatasets datasets portedintable figure andfigure comparedwith gan int cls stackganachieves improve from to and improvementonoxford from to asshowninfigure the samplesgeneratedby gan int cls colorofthebirds beak andlegs whichmake olution figure imagesgenerated fromthetextbystage iandstage iiofstackgan figure forgeneratedimages column trainingimages columns byutilizingstage iidiscriminator the neighborretrieval tionconstraints gawwn scoreoncubdataset ours tailsthangan int cls asshowninfigure however scriptions incomparison ourstackgancangener ate photo tions figure iand stage asshown intheþrstrowoffigure inmostcases stage igan textdescriptions however stage iimagesareusually especially forforegroundobjects stage iigangenerates scriptions forcaseswherestage iganhasgenerated stage iigancompletesthe details forinstance inthe stcolumnoffigure witha satisfactorystage iresult stage iiganfocusesondraw inallotherexamples iiimages in manyothercases stage fectsofstage again forexample whilethestage iimageinthe thcol describedinthetext ii gan insomeextremecases the thcolumnoffig ure evenwhenstage shape stage backgroundfromstage iimagesandþne tunethemtobe ii importantly imagerelations iidiscriminator dofour stackgan itsnearestneighbors byvisuallyinspect seefigure wecanconclude componentanalysis inthissubsection the thedesignofstackgan asshownintheþrstfour rowsoftable ifstage images such ure stage igan sampleswithout figure ca helpsstabilizethe eratedsamples row withoutca stage iganfailstogen erateplausible samples zisusedforeachcolumn row withcabut þxingthenoisevectors methodcatexttwice inceptionscore stage igan no yes stage igan no yes stackgan yesno noyes yesyes stackgan yesno noyes yesyes table samplesgener ca althoughstage samples eratedbystackgan inaddition bydecreasingthe outputresolutionfrom to theinception scoredecreasesfrom to notethatallimagesare scaledto thus therefore the stackgandemon stratesthatour intothelargerimages forthe stackgan ifthe the to itindicatesthat iihelpsreþne stage iresults theresultsof stackganmodels wealsoinvestigatethe ca denotedasòno caóintable to figure alsoshowsthat stage igan and stackgan figure lefttoright tenceembeddings thenoisevector incontrast withoutusingca samplesgeneratedby stage consequently theproposed tofurtherdemon ifold asshowninfigure weþxthe noisevector thosesentences theresultsshow birdshapes onbirdappearances fromblacktobrown conclusions inthispaper ialnetworks stackgan tionforsynthesizing photo realistic images theproposed to sketch reþnementprocess stage igansketchestheob textdescriptions stage stage yieldinghigherreso extensivequantita ourproposedmethod to image generativemodels images withmorephoto realisticdetails anddiversity references arjovskyandl bottou in iclr brock lim ritchie andn weston neuralphoto in iclr che li jacob bengio andw li mode in iclr chen duan houthooft schulman sutskever andp abbeel infogan in nips denton chintala szlam andr fergus deep sarialnetworks in nips doersch arxiv gauthier technicalreport goodfellow pouget abadie mirza xu warde farley ozair courville andy bengio in nips he zhang ren andj sun deepresiduallearning forimagerecognition in cvpr huang li poursaeed hopcroft ands belongie in cvpr ioffeandc szegedy batchnormalization accelerating in icml isola zhu zhou anda efros image to image in cvpr kingmaandm welling auto encodingvariational bayes in iclr larsen nderby larochelle and winther similaritymetric in icml ledig theis huszar caballero aitken te jani totz wang andw shi photo realisticsingleim agesuper incvpr lin maire belongie hays perona ra manan dollr andc zitnick microsoftcoco common objectsincontext in eccv mansimov parisotto ba andr salakhutdinov in iclr metz poole pfau andj sohl dickstein unrolled in iclr mirzaands osindero ialnets arxiv nguyen yosinski bengio dosovitskiy and clune plug conditionaliter in cvpr nilsbackanda zisserman automatedßowerclassi in iccvgip odena olah andj shlens in icml radford metz ands chintala unsupervisedrepre sarialnetworks in iclr reed akata mohan tenka schiele and lee in nips reed akata schiele andh lee learningdeep representationsofþne in cvpr reed akata yan logeswaran schiele and lee to imagesynthesis in icml reed vandenoord kalchbrenner bapst botvinick andn defreitas technicalreport rezende mohamed andd wierstra stochastic tivemodels in icml salimans goodfellow zaremba cheung radford andx chen gans in nips szegedy vanhoucke ioffe shlens andz wojna in cvpr snderby caballero theis shi andf huszar resolution in iclr taigman polyak andl wolf unsupervisedcross in iclr vandenoord kalchbrenner andk kavukcuoglu in icml vandenoord kalchbrenner vinyals espeholt graves andk kavukcuoglu in nips wah branson welinder perona ands belongie thecaltech ucsdbirds dataset technicalre portcns tr wanganda gupta in eccv yan yang sohn andh lee attribute image con in eccv zhao mathieu andy lecun energy basedgenerative adversarialnetwork in iclr zhu kr ahenb uhl shechtman anda efros gen ineccv 
multi references renjiezheng mingboma lianghuang schoolofeecs corvallis or baiduresearch sunnyvale ca zheng renj me cosmmb liang huang sh gmail com abstract neuraltextgeneration includingneuralma chinetranslation imagecaptioning andsum marization however duringtrainingtime typicallyonly ple encesavailable referencesinnistmt evaluations and referencesinimagecap tioningdata butmoreimpor tantly referencesby atenewpseudo references theseapproaches bleu andimagecaptioning bleu cider introduction inaddition translation mt sutskeveretal bahdanau etal bysimpleadaptation practicallyvery rushetal nal lapatietal venugopalanetal xuetal decoderframe work sutskeveretal whichgeneratesa variable bah danauetal xuetal thereare accuracy convs gehringetal and transformer vaswanietal however all able duetothenon erationunliketasks inmt even glereference bitext forexample the nistchinese to englishandarabic to english mtevaluationdatasets haveintotal around chinesesentencesand ara bicsentenceseachwith lations ontheotherhand forimagecaption ingdatasets butalsofortrain ing themscoco linetal dataset provides sandabstract vedantametal evenprovide referencesperimage canweuse howmuch ences annotatedrefer ences karpathyand fei fei uniform andshufmethods ex plainedinsec tothebestofour knowledge actually dreyerandmarcu moreimportantly wefur pseudo inparticular we ferentreferences seeexamplesinfig this canbeviewedasamodern neuralversionof sequencealignment barzilayandlee wecanthengen fromthelattice firstly formulti imagecaptioningtasks section secondly network intolattices references section bleu andimagecaptioning bleu cider pseudo references section wedonotchange ourmul wherethe th trainingexample includes onesourceinput andaref erenceset of references notethatthefollowing sampleone uniform and shuf areorderedsets sampleone ences foreachexample werandomlypickoneof the notethat suchas karpathyandfei fei butneverusedinmt thisapproach canbeformalizedas sampleone rand uniform byusingsampleone soweintro duce uniform whichbasicallycopies training example reference uniform isbasedonuniform butshufallthe eachepoch so formallyitis shuf shuf uniform note thatweusemini batchduringtraining whenwe equivalent pseudo referencesgeneration tocoveralargernumber wewanttogenerate morepseudo ingones ences intoalattice weachieve finally wegeneratemorepseudo referencesby score thenistchinese to datasetasanexample indonesia figure pseudo referencescanbe generated issoftwordalignment pseudo naiveidea hardwordalignment sioniteratively ateachtime weselecttworefer wecande showninfig assumethatwedoa erences indonesia its opposition and foreign and ifwefurthercom wecanmergeat indonesia opposition to and foreign which givesthelatticefig bysimplytraversing thelattice newpseudo referencescanbe generated forexample however suffersfrom twoproblems forexam ple inthepreviousexample reiterated repeats and reiterates shouldbemerged together similarly military troops and armies ifthe figure to wecangener whichcan generate pseudo references merged dataset fig toabrickbuilding smallentry two elephants atﬁ to ﬂandatﬁ however to ﬂin itisaprepo second andshouldnotbemerged thus thelat ticeinfig pseudo references therefore alignment barzilayandlee how ever word forexample infig therearetwo indonesia if both indonesia to indonesia inthe thisincor thus formally givenasen tencepair and tutionmatrix whosecell andword basedmul webuildalanguage model lm ofeachword betweensentences fig lm mousaandschuller theoptimiza th word shid denstate weconcatenate resenteachword inasentence wethen cosine similarityscoreof word and as cosine figure figure fig referencesoffig dynamicprogramming weneed unfortunately thus weuseiter sentencepairs durbinetal anoptimalpathfrom to this fig thereisagap opt opt opt opt intuitively we alignedwords weorder opt thescoreofbottom rightcell infig figure tutionmatrix usingthisorder unlessboththe thiswillpre scales in it alignunrelatedwords totacklethisproblem we deductaglobalpenalty fromeachcellof withtheglobalpenalty thedpalgorithmwill notalignawordpair unless we forexample infig fig afterthat the latticeturnsintofig bymergingthetwo references alignmentscore assumingit sthelasttwosen tences wealign mergetothelattice seefig references selectionbybleu wegeneratepseudo forexample ifwetra wecan generate pseudo refrencesintotal then tobal references foreachexample ofpseudo figure pseudo referencesand wecalculateallpseudo references ences andonlykeeptop pseudo references withhighestbleuscore experiments proposedalgorithm machinetranslation to pairsof referencedata nist table dataset wepre trainourmodelona mpairs weusethenist astest sets fig proach andonly calculatethetop averagebleuanalysis fromthewecan thenum experiments untilwecollect nolessthan references wetrainabidirec trainingdataset penningtonetal wordembeddingsizeof dimension for weemploybyte pairencoding bpe sennrich etal kand we adoptlengthreward huangetal to weuseatwolayerbidi lstmasthedecoder weperformpre training for dataset withabatchsizeof wordembedding sizeof beamsizeof learningrateof learningratedecayof anddropoutrateof epochsandusethe bestbatchsizeamong foreachup datemethod soitis andlearningrate decayas table besidestheorig inal wegener and referencesusinghard we sampleone uniform shufe erence first ref accordingtotable shufwithoriginal refer enceshas task pre training training validation testing machinetranslation ofexamples ofrefsperexample imagecaptioning ofexamples ofrefsperexample table ofrefs method bleu pre train first sampleone uniform shuf includespseudo refs hardalign softalign sampleone uniform shuf sampleone uniform shuf sampleone uniform shuf sampleone uniform shuf table baseline ofrefs method bleu pre train first shuf uniform table bleuonthemttestset includespseudo baseline with referenceshas bleuimprovement fromfig pleonemethod references thismay eachepoch imagecaptioning weusethewidely follow ingpriorwork weusethekapathysplit karpathy andfei fei table showsthestatisticsof thisdataset weuseresnet heetal toex tractimagefeatureof featuresizeandsimple toanlstmde ofrefs method bleu cider first sampleone uniform shuf includespseudo refs hardalign softalign bleucider bleucider sampleone uniform shuf sampleone uniform shuf sampleone uniform shuf table bleu tionset baseline ofrefs method bleucider first sampleone shuf shuf table bleu withsoft includespseudo baseline coder wetraineverymodelfor epochsand lectthebestmodel foreveryupdatemethod we andweuseabeamsizeof fig wesetglobalpenaltyas andcalcu latethetop agebleuanalysis ithasworsequalityand table with image originalreferences id pseudo references bleu table references theselected pseudo with references figure taskondev setwithsoftalignment this differentcaptions pects thehigh table with originalreferences whichisadoptedin previouswork karpathyandfei fei bleuscoreand cider casestudy fig pseudo bleuscore pseudo referencesintotal allthetop pseudo references bleuscoresareabove bleuscore ences is references references figure conclusions referencetraining methodsandaneural framework ourproposed acknowledgments grantn andnsfgrantsiis andiis wethanktheanony forproofreading references dzmitrybahdanau kyunghyuncho andyoshuaben gio corr bootstrapping sequencealignment in proceedingsoftheacl volume learning toparaphrase multiple sequencealignment in proceedingsofthe volume hyter meaning uation in proceedingsofthe conferenceof humanlanguagetech nologies pages tionallinguistics richarddurbin seaneddy anderskrogh and graememitchison ysis acids jonasgehring michaelauli davidgrangier denis yarats andyanndauphin convolutionalse in icml kaiminghe xiangyuzhang shaoqingren andjian sun nition recognitioncvpr lianghuang kaizhao andmingboma when ation modulobeamsize in emnlp fei deepvisual tions in pages tsung yilin michaelmaire sergej belongie lubomird bourdev rossb girshick jameshays pietroperona devaramanan piotrdoll ar and lawrencezitnick microsoftcoco com monobjectsincontext amrmousaandbj ornschuller contextualbidi rectionallongshort sentimentanalysis in proceedingsofthe thcon volume longpa pers rameshnallapati bowenzhou andmingboma classifyorselect corr jeffreypennington richardsocher andchristo pherd manning glove globalvectorsfor wordrepresentation in emnlp pages alexanderm rush sumitchopra andjasonweston tencesummarization ricosennrich barryhaddow andalexandrabirch subwordunits arxivpreprintarxiv ilyasutskever oriolvinyals andquocv le works proceedingsofthe thinternationalcon ashishvaswani noamshazeer nikiparmar jakob uszkoreit llionjones aidann gomez lukasz kaiser andilliapolosukhin attentionisall youneed in nips ramakrishnavedantam lawrencezitnick and deviparikh cider consensus basedimage in cvpr marcusrohrbach jeffrey donahue raymondj mooney trevordarrell and katesaenko sequencetosequence videoto text in iccv kelvinxu jimmyba ryankiros kyunghyuncho aaroncourville ruslansalakhudinov richzemel andyoshuabengio show attendandtell tion proceedingsofthe icml kelvinxu jimmyba ryankiros kyunghyun cho aaronc courville ruslansalakhutdinov richards zemel andyoshuabengio show attendandtell in proceedingsofma 
zhewang weihe huawu haiyangwu weili haifengwang enhongchen hefei china baiduinc beijing china xiaose mail ustc edu cn cheneh ustc edu cn hewei wu hua wuhaiyang liwei wanghaifeng baidu com abstract inthispaper weproposeanoveltwo topicsofthe swritingintent decoderframework theproposedplanning user sintent of the introduction duringthehistory beautifulscenery love friendship etc suchas rhythmical andtonalpatterns table poetryinchina veorsevencharacters ping theleveltone orze thedownward tone wang thewell inrecentyears mostap tosaetal wuetal netzeretal oliveira oliveira geneticalgorithms manurung zhouetal manurungetal summarizationmethods yanetal jiangandzhou heetal togeneratepoems morerecently promisingdiscipline to lem zhangandlapata wangetal yietal swritingintents usually asetofkeywords theuser in addition corpus butasweknow thewordsusedinpoems aredifferent frommodernlanguages asaconsequence barackobama inthispaper stage procedure thecontentsofpoems ﬁwhattosayﬂ areexplicitlyplanned ﬁhowtosayﬂ isconducted givenauser asentenceor internationallicense licensedetails http creativecommons org licenses by arxiv cs cl dec iii ppzzp zzpp zppz aaa ppzzp table prepresentsthe level tone tone therhyming topicsfor topic thepoemplanning topics andeachsub topicisrelatedto decodermodel rnnenc dec wemodifythernn enc theplanningbased first swritingintent second suchaslarge encyclopedias asaconsequence ancientpoems squery barackobama toaseriesofsub topicssuchas outstanding power etc fold first weproposeaplanning framework topicofeachline second weusearnnencoder decoderframework togeneratethe poemlinebyline section section section concludesthepaper relatedwork oliveiraetal proposedapoem netzeretal employedamethod tosaetal andwuetal usedaphrasesearch greeneetal coltonetal describedacorpus yanetal severalconstraints manurung andzhouetal poems smt jiangandzhou usedansmt heetal recently zhangandlapata rnn theapproachgener rnnlm mikolovetal wangetal figure usinganend to wangetal there fore yietal nmt itintotheline decodermodel sutskeveretal first wedon tconstraintheuser sinput it canbesomekeywords phrases second weuseplanning sinput topic which sintentwhilethenext heetal zhangand lapata wangetal yietal third therhythmortonein zhouetal yanet al zhangandlapata yietal iscontrolled finally zhangandlapata yietal approaches overview we proposeaplanning ppg theuser swritingintentas inputwhichcanbeaword asentenceoradocument poem thetwo lineswith representingthe thlineofthepoem keywords where isthe topicforthe thline isgeneratedbytaking and asinput where generatedpreviously from to andeachlineis poemplanning keywordextraction theuser mustbeequaltothe numberoflines inthepoem topic if theuser sinputquery istoolong mihalceaandtarau itis agraph brinandpage eachcandidatewordis occurrence words thetextrankscore ji jk where ij and with and brinandpage andtheinitialscoreof issetto keywordexpansion iftheuser sinputquery expansion rnnlm basedmethod rnnlm mikolov etal argmax where isthe thkeywordand topicofoneline ally lines werankthewords thenthewordwiththe inthisway wecanextractakeyword sequenceforeverypoem model knowledge basedmethod theabovernnlm squery containsout of domainkeywords forexample tosolvethisproblem weproposeaknowledge edgetogeneratesub topics sugges tionsofsearchengines lexicaldatabases wordnet etc givenakeyword thekeyideaofthe inthispaper weusetheencyclopedia around the part of corpus poemgeneration thisprocedurecanbe figure to decoder rnn enc dec bahdanauetal givenakeyword whichhas characters andtheprecedingtext which has characters weencode and into withbi gru choetal models then weintegrate intoavector of where weset and textiszero therefore forthedecoder andforeach generationstep isgeneratedbasedon contextvector andprevious generatedoutput argmax aftereachprediction isupdatedby tj isthe soutput theweight tj iscomputedby tj exp tj exp tk where tj tanh tj attimestept canbeas where corpus argmax logp experiments dataset inthispaper samelengthof or characters wecollected poemsforvalidation poemsfortesting system inthisway keywordsfor everyquatrain weextracted keywordsequences whichis seesection forknowledge based expansion weusebaidubaike the keyword theprecedingtext thecurrentline foreverypoem takethepoemintable asexample decmodel proposedinsection keyword theprecedingtext currentline table training decmodel wechosethe andinitializedbyword vec mikolovetal encoderscontained hiddenunits zeiler thevalidationset evaluation evaluationmetrics zhangandlapata schatzmannetal mouetal atopic liuetal has such http baike baidu com poeticness fluency coherence meaning table models poeticness fluency coherence meaning average char char char char char char char char char char smt rnnlm rnnpg anmt ppg table diacritics and indicate thatourmodel ppg asbleuandmeteor therefore wecarryoutahuman following heetal yanetal zhangand lapata ﬁpoeticnessﬂ ﬁfluencyﬂ ﬁcoherenceﬂ ﬁmeaningﬂ thescoreofeach aspectrangesfrom to character quatrainsandtwenty characterquatrains expertsandthe baselines processing smt heetal rnnlm graves etal trainthernnlm rnnpg intheapproachofrnn basedpoemgenerator zhangandlapata thelineis anmt anmt decframework bahdanau etal results planning ppg theresultsare characterand anmtis rnnlmandrnnpg decframework thesub withouttheguideofsub topics incontrast which wronglympashp cannotdistinguish successfullyhpashp normalgroup expertgroup table writtenpoems hp frommachine generatedpoems mp andifeelblueatsunset table theleftoneisamachine generatedpoem and topicforeveryline itcanbe sub topicpredictionmodel humanpoet withhumanpoets turing poemsfromthetestset theinputandgenerated therefore themachine generated writtenpoems generatedones wehad evaluatorsintotal allofthemwerewell theotherthirty tothenormalgroup intheblindtest time wecanseethat ofthemachine generatedpoems arewronglyasthehuman thisnumberdropsto humanpoets themachine writtenpoems table generationexamples table showssomeexamples is beer poemplanningmodelare beer aroma cool and drunk thetitleoftheright oneisanamedentity xinbing whowasafamouswriter threekeywordsbesides xinbing springriver stars and thepast whichare sworks beer xinbing table inthispaper weproposedanoveltwo theuser topics decoderframework thernnenc humanpoets our inthefuture wewill suchasplsa ldaorword vec wewillalsoapplyour songiambics yuanquetc acknowledgments program no cb grantno yfb grant no grantno wk qiliu tongxu linlixu biaochangandthe references bahdanauetal dzmitrybahdanau kyunghyuncho andyoshuabengio arxivpreprintarxiv brinandpage theanatomyofalarge engine computernetworks choetal kyunghyuncho bartvanmerri enboer caglargulcehre dzmitrybahdanau fethibougares holgerschwenk andyoshuabengio decoderfor arxivpreprintarxiv coltonetal simoncolton jacobgoodwin andtonyveale full facepoetrygeneration in iccc graves alexgraves arxivpreprint arxiv greeneetal ericagreene tugbabodrumlu andkevinknight in emnlp heetal jinghe mingzhou andlongjiang in twenty jiangandzhou longjiangandmingzhou proach in proceedingsofthe volume pages liuetal chia weiliu ryanlowe iulianvserban michaelnoseworthy laurentcharlin andjoelle pineau arxivpreprintarxiv manurungetal rulimanurung graemeritchie andhenrythompson manurung hisarmanurung in emnlp mihalceaandtarau textrank in emnlp mikolovetal tomasmikolov martin at lukasburget jancernock andsanjeevkhudanpur in interspeech volume page mikolovetal tomasmikolov kaichen gregcorrado andjeffreydean efestimationof arxivpreprintarxiv mouetal lilimou yipingsong ruiyan geli luzhang andzhijin forwardsequences acontent textconversation in proceedingsthe netzeretal yaelnetzer davidgabay yoavgoldberg andmichaelelhadad gaiku generating in guisticcreativity pages oliveiraetal hugogonc alooliveira raquelherv as albertod andpablogerv as adaptinga in iccc oliveira hugogonc alooliveira anoverview universidadede coimbra oliveira hugogonc alooliveira poetryme computa tionalcreativity conceptinvention schatzmannetal jostschatzmann kallirroigeorgila andsteveyoung in dialogue sutskeveretal ilyasutskever oriolvinyals andquocvle neuralnetworks in pages tosaetal naokotosa hidetoobara andmichihikominoh hitchhaiku in icec pages springer turing alanmturing mind wangetal qixinwang tianyiluo dongwang andchaoxing withneuralattention basedmodel corr abs wang liwang wuetal xiaofengwu naokotosa andryoheinakatsu newhitchhaiku aninteractiverenku in icec pages springer yanetal ruiyan hanjiang mirellalapata shou delin xueqianglv andxiaomingli poet optimization in ijcai yietal xiaoyuanyi ruoyuli andmaosongsun encoder decoder corr abs zeiler matthewd zeiler adadelta corr abs zhangandlapata neuralnetworks in proceedingsofthe emnlp pages doha qatar october zhouetal cheng lezhou weiyou andxiaojunding journalofsoftware 
simple pdf file this is small demonstration pdf file just for use in the virtual mechanics tutorials more text and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text boring zzzzz and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text and more text even more continued on page simple pdf file continued from page yet more text and more text and more text and more text and more text and more text and more text and more text oh how boring typing this stuff but not as boring as watching paint dry and more text and more text and more text and more text boring more little more text the end and just as well 
submitted revised published jieshen js rutgers edu rutgersuniversity piscataway nj usa pingli pingli gmail com baiduresearch bellevue wa usa editor sujaysanghavi abstract largestabsolute weestablisha tight compressedsensing rip basedalgorithms sparsityisunknown machinelearning intermsoflarge scalemachinelearning ayetchallengingprob thatattemptedthe weprovethe foranumberofpreva convex keywords sparsity hardthresholding compressedsensing introduction olshausenandfield statistics tibshirani efronetal signalprocessing chenetal donohoetal donoho cand esandwakin chandrasekaranetal tonamejustafew inordertoobtain asparsesolution amongwhichtwopromi troppandwright forinstance as omp patietal repeat tropp troppandgilbert inspiredbythesuccess ofomp cosamp needell jieshenandpingli license cc by see https creativecommons org licenses by http jmlr org papers html henand andtropp andsubspacepursuit sp daiandmilenkovic madeimprovementbyse rip cand esandtao interestingly the the iht algorithm daubechiesetal blumensathanddavies foucart then htp whichcombinedtheidea ofcosampandiht jain etal recently bahmanietal andyuanetal problemnon convex the suit chenetal donohoandtsaig andlasso tibshirani thedifferenceis thatlassolooksforan cand esand tao another cand esandtao which ofthegradient interestingly cand esetal aseriesof vanishes cand esandtao cand es wainwright caietal foucart inthispaper ht iht cosamp sp machinelearning yuan andzhang andstatistics ma ourmotivationistwo fold fromahighlevel com theseht scaleproblems troppandwright nevertheless thisnatu basedalgo forpractitioners thesolution however langfordetal duchiandsinger xiao itishardforthe convergence forageneralvector thehardthresholded signal inmagnitude elementsof tozero ties hence isalways sparse the numberofnon moreover theresultantsignal isabest at ight oundof ard hresholding intermsofany norm thatis forany sparsevector kh kh it truesparsesignal here obtainedby fullgradientdescent then sparse is totheonebefore forcomparison defazioetal projection hence tighteningthe bound wheretheproxy itself inotherwords since doesnotminimize theobjectivefunction thedeviation makes asanexample nguyenetal vanishing we inthiswork ourboundistight hencecannotbe holdsforallchoicesof sparsesignals andforgeneralsignals orrip like conditionassumed inthecontextof compressedsensing itmeansthatinessence formachinelearning itsuggeststhat finally large ouralgorithm henand returningto ouranalysisshows cansuchanalgorithm succeed isnotapplicableinthe challengingscenario notation todenoteavector eithercolumnorrow andits thelementisdenotedby the normofavector isdenotedby thesupportsetof indicesofnon zeros isdenotedby supp whosecardinalityis writtenas supp or formatricesandits th entryisdenotedby ij are foraninteger supposethat isasubsetof we whichretainselements containedin andsetsotherstozero thatis if otherwise inparticular let inthisway ontoan ballwithradius thatis max roadmap alongwithawhy theconventionalbound isnottight section scaleprob finally weconcludethe paperinsection thekeybound isnottight hardlybeattained toseethis sparsesignal anda generalone kh kh kh at ight oundof ard hresholding isabest themajor issueoccursin thoughitisthewell and doeshavea structureœitis sparse wemusthave forsome thatis ifandonlyif toseethis let bethesupportsetof and bethecomplement let and likewise we and asthecomponentsof supportedon and respectively hence indicates and whereweassume since immediatelyimplies doesnothold if thenwehave since containsthe otherwise the factthat and implies andhence therefore weobtain when happens however weinrealityhave kh inother words thefactorof in arbitraryconstant inthissense isnottight theorem let beany sparsesignal forany kh min min inparticular and suchthatthe equalityholds remark maximumof when in auniversalmanner when tendsto isgivenby withrespectto thus themaximumvalueof equalsone eveninthiscase wethat max remark theconstantbound weshowinsection thatitcannot remark when isnotexactly sparse kh kh kh thus is sparse proof sketch denote henand let bethesupportsetof andlet beitscomplement weimmediatelyhave let bethesupportsetof likewise we and for duetotheconstruction wehave mustbezero denote and and wehave maximumof weassume below fixing wecanviewtheleft handsideoftheabove ontheotherhand weobtain werecallthat containsthelargest absoluteelementsof while hassmallervalues inparticular theaveragein islargerthan thatin whichgives notethat hence min andoptimizingover min min finally min min at ight oundof ard hresholding remark tightness and byabackwardinduction notethat equals ifandonlyif hence wepick where and onlydependson and notonthecomponentsof or donotviolatetheof weneedto or or notethatthereis sparsevector letusconsiderthecase and sothat and thus thestequalityof havesamemagnitude thefact alsoimplies isasubsetof duetothe of andthesparsityof hencewehave finally picking aswedidin istight however ifthereis forinstance letusfurther assumethatthesignal is sparse if then isazerovectorand readsas otherwise wehave min and isimprovedto min min henceforth isgivenby min min notethatthefact ofjainetal acloselyrelatedbound wasestablished kh kh kh whichalsoimproveson provided however oneshortcomingof isthatthe forcomparison our henand inthissection processing sincemostoftheht toderivethe convergencecondition iht blumensathanddavies andcosamp needell andtropp tobeclear eitherconvexornon convex rather wefocuson compressed sparsesignal fromasetofits perhaps noisy measurements ax where isaknown sensingmatrixwith ingeneral determinedsystem yet that is fora suchanovelideawas asthe bycand esandtao whichrequires suchthatforall sparsesignals ax the ric aboveinequalities notethat sparse sparsevectors and and and forwhich guaranteesthat implies todate gaussianmatrices optimalrip verysmall partialfourier matrices fastcomputation lowmemoryfootprint notably itwasshown whelmingprobability adamczaketal lietal apartiallistincludes iht cosamp spandregularizedomp needellandvershynin eningtheripcondition wangandshim moandshen caiandzhang mo baraniuketal provided log hence surements thatbeingsaid sincetheconstant aboveisunknown at ight oundof ard hresholding phasetransition donohoandtanner donohoetal for wainwright ananalogousresult notably inblanchardandtanner phase transitionforiht inthepresentpaper wefocus inthelanguageofrip sparsesignal thatis ihtstarts withanarbitrarypoint andatthe thiteration ax usedtheparameter however inpracticeonemay thus as blumensathanddavies appliedthebound aswehaveshown theirresults theorem considerthemodel andtheihtalgorithm pick andlet be then undertheripcondition forall where isgivenbytheorem blumensathanddavies required cand es caietal foucart incontrast see appendixcfordetails inviewof requirementof thesparsityparameter which in blumensathanddavies whengraduallytuning largerthan italwaysrequires if theconditionturnsout since theorem becomeslarger forexample suppose inthis henand case theorem whereas blumensathanddavies requires in thatamongvariousreal worldapplications ontheotherhand imumchoiceof tobemoreprecise argument require log valueof first itisknownthat columnsof isometryconstant bahandtanner hardtoestimate also left isoneofthemostefalgo let ax andproceedsasfollows supp supp argmin supp cosampprunesthe onasmallsupportset inparticular inthelaststep errorineachstep owingtotheorem theorem considerthemodel pick andlet be then undertheripcondition itholdsthatforall where isgivenbytheorem at ight oundof ard hresholding roughlyspeaking hence itismonotonically in fact toseethis incomparison needellandtropp derivedabound andfoucartandrauhut theorem improveditto forageometricrateof jainetal forcosamp scaleoptimization canbevery large therefore formally weareinterested min opt butnotlimitedto the forall wehave and thelossfunction ax istheleast forall wehave and thenegativelog log exp forwhich log exp toeasenotation wewilloftenwrite as and as for isallowedtobenon convex hence in norm constraint lohandwainwright herewechoosethe fastprojection previouswork agarwaletal inourproblem yet we ht svrg tooptimize tackling scalemachinelearning stochasticmethods regularized svrg xiaoandzhang andsaga defazioetal non convex problem ismorediftoana henand usuallythesolutionis notsparse the norm ness seelangfordetal xiao duchiandsinger ontheissue bahmanietal jainetal constrainedproblem inabatchscenario whichis notpracticalforlarge rithmisnguyenetal nonetheless statisticalmodels opt and thetrueparameter forinstance considerthemodel minimizing doesnotamountto recovering infact theconvergenceto opt isonlyguaranteedtoan accuracybythe statisticalprecision oftheproblem opt whichisthebest agarwaletal and algorithm ht svrg require trainingsamples maximumstagecount sparsityparameter updatefre quency learningrate radius initialsolution ensure optimalsolution for to do set for to do uniformlypick andupdatethesolution endfor uniformlychoose andset endfor algorithm ouralgorithm algorithm wherethepri owenandzhou is sparse for andhence isstill sparse constraintin readsas is properlychosen whichdependson at ight oundof ard hresholding oldingoperation notethatitis if is not sparse usuallyitisdense isnotequalto inaddition defazioetal whichenjoysthe non tanceuptoamultipleof ifusingthebound inaccurateiterates rip likecondition which combinedwiththeorem thresholding notethat seetheorem necessary moreover withhardthresholding svrgforapprox imatinganarbitrary sparsesignal globaloptimumof andforatrueparameter ssumption negahbanetal jain etal issaidtosat rsc withparameter ifforallvectors with itholdsthat restrictedsmoothness issaidtosatisfythe rss withparameter ifforallvectors with itholdsthat forall here werecallthat wasintroducedin andtheparameter rouxetal svrg johnsonandzhang henand andsaga defazioetal agarwaletal wealsostressthatthe whichismilder thanours negahbanetal pper oundof rogress forbrevity letusdenote the andwewrite kr max kp aswillbeclear kr svrgcanbetothepoint for kr kr wherewerecallthat and areusedinthe algorithmand estimate virtually scalesas kr which wedenote thesamples theorem consideralgorithm anda sparsesignal ofinterest assume and pickthestepsize if thenitholdsthat fi and providedthat is largeenough inparticular for wehave for wehave at ight oundof ard hresholding remark forthetheoremtohold dueto hence theconventionalbound isnotapplicable incontrast theorem assertsthatthis slightlylargerthan remark withtheconditionson and thecoef issuflarge remark thetheoremdoes not thisis because mightbelessthanzero however specifying doesgiveconvergence results yper arameter etting hyper parameters and svrg werequire whichimplies recallthat isgivenintheorem ingeneral hence wehave toourknowledge forompand injainetal however differentwayinthatht ofjainetal gives whereasusingtheorem thesparsityparameter dependsonly onthedesiredsparsity inthisregard ourboundisvital intuitively ht svrgperforms inordertomitigate thevariance inthislight shouldnotbetoosmall otherwise alarge toquantitatively analyzehow shouldbeselected letusconsiderthecase forexample thecase followsinasimilarway inordertoensure wemusthave inparticular picking hastosatisfy henand johnsonandzhang when optimization nesterov with and inmind parameters due to bytheorem weknowthat thus alargerquantityof and henceafasterrate interestingly for is concentrates hence proposition fix and intheorem is inthe attainstheminimum nowlet thisgives kr kr lobal inear onvergence constrained optimizationprogram corollary assume and considertheht parameters givenin thenthesequence opt of opt opt kr opt kr opt proof whenever opt thecorollaryreadsas opt opt we defazioetal inappendixe in ax svrg when ischosenastheleast squareslossinthat opt and ax ontheotherside max kr at ight oundof ard hresholding ofshenandli nowwespecify forinstance in itishencepossibleto corollary assume and let betherssparameterof atthesparsitylevel considertheht parametersgivenin thenthesequence recoversa sparsesignal withageometricrate kr kr remark therssparameter of alwaysrangesin whichissimplyby omputational omplexity pgd studiedinjainetal svrg first weremarkthatthe of atsparsitylevel we write pgdrequires log iterations hence nc log forht svrg inviewofcorollary hence ht svrgneeds log iterationswhere kr canbemadeassmallas withsufsamples to beinthesequel ineachstage ht followedby times stochasticupdates therefore svrgisgivenby log bynotingthefact inthescenario ht svrgsimproveson pgdintermsoftimecost statisticalresults agivensparsesignal withappropriatehyper parametersettings on kr assuggestedby thereby kr thanpgd westudytwoproblems raskuttietal notably rankmatrixregression sparse agarwaletal parse inear egression ax henand where isthedesignmatrix istheresponse issomenoise and isthe and notethatwhenwe inorderto approximately estimatetheparameter ingnon convexprogram min forouranalysis satisfy jj thenoise isindependentof anditsentriesarei proposition andtheprogram as sume exp min log max log log kr log kr opt opt log above min and max respectively werecallthat and and and isthersspa rameterof theestimationfor and kr literature raskuttietal whilethatfor inbellecetal amountstoestimating max kh inorderto estimate kr opt noticethat kr opt kr opt kr kr opt kr opt kr nowwelet const andget min log log supposethat min log and log with thenourassump tions and log log and log at ight oundof ard hresholding forcorollary asfaras log opt log wehave opt max min opt max min opt svrgtoapproximate aglobaloptimumof upto opt returningtocorollary toguaranteethat itsuftopick log log finally situation min log and log log log providedthat max log log thus ht svrgismoreefthanpgd parse ogistic egression pr exp where iseither or likelihood min log exp linearregression see forexample negahbanetal infact aconcurrentwork onarxiv lietal madetheirworkpublic svrgwaspresented tobenon convex proposition henand experiments inthissection svrgalgorithm ontwotasks sparserecovery compressedsensing andwevisualizethe modelslearnedbyht sparserecovery datageneration thedatadimension isedas andwegeneratean gaussian randomsensingmatrix whoseentriesarei then sparsesignals isuniformlychosen thatis trials the measurements foreachsignal isobtainedby ax whichisnoisefree inthisway baselines ihtandpgd bothof yet pgdismoregeneral tobelargerthanthe truesparsity foriht foriht hence svrg evaluationmetric wesayasignal if inthisway hyper parameters ifnotweuse and forht svrg svds aa forht svrgandpgd where svds aa aa sinceforeach stage ht times weruntheihtandpgd for hase ransition tothisend we from to roughlywithastepsize wealsostudythe whichrangesfrom to roughlywith stepsize at ight oundof ard hresholding ht thissuggeststhatht large figure thevalues rangefrom to the asastochasticvariant ht infigure curves inthisway particularsparsity forinstance percentsofthe sparse signalsarerecovered wehavetocollect measurements thatusing measurements figure left and thesparsity right andfigure orexact recovery nowweareto and tothisend fromfigure where withstepsize andrun thealgorithms redcircleforempir icalobservationofht svrg whichare henand forexample wethat isalmostlinearwith especially thecurveofht which againvht figure and percentageofsuccess svrg ht whichagainthatht svrgisanappealing nfluenceof yper arameters next parameters thesparsityparameter updatefrequency andstepsize svrg wesetthetrue sparsity andcollect notethat and svds aa infigure theorem isdeterministic the proposition hence butht aswehaveobservedin figure clearly wearenot forourpurpose weseta threshold thatis percentsofthesignals areexactlyrecovered otherwise theleftpaneloffigure vestheconditionthat hastobelargerthan whilethesecond rate finally workswell and when within stages frequency infigure thatforany ht islargeenough at ight oundof ard hresholding figure convergenceofht wehave measurementsfor the elementsarenon zero thestandardsettingis and left isnotlargeenough ht middle asmall right figure suflargevaluefor ht svrgalwaysconverges ht svrgwillconvergeif islarge enough for thelog error thousandsiterations weillustratedthatht here westudytheper consistingof by hencetotally classes theupdatefrequency isedas asinthe previoussection svds aa sinceforthereal worlddataset thetruesparsity isactuallyunknown first wevisualizevepair svrginfigure whereeachrowis andthesubsequentred http yann lecun com exdb mnist henand figure sityparameter forexample recognizingadigitisﬁ ﬂorﬁ ﬂwiththesparsity inparticular foreachpair welabelthe apparently figure wevisualize modelslearnedbyht svrgunderdiffer is vs vs vs vs ﬂandﬁ vs whereforeachpair note thathere the ratherthan log opt duetothefact isnp hard generallyspeaking ht svrgconverges stages itisnotsurprisingto lower objectivevalue however at ight oundof ard hresholding figure the demonstratethe wherecurveswith sparsity generallyspeaking ht svrgconvergeswithin stageswhichisavery fastrate tasks italready inthispaper operator constrainedproblems andhavedemonstrated in addition scalesetting hencejustifyingthat moreover method nguyenetal ht thisposesa henand sparsity constrainedproblem acknowledgments we and intoourattention weappreciate huanxu finally nsf bigdata andnsf iii appendixa technicallemmas thefollowingisa innguyenetal lemma foragivensupportset is rssandis then forallvectors and with supp proof hr forallvectors and wehave supp whichisequivalentto where supp ontheotherhand weobtain hr supp providedthat supp wepick clearly forsuchachoiceof wehave supp supp hence byassuming that supp isnotlargerthan weget at ight oundof ard hresholding nowexpanding andrearrangingthe lemma considertheht let let supp supp supp put assume forall denote kp proof wehave kp kp kp hp kp kp hr hr kp hr where isbyalgebra applieslemma andthefactthat weobtainthefollowing kp kp theproofiscomplete corollary if wehave henand appendixb proofsforsection proofoftheorem proof isazerovector inthefollowing weassumethat isnotazerovector denote let bethesupportsetof andlet beitscomplement weimmediatelyhave let bethesupportsetof letussplitthevector asfollows likewise wedenote wehave inthisway hence maximizing and firstly weconsiderthecaseof whichmeans implying inthe following weconsider inparticular weconsider maximumvalueof arranging weobtain letus andthefunction at ight oundof ard hresholding thus indicatesthat notethat isa where istheidentitymatrix attainsthe whichisgivenby isnon empty werequirethat implying weobtain werecallthat containsthelargest absoluteelementsof while hassmallervalues inparticular theaveragedvalueof isno greaterthanthatof inmagnitude notethat hence min andoptimizingover gives min min pluggingbackto weobtain min min theproofiscomplete henand appendixc proofsforsection proofoftheorem ourproofandtheirs wheretheorem applies proof wheretheyreached old whiletheorem gives new therein weobtain bynotingthefactthat where and wehaveanewboundforeq new to notethatspecifying gives proofoftheorem infoucartandrauhut whichgivesthebest wheretheorem applies incaseof confusionbynotation thereadermay thedifferenceisineq offoucartandrauhut notethattoderivethis inequality whichgives old gives new at ight oundof ard hresholding andeq therein weobtain where to notethatsetting givestheoldbound offoucartandrauhut appendixd proofsforsection proofoftheorem proof fixastage letusdenote sothat byspecifying supp supp supp supp itfollowsthat thus and and wealsohave the theunbiasednessof forthesecondterm weuselemma toboundit putthemtogether henand conditioningon for wehave kp kp kp where applieslemma appliesassumption andwewrite kr for brevity conditioningon andtakingthe wehave lm lm lm wherewerecallthat seealgorithm the forbrevity wewrite kr basedon case lm at ight oundof ard hresholding whichimplies lm pick suchthat weobtain wemustimpose putting and togethergives thus wehave fi case inthiscase implies lm lm henand thatis thus werequire whichisequivalentto combiningitwith gives empty weimpose sothat thus bycombining and and is theproofiscomplete proofofcorollary proof wehave max max max at ight oundof ard hresholding supposethat rsswithparameter itfollowsthat kr recallthat kr kr henceusing gives max kr kr finally see lemma inshenandli max kr kr kr theproofiscomplete appendixe ht saga defazioetal asshowninalgorithm andtheoneproposed indefazioetal hence sparsesolution theorem of sparse then producedbyalgorithm sparsityparameter proof thelyapunovfunction asfollows henand algorithm ht saga require thecurrentiterate andofeach attheendofiteration thestepsize ensure thenewiterate pick uniformlyatrandom take andstore inthetable changed updatethenewiterate asfollows weexamine wehave also fortheterm wehave therefore at ight oundof ard hresholding and suchthatthe positive thatis werequire pick wethetwoinequalities pick werequire ln ontheotherhand bytheorem wehave thus werequire ln byalgebra dueto weknow sufwhereweassume picking ln completestheproof henand references radosawadamczak alexandere litvak alainpajor andnicoletomczak jaegermann re randomsampling alekhagarwal sahandnegahban andmartinj wainwright methodsforhigh matrices sohailbahmani bhiksharaj andpetrost boufounos greedysparsity richardbaraniuk markdavenport ronalddevore andmichaelb wakin asimpleproofofthe pierrec bellec guillaumelecu andalexandreb tsybakov slopemeetslasso improvedoracle boundsandoptimality corr abs peterj bickel ya acovritov andalexandreb tsybakov dantzigselector pages jeffreyd pressedsensing davies journal davies ap tonyt caiandanruzhang rankmatrixrecovery tonyt cai liewang andguangwuxu ieee emmanuelj cand es emmanuelj cand esandterencetao ieeetransactionson informationtheory at ight oundof ard hresholding emmanuelj cand esandterencetao thedantzigselector ismuch largerthan emmanuelj cand esandmichaelb wakin ieeesignal processingmagazine emmanuelj cand es justink romberg andterencetao ex ieeetransactionson informationtheory venkatchandrasekaran benjaminrecht pabloa parrilo andalans willsky theconvexge scottshaobingchen davidl donoho andmichaela saunders pursuit ingriddaubechies micheldefrise andchristinedemol mathematics aarondefazio francisr bach andsimonlacoste julien saga in proceedingsofthe th pages davidl donoho compressedsensing davidl donohoandjaredtanner proceedingsoftheieee davidl donohoandyaakovtsaig fastsolutionof solutionmaybesparse davidl donoho michaelelad andvladimirn temlyakov davidl donoho iainjohnstone andandreamontanari theory johnc duchiandyoramsinger splitting bradleyefron trevorhastie iainjohnstone androberttibshirani leastangleregression the annalsofstatistics henand simonfoucart siamjournal onnumericalanalysis simonfoucart constants in sanantonio pages springer newyork ny applied birkh auser prateekjain ambujtewari andinderjits dhillon in proceedingsofthe pages prateekjain ambujtewari andpurushottamkar dimensionalm estimation in proceedingsofthe processingsystems pages reduction in proceedingsofthe systems pages johnlangford lihongli andtongzhang journal pingli cun huizhang andtongzhang in proceedingsofthe pages xingguoli tuozhao ramanarora hanliu andjarvishaupt corr abs po linglohandmartinj wainwright high convexity po linglohandmartinj wainwright regularizedm statistical zongmingma theannalsof statistics qunmo corr abs qunmoandyishen tropp cosamp curatesamples at ight oundof ard hresholding processing sahandnegahban pradeepravikumar martinj wainwright andbinyu aframeworkfor high in proceedingsof the pages yuriinesterov abasiccourse volume of appliedoptimization springerus namh nguyen deannaneedell andtinawoolf corr abs brunoa olshausenanddavidj field astrategy employedbyv visionresearch artowenandyizhou association yagyenshc pati raminrezaiifar andperinkulams krishnaprasad in conference recordofthe systemsandcomputers pages garveshraskutti martinj wainwright andbinyu balls nicolasleroux markw schmidt andfrancisr bach in proceedingsofthe thannualconference pages jieshenandpingli corr abs jieshenandpingli in proceedingsofthe pages jieshenandpingli in proceedingsofthe pages roberttibshirani society seriesb methodological joela tropp greedisgood ieeetransactionson informationtheory joela troppandannac gilbert matchingpursuit henand joela troppandstephenj wright problems proceedingsoftheieee martinj wainwright lasso pursuit linxiao linxiaoandtongzhang tion xiao tongyuanandtongzhang journal xiao tongyuan pingli andtongzhang journalofmachine learningresearch tongzhang ieeetransactionson informationtheory 
eep aptioningwith ultimodal ecurrent eural etworks rnn junhuamao losangeles baiduresearch mjhustc ucla edu weixu yiyang jiangwang zhihenghuang baiduresearch wei xu yangyi wangjiang huangzhiheng baidu com alanyuille losangeles yuille stat ucla edu bstract inthispaper rnn model imagecaptionsaregen networks images thesetwosub toformthewholem rnnmodel iaprtc flickr flickr kandmscoco of the artmethods inaddition weapplythe andachieves of the artmethodswhichdi theprojectpageof thisworkis www stat ucla edu junhua mao rnn html ntroduction plications imageretrieval thanks recent manyprevious thus inthiswork rnn model toaddress andthetaskofimageand sentenceretrieval thewholem avisionpartanda multimodalpart mostrecently rnnmodel areavailableat https github com mjhucla mrnn cr http arxiv org abs maoetal ourwork arxiv cs cv jun figure fromiaprtc dataset weshowa allthepeople www stat ucla edu junhua mao rnn html cnn themulti layerrepresentation ourm seedetailsinsection the simultaneously intheexperiments iaprtc grubinger etal flickr rashtchianetal flickr youngetal andmscoco linetal of the artperformance retrievingim elated ork guage forcomputervision krizhevskyetal works cnn with layers denotedasalexnet russakovskyetal this girshicketal designaobjectde tectionframework rcnn basedonthiswork recently simonyan zisserman proposea cnnwithover layers denotedasvggnet fornaturallanguage rnn showsthestate of the artperformance inmanytasks mikolovetal recently kalchbrenner blunsom choetal andsutskeveretal image sentenceretrieval hodoshetal fromeetal socheretal socheretal inthis way recently karpathy etal figure rnn neuralnetwork rnn architecture thesimplernn ourm rnnmodel theinputs representsthe wordsinasentence weaddastartsign start andanendsign end the itconsistsofvelayers arecurrentlayer amultimodallayerand asoftmaxlayer dimensionofthelayer bestviewedincolor odsforthistask theyparse mitchelletal gupta mannem each kulkarnietal usesacon this composingthere trievedcaptions kuznetsovaetal whichismorerelated toourmethod sentencesand images usingforexample srivastava salakhutdinov and topicmodels barnardetal jiaetal morecloselyrelated whichisbuiltonalog bilinearmodel mnih hinton vewords whereasinourmodel shortlyaftermaoetal kirosetal karpathy fei fei vinyalsetal donahueetal fangetal chen zitnick itdemonstratesthe fromthesemethods firstly weincorporateatwo rnn layerword embedding secondly theimage of the intheexperiments we performing odel rchitecture rnn orelmannetwork elman theinputwordlayer therecurrentlayer andtheoutputlayer theactivationofinput re isdenotedas and respectively denotes thecurrentwordvector whichcanbeasimple of theone hot representation non zeroelement mikolovetal where and and areelement wisesigmoid and thesub accordingly weneedto rumelhartetal urm rnn model rnn isshowninfigure it therecurrentlayer themultimodal layer andthesoftmaxlayer iten mostofthesentence karpathyetal fromeetal socher etal kirosetal usepre theirmodel incontrast trainingdata thestate of the artresult seefigure dimensions thecalcula insteadof denotedas attime denotedas wemap andadd themtogether whereﬁ ﬂrepresentselement wiseaddition weset tobetherlinearunit relu nair hinton krizhevskyetal functionisadopted seesection reluisfaster non bptt isconducted previouswork mikolovetal useheuristics toavoidthisproblem thetruncatedbptt stopsthebpttafter timesteps where isahyperparameter becauseofthegood propertiesofrelu wesetupa rnnmodel seefigure theword embeddinglayerii fortheimagerep resentation th layerofalexnet krizhevskyetal or th layerofvggnet simonyan zisserman tures whereﬁ ﬂdenoteselement wiseaddition denotes theimagefeature istheelement lecunetal tanh boththesimplernnandm whichisdifferent fordifferentdatasets rainingthem rnn totrainourm rnnmodelweadoptalog itisrelatedtothe perplexity of asentence is calculatedasfollows log ppl log where ppl giventheimage given and previouswords itcanbecalculated bytheperplexity log ppl where and receptively denotesthelengthof th sentences and entence eneration mage etrievaland entence etrieval weusethetrainedm sentencesgeneration imageretrieval re sentenceretrieval start orarbitrary ofthenextword thenextword inpractice afterthat end fortheretrievaltasks givenanimage accord ingtotheprobability perplexity tosolvethisproblem kirosetal perplexity differentfromthem weusethe where denotesthequeryimage and areimages weapproximate this intheexperiments probability earningof entenceand mage eatures andthe visionpart thealexnetorvggnet forthelanguagepart asmentionedabove learntheirparameters forthevisionpart weusethepre trainedalexnet krizhevskyetal orthevggnet simonyan zisserman onimagenetdataset russakovskyetal recently karpathyetal girshicketal wholeframe intheexperiments etal inthispaper infuturework andtheparameters them whichallows thehyperparameters suchas aretunedviacross validation onflickr ittakes msonaverageto generateasentence onasinglecorecpu xperiments atasets iaprtc grub ingeretal flickr rashtchianetal flickr youngetal andms coco linetal iaprtc theworld people animals cities landscapes etc foreachimage onaverage thereareabout previousworks guillauminetal kirosetal with imagesfortrainingand imagesfortesting flickr foreachimage itprovides providedbythedataset thereare imagesfortraining imagesfortesting flickr foreachimage italsoprovidesve sentencesannotations itconsistsof crowd images wefollowthe previouswork karpathyetal whichused imagesfortesting thisdataset aswellas theflick kdataset mscoco trainingimages and validationimages foreachimage werandomly sampled validationset valuationmetrics sentencegeneration seeequ and bleuscores andb papinenietal bleu similarly thoughithasdrawbacks forsome images socheretal fromeetal karpathyetal weuser asthemeasurement kistherecall higherr kusuallymeansbetter retrievalperformance ther kscores sentenceorimage foriaprtc datasets kirosetal esultson iaprtc areshownintable ours rnn baseservesasa rnnmodel rnnexceptthatitdoes asmentionedinsection words therefore fromtable itsbleuscoreislow table showsthatourm state of the www stat ucla edu junhua mao rnn html evaluationtoolbox chenetal were kirosetal updatedoneshere ppl lbl mnih hinton mlblb alexnet kirosetal mlblf alexnet kirosetal guptaetal gupta mannem ours rnn base ours rnn alexnet table dataset ﬁbﬂisshortforbleu sentenceretrival imagetotext imageretrival texttoimage medrr medr ours rnn table kandmedianrank medr foriaprtc dataset sentenceretrival imagetotext imageretrival texttoimage medrr medr random sdt rnn alexnet socher avg rcnn devise avg rcnn deepfe alexnet deepfe rcnn ours rnn alexnet table resultsofr kandmedianrank medr forflickr kdataset alexnetﬂdenotesthe rcnnﬂdenotes fortheretrievaltasks wereportr forfuturecomparisons theresultshowsthat top top we seesup esultson lickr ther of the artmethods sdt rnn socheretal devise fromeetal deepfe karpathy etal alexnet avg deepfe rcnn karpathyetal however theevaluationmetrics the ppl andb rnn alexnetmodelin thisdatasetare and respectively sentenceretrival imagetotext imageretrival texttoimage medrr medr flickr random devise avg rcnn deepfe rcnn rvr mnlm alexnet mnlm vggnet nic lrcn deepvs ours rnn alexnet ours rnn vggnet mscoco random deepvs rcnn ours rnn vggnet table resultsofr kandmedianrank medr forflickr flickr kmscoco ppl ppl rvr deepvs alexnet deepvs vggnet nic lrcn dmsm ours rnn alexnet ours rnn vggnet table ourm rnndim lstmnoyesyesyesnono table lrcnhasastackof four weachievesstate of the lstm hochreiter schmidhuber canbetreatedasa esultson lickr and mscoco of the devise fromeetal deepfe karpathyetal mnlm kirosetal dmsm fangetal nic vinyalsetal lrcn donahueetal rvr chen zitnick anddeepvs karpathy fei fei areshownintable andtable respectively wealso table dmsm nic lrcn candidates ciderrouge lmeteor rnn greedy rnn greedy rnn beam rnn beam table simonyan zisserman outperformsthe state of the artmethods metrics them arenovel chenetal select reference sentences rnnmodel wetrainsev inparticular weshow thatthetwo mance about forb duetothelimitedspace section earest eighboras eference recently devlinetal ofthe captionw inspiredbythismethod weadoptthem egy maoetal denotedasm rnn shared togenerate scheme wekeepthe end these mostprobable andcanbetreatedasthe hypotheses inourexperiments weset set seedetailsinsection scores devlinetal ofthehypothesesw neighborimages seedetailsinsection theoneisthe simonyan zisserman weresize pixels windows then then train valandtestsets at https github com mjhucla mrnn cr figure com thefeaturesbythem thefourcorners ontheresizedimage finally weaverage dimensionalfeature rnnmodel itcanbecalculatedas where see equation and wethatcomparedtothe thefeaturesbythem visualinformation bananas onsensus eranking supposewehavegetthe wefollow devlinetal etal rnnmodel more foreachhypothesis wecalculate ciderrouge lmeteor rnn shared rnn shared nnref bleu rnn shared nnref cider rnn shared nnref bleu orcale rnn shared nnref cider oracle mscoco testserver ciderrouge lmeteor rnn shared rnn shared nnref bleu rnn shared nnref cider table resultsofm rnn borsasreferences rnn shared nnref rnnmodelon nearestcaptions the asentence levelbleuscore papinenietal orasentence levelcider vedantametal wecross and forthebleu basedsimilarity theopti mal and are and respectively forthecider basedsimilarity theoptimal and are and respectively xperiments forbleu wegetanimprovementof setand setintermsofbleu score forthecider based consensusreranking wegetanimprovementof pointson themscocotest setintermsofcider iscussion althoughthehy someofthemcapture sensusreranking more accordingtothescores bleuorcider seerowswithﬁ oracleﬂ high bothforthem rnnmodelitselfandthe rerankingstrategy onclusion rnn state of the artinthreetasks sentencegeneration figure cider adeepcnnandthesetwo sub ourm cknowledgments wethankandrewng kaiyu changhuang duohaoqin haoyuangao jasoneisnerforuseful reviewersfromiclr andnips deeplearningworkshop forminds brainsandmachines cbmm and aro cs eferences barnard kobus duygulu pinar forsyth david defreitas nando blei davidm andjordan michaeli jmlr chen fang lin ty vedantam gupta dollr andzitnick microsoftcoco captions arxivpreprintarxiv chen xinleiandzitnick clawrence generation arxivpreprintarxiv cho kyunghyun vanmerrienboer bart gulcehre caglar bougares fethi schwenk holger andbengio yoshua machinetranslation arxivpreprintarxiv devlin jacob cheng hao fang hao gupta saurabh deng li he xiaodong zweig geoffrey andmitchell margaret arxivpreprintarxiv devlin jacob gupta saurabh girshick ross mitchell margaret andzitnick clawrence ex arxivpreprintarxiv donahue jeff hendricks lisaanne guadarrama sergio rohrbach marcus venugopalan sub hashini saenko kate anddarrell trevor long arxivpreprintarxiv elman jeffreyl cognitivescience fang hao gupta saurabh iandola forrest srivastava rupesh deng li doll ar piotr gao jianfeng he xiaodong mitchell margaret platt john etal andback arxivpreprintarxiv farhadi ali hejrati mohsen sadeghi mohammadamin young peter rashtchian cyrus hock enmaier julia andforsyth david in eccv pp frome andrea corrado gregs shlens jon bengio samy dean jeff mikolov tomas etal devise adeepvisual in nips pp girshick donahue darrell andmalik in cvpr grubinger michael clough paul uller henning anddeselaers thomas theiaprtc bench mark in ontoimage pp guillaumin matthieu verbeek jakob andschmid cordelia in eccv pp gupta ankushandmannem prashanth in iconip gupta ankush verma yashaswi andjawahar cv images in aaai hochreiter seppandschmidhuber urgen longshort termmemory neuralcomputation hodosh micah young peter andhockenmaier julia task data jair jia yangqing salzmann mathieu anddarrell trevor learningcross multinomialdata in iccv pp kalchbrenner nalandblunsom phil in emnlp pp karpathy andrejandfei fei li deepvisual tions arxivpreprintarxiv karpathy andrej joulin armand andfei fei li imagesentencemapping in arxiv kiros ryan salakhutdinov ruslan andzemel richards unifyingvisual semanticembeddings arxivpreprintarxiv kiros ryan zemel andsalakhutdinov ruslan in icml krizhevsky alex sutskever ilya andhinton geoffreye imagenetwithdeepcon in nips pp kulkarni girish premraj visruth dhar sagnik li siming choi yejin berg alexanderc and berg tamaral babytalk in cvpr kuznetsova polina ordonez vicente berg tamaral andchoi yejin treetalk compositionand linguistics lecun yanna bottou eon orr genevieveb andm uller klaus robert efbackprop in neuralnetworks tricksofthetrade pp springer lin tsung yi maire michael belongie serge hays james perona pietro ramanan deva doll ar piotr andzitnick clawrence microsoftcoco arxiv preprintarxiv mao junhua xu wei yang yi wang jiang andyuille alanl mao junhua xu wei yang yi wang jiang huang zhiheng andyuille alan learninglikea child arxivpreprint arxiv mikolov tomas at martin burget lukas cernock jan andkhudanpur sanjeev recur in interspeech pp mikolov tomas kombrink stefan burget lukas cernocky jh andkhudanpur sanjeev exten in icassp pp mikolov tomas sutskever ilya chen kai corrado gregs anddean jeff distributedrepresen in nips pp mitchell margaret han xufeng dodge jesse mensch alyssa goyal amit berg alex ya maguchi kota berg tamara stratos karl anddaum eiii hal midge generatingimage in eacl mnih andriyandhinton geoffrey in icml pp acm nair vinodandhinton geoffreye in icml pp papineni kishore roukos salim ward todd andzhu wei jing bleu amethodforautomatic in acl pp rashtchian cyrus young peter hodosh micah andhockenmaier julia collectingimageanno tationsusingamazon smechanicalturk in naacl hltworkshop pp rumelhart davide hinton geoffreye andwilliams ronaldj back propagatingerrors cognitivemodeling russakovsky olga deng jia su hao krause jonathan satheesh sanjeev ma sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexanderc andfei fei li simonyan karenandzisserman andrew scaleimage recognition arxivpreprintarxiv socher richard le manning andng in tacl srivastava ruslan in nips pp sutskever ilya vinyals oriol andle quocvv works in nips pp vedantam ramakrishna zitnick clawrence andparikh devi cider consensus basedimage arxivpreprintarxiv vinyals oriol toshev alexander bengio samy anderhan dumitru showandtell aneural arxivpreprintarxiv young peter lai alice hodosh micah andhockenmaier julia denotations in acl pp upplementary aterial rnn model rnn rnn noembinput rnn onelayeremb rnn emboneinput rnn visinrnn rnn visinrnn both rnn visinrnn both shared table rnnmodelsontheflickr kdataset seefigure figure rnnmodels inthissection two intuitively levelse asaninputto themultimodallayer tovalidateitsefy rnnnetworks rnn noembinput rnn onelayeremb rnn emboneinput ﬁm rnn noembinputﬂ denotesthem timodallayeriscutoff therecurrentlayerand ﬁm rnn rnnmodelwhosetwoword dimensionalword embeddinglayer therearemuch rnn originalm rnn ifthedictionarysize islarge ﬁm rnn multimodallayer table itvthe rnn rnn visualinrnn rnn visualinrnn both andm rnn visualinrnn both shared form rnn visualinrnn els aresharedform rnn visualinrnn both shared table rnnmodel inpractice rnnmodel increasing hochreiter schmidhuber mightsolvetheproblem work iaprtc inadditiontother kandmedr wealsoadoptexactly kirosetal usesashortlistof thisshortlist sentenceretrieval task orsentences areshowninfigure themethod bowdecaf isa strongimagebasedbag of wordsbaseline kirosetal kirosetal ourm rnnmodeloutperforms hecalculationof bleu score tocalculatebleu ninthepaperwhere score wecomputethe gramprecision papinenietal thenwecomputethe geometricmeanof uptolength bp min bp log where we where and corpus longerorshorter imagetotextcurve texttoimagecurve figure tc dataset topfewretrievals ismostimportant 
proceedingsofthe pages bulgaria august 
insightsfromlarge scaleanalysesof zengtaojiao zhuoranwang guanchunwang haotian huawu haifengwang baiduinc no shangdi thstreet haidiandistrict beijing china heriot wattuniversity edinburgh uk cambridge uk abstract tems sds china kdialogsfroma bookingsdsand basedon suchdata wealsoexaminehowsuch userintentions related andtime of day relatedfactors theinthisresearch introduction machineinter faces spokendialogsystems sds havebeenincreasingly decade cisionprocesses pomdps naturaldialogs youngetal thedialogpoliciesin simulations schatzmannetal youngetal or crowdworkers gasicetal however theintentions tal whichposesthefollow inghigh levelquestions whatexter weempiricallyanalyze china mostlyimplicit userbehaviorpatterns copyright intelligence www aaai org allrightsreserved publicdeployments williams thisworkisfo relatedsystem characteristics thisisthe thesys tem sds whilethe secondsystem sds bothsdsarerule based bohusand rudnicky one best cisions thesds but wherere arately ineverydialog forbooking thereare requiredslots namelydeparturecity destinationcityand departuredate and seatgrade airline etc forhotelreservation the required check inandcheck outdates with star rating etc kdialogs with duringjanuarytomay denotedasds and mdialogswith mturns collectedfromsds duringjanuarytojune denoted asds inds with forbookingand forhotelreservation thereare ofthedialogsinds thatcontaincompound andhotelbookinggoals intheremaining ofthedialogsinds the etc rate forsds andsds are and respectively whatmakesdialogsfail required oursds flight departure destination date ds ds hotel destination check in check out ds table andds firstly forbothtasks aswhen planningatrip herdeparture anddestinationinmind inmanycases usersmayjust nevertheless onthe otherhand inmanysituations whenusergoalisclear usersmayprefer betterdialog takingtheintentionof byidentifyingrel basedapproaches we that inbothds andds thereareonlyaround worse herde throughoutthispaper stands sincethispaperaims toanalyzemacro itisimpractical thedata notethat ingtouser mostof bydefault therefore intable tentions intuitively sdestinationwill inaddition the busyhoursandleisure hours therefore of lows characteristics weanalyzethe sdepartureand ingsarehighlighted inchina suchaslhasa sanya lijiang etc locations toexplainthis searchingforaback home especiallywhen spectives firstly wecomputethe missing named datemissingrate to named notehere we astheexamplesof interestinds aretoosparse the which thoughthe acteristicsofuser sdestination ulartourismplace canuser sintention takingsuch orusabilityofthesds roundingthem excepttourism ds departure ds destination ds departure ds destination rank city etc rank city etc rank city etc rank city etc hongkong taipei xidai chongqing lhasa jinjiang hongkong taipei jieyang chongqing lhasa qingdao guilin xi an kunming dalian kunming shanghai haikou xi an urumqi beijing lijiang shenyang haikou lanzhou xieyang harbin yinchuan hongkong yinchuan nanjing lijiang shenyang sanya lanzhou guilin tianjin sanya kunming haikou haikou dongguan sanya taizhou xidai cixi lijiang yiwu sanya nan an zhangjiajie suzhou lijiang fuqing dali dongguan lhasa foshan lhasa total total total total table andds citieswithlessthan and occurrencesin ds andds areout respectively datemissing perceivedbrowsing rank city rate rank city rate huangshan dali dali zhangjiajie lijiang lijiang lhasa sanya zhangjiajie lhasa overall overall table incomparison forexample usersgoingtoindustry however functions insuchcases nevertheless insteadofﬁguess strip ﬁokay tolhasa stravelpur personalized remark as state of the manusers gasicetal however duetothecom youngetal thomsonandyoung logpolicies inaddition question therefore remark althoughaproperrule basedimplementation suchasours rates sinceetcdoesnot moreover recommending however dependent tions time of day relatedfactors asmentionedabove sbehaviorswhenin teractingwithansds weplotthetask dependentetcrates figure dailytimeperiods hours figure hours figure hours inds andds hours infigure morningaround am whilefords bookingdi alogsonly around am ousthanthatinds moreinterestingly highestetcrates lows firstly suchasthemidnight may neys hotelsbooked onthecontrary intheearlymorning andthere wecarryoutatime and respectively while remark time of day onsds orfordialogpolicy ityofsds inaddition onecaninfigure thatthereareno bothds andds during pmto pm userswithuncer themainreason inbothfigure and however onepossibilitycould conclusion basedonlarge alogsystems themaincontribu aswellastime of day relatedfactors icalevidences acknowledgements pro gram no cb sicsapecegrant references bohus andrudnicky theravenclawdialog managementframework comp speechlang gasic breslin henderson kim szummer thomson tsiakoulis andyoung on in icassp schatzmann thomson weilhammer ye and young agenda in naacl hlt thomson andyoung bayesianupdateofdia loguestate tems comp speechlang williams in sigdial williams resultsfrom twopublicdeployments ieeej selectedtopicssig proc young gasic keizer mairesse schatzmann thomson andyu thehiddeninformation statemodel basedspo comp speechlang young gasic thomson andwilliams pomdp are view proc ieee pp view publication statsview publication stats 
jingyuanzhang chun talu bokaicao yichang andphilips yu àbigdatalab bdl baiduresearch sunnyvale ca usa il usa santaclara ca usa tsinghuauniversity beijing china zhangjingyuan baidu com clu caobokai psyu uic edu yichang huawei kgs havebeenwidelyused whilekgscannot however itis weproposeatensor entitiesand textdescriptions aware multi tamure thatcanlearn frombothkgsandnews parameters furthermore large scalekgsandnewstexts of the artmethods onreal worlddatasets keywords tensorfactorization embedding ntroduction knowledgegraphs kgs suchasfreebase anddb pedia here handtaretwoentities headandtail and risarelation entitiescanbepersons organizations locations etc beperson location thekgs naturallanguage however current worldrelationships emergingovertime fortunately thereis timely considerfigure asanexample whereanemerg https www freebase com http wiki dbpedia org forsimplicity andarelation knowledge graph kg byron howard rich moore walt entity relationships in kg emerging relationships directedby directedby producedby figure althoughthere manypieces thepublisher andthedirectors forexample thecurrentkgscan in addition eventdetection etc however themainfocusofthese forinstance pathranking algorithm pra randomwalktechniques andrelationsintoalow tailentitiesinkgs however itisnontrivialto basedmethodsto fromtheotheraspect toembedlarge scaletexts forexample lineis proposedin toembedtextsintoalow dimensional occurrence networkfromtexts laterpteisproposedin toimprove afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd figure tail entitiesandnewstexts therefore fromnewstexts recently kgandthetextsjointly however themethodin sotheirindirect thoughthemethodin aimsto toaddresstheissue inthispaper weproposeatensor but alsotherelationtype afourth speciþcally headentities relations tailentities atensorstructure spacesofentities relationsandtexts figure showsthe asthe interactions tensorproduct ofentities relationsand news inthismanner itcandealwith noteworthily wouldbeproblematic first ingthefourth ofentities makingitchal second duetothe itistime consuming overþtting lastbutnotleast whichconsistsof ischallenging wefurtherde velopa text aware multi tamure byfactorizingthe descriptions since overþtting insummary weformulateanewtask connectingemergingre lationships sources newstexts basedframeworktocon thenewstexts order tensorformulation bycapturing notonlythefourth butalsolower orderinteractions inthebuilttensor thelower orderinteractionscan relations andtextdescriptions seesectioniii fordetails furthermore large scaleapplications of the artmethodsvia tensorflowonreal worldkgandnewsdata sectionii formulatestheproblem sectionivpresentsthe relatedwork ii reliminary inthispaper beforeproceeding weþrst tableilists basicconcepts deþnition entity anorganization oralo cation etc afþliationtypeoran organization locationtype formoftriplets where hisaheadentity tisatail entityand risarelation weuse deþnition knowledgegraph kg aknowledge graph gkg ekg ekg afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd tablei listofbasicsymbols arelationship setorspace tominclusively denotesinnerproduct denotestensorproduct outerproduct denoteshadamard element wise product ekgisthesetofknown relationships eachdirectededgein ekgcanberepresented byatriplet wheretheentities handt ekg and therelation nowadays worldknowl edge largevolumesof arearising withtime deþnition emergingrelationship anemergingre lationship exists ifitslabel intherealworld ekgort ekg forexample infigure zootopia producedby walt disneystudios publisherentity waltdisneystudios similarly zootopia directedby byronhoward and zootopia directedby rich moore inthefollowing deþnition tensor þrstorder andmatrices secondorder following an thordertensoris denotedby ri ááá imanditselementsby xi ááá im spanningthe rangefrom ááá otherwisespeciþed foranarbitrarymatrix ri its throwand xiandxj respectively the sizedtensors ri ááá imisdeþnedby ááá imi xi ááá imyi ááá im theouterproductof mvectors rimform isan ááá ááá im ááá imforallvaluesof theindices inparticular for ááá andy ááá itholdsthat ty forageneraltensor ri ááá im itscandecom parafac cp factorization is ááá wherefor ááá arefactor matricesofsize im and isusedforshorthand problemstatement gkg amongentities candidates dfromnews intheentityset ekgofthekg wedenote enews anddenote ekg enews thetask functionf wheretheentities handt rand ekg iii roposed method inthissection text aware multi tamure tensor basedscorefunction relationsfromnews thekgorthenews texts isavailable afterthat basedmodel ner techniquesin kgarenewentities we news weaimtodetermine news themost afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd relationtype ridextractedfromnews fnews id ur di dtur whereur welet ir rid whose columnsarethevector ur let er rirdenotetherelation typeindicatorvector er ááá ááá wecanobservethat order relationtype fnews dtuer er giventhekgonly tail let ie denotethenumber ofentities andlet eh rieandet riedenotethe respectively wecanform fkg ethvret er eh et wherevr rie and rir ie vr vr obviously er eh et wherew rir ie ie learned such besides inorderto notavailableinthekg weaddaòco occurrenceórelation text awaremulti relationallearning suchthatthecomplex entitiesandtext how ever ordertensor iso ir ie ie id relationshipsinnews thetensor moreover wwouldbeproblematic first duetothelargenumber ofparameters ir ie ie id thelearningprocedure sparsecounterpart er eh et second sincetheweight itcannotmodelthe toaddresssuchissues we theweight wherem rir kandm rie headentitiesand tailentities respectively rid krepresentsthe fromeq andeq er eh et etrm ethm ettm dtm etrm ethm ettm dtm rt dtm where isthehadamard elementwise product etrm ethm andt ettm aretheembedding headentity handtail entityt respectively lower onecan noticethateq onlymodelsthefourth orderinteractions entities handt and however thelower order forexample iftheheadentity isaperson òrichmooreó therelationtype risunlikely tischosen inthesampleinstance inthiscase eventhe thus orderinteractions inthepredictivemodel vectorsineq asfollows rt bh bt dtm bv wherebv rid bhandbt riearethebiasvectors toillustratewhy orderinterac tions wecandecomposeeq intotwoparts theþrst partrt bt dtm bv modelsthefourth whilethesecondpart afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd eteherdrm bht btdtm bvfigure theworkßowoftamure rt bh bt dtm bv withthebiasvector bhmodelsthethird orderinter actionsbetween withouttheheadentity hinvolved othertypesofthethird wenamethemodelineq as text aware multi tamure theworkßow aftermapping theheadentity bt bt dtm bv textsvia rt bh bt dtm bv clearly relationtypes entities ized undersparsity therefore sparsedata further thebiasvectors forrelationships afterfactorizing theweighttensor moreover themodelcomplexityis ir ie id parameters giventhetrainingset tamure relationtypesand followingthe samestrategyasin weminimizeamargin based algorithm thetamurealgorithm input thetrainingset entities eandrelationtypes margin embedding dimensionk output embeddingsofentities initializeembeddings uniform foreach eandr whilenotconvergence do sampleamini batchofsize mdbatch sample sbatch for dbatchdo sample sbatch endfor thegradientofeq on sbatch normregularization endwhile max where isamarginhyper parameter and iscomposedoftraining kg inthekg disalways replaced accordingly thelossfunctionineq favorshigherscores optimizer inmini batchmode withtheadditionalmax whichconstrainsthe than thedetailed ateach mini batch asinglecorrupted theparametersarethen rate normregularization constraint iv xperiments inthissection afterintroducingthe wecomparedifferent baselinemethods afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd dataprocessing corpusinthefb dataset fb contains corpuscoupled therearearound infreebase dependencypaths forexample obamaisthe nsubj president prep ofobj objectó intheexperiment weextractedn gram intheprocessofn gram featureextraction thosen gramfeatures infb asubsetof almost corpus tamuremethod fb costofhuman labeling intheexperiment relationshipswith kg the tamuremodelasin summarizedintableii comparedmethods wecomparethe followingninemethods transe totailentities low thescorefunction oftranseis skip gram itisthestate of the artwordembedding model aword https www microsoft com en us download details aspx id deepwalk itlearnsembeddings graph byviewingentities thesametypeofnodes thenweapplythe line itisthe large scale informationnetwork embeddingmethod line similartodeepwalk linetreatsentities theweightsarethe occurringinnewsorkgs pte itisthe predictive text embeddingmethod pte the word wordco occurrencegraph anentity entitygraph aword relationgraph transe sg graphs dingsfromkgsandskip fromnewstexts since thedatasetfb news rescal itfocusesonthekgand rescal tamure kg basedframe workonthekgonly weþrstbuildathird order titiesfromthekg thentheproposedmulti relational kgis rt bh bt tamure hi basedframe workwiththehigh order fourth order interactions only thelower hi tamure itisthe text aware multi tamure proposedinthispaper we buildafourth relationships forfaircom parisons for afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd tableii news validation test texts text descriptions entities entities existing entities emerging entities relationships existing relationships emerging relationships training relationships validation relationships testing relationships alltheabovemethods foragivenentity itsembedding theadamalgorithm rescal tamure kg tamure hiandtamure the themini batchsizeissetas issetas andthemarginissetas intheexperiments þerasin valuedoutputscores validationset skip gram deepwalk lineandpte wefollow toapply headentities sincetheremight wemeasure labeleval microf andmacrof andconsiderthemicro macro averageof precision andrecall averageaccuracy avgacc averageauc avgauc andhammingloss hl relationtypes inthissection methods rizedintotwogroups inordertoshowthe wenot relationships isreportedintableiii relationships speciþcally baseline onthe http www csie ntu edu tw cjlin liblinear figure forthe micro andmacro metrics thelargerthevalue thebetter theperformance forthe hlmetric thesmallerthebetter hlmetricand onthe avgacc metric furthermore in tamure and hlandavgauc respectively tamure entities andtextdescriptions onlyonenewentity afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd tableiii òvalue rank the bettertheperformance methodskgembedding textembedding kg textembedding kg skip gramdeepwalklinepte transe sgtamure hitamure micro macro avgacc avgauc hl methodskgembedding textembedding kg textembedding kg skip gramdeepwalklinepte transe sgtamure hitamure micro macro avgacc avgauc hl methodskgembedding textembedding kg textembedding kg skip gramdeepwalklinepte transe sgtamure hitamure micro macro avgacc avgauc hl however transecannot thekg innews kgembedding transeandrescal incontrast theproposedtensor based framework tamure kg newsinformation tamure the reasonliesinthetwo forthesemodels withtwoseparatesteps itis ofembeddings amongentities it furthermore comparedwithtranse sg tamure hi transe sgcombinesthe inthekg however transe sgembedsthekgandthe newsseparately hence tively theproposedtamure hiandtamuemethodsin andthereby lower itoutperforms tamure hitoalargeextent insummary withatensorstructure theproposed theentitiesinfb inthefollowing weassessthe newentities intherangeof to weselectthe transe rescal tamure kg tamure hiandtamure andrepresent duetospacelimit we afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd figure micro andhl similar macro avgacc andavgauc fromfigure inaddition lessthan ofemergingentities withlessthan of thereby however thenews parameteranalysis inthefollowing wedemonstratethe infigure weshowthe resultson micro andmacro metrics whereòoneó entitynotinthekg itcanbeobservedthat withalarger embeddingsize bothmicro andmacro however whentheembed dingsizeislargerthan therefore inourexperiment dueto spacelimit avgacc avgauc andhlmetrics inßuenceofepochs figure oftamure respectively òembed kóon theleftoffigure epochs regardless figure tamure inourexperiment as elated work knowledgegraphs kgs ofreal worldrelationsabout however knowledge graph kg relationtypes thetaskofkg inaddition view forinstance thepathranking algorithm pra recently the researchwork in ourwork hence furthermore textsjointly however themethodin embeds effectively thoughthemethodin aimstoconnect relationtypes incontrast handlesmulti order tensorstructure recently thereare scaletexts for example and toembedtextsintoalow ahomogeneouswordco laterpteisproposedin afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd thesemethods therefore they effectively vi onclusion inthispaper tensor withan efþcient text aware multi tamure of the artmeth odsonreal worlddatasets acknowledgement cns andnsfc wegrate references dong gabrilovich heitz horn lao murphy strohmann sun andw zhang òknowledgevault aweb scale óin kdd hoffart altun andg weikum withambiguousnames óin www zhang lu zhou xie chang andp yu òheer news óin ieeebigdata chang yih yang andc meek òtypedtensordecompo óin emnlp bordes weston collobert andy bengio òlearningstruc óin aaai garc õa dur an bordes andn usunier òeffectiveblendingof twoandthree relationaldata óin ecml pkdd nickel jiang andv tresp óin nips socher chen manning anda ng òreasoningwithneural óin nips wang zhang feng andz chen òknowledgegraphembed óin aaai gardnerandt mitchell óin emnlp lao mitchell andw cohen óin emnlp bordes usunier garcia duran weston and yakhnenko relational data óin nips lin liu sun liu andx zhu òlearningentityand óin aaai mikolov sutskever chen corrado andj dean òdis ality óin nips perozzi al rfou ands skiena òdeepwalk onlinelearning óin kdd tang qu wang zhang yan andq mei òline large óin www tang qu andq mei òpte large óin kdd wang zhang feng andz chen jointlyembedding óin emnlp koldaandb bader siamreview cao zhou li andp yu òmulti viewmachines óin wsdm lu he shao cao andp yu òmultilinear taskmulti viewlearning óin wsdm nadeauands sekine classiþcation vol no http nlp stanford edu software crf ner shtml kingmaandj ba òadam iniclr srivastava hinton krizhevsky sutskever and salakhutdinov òdropout fromoverþtting jmlr toutanova chen pantel choudhury andm gamon inacl nickel tresp andh kriegel òathree learningonmulti relationaldata óin icml ghamrawianda mccallum òcollectivemulti labelclassiþca tion óin cikm tsoumakas katakis andi vlahavas òminingmulti labeldata angeli tibshirani wu andc manning òcombiningdistant óin emnlp zeng liu chen andj zhao óin emnlp gardner talukdar krishnamurthy andt mitchell òin knowledgebases óin emnlp ji he xu liu andj zhao óin acl gardner talukdar kisiel andt mitchell òimproving cues óin emnlp neelakantan roth anda mccallum òcompositionalvector óin acl cao kong zhang yu anda ragin òmining identiþcation óin icdm zhang kong jie chang andp yu òncr ascalable network basedapproachtoco rankinginquestion and answersites incikm zhang jie rahman xie chang andp yu òlearning basedmodeling óin cikm zhang cao xie lu yu anda ragin side óin sdm afe wzjw id zojqhfhbpoqzhzxq xvuubgahnq ddfd 
thisisan openaccessarticle http creativecommons org licenses by use distribution doi emergingtrends china andmynewjobatbaidu kennethwardchurch baidu sunnyvale ca usa mail kennethchurch baidu com received april accepted april abstract china ai iwould shappening inchinaoverthepast years yearsago bothin kai china westin majorbreakthroughsby mynewpositionatbaidu mentioningchina inow workforbaidu but youwillsoon china world wedo suchasspeech to speechtranslation wedo suchasself drivingcars educationinchina https en wikipedia org wiki baidu https www theverge com circuitbreaker baidu translator gadget announced https www wired com baidus self driving car has hit the road church aclmembership foreachcountry aclmembership interest it growth buttheyarebigger theﬁrst timeivisitedtsinghua the mit ofchina overtheceiling nevertheless iwasimpressed withtheaudience upsthatnoone whereasinchina sincetherewas abroad sincethen towardtheend oftheexpansion ihadanopportunityin ofdeansinchina isuggestedthatmyson butmygreat hisday foreseeablefuture inchina thegapisclosing iamtoldthattsinghua good tables and itis shouldwecountboth thelistofschools intables and comefrom https www usnews com education best global universities search region country china subject computer science name https www usnews com best graduate schools top engineering schools computer engineering rankings int int aswellas https admission princeton edu how apply admission statistics https news harvard edu gazette story college admits percent of applicants https collegeadmissions uchicago edu apply applicants international https www yale edu about yale yale facts https undergrad admissions columbia edu ask faq topic http web mit edu facts enrollment html https diversityandaccess stanford edu diversity diversity facts https www upenn edu about factsemergingtrends table topschoolsintheworld schoolstudents international princeton harvard universityofchicago yale columbia mit stanford average table appreciated schoolstudents international tsinghuauniversity zhejianguniversity southeastuniversity pekinguniversity average moreimportantly kstudents school kstudents school topschoolsinchina numberscanbeabitmind numbing completelysoldout lastminute ontherevival ofempiricism ofoldfogies back in thatwasempiricism thesedays workingondeepnets church and especiallyinchina speech kai fuleepointedoutthat to overthe yearssince phones banking loans facerecognition payments self aschairman kai inchina peoplelikehimare treatedlikerockstars kai aftergraduatingfrom columbiainai sincethen hehasheld microsoftandgoogle kai fuhasﬁrst helistedfour twoof hisfourreasonsare talent china stoptalent mostnowseechina sbigcompanies suchasbaidu alibaba andtencent medium sizedcompanies suchasmeituan didiandxiaomi andstartups suchasvipkidandface asbetteroptions withoutthebesttalent thedevelopment and leadership theheads promotion technology theydon tunderstandchinese don term commitmenttochina andtheir these inwhatwedo technologyreview mit salumnimagazine citesaboldchinese http engineering columbia edu kai fu lee speech https edition cnn com asia china innovation kai fu lee index html https pandaily com kai fu lee american companies struggle china https www technologyreview com chinas ai governmentaiplan major breakthroughsby sotoo andwhatwedo technologyreview quotesandrewng it istheglasshalffull orhalfempty thewestshouldn fearchina sairevolution itshouldcopyit buttherearealsofears especiallyinthewest largely andtheeast china startitseconomy in recentdecades ofpeopleoutofpoverty but inequality buttherearealsomany thistrendholdsfor facerecognition things downsides whilethewest thepeoplethatitalk wereﬁrstintroduced buteventually http www gov cn zhengce content content htm https www cnbc com kai fu lee robots will replace half of all jobs html https pandaily com kai fu lee disagree with elon musks ai will destroy humanity http www dailymail co uk news article china installs million ai equipped street cameras html church anansweringmachine mayormay someofthem commercially therehavebeenafew parents unlockourphones building allineedismyface itishardto electronicpayments cashisnotusedmuchin chinathesedays face tothecamera andthatisthat anyone canstealacreditcard withorwithoutachip aspointedoutbykai fuleein million such percentfrommerchants return themselveswith percent and moreopenmodel yearsago andcon sequently inadditiontotheprice wealso aswellastwolegal currencies oneforlocals rmbs fmbs intheory butinpractice fmbswereworth friendshipstores availableelsewhere but https news filehippo com facial recognition app to find missing children https nakedsecurity sophos com facial recognition reunites missing boy with his dad after four years http www thedrum com news marketing moments american express dont leave home without itemergingtrends seenthem fmbs change needlesstosay myhostwashorriﬁed sohethrew inafreebattery gameisplayed butineededtheclock remarkably thegame butattheendoftheday thanitwasworth everytimeigotochina itisacountry thesedays thereisless negotiating andalmostnocash itcanstillbedone barely snotthewaythe evenmorerecently weplayedlotsofgames blackmarket negotiatingcanbefun from fast methods kai fuleeiscorrectwhenhe concludingremarks whenimentionedthat madeinjapan usedtobetruefor madeingermany sotoo astheeconomy improvesinchina madeinchina usedtobeassociated withpoorquality beenverygood yearsago andtheyare andlongterm inwhatwedo bothintheprivate kai andchina major breakthroughsby 
searchonthegpu jingbozhou qiguo jagadish lubo skr al siyuanliu wenhaoluan anthonyk tung yuejiyang yx yuxinzheng yx univ ofmichigan annarbor baiduresearch tencentinc jzhou qiguo krcal luan atung yueji yuxin comp nus edu sg jag umich edu sliu ntu edu sg abstract workonthegpu calledgenie aimingtoreducethepro ofdifferentdatatypes butmanypopularones are anddemonstrate sensitivehashing lsh named annsearch andanoveldata structurec thispurpose lifedatasets ntroduction thesequeriescould bothhumansandclient applications forexample imagematching sift scale featuresandmatching parallelizationfor units gpus years thenvidia gtxtitanx has gbofddr memoryatapriceof thenvidia has gbofddr memoryatapriceof us dollars furthermore installed however effectivelyex particularlyaswewor single instruction multiple data simd architecture concur rentcontrol https github com sesame nus genie cess tion tothisend wedevelop basedgen erici nverted inde xframework calledgenie wealsonameoursystem asgenie match countmodel countmodel butthesystemis aresupportedbygenie œjustthatmanyare aswewill acrossinpractice asananalogy considerthemap reduce model not reducemodel butmanycan forthosethatcan hadooptakescareof burden inasimilarway oursystem genie canabsorbthe burdenofparallelgpu ourproposedmatch countmodelthecommon types inverted index likestructure bythe lsh schemeunder shotgunandassembly sa wepresent countmodel wepropose butnot geniealsoexploitsgpu called countpriorityqueue pqforshort whichcan duetothesimd architecture thetop whichiswidely inpreviousstudy whichiscalledk selectionin andshort listsearchin orapriority ournoveldesignofc pqcan and result thegpucanbeovercome throughputofgenie thenovelstructureofc pqcanalso there withinabatchonthegpu toovercome weintroduceamultiple bygenie weproposeanewconcept calledtolerance ann search whichisin annsearch thenweprove that geniecansupportthe anoth we data genie onthegpu gpu countmodel especially pqto weproposethenew conceptof ann effectivelysupport typesofreal andefyofgenie techniquereport ii reliminariesand verview inthissection weuse asarunningexample match countmodel givenauniverse an object in gˆ formsa dataset ds query isasetof items whereeachitem isasetofelements from isasubsetof queryset is as qs fig example theuniverse isaset of orderedpairs where and an tuple asillustratedinfig the inthere ranges as where aset ofpairs withvalue aswecanseefrom fig query and canberepresentedas informally givenaquery andanobject thematch countmodel mc wegiveaformal ofthematch countmodelasfollows match countmodel givenaquery andanobject wemapeach queryitem where containedbytheitem whichisalsoasubsetof countmodelisthesumof theintegers mc forexample infig for and wehave andc then wehave mc ingenie respecttothequery accordingtothemodel mc to obtainthetop objectsofquery efsupportthematch andqueries fig invertedindex valuesasorderedpairs discretized keyword postingslist comprises givenaquery wecanquickly ordered pairs afterthat wecan geniewithlshandsa countmodelhasthexi justas map wedo however at amonggenie lshandsainfig howtoorganizedata bypreviousliterature ofthispaper transformedbylsh high dimensionalpoint insuchascheme givenaquerypoint buckets meanwhile sets including jaccardkernelforsets radialbasisfunction rbf kernel forfeaturesketches we transformedbysa includingdocuments sequences treesandgraphs canbe transformedwiththesa scheme the units ﬁshotgunﬂ gramsforsequences andstarsforgraph after thedecomposition unit aparticularsub atquery time smallsub unitswith thequeryobject thematch which this plexstructureddata fig lshandsa iii nverted ndexonthe gpu genie pqfor short finally weproposeamultiple thegpu fig structure listarray inthegpu sglobalmemory thereisalsoa positionmap in when processingqueries this look fig fig onsomeattributes whenweinvokeaquery weobtain itspostingslists thenwe useoneblockofthegpu asmallbatchofthreads upto andcontrolsthe foraquery with queryitems ifthereare queries therewillbeabout inparallel duringtheprocess thepostingslist counttable to postingslists therefore thesystemworksina intheinvertedindex postingslists we lists werefer readersto countpriorityqueue calledcountpriority queue pqforshort gpu genie pqhastwostrongpoints thoughhowtoretrieve thetop pqcanthistask withsmallcost and pqcanreducethespace requirementofgenie thisproblemisalso inpreviousstudy thisproblem however thekeyideaofc pqistouseatwo leveldatastructureto storethecountresults thequeryresult ofc pq takingadatasetwith pointsasanexample thousandqueries about gb thesize is queries points bytes gb which toreducespacecost second weonlyneedtoallocate several insteadof inbitmapstructure eachobjectisbounded count actually weusuallycaninfera forexample themaximumcount fig anillustrationofthec pq pq fig shows themainstructuresofc in thelowerlevel wecreatea bitmapcounter whichallocates severalbits upto bits intheupper level thereisa hashtable idanditscountvalue then apivotaldevice called gate determineswhichid thegatehastwomembers zipperarray andathresholdcalled auditthreshold inthe followingcontext bc thehashtableas ht theauditthresholdas at and thezipperarrayas za first onlyafewobjectsinthe inthebccannotbetop abandoned second thresholdforthetop result at astop results seetheorem thesizeofthe za ingate basedonthe countvaluebound za zais basedindexingarray theindexstartsfrom denotedas zc andvalue za min zc the at ingate za at and za at iftherearealready za intheht already candidatesifthetop countthresholdisjust therefore the at increaseby when za at pqinalgorithm for eachquery onequeryitem pq withalgorithm when weimmediately at line ifitis wewillinsert orupdate anentryintotheht count meanwhile wewillupdatethe za line if za at islargerthan wealsoincreasethe at byoneunit line algorithm forathreadinablock itaccesses object intheinvertedindex then val bc bc val if val at then putentry val intotheht za val while za at do at wepresenttheorem pq theorem updatingc pq thetop and at supposethe matchcountofthe thobject ofaquery is mc mc thenwehave mc at accordingtotheorem wecanselectthetop objects greaterthan at only matchcountequalto at webreaktiesrandomly pqwith datainfig example givenadataset andaquery infig wewanttothetop resultof fromthe objects initiallywehave at za bc and ht foreasyexplanation by and asshowninalgorithm list westaccess andget bc since bc at wehave ht and za notethatzais basedarray thus aftertheupdating za since za at and at thenwehave at thenwe update bc and bc withoutchanging and are smallerthan at withthesamemethod afterprocessing weonly have bc ht za at and bc thereisno init for wehave bc since bc at wehave ht za and at wealso have bc finally wehave ht and at bytheorem resultis at ht toselect theobjectequalto whichisjust here weproposea accordingtotheorem max count value weadoptalock freesynchroniza problem hoodschemeinthec smallerthan at inthehtcannotbetop candidates seetheorem at at memorybandwidth forthis problem andthen when introducedbefore afteraround wecollectall resultsfromeachpart results asthetop resultsamongthetop resultsof eachdatapart fig fig spectives first powerofthegpu postingslist therefore inasmanneraspossible second pqcanthetop kselectiontaskwitha scanningthehash tableonlyonce thisdata and fortop kselection sectionvi andsectionvi third inc pq mostopera theht besides since thethreadshave fourth times ourexperimentin sectionvi iv eneric anns earchwith lsh followed inthissection accordingtothe in ahashingfunction issaidtobelocality sensitiveifit pr sim measure here sim toanumberin where sim means and are identical annsearchproblem infig wetreateach forpoint underhashfunction isapair isasetofpoints whosehashvalueby is if and givenaquerypoint wealsoconvert anysimilaritymeasure canbe supportedbygenie space weusuallyresortto sensitivehashing functionfamily sim insectioniv re byparameters forexample thesignatureofthe weproposea re afterobtainingthe lshsignature thus tion notethat re lshcanbesmallenough fig re hashingmechanism where isalshfunctionand fig similarity thenumber withconstraint pr where casestudy wetake theannsearchonashift study theauthorsin proposeanlshfamily calledrandom binninghashing rbh forlaplaciankernel exp known asfarasweknow ithasnotbeen appliedtoannsearch foundin inexperiment weusethere hashing theoreticalanalysis countmodel forthispurpose called tolerance ann ann givenasetof points inaspace sim the suchthat sim sim ann whichistoapoint sothat sim sim withhighprobability someexistingworks like though withoutexplicit errorboundand ann genieforaquery isthe annof givenapoint and aquery supposethereare functionsin satisfying where countmodel weprovein theorem thatthereturnofmatch countmodelongenie and sim withhighprobability theorem sim anlsh family where toadomain with ln wecanconvertapoint and aquery count model whichare and thenwehave mc sim givenaquerypoint andproperof stated intheorem annof theorem givenaquery andasetofpoints ourmatch which mc sim withtheprobabilityat least supposethetruennof is andthetop countmodelis thenwehave sim sim theorem whichmaybeverylarge tasks dowereallyneedsuch beforeexploitingthis if islarge enough canbefactorizedas collisionscausedby pr pr pr pr where sim thus wehave pr suppose canre hash intoavery largedomain wecanclaimthat pr forsimplicity letusdenote mc anestimation of mle canbe mc eqn pr pr eqn thesimilaritymeasure sim therefore thereis noclosed nevertheless eqn countmodel ifwe ed and wecaninferthe pr accordingtoeqn fig bythismethod asimilarhasalsobeen illustratedin fig hashfunctions beingm appearsat whichis which is ln showninfig isdataindependent thus insteadofusing theorem eqn likefig earchingondatawith sa andassemblyﬂ sa givena dataset thenwebuild andthe uniqueunit whenaquerycomes itisalsobrokendownas asetofsuchsmallunits afterthat geniecaneffectively objectanddataobjects thereturnofmatch measure orbe editdistance to candidates shortdocumentdataand inthissection wedecompose thesequence intoasetof gramsusingalength sliding window givenasequence andaninteger the gramis alength subsequence of sincethesame grammay weintroducethe ordered gram whichisapair gram where denotesthe th same graminthesequence therefore wedecomposethe sequence intoasetofordered gram ingenie gramas example forasequence aabaab thesetof ordered grams of is aab aba baa aab where aab denotes thestsubsequence aab in and aab denotesthe secondsubsequence aab in duringquery process intoasetof ordered grams usingslidingwindows genie largecountintheindex lemma supposethesame gram appears times insequence and timesinsequence thentheresult returnedbythematch countmodelis mc min theresultofthematch theorem and is then thereturnofthematch countmodelhas mc max fj jg accordingtotheorem match forthequery candidatesfrom andthe candidatesto obtainthe thedetailofthe in withthismethod thoughwe candidatesarethereal top inotherwords afterthe vwecanknowwhether isthereal thmost theorem forthe thcandidates returnedby andquery is mc amongthe candidates wecan amongthe candidates and is ed if thentherealtop resultsare untilthe conditioninlemma however asshowninour experiment inthisapplication webuildaninverted document documents ifaword occursinthedocument otherwise itiszero theoutputofthematch countmodel whichisthe numberofwordco isjustthe innerproduct data infig value granularitylevel thetop selectionquery thetop tuples function sql orderby ingenie weuse count model vi xperiments settings datasets weusevereal system ocr it contains dimensions gb werandomlyselect testset andremovethem fromthedataset whichisfurtherre sift thisdataset contains msiftfeatures whichare dimensionalpoints itstotalsizeis gb werandomlyselect themfromthedataset lshfamily http largescale ml tu berlin de instructions http lear inrialpes fr jegou data php buckets dimensionalspace sift large wealsoextract theilsvrc imagedataset gb sifttoprocessthedata dblp fromthedblpwebsite gb werandomly choose andthenmodify thisdatasetistoserve specially weset and bydefault tweets thisdatasethas mtweets weremovestop wordsfromthetweets thedatasizeis gb we reserve ktweetsasaqueryset seesectionv adult which contains krowswith attributes mixedofnumerical andcategoricalones fornumericaldata wediscretizeall valueinto wefurtherdupli cateeveryrow times thus thereare minstances withsizebeing gb weselect ktuplesasqueries discretized value discretized value weuseit seesectionv competitors gpu lsh weusegpu lsh asacompetitor furthermore sincethere isnogpu kernelspace westillusegpu forannsearchofgenie wetheparametersof gpu qualityasgenie we onlyuse mdatapointsforgpu lshonocrdatasetsince gpu gpu spq likemethod ongpuasacompetitor computematch thenweuse agpu basedfastk selection wenamethis top whichdenotesgpufast http image net org challenges lsvrc index data http dblp uni trier de xml https dev twitter com rest public ﬁcityﬂ etc itis http archive ics uci edu ml datasets adult http gamma cs unc edu knn riorityq ueue wegivean introductiontospqin notethatforannsearch we notoriginaldata cpu idx memory we countmodelforeach object with klogn worst caseperformance inc stltogetthe largest cpu lsh weusecpu lsh forannsearchinhigh codefromauthors website appgram thisisoneofthestate of the artmethodsfor notethatappgram sinceappgram whilegenieonlydoes thussometrue knnsmaybemissed havetruetop explorethetrueknns insectionvi gen spq insteadofc pq foreachdataset however insteadofusingc pq seesection iii weusespq spq environment gpuplatform titanxwith gbmemory incuda otherprogramswereinc oncentos server with unlessotherwiseweset andsetthe all times bydefault foronebatchprocess canbefoundin forannsearch weusethemethod thereforethenumber ofhashfunctionsis efofgenie wecomparetherun wedonot axisislog scaled ourmethod outperformsgpu http ss sysu edu cn fjl lsh lsh source code tar gz tablei genie for queries theunitoftimeis second stage ocr sift dblp tweets adult indexbuild indexloading query transfer match selection gpu furthermore gpu queriesinparallel foronebatchprocess butgenie cansupportmorethan queriesinparallel aswecanseefromfig gpu therunningtimeof gpu thisisbecausegpu query thus gpu thereare queries notethatweonlyuse datapointsforgpu lshonocrdataset fig queriesforone batch wethequerynumberas inthisexperiment the ofdatasize nevertheless therunningtimeofgpu lshis lshusesmanylshhash shortblocks therefore fig lshfor alargernumber upto ofqueriesonsiftdata though gpu withsetting wecanseethatthetime costofgpu lshtoprocess seconds queries with batches in seconds timepr stagesofgenie theﬁindex thisisanone timecost the ﬁindex queriesperbatch theﬁquery theﬁquery pq fordblpdata matchﬂ thisourdesignchoice ocr sift dblp tweets adult fig ocr sift dblp tweets adult fig thequerynumberis tableii unningtimeof genie sift large datasetfor queries unit second sift large genie gpu lsh cpu lsh tableiii genie withmultiple able ii unit second sift large indexloading resultmerging genie total ifthe seesectioniii table sift largedataset inthisexperiment wesetthedatapart foreachloadingas mdatapoints loadings queries with secondson msiftdatapoints italsoshows sincegpu lshcannothandle mpoints weestimatetherunning timeofgpu lshon mand andresultmergetime wecanseethatgpu lshhasalmost index loading memoryand resultmerging therunningtimecost wecanseethatthe discussion itisnot basedal gorithmslikecpu lsh cpi idxandappgram lshis tableiv unit mb dataset ocr sift dblp tweets adult genie gen spq the mainbottleneckofgpu lshistoselecttop kcandidates whosemethod computation meanwhile genieoutperformsgpu gpu spqusesak whereasgenieonly whichis at spq effectivenessofc pq infig gen spq pq wecan seethat withthehelp ofc oftheoneofgen spq withthe datasize thenwe gb effectivenessofgenie inthissection annsearchwithgenie lsh theused evaluationmetricis approximationratio whichis time wetheparametersofgpu lshandgenie adetailed ocr sift dblp tweets adult fig theeffectivenessofc pq tablev redictionresultof ocr databy nn method precision recall score accuracy genie gpu lsh methodcanbefoundin fig fig approximationratiov valueofkonsiftdata fig lshand genie varying whereasgpu when issmall lshwithsmaller thereason stopconditions thuswithlarger approximationratio gpu gpu lshuses gpu thus largerthan weuse only lshsince weincreasethenumber ofhashtables as tablesforgpu lshissetas performanceofgpu genie whichwill after thesearchongenie notobtainrealtop results seesectionv tablevishows searchresults queries aswe seefromtablevi withlessthan genie evenwith than ofqueries tablevi ccuracyoftop searchon dblp dataseton genie querylength and percentof accuracy latencytiem similaritysearchis typing where withinminimum forthe querieswitha latencytimeof second asshowninfig andtablei querytransfer matchingin accuracy adiscussionon isin vii elatedwork spatialindexmethods dimensionalityishigh nearestneighbour werefer toscaleupthe similaritysearch thelsh charikar inves wangetal differentdatatypes havebeendevised ofsub units grams forsequences andstars forgraphs sometimes avstepisnecessary editdistance betweenthe similaritysearch however thereareafew gpu panetal level lshalgorithm the space therearesomeworks aninverted likeindex asfarasweknown tree somegpusystemsfor key pq whichcan basedapproach likelossycounting andspacesaving andsketch basedapproach like count min andcount sketch however bothap whereasc pqcanreturn theexacttop frequentitems moreover severalfrequent itemalgorithms min requirepriorityqueue likeoperations makingthemnontrivial viii onclusion inthispaper wepresentedgenie framework inthematch countmodel like support thelshscheme withthesascheme inparticular weinvestigatedhowto highdimensionalspace anddocumentdata andtop cknowledgments itis sgfunding theworkbyh andiis eferences luo wong andl leong trees onthegpu ﬂin asp dac pp wang ding tung andz zhang ﬁefandeffective grams pvldb vol no pp ding he yan andt suel ﬂin www pp panandd manocha ﬁfastgpu ﬂin gis pp charikar rithms ﬂin stoc pp datar immorlica indyk andv mirrokni ﬁlocality sensitive stabledistributions ﬂin socg aparicio chapman stupka putnam chia etal ﬁwhole rubripes science vol no pp she jiang clark liu cheng tuzun church sutton halpern ande eichler nature vol no pp alabi blanchard gordon andr steinbach ﬁfastk selection jea vol pp zhou guo jagadish kr al liu luan tung yang andy zheng arxiv yan yu andj han databases ﬂin sigmod pp yang kalnis anda tung structureddata ﬂin sigmod pp he agarwal ands prasad corearchitectures ﬂin hipc garc lefebvre hornus anda lasram ﬁcoherentparallel hashing acmtog vol no moazeniandm sarrafzadeh ﬁlock processors ﬂin saahpc pp indykandr motwani towards ﬂin stoc pp wang shen song andj ji asurvey arxiv rahimiandb recht scalekernel machines ﬂin nips pp satuluriands parthasarathy fastsimilaritysearch pvldb vol no pp sutinenandj tarhio ﬁfiltrationwithq matching ﬂin pp jegou douze andc schmid ﬂin eccv lichman http archive ics uci edu ml panandd manocha ﬁbi nearest neighborcomputation ﬂin icde pp gan feng fang andw ng ﬁlocality ﬂin simod pp sun wang qin zhang andx lin ﬁsrs solvingc spacewithatinyindex pvldb vol no pp tatikondaands parthasarathy ﬁhashingtree structureddata meth odsandapplications ﬂin icde pp ao zhang wu stones wang liu liu ands lin pvldb pp zhouanda tung ﬁsmiler asemi systemforsensors ﬂin sigmod pp zhang wang yuan guo lee andx zhang ﬁmega kv memorykey value stores pvldb vol no pp hetherington rogers hsu connor andt aamodt onheterogeneouscpu gpusystems ﬂin ispass pp cormodeandm hadjieleftheriou streamsofdata cacm vol no pp mankuandr motwani streams ﬂin vldb pp metwally agrawal anda abbadi ﬁanintegratedef tods vol no pp cormodeands muthukrishnan thecount journalofalgorithms vol no pp charikar chen andm farach colton indatastreams tcs vol no pp 
classification chaoqiao bohuang guochengniu darenli daxiangdong weihe dianhaiyu huawu àbaiduinc beijing china china qiaochao huangbo niuguocheng lidaren daxiangdong hewei yudianhai wu hua baidu comabstract suchamethodcan however gramislarge inthispaper speciþcdis grams without inourmodels parts localcontext introduction search queryclassiþcation andsentimentanalysis asimpleyeteffec of words andtrainaclassiþer supportvectorma chines joachims fanetal andnaivebayes mccallumetal although bag of therepresentationsdo pangetal exploitedn grams pangetal wang manning joulin etal althoughn gramsareveryuseful thenumberof gram nincreases exploitlargen grams grammodelisvery large recently joulinetal whichcanlearnanduse grams morespeciþcally theembeddingofann gramisdeþnedasa low gram notethatthen berepresentedasone hotvectorswang manning chao qiao outlook com bohuang gmail com inthispaper gramsforaspeciþctask classiþcation fol zhang however dependentandacquired supervisedlearning forregionembeddings intuitively the extended gramthusconsistsof twoparts named òlocalcontextunitó the regionembeddingofann gram inthispaper andthebagof network of the related bag of words inaddition thehand craftedn manning suchasnaivebayes mccallum etal joachims fanetal however recently thepre word vec mikolovetal socheretal ontheotherhand tuneonpre trainedword iyyeretal fasttext joulinetal information cnn kim johnson zhang zhangetal rnn tangetal lai etal yogatamaetal morerecently thetransformer vaswanietal ase although inthe restofthissection cnnandtransformer whicharethemost relevanttoourwork fasttext andusesafullcon fasttextuseshand craftedn gramsas similarly differently ourmodelsdirectly hand cnncnnisafeed wordsare both wordindependent whichcanbe unlikecnn whichare worddependent moreover transformer vaswanietal thetransformer introduced therearealsosome differenceshere whilevaswanietal inourmethod theircontextunits sinandcos method inthispaper morespeciþcally with wistandingfor thei th startingfrom wordofthedocument weuse region todenotethe wi forinstance givenasentencesuchas hotel region meansthesubsequence foodisnotverygood inthiswork word context wordregionembedding figure ocalcontextunit word embeddings moreformally theembedding ewofword wis rh vwithalookuplayer where visthesizeofthe vocabulary histheembeddingsize asmodelparameters formally kwi rh ofwiasa rh eachcolumnin ofwi infact wordtoitscontext andwecallthe formally let piwi wi tini thwordõsview and kwi tbethe thcolumnin kwi giventheunit ewi tofwi weuseanelement wisemultiplication denotedby tocompute piwi piwi kwi ewi wi function aparticularcolumnof kwi kwi ofkwicanbe ewiitself whichtransforms ewitothesamespaceas ord we region inthispaper word words forexample inthesentence theoccurrenceofword to contextview obtained word to contextview word formally weusethecontextunit kwiofmiddleword region thentheword contextre gionembedding matrix max piwi cpiwi piwi piwi wheremax finally weget region withdimension figure ashows forinstance inthesentence inthe region contextunitof food is not veryandgood theembedding ofregion ontext wordregionembedding andwecallthiscontext wordregionembedding similarly fora region thenthecontext matrix max pi cwipi wi pi wipi cwi figure theword whilethecontext fortextclassiþcation sized sizevectors we formally denotesthesoft signfunctionand or experiments atasets toevaluateourmodels therearein total newsclassiþcation question answer respectively table usedinourexperiments etal isemployed aselines wereport then volutionalmodel char cnn ofzhang lecun rentnetwork char crnn ofxiao cho vdcnn ofconneauetal lstm ofyogatamaetal andthe bigramfasttext bigram fasttext ofjoulinetal table statisticsofdatasets lengthstrain samplestest samplestasks yelpreviewpolarity amazonreviewpolarity amazonreviewfull agõsnews news yahoo answers ontologyextraction mplementation details fordatapreprocessing convertedtolowercase of vocabulary oov items additionally lengthof cpaddingare forourmodels dataset is theregionsizeis andthebatchsizeis foroptimization theembeddings adam kingma ba isusedastheoptimizer likel normalization ordropout gpus thecode esults table testsetaccuracy modelyelpp yelpf amz amz agsogouyah dbp bow ngrams ngramstfidf char cnn char crnn bigram fasttext vdcnn lstm region emb region emb table lishedresults onsixdatasetsofeight state of the datasetsexceptvdcnn as aresult detailed reportedinappendixa https github com text representation local context unit furthermore whichis infact bothofourtwoproposed xploratory experiments inthissubsection componentofourmodel ourmodels contextregion sotheselection whereaslarge luckily ofdatasets actually figure andwhenthe sizeequalsto accuracy butstillgetsa promotion intuitively equalsto figure regionsize andembeddingsize onyelpreviewfull dataset and unigram fasttext bigramfasttext cnnandours weuseregionsize forcnnandours furthermore hereweuse inþgure sizecombinationcanbe forexample insentimentanalysis word however followingwords mensions figure dimensions infact isrelativelylarge dimension regionsize times where speciþcnumbers infasttext incnn and inours ffectof contextunit inthissection table table forfast text unigram embeddingdimensionis forfasttext win pool region emb scalar and region emb ourmodel regionsizeis fasttext unigram fasttext win pool region emb scalar region emb ourmodel firstly unigramfasttext wecallitfasttext win pool win pool of thanunigramfasttext secondly win pool distinguish able henceitcanberegarded wenamethis methodw region emb scalar comparedtothenon scalarmethod sizereduction furthermore region emb ourmodel region emb scalar where shapewith by wordsamples however but modiþerslike very good badandnounslike food foreachrowoftheþgure anormalizedl normofeach regionsize isadopteddefault annotation li while ri denotedastheright part intheperspectiveof however rithanli sodoes but forword very restofall veryhappy orverysad forword good notgood verygood andnotthatgood separately thepositionof sodoesword bad nounsfoodandmorning theheatmapofword deliciousfood orfoodwasmediocre whiletheword actually isualization inthissubsection classiþcation herewe categoryversion notthe figure analysisdataset regionsizeis and embeddingsizeis surroundingwords table greendenotespositive noc unit andwithcontextunit withc unit phrasenoc unit getyourwallet ready withc unit getyourwallet ready noc unit nothingremarkable but withc unit nothingremarkable but forclarity intable welisttwocases wordsandartiþcially rediftheyarenegative the weabbreviatethemas withc unit andnoc unit respectively forsentence getyourwalletready whichiscontext free thepolarityof crazy ispositive and highisnegative becausetheintensity ofcrazy ishigherthan high thepolarityofphrase pricesarecrazyhigh istotallypositive whichis amistake butwithcontextunit dependent undertheinßuenceof high crazy vanishesandphrase pricesarecrazyhigh foranothercase nothingremarkable butnotbadeither withoutcontext unit remarkable ispositive while nothing not bad performnegative respectively butwithcontextunit but weakens notandbadßips asaresult phrase butnotbadeither performspositive overall conclusion onitscontextwords of the and weare supervisedlearning atthesametime wearealso textclassiþcation acknowledgments program no cb references alexisconneau holgerschwenk lo õcbarrault andyannlecun arxivpreprintarxiv rong enfan kai weichang cho juihsieh xiang ruiwang andchih jenlin liblinear aug mohitiyyer varunmanjunatha jordanboyd graber andhaldaum eiii deepunorderedcompo in proceedingsofthe rdannualmeeting volume longpapers volume pp thorstenjoachims makinglarge technicalreport technicalreport sfb komplexit universit atdortmund neuralnetworks arxivpreprintarxiv semi viaregionembedding in pp armandjoulin edouardgrave piotrbojanowski andtomasmikolov textclassiþcation arxivpreprintarxiv yoonkim arxivpreprint arxiv adam arxivpreprint arxiv siweilai lihengxu kangliu andjunzhao classiþcation in aaai volume pp jiweili xinleichen eduardhovy anddanjurafsky innlp arxivpreprintarxiv andrewmccallum kamalnigam etal siþcation in aaai volume pp madison wi tomasmikolov ilyasutskever kaichen gregscorrado andjeffdean distributedrepresen in cessingsystems pp bopang lillianlee thumbsup in proceedingsoftheacl volume pp richardsocher alexperelygin jeanwu jasonchuang christopherdmanning andrewng andchristopherpotts treebank in proceedingsofthe cessing pp duyutang bingqin andtingliu in emnlp pp ashishvaswani noamshazeer nikiparmar jakobuszkoreit llionjones aidanngomez ukaszkaiser andilliapolosukhin ini guyon luxburg bengio wallach fergus vishwanathan andr garnett eds advancesinneu pp curranassociates inc url http papers nips cc paper attention is all you need pdf baselinesandbigrams simple classiþcation in proceedingsofthe linguistics shortpapers volume pp efþcientcharacter arxivpreprintarxiv daniyogatama chrisdyer wangling andphilblunsom stat arxivpreprintarxiv xiangzhang junbozhao andyannlecun character siþcation in pp indetail forallthe datasetsintable hyperparam theregionsizeis andthebatchsizeis epochindexstartsfrom and table datasetvocabulary sizew cregion sizeparameters time mins accuracy yelpp yelpf amz amz ag sogou yah dbp pandyelp onaccuracy table datasettriesnum performancevariance yelpp yelpf 
weihe zhongjunhe huawu andhaifengwang baiduinc no shangdi thstreet beijing china hewei hezhongjun wu hua wanghaifeng baidu nmt conductsend to end guagedecoder however themethodhassome limitations consumingtrainingand decoding of vocabularyproblem furthermore translations in smt features suchasatranslation modelandan gramlanguagemodel withthenmtmodel underthelog linearframework qualityofthestate of the to to introduction sutskever vinyals andle andbahdanau cho andben gio to endtranslation nmt typically anen anda intoconsideration however annmtsystemusually usesthetop whichcauses aseriousout of vocabulary oov problem whenoov thetransla correspondingauthor zhongjunhehezhongjun baidu com copyright intelligence www aaai org allrightsreserved shorttranslations sourcesentence monolingualcorpus therefore itisdifﬁcultforannmt whichisproventobeuse translation smt luongetal oovwordsinapost processingstep gulcehreetal rnn how ever problems intuitively ofthesmtcomponents the gramlanguagemodel nevertheless thecurrentnmt quality inthispaper linear framework weincorporate smtfeatures includingthe translationmodel gramlanguagemodel based smtapproach koehn och andmarcu andem tion andthe tences ourmethodhasthefol lowingadvantages thelog ilyextended linear fromtheword proceedings of the thirtieth aaai conference on artificial intelligence aaai dajia doubei chuanran releihengliu unkeos decoderencoder henc dec figure decoder problem onibmmodels brownetal thetranslationta tem thetranslation usedinapost processstep luongetal longtranslation weaddan linear framework scalemono the whileintheshallow fusion gulcehreetal rescoretop devlinetal aulietal choetal li liu andsun zhaietal themaindifference smtframework enhanced sourcenmt systemgroundhog bahdanau cho andbengio https github com lisa groundhog groundhog to endtrans lation we contain ingabout millionsentencepairs collectedfromthe web experimentsonchinese to of the artnmtsystem background decoder are network andthelog linearmodels thedominantframework rnnencoder decoder figure encoder decoder bahdanau cho andbengio for chinese to englishtranslation givenasourcesentence fintoasequence ofvectors previouslygenerated schusterandpaliwal withahiddenlayer attheencodingstep theencoder fintowordvectors xirkx where ofthesourcelanguage hienc genc xi hi enc where the tanhfunction hienc hienc hienc sentence atthedecodingstep quenceiscomputedas jj yj yj yj jj gdec sjdec yj cj where sj whichiscom putedby sj dec gdec sj dec yj cj gdecandgdecarenon thecon textvector statesoftheencoder cj txi jihienc where theweight yjistranslatedfroma sourceword xi bahdanau cho andbengio used afeed modeltoestimate basicrnnencoder ﬁxedlength followingchoetal bahdanau cho andben gio reset gatesandupdategates the reset which the updategatescontrol short term memory lstm sutskever vinyals andle butmuchsimpler thernnencoder andperformsanend to endtranslation however underthe currentarchitecture log linearmodels thewidelyusedlog ducedbyochandney exp mi ihi eexp ihi where hi iistheweight thestrengthofthelog easilyaddedintoit astandardphrase basedsmt koehn och andmarcu typicallycontains features thebi andp the xicjsjyj yj yj gramlangugemodel wordtranslatio jijip yj xi figure illustrationofthelog linearnmt topredict thetargetword yj suchasthe the gramlanguagemodel bi plex andplex the languagemodel thereorderingmodel thewordpenalty and thephrasepenalty recently toimprovelanguagemod eling devlinetal translationmodeling aulietal choetal andphrasereordering li liu and sun zhaietal thenmtmodelviathelog linearframework makingthe nmtmodelextendable log linearnmt weusefig ure toillustrateouridea getword yj rnn gramlan guagemodel thetranslationtable estimatingfromword translatethelow knownwords weusea log featuredeﬁnition thernnencoder decoderfeature thisfeatureisthe hrnn jj log yj sj cj thebi ateach stepofdecoding sourcewords htp jj ii jilog yj xi htp jj ii jilog xi yj where targetword estimated bythernnencode decoder section rnnencoder decoder andp och andney andthe grow diag ﬁnal koehn och andmarcu method lows xn yn wheren istheco words xandy thestandard gramlanguagemodel hlm jj log yj yj pus thewordrewardfeature hwp jj weadd thetranslationcanbe asmentioned thenmtencoder riousoovproblem thepost processingmethod luong etal duringdecoding seefigure forillustration weﬁrstlyﬁnditscor respondingsourceword the unk symbol hengliucrossﬂow hengliuover translationtable figure translationtable bythernnencoder decoder hengliu thenweobtaintrans theﬁnal linearmodel bah danau cho andbengio ˆy argmax yp givenasourcesentence initialstate ateachtimestep thedecoderselectstop states nisthebeamsize of sentence eos symbolisproduced theﬁnaltranslation highestscore lationunderthelog linearframework ateachdecoding state equation toselecttop cabulary inourdecoder foreachwordintar getvocabulary the weightsofthelog dardminimum error rate training mert och al gorithm tospeedupthedecoder weuseapriorityqueue huetal to englishtransla tion theweb containingabout billionenglishwords sofarasweknow thisisthelargest systemdevtest groundhog tm wr lm pbsmt table tm translationmodel wr wordreward lm language model pbsmt phrasebasedsmt weusednistmt oursystemonnistmt insensitivebleu papinenietal thefeature dardminimum error rate training mert och to maximizethesystems weusetheopen sourcenmtsystem groundhog bah danau cho andbengio baselinesystem wesetbeamsizeto fordecoding as acomparison basedsmt pbsmt system whichisare implementation ofthestate of the artphrase basedsystem moses koehn etal andthe forthesmt system wesetthestack limitto andthetranslation option limitto training totrainthe groundhog system welimitthevocabulary to getlanguages bol unk abackwardrnn andeachhas hiddenunits the decoderhas hiddenunits thewordembeddingsare dimensional amini sgd togetherwithadadelta zeiler areusedto trainthenetworks eachmini batchofsgdcontains sentencepairs ofparameters and weranboththe card nvidiateslak forthe pbsmtsystem thegiza ochandney andthe grow diag ﬁnal koehn och andmarcu method wetraineda gram languagemodel stolcke withkn discountonthe thewordtranslationta integratedwiththe groundhog system resultstable weobservedthat moreover oursys largetrainingcorpus speciﬁcally table wardfeatures mentsoverthebaseline tm wr provements firstly sec ondly tionsofunknownwords thirdly thewordrewardfeature theaverage groundhogare and respectively thisindicates lem thenextsection weaddeda gramlanguage tothe groundhog system row lm asthe gramlanguage improvetheﬂuency comparedwiththe groundhog system oursystem groundhog tm wr lm mentof pointsinbleuscore whichisstatistically signiﬁcantatp level riezlerandmaxwell method asanexample the chuanshu transmission infact ourmethodproduces groundhog sourceandtargetwords inthisexample thetranslationta chuanshu transmission with highprobabilities we employaconventional gram languagemodeltoim provethelocalﬂuency forexample thereisanotheren try chuanshu transfer forthechineseword chuanshu the because plm transmission seriesofhighspeed isgreater thanplm transfer seriesofhighspeed sourcer pbsmtyes likether laptop wirelesstransmission groundhogyes likether laptop ourmethod yes likether laptop aseriesofhighspeed transmissionofthe source crossﬂowoftears andtearsunk ourmethod andtears crossﬂow table translationexamples systemoovpercentage devtest pbsmt groundhog ourmethod table thepbsmt table pbsmt groundhog andoursystemsonnist andnist test sets alignmenterror however forthe groundhog asshownin thesourceword hengliu isnottranslatedby groundhog linearframework thiswordwas crossﬂow moreover inthepbsmtsystem asweknow pbsmtsystemex tractsword alignedbilingual corpus however ment phrasetable ideally thernnencoder decoderislim rnnencoder andpbsmt table thetranslationtable therow ourmethod sharesthesame settingswithsection experiment inthesesettings thetrans butalso systemdevtest ourmethod oov table ourmethod groundhog tm wr lm figure therow oov meansthat decoder butnotgenerate itisobserved provementof figure sizeinthedecoder wecanseethatthe sizeincreasesafter ontheotherhand theperformanceof size thereasonisthat withthebeamsize increasing inthispaper smtcomponents thetranslationtable thelanguage model underthelog linearframework whichmakesthe thetranslationtable istrainedonword dardphrase basedsmtmethod andthelanguagemodel theproposed chitecture andthelanguage lingualcorpus experimentsonchinese to englishtrans asanewapproach provement currentrnnencoder decoderisactuallya word inthefuture weplantoim localwordreordering idiomtranslation etc acknowledgements programofchina programno cb we references auli galley quirk andzweig joint works in proceedingsofthe bahdanau cho andbengio neuralma in arxiv cs cl brown pietra pietra andmer cer translation parameterestimation computationallinguis tics cho vanmerrienboer gulcehre bahdanau bougares schwenk andbengio learning decoderforstatis in proceedingsofthe con cessing emnlp devlin zbib huang lamar schwartz and makhoul in proceedingsofthe linguistics gulcehre firat xu cho barrault lin bougares schwenk andbengio on in arxiv cs cl hu li lan wu andwang op in mtsummitxv koehn hoang birch callison burch fed erico bertoldi cowan shen moran zens dyer bojar constantin andherbst moses translation in acl demonstrationsession koehn och andmarcu statisticalphrase basedtranslation in proceedingsofhlt naacl li liu andsun foritg basedtranslation in ing luong sutskever le vinyals and zaremba in proceedingsofthe rdan ticsandthe languageprocessing volume longpapers och andney inproceedingsofthe och andney och in proceedingsofthe stannual riezler andmaxwell onsomepitfallsin in pro orsum marization schuster andpaliwal bidirectionalrecur rentneuralnetworks signalprocessing ieeetransactions on stolcke srilm toolkit in volume sutskever vinyals andle sequenceto in nips zeiler adadelta method in arxiv cs lg zhai zhang zhou andzong rnn based in proceedingsofthe linguistics 
thepowerofgradients jiajihuang rewonchild vinayrao huangjiaji rewon vinay baidu com hairongliu sanjeevsatheesh adamcoates liuhairong sanjeevsatheesh adamcoates baidu com bordeauxdr sunnyvale ca abstract and forspeechrecognition scoresand otherlikelihood gradient however arestillnotwell understood egl approachinactive learningforend to endspeechrecognition wejustify egl fromavariancere ductionperspective andobservethat egl experimentally weshowthat egl oralternatively reducethenumberofsam plestolabelby introduction state of the asr systems however isexpensiveandtime consuming howmuch scale end to endasrsystems however culatingscores inparticular bels ongradient egl egl haspreviouslybeenas sparameters andexperimentally weshow egl tobesu additionally weobservethatthe egl suggesting egl equalcontribution nips barcelona spain arxiv cs cl dec in egl butourworkisthewe knowoftoapply egl gradient forexample convergence problemformulation denote asanutteranceand transcription where and rnn pairs denotedas negativelog min log outtestdataset inmanyappli cations is queried forthe forwhichthelabel arethenobtained wediscussseveral such querystrategies below scores an labels bytakingtheinstances sampledregionsofthe inputspace intuitively rameters kr motivatedbythis intuition egl dientlength egl interprets egl egl anestimator hasthefollowingform where isthetrueparameter and isindependentof data sothatthe pairsaredrawnfrom where def log log withrespectto wehaveasymptotically eq ontestdata overthetestset criteriain tr substitutingeq into wehave log log whichisequivalentto max kr dyd inadvance fromapre trainedmodel tobeclosetothetrue the argmax kr forrnns propagation another practicalissueisthat egl the therefore weonlymarginalize overthe asin the egl methodin isalmostthesameaseq exceptthegradient snormisnotsquaredin egl wedenoteeq as egl in subsequentsections experiments egl inourexperiments thernntakesin convolutionallayers followedbyseven bi allrecurrent ateachtimestep overthecharacters ctcloss wiseprobabilities abasemodel istrainedon hours kinstances then itselectsasubsetofa hour minstances unlabeleddataset wequerylabelsforthe of instances hour kinstances testsetand wereportctcloss charactererrorrate cer andworderrorrate wer thescoremethods fromthe entropy ctcloss pctc predictedctc in thefollowingsections ctclossondevset cerondevset werondevset figure egl smalleramountsofdata weimplement egl labels andcompareitwith random selectionbaseline entropy and pctc eachmethod set fig reportsthemetrics exactvalues arereportedintable intheappendix allthe random baseline moreover egl showsasteeper more whenquerying oftheunlabeled dataset egl has lowercerand lowerwerrelativeto random theperformance of egl atquerying isonparwith random at suggestingthatusing egl canleadtoan approximate nessofaninstance thesetwomethods andplotthemina dranking vs aplotcloseto fig showstheranking vs rankingplotsbetween pctc and entropy egl and entropy weob servethat pctc rankingsand entropy rankings fig areverycorrelated thisislikelybecause incontrast egl en tropy fig thissuggests egl based pctc rankingsvs entropy rankings egl rankingsvs entropy rankings figure rankingsarenor malizedto with in pctc and entropy in egl entropy and pctc egl butuninformativeby entropy egl and entropy informativeness withbackgroundnoise orwords fortrainingend to weformallyexplained egl performanceonend to randomselection we alsoshow egl singlebatch inthefuture appendix table smallerisbetter bestinbold query ctc cer wer random entropy pctc egl random entropy pctc egl random entropy pctc egl references amodei ananthanarayanan anubhai etal deepspeech end to endspeech in graves fernandez gomez andj schmidhuber in proceedingsofthe rd riccardiandd hakkani tur activelearning recognition settles technicalreport madison settlesandm craven in associ sourati akcakaya leen erdogmus andj dy asymptoticanalysisof arxiv varadarajan yu deng anda acero in speechandsignal processing zhangandf oles in proceedings ofthe zhaoandt zhang minimization in proceedingsofthe 
deepspeech scalingupend to end speechrecognition awnihannun carlcase jaredcasper bryancatanzaro gregdiamos erichelsen ryanprenger sanjeevsatheesh shubhosengupta adamcoates andrewy ng abstract wepresentastate of the to enddeeplearning systems thesetradi in contrast groundnoise reverberation orspeakervariation optimized fortraining oursystem calleddeepspeech achieving erroronthe fulltestset widelyused state of the introduction andhand inthispaper wedescribeanend to endspeechsystem calledﬁdeepspeechﬂ combinedwitha languagemodel rnn becausethis ornoiseinfact oursystemexcels hub corpus achieving error includingspecialized inputfeatures acousticmodels hmms theintroduction usually asaresult incontrast to endusingrecurrent neuralnetworks ourmodelistrainedend to endtoproduce contactauthor awnihannun baidu com arxiv cs cl dec tappingtheofend to enddeeplearning however wemust ii thisproblemhas fern andez gomezandschmidhuber meanwhile rapidtrainingof multi gpucomputation system al whoapplied builtspeechfeatures additionally takentogether theseideassufto buildanend to onthefullswitchboard hub further ourownconstruction systemsachieve error useinsection section andourdatacapture andsynthesisstrategy section state of the section andourconclusions rnntrainingsetup rnn letasingleutterance andlabel besampledfroma trainingset eachutterance isatime seriesoflength whereeverytime weusespectrogramsas ourfeatures so denotesthepowerofthe the transcription with where space apostrophe blank layersofhiddenunits foraninput aredenoted istheinput forthelayer ateachtime alongwitha contextof framesoneachside theremainingnon foreachtimestep thus foreachtime the layersarecomputedby where min max istheclipped relu thefourthlayerisabi directional recurrentlayer wetypicallyuse forourexperiments inpractice notethat to forthe thutterance while theunits to theh non recurrent where andcharacter inthealphabet exp exp here and denotethe thbias respectively wecomputethectcloss tomeasure theerrorinprediction duringtraining withrespectto fromthispoint computingthe restofthenetwork weusenesterov figure andwedonotuselong short term memory lstm circuits thissmall computingthereluout wise nonlinearity weusemomentumof convergence regularization section therecurrent weuse severaltechniques ratebetween andvote oraveragetheresults howeverwefounditto ms halfthebankstepsize totheleftandright thenforward languagemodel readablecharacter leveltranscriptions themostlikelychar the table showssomeexamples trainingset inpractice thisishardtoavoid hear allofthe therefore weintegrateour systemwithann textcorpora forcomparison millionutterances then millionphrases words rnnoutput decodedtranscription table left tionofalanguagemodel right giventheoutput wherethelanguage weaimtoasequence that log log lm word count where and aretunableparameters setbycross validation thatcontrolthetrade offbetween thernn theterm lm denotesthe accordingtothen grammodel šsimilar optimizations asnotedabove speedexecution andthusfasttraining forexample optimizedblas calls whenfullyunrolled weusethekenlmtoolkit totrainthen weusemulti gpu training work asweexplain dataparallelism first eachgpuprocesses singlematrix forinstance vectormultiplication inthe recurrentlayer where where correspondstothe thexample attime thegpuismostefwhen is relativelywide examplesormore onegpuaspossible wetypicallyuse or however this asimilarsolutionwas modelparallelism to thatis processing asmanyexampleson asmany gpusfailstoyielda speedupintraining asmanygpus astheminibatch within eachgpushrinks most bandwidthlimited toscalefurther model ﬁmodelparallelismﬂ since independent unfortunately naivelysplittingthe rnntoplace and compute whichdependsonboth and thus time dimension withthe halfofthetime series from to toanothergpu at themid point and andswaproles andthesecondgpu striding sincetheseare asaoptimization orstrides ofsize thisis withastep sizeof inthelayer weusethecudnn library wsjread fisherconversational baiduread table thewallstreetjournal switchboard andfisher trainingdata large foroursystemweneed ofsufscale of speakers forcomparison inourwork down capturinglabeleddata readspeech however toaorder forexample to ifnecessary echoesor or thereare however forexample inordertotake hoursofclean speechandcreate hoursofnoisyspeech hours wecannotsettlefor say thus withalengthof hours weusealargenumber ofshorterclips toensureagoodmatch recordings ﬁlombardeffectﬂ noisearoundthem this involuntary theirvoice experiments describedinsection topredictcharacter level transcriptions toyieldaword leveltranscription theworderrorrate wer conversationalspeech switchboardhub full hub ldc switchboard andﬁhardﬂ callhome instances weusethefullset whichisthe swb andfisher fsh hourcorpuscollected hours inpartthisisbecause trainingonthefull tionedinsection hours khz wecompute spectrogramsof thebanksarecomputed overwindowsof msstridedby ms mel particularlywhen trainedon hourswitchboard weapplyasimpleformof otherthanthis we fordecoding weusea again cross validationonaheld outdevelopmentset hiddenlayerseachwith neuronstrainedon only hourswitchboard thedeepspeechswb rnnseach with hiddenlayersof hourcombinedcorpus allnetworks aretrainedoninputsof framesofcontext dnn gmmsmbr usesase hmmsystemto testsetisthebest absolutewerand relative themodelfrommaas etal dnn hmmfsh achieves hourcorpus state of the using modelswbchfull veselyetal gmm hmmbmmi veselyetal dnn hmmsmbr maasetal dnn hmmswb maasetal dnn hmmfsh seideetal cd dnn an kingsburyetal dnn hmmsmbrhf an sainathetal cnn hmm an soltauetal mlp cnn vector an deepspeechswb deepspeechswb fsh table publishederrorrates wer noisyspeech of noisyand noise freeutterancesfrom speakers backgroundradioortv washingdishesinasink acrowdedcafeteria arestaurant andinsideacar drivingintherain as wellasnewsclippings phoneconversations internetcomments publicspeeches andmoviescripts to noiseratio snr ofthenoisysamples butwe aimedforansnrbetween and db morethan hours listed intable sincewetrainfor to ourmodel learnsfromover hoursofnoveldata weuseanensembleof networkseachwith hidden layersof neurons exampleconsistent thefeaturesare of msstridedby msandanenergyterm audioareresampledto khzpriortothe featurization finally stagesoftraining asdescribedinsection weusea wetrainthelan guagemodelon ofthe onlythemostcommon wordsarekept the restremappedtoan unknown token wit ai google speechapi bingspeechand appledictation emptyresult wetrainedtwo rnns oneon hoursplusnoise on the werand werforthe commoncrawl org wit basedapis weused thewindows errorincaseswherean apifailedtorespond however onthe noisyutterances weroverthecleanmodel wer absoluteand relativeimprovement systemclean noisy combined appledictation bingspeech googleapi wit ai deepspeech table results wer for scoresarereported only clean relatedwork thesesystems recognitionpipeline mechanically to endspeech forexample gravesetal ctc byrnnsand withlstmnetworks wesim withactivations rnnusedbyhannunetal byfocusing onscalability complexlstmmachinery studied includinggpus scaledlresults earlyportsofdl andlocallyconnected networks andblasareavailable indeed usinghigh itispossible billionconnections usingclustersofgpus inother suchascomputervision inspeechrecognition however fromtensofhours hours with tofullyutilize labeledutterances thisapproachis conclusion wehavepresentedanend to enddeeplearning ingstate of the clear conversationalspeech gputrainingand systemmusthandle combined thesesolutionsen ableustobuildadata we acknowledgments forhis wealsothankianlane danpovey danjurafsky dario amodei andrewmaas references bourlardandn morgan ahybridapproach kluwer academicpublishers norwell ma chetlur woolley vandermersch cohen tran catanzaro ande shelhamer cudnn cieri miller andk walker thefishercorpus speech to text in lrec volume pages ciresan meier masci gambardella andj schmidhuber flexible high internationaljoint pages ciresan meier andj schmidhuber multi in pages coates carpenter case satheesh suresh wang wu anda ng in coates huval wang wu ng andb catanzaro deeplearningwith cotshpc in coates lee anda ng ananalysisofsingle learning in pages dahl yu deng anda acero context dependentpre speech andlanguage processing dean corrado monga chen devin le mao ranzato senior tucker yang anda ng in ellisandn morgan sizematters in icassp volume pages ieee glorot bordes andy bengio in thinterna pages graves fern andez gomez andj schmidhuber in icml pages acm gravesandn jaitly towardsend to works in icml grosse raina kwong anda ng shift arxivpreprintarxiv hannun maas jurafsky anda ng first abs http arxiv org abs pouzyrevsky clark andp koehn scalablekneser neylan guagemodelestimation in proceedingsofthe bulgaria hinton deng yu dahl mohamed jaitly senior vanhoucke nguyen sainath andb kingsbury speechrecognition november hinton srivastava krizhevsky sutskever andr salakhutdinov improv abs http arxiv org abs junqua nizers kincaid oppe andd young itpackv dusersguide kingsbury sainath andh soltau freeoptimization in interspeech krizhevsky sutskever andg hinton neuralnetworks in pages le ranzato monga devin chen corrado dean anda ng building high in machinelearning lecun boser denker henderson howard hubbard andl jackel neuralcomputation lecun huang andl bottou in volume pages lee pham largman anda ng in systems pages maas hannun lengerich qi jurafsky anda ng increasing abs http arxiv org abs maas hannun anda ng acousticmodels in speech andlanguage processing mohamed dahl andg hinton ieee transactionsonaudio speech nairandg hinton in pages povey ghoshal boulianne burget glembek vesel goel han nemann motlicek qian schwarz silovsky andg stemmer thekaldispeech recognitiontoolkit in asru raina madhavan anda ng large processors in renals morgan bourlard cohen andh franco sainath kingsbury mohamed dahl saon soltau beran aravkin andb ramabhadran in asru sainath rahmanmohamed kingsbury andb ramabhadran deepconvolutional in icassp sapp saxena anda ng objectrecognition in aaaitwenty schusterandk paliwal ieeetransactions onsignalprocessing seide li chen andd yu dependentdeepneural in asru soltau saon andt sainath convolutional neuralnetworks in icassp sutskever martens dahl andg hinton in sutskever vinyals andq le http arxiv org abs szegedy liu jia sermanet reed anguelov erhan vanhoucke and rabinovich vesely ghoshal burget andd povey sequence neuralnetworks in interspeech 
deepspeech end to darioamodei rishitaanubhai jingliangbai ericbattenberg carlcase jaredcasper bryancatanzaro qiangcheng guoliangchen jiechen jingdongchen zhijiechen mikechrzanowski adamcoates gregdiamos keding niandongdu erichelsen jesseengel weiweifang linxifan christopherfougner lianggao caixiagong awnihannun tonyhan lappivainojohannes bingjiang caiju billyjun patricklegresley libbylin junjieliu yangliu weigaoli xiangangli dongpengma sharannarang andrewng sherjilozair yipingpeng ryanprenger shengqian zongfengquan jonathanraiman vinayrao sanjeevsatheesh davidseetapun shubhosengupta kavyasrinet anuroopsriram haiyuantang liliangtang chongwang jidongwang kaifuwang yiwang zhijianwang zhiqianwang shuangwu likaiwei boxiao wenxie yanxie daniyogatama binyuan junzhan zhenyaozhu bordeauxavenue sunnyvaleca usa no xibeiwangeaststreet kejiyuan haidiandistrict beijing china abstract weshowthatanend to enddeeplearningap languages ofhand works end to environments techniques thisallowsus asaresult inseveral cases onstandarddatasets finally usingatechnique ter delivering introduction decadesworthofhand goneintocurrentstate of the tion asr pipelines to end usingdeep contactauthor sanjeevsatheesh baidu com proceedingsofthe rd learning newyork ny usa jmlr cpvolume copyright bytheauthor in hannunetal and graves jaitly this endtoend bootstrapping alignment clustering hmmmachineryoften usedtobuildstate of the artasrmodels onsuchasys tem builtonend to enddeeplearning wecanemploya sets puting workarchitectures tecture andcomputational andourdatacapturing levelperfor mance tothatend son webeginwitha end to endspeech recognition section de themodelandsection explainshowtoefcom putethem ananalysis sentedinsection deepspeech end to relatedwork feed forwardneuralnet years ago bourlard morgan renalsetal re time robinsonetal waibeletal more mohamedetal hin tonetal dahletal jaitly vanhoucke seideetal abdel hamid etal sainathetal recurrentneuralnet of theartrec ognizers gravesetal saketal and tion sainathetal end to search hmm graves jaitly andstandalone hannunetal thernn encoder ingphonemes chorowskietal orgraphemes bah danauetal chanetal thectcloss function gravesetal coupledwithanrnnto to graves jaitly hannunetal maasetal thectc phonemes miaoetal saketal thougha successofthethusfar krizhevskyetal le etal tialperformancegains rainaetal whichweresub krizhevskyetal ormoregpus coatesetal wetakeadvantageof level chetluretal webuiltonthe pastworkinusingmodel parallelism coatesetal data parallelism deanetal oracombinationofthe two szegedyetal hannunetal tocreatea speechrecognition to end speechrecognition withover hoursoflabeledspeech usedin hannunetal dataaugmentationhas lecunetal sappetal coatesetal andspeechrecognition gales figure etal hannunetal forex ample panayotovetal modelarchitecture figure andlays tailinthispaper oursystem onein hannunetal work rnn uniorbidirectional lay to tion gravesetal calculated on mswindows language ateachoutputtime step thernnmakesa prediction where inenglishwehave space apostrophe blank wherewehave addedthe space for erswithclippedunits relu min max deepspeech end to architecture baselinebatchnorm gru layer rnn layer rnn layer rnn layer rnn layer rnn nosortagrad table varydepthofrnn grad allnetworkshave thenumberofhidden nesecharacters atinferencetime weuseaspecial izedbeamsearch hannunetal tothetran scription thatmaximizes log rnn log lm wc wherewc isthenumberofwords english orcharac ters chinese inthetranscription theweight con thectcnetwork theweight thetranscription developmentset we rentlayers however creases tion batchnorm ioffe szegedy though laurentetal incontrast thevariantofbatch attimestep arecomputed atthesametimestep figure withoutbatchnorm bn manceonthedeeper network whichhas layersin total thanthe shallower network inwhichonly ofthe layersis abidirectionalrnn asin laurentetal anaturalextension immediately beforeeverynon linearityasfollows latedoverasingletime stepoftheminibatch wedidnot thistobeeffective analternative sequence wise normalization istobatch therecurrent computationisgivenby ofthesequence figure wisenormalization table witha work andusethesefor evaluation ioffe szegedy sortagrad wetrainingwithctc weexperimentwith atrainingcurriculum bengioetal zaremba sutskever deepspeech end to orrnnswithmanysteps layersofweights pas canuetal zero scentquitevolatile weusethelength shorter easier utterances afterthe minibatches table layermodelwith recurrentlayers ing withoutbatchnorm stable vanilla rnnswhich aremodeledbyequation withreluactivations more term memory lstm units hochreiter schmidhuber gru choetal bah danauetal but showthatforaednum nectednetworks convolution and frequencydo main andinthetime onlydomain inallcases weuseaﬁ same ﬂconvolution insomecaseswespecify astride subsampling figure contextsizeof of utterances ﬁregulardevﬂ andamuchnoisier datasetof utterances ﬁnoisydevﬂ randomlysam pledfromthechime developmentdatasets barker etal dconvolu dconvolutions whileprovid ingasmalloncleandata thechangefromone layerof dconvolution improveswerby models online low however imply mance saketal ourmodels outanylossinaccuracy showninfigure thelayer sactiva tions andthusallowsustocon thelookahead where the activations forthenewlayerattime step are for layers deepspeech end to layer layer layer layer layer layer table inallcases theconvolutionsare followedby recurrentlayersand fullyconnectedlayer for sortagrad andhas mparameters adaptationtomandarin development forexample oneoften needstohand shanetal suchastonesinman darin shanetal niuetal sinceourend to thesetimecon thishasenabledus toquicklycreateanend to tionsystem usingtheap char acters sincehybrid chinese weincuranout containedinthisset asour testsethasonly outofvocabcharacters insection we speechmodels systemoptimizations anda precisionex aflops wecreated mancecomputing hpc infrastructure althoughmany nvidia single precisiontflop machines granted therefore we createdcustomizedall developedafast andusecustommem oryallocators takentogether tosustain overall eachnode whereeachgpu andregressions inthissetup however thegpus usingan all reduce opera tion tionalcycles mitigatethisissue deanetal rechtetal reduceopera tionitself achievinga toreducecpu loads similarly wehave usedhighly cpu sincenopublic well wedevelopedafast deepspeech end to trainingdata large labeledtrainingdata we use million utterances hoursof millionutterances datasetconstruction scriptions ondlongclips for agivenaudio transcriptpair themostlikelyalign mentiscalculatedas argmax align ctc modeltrainedwithctc anaccuratealignment however wefoundthatthisap rectionalrnn webuild therawctc cost the theratioof thenumberof thetranscription thisdataset fortheenglishdataset wethatthe to while retainingmorethan oftheexamples additionally dband db justasin hannunetal sainath etal scalingdata thisisdonebyran foreach dataset epochswith early size rel ative implyingthat table themodelhas layers layersof dconvolutionand recurrentlayers with mparameters results system weuse collectedinternally epochs orthefullmandarin sutskeveretal utterances ifthenormof itisrescaledto pascanuetal thebestonaheld chosenforevaluation aftereachepoch weuse amomentumof forallmodels english layersof dconvolution followedby grucellseach layerwith inthetestsets setonceonaheld outdevelopmentset weobtainameasure forexample lippmann weron thewsj eval set andautomatictypoand spellcorrections asrwizard of oz that westrivetooutperform deepspeech end to testsetourshuman read wsjeval wsjeval librispeechtest clean librispeechtest other accented voxforgeamerican canadian voxforgecommonwealth voxforgeeuropean voxforgeindian noisy chimeevalreal chimeevalsim table crowd everyaudioclip onaverageabout secondslongeach wercalculation unitedstates secondspertranscrip tion thehand whilethe ror enchmarkresults to nition thewallstreetjournal wsj corpusofreadnewsarti books panayotovetal table showsthatoursys temoutperformscrowd out of testsets http www voxforge org dataset manydifferentaccents categories american canadian indian commonwealth andeuropean datawith of examples accent finally challenge barkeretal irish southafrican alanguage andwithaddednoise usingall channelsof provements yoshiokaetal weusea single chan nelforallourmodels sinceaccesstomulti channelaudio isnotyetpervasive noisetocleanspeech mandarin intable examplesofnoisy speech codingparameters by relative architecturedevtest layer rnn layer rnn layer rnn batchnorm layer rnn batchnorm frequencyconvolution table each millionparameters testhumanrnn utterances committee utterances individual table thesethas chinese speakers thesecondhas table deployment designedforrealtime transcription ers searchfordecoding whilestillproviding deepspeech end to withthisscheduler wecan withincreasedlatency regardlessof thisscheduling achievingrel samplesper batch loadmedian ile streams streams streams table latencydistribution ms versusload weseeintable tencyof ms anda ms whenloadedwith concurrentstreams thisserveruses onenvidiaquadrok gpuforrnnevaluation as designed serverloadgrows keepinglatencylow precision arithmetic but improvesefy wewroteourown bit matrix substantially the gramlanguagemodel lookups weemployaheuristic least inpractice wethat workswell characters this timebyafactorof relative of the art wehave end to first wehave forexample while hoursofman darinspeech hoursof datacanenhanceperfor similarly grammodelswithour deepspeechsystem finally handledinpost processing its thus ties awarenessforend to search conclusion end to sincetheapproachis highlygeneric pliedtonewlanguages creatinghigh performingrecog englishandman darin guages finally getheronagpuserver to workarchitectures byawelloptimized scalemodelson overall thevalueofend to willcontinuetoscale references abdel hamid ossama mohamed abdel rahman jang hui and penn gerald ceptstohybridnn in icassp bahdanau dzmitry chorowski jan serdyuk dmitriy brakel philemon andbengio yoshua end to endattention based abs http arxiv org abs barker jon marxer ricardvincent emmanuel andwatanabe shinji thethird chime challenge dataset taskandbaselines submittedto ieee workshop asru deepspeech end to bengio yoshua louradour jéreome collobert ronan andwe ston jason curriculumlearning in onmachinelearning bourlard andmorgan ahybridapproach norwell ma chan william jaitly navdeep le quoc andvinyals oriol listen attend andspell abs http arxiv org abs chetlur sharan woolley cliff vandermersch philippe cohen jonathan tran john catanzaro bryan andshelhamer evan cudnn url http arxiv org abs cho kyunghyun vanmerrienboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger and bengio yoshua rnnencoder in emnlp chorowski jan bahdanau dzmitry cho kyunghyun andben gio yoshua end to ingattention basedrecurrentnn firstresults abs http arxiv org abs coates adam carpenter blake case carl satheesh sanjeev suresh bipin wang tao wu davidj andng andrewy in coates adam huval brody wang tao wu davidj ng an drewy andcatanzaro bryan deeplearningwithcots hpc in dahl yu deng andacero context dependent pre recognition speech andlan guageprocessing dean jeffrey corrado gregs monga rajat chen kai devin matthieu le quoc mao mark ranzato marcâ zaurelio senior andrew tucker paul yang ke andng andrew in advancesinneu gales ragni aldamarki andgautier support in asru pp graves andjaitly towardsend to endspeechrecognition in icml graves fernández gomez andschmidhuber con in icml pp acm graves alexandjaitly navdeep towardsend to endspeech in proceedings ofthe icml pp graves alex mohamed abdel rahman andhinton geoffrey in icassp sak hasim senior andrew andbeaufays francoise long short in interspeech hannun awni case carl casper jared catanzaro bryan di amos greg elsen erich prenger ryan satheesh sanjeev sengupta shubho coates adam andng andrewy deep speech scalingupend to endspeechrecognition http arxiv org abs hannun awniy maas andrewl jurafsky daniel and ng andrewy first abs http arxiv org abs hinton deng yu dahl mohamed jaitly senior vanhoucke nguyen sainath and kingsbury inspeechrecognition november hochreiter seppandschmidhuber jürgen longshort term memory neuralcomputation ioffe sergeyandszegedy christian batchnormalization ac shift abs http arxiv org abs krizhevsky alex sutskever ilya andhinton geoff imagenet in pp laurent cesar pereyra gabriel brakel philemon zhang ying andbengio yoshua batchnormal abs http arxiv org abs le ranzato monga devin chen cor rado dean andng buildinghigh levelfeatures in internationalcon lecun yann huang fujie andbottou léon learningmeth lighting in vol ume pp lippmann richardp mans speechcommunication maas andrew xie ziang jurafsky daniel andng andrew lexicon networks in naacl miao yajie gowayyed mohammad andmetz florian eesen end to wfst baseddecoding in asru mohamed dahl andhinton acoustic ieeetransac tionsonaudio speech url http ieeexplore ieee org xpls abs_all jsp arnumber jaitly nguyen seniorandvanhoucke application recognition in interspeech deepspeech end to niu jianwei xie lei jia lei andhu na context dependent nitionapplications in apsipa panayotov vassil chen guoguo povey daniel andkhudanpur sanjeev librispeech audiobooks in icassp pascanu razvan mikolov tomas andbengio yoshua onthe abs http arxiv org abs raina madhavan andng large scaledeepunsuper in thinternational recht benjamin re christopher wright stephen andniu feng hogwild alock ticgradientdescent in cessingsystems pp renals morgan bourlard cohen andfranco tion robinson tony hochberg mike andrenals steve theuse pp sainath tara vinyals oriol senior andrew andsak hasim convolutional longshort termmemory fullyconnecteddeep neuralnetworks in icassp sainath taran rahmanmohamed abdel kingsbury brian andramabhadran bhuvana worksforlvcsr in icassp sak hasim senior andrew rao kanishka andbeaufays fran coise abs http arxiv org abs sapp benjaminn saxena ashutosh andng andrew afast nition in aaaitwenty gence seide frank li gang andyu dong conversationalspeech in interspeech pp shan jiulong wu genqing hu zhihong tang xiliu jansche martin andmoreno pedro nese in interspeech sutskever martens dahl andhinton ontheimpor in th szegedy christian liu wei jia yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru van houcke vincent andrabinovich andrew goingdeeperwith convolutions waibel alexander hanazawa toshiyuki hinton geoffrey shikano kiyohiro andlang kevin phonemerecognitionus ingtime delayneuralnetworks nalprocessing speechand signalprocessing yoshioka ito delcroix ogawa kinoshita yu fabian espi higuchi araki andnakatani thenttchime system advancesinspeech microphonede vices in ieeeasru zaremba wojciechandsutskever ilya learningtoexecute abs http arxiv org abs 
pages nagoya japan october 
workshoptrack iclr ookahead onvolution ayerfor nidirec tional ecurrent eural etworks chongwang daniyogatama adamcoates tonyhan awnihannun boxiao baiduresearch sunnyvale ca usa contact dyogatama baidu com bstract rnns machinetranslation part of speechtagging andothers however latencysetting weevaluateour our errorrates ntroduction wheregivenaninput inthispaper wewillreferto astimesteps manyreal for example inspeechrecognition languagemodeling audioclips aword thenextword associated withthisinput rnns mikolovetal sutskeveretal amodeietal interalia therearetwogeneral typesofrnns forbidirectionalrnns where and aremodelparameters similarly inthebackwardpass wecompute theoutputattimestep isthencomputedas where onlytheforwardpassis performed is wp thiswork hochreiter schmidhuber andgrus choetal aswell whileanincrease an online low latencysetting equalcontribution andbolduppercase workshoptrack iclr unidirectionalrnns on theotherhand inthispaper timesteps context weshowhowwe figure ookahead onvolution showninfigure the supposeattimestep steps weaparameter matrix theactivations forthenewlayerattime step are where denotesanelement wiseproduct theoutputattimestep isthencomputedas fora non linearfunction and xperiments englishandchinese model amodeietal itisacharacter unidirectional forward thesecond to convolutionlayer to last thelastlayerisa gravesetal seeamodeietal datasets thewsj baidu testutterances https catalog ldc upenn edu ldc workshoptrack iclr table worderrorrates english chinese forcompetingmodels we usefuturecontextsize inallourexperiments model english chinese nolm smalllm nolm smalllm forwardrnn forwardrnn lookahead conv bidirectionalrnn results table sinceourfocus wehave iscussion layeraboveall unidirectional recurrentlayers first thisallowsus layer second thisresultsinbetter representations tofeedtothe layer weplantorunmore eferences amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos elsen engel fan fougner han hannun jun legresley lin narang ng ozair prenger raiman satheesh seetapun sengupta wang wang wang xiao yogatama zhan andz zhu deep speech end to arxive prints kyunghyuncho bartvanmerrienboer caglargulcehre dzmitrybahdanau fethibougares holger schwenk andyoshuabengio in proc ofemnlp alexgraves santiagofernandez faustinogomez andjurgenschmidhuber in proc of icml longshort termmemory neuralcomputation tomasmikolov martinlukasburget janﬂhonzaﬂcernocky andsanjeevkhudanpur in proc ofinterspeech workshoptrack iclr ilyasutskever oriolvinyals andquocv le in proc ofnips 
zhichaoli yiyang xiaoliu fengzhou shileiwen weixu baiduresearch lizhichao yangyi liuxiao zhoufeng wenshilei xuwei baidu attention ram themodellearnstode cidewhentostoponthe toachievethis weaddan additionalcontinue theissim asram andstan nition https github com baidu research dt ram introduction tention competitiontheoryﬂ hancedbytop the with visualattention formancesincrease clutteredbackground tionmodel ram thatmimicsthehumanvi ram byiterative regions withaedresolu tion easy moderate hard figure tawnyowlimages whenrecogniz humanmayspenddiffer entlengthoftime time besidesattention images backgroundclutter occlusion objectscale forexample ifthe figure withouthesitation however figure figure peoplemayspend inspiredbythis dt ram continue stop actionatevery timestep duringeachstep dt ramwillnotonlyupdate thenextattention themodel butcanbeviewedasa where thiscouldbringdt especial althoughdt ramisanend to endrecurrentneuralar chitecture etersfromscratch recognition creases thismakespol hardertooptimize lumlearning wegradual indt ramwiththepre trainedramanditwith reinforce wealso aswellast wodatasets cub andstanford cars experimental resultssuggestthatdt ramcanachievestate of the art compared toram time relatedwork sion networks mnih etal developthere ram forimagerecogni tion ba etal followthe jectsinimages sermanet etal furtherextendramto imagerecognition sinceproblem be sidesrecognition translation imagecaptioning imagequestionan swering thesoftatten cues backpropogation pomdp such toop backlowlevelfeatures itusesbothtop downand bottom sohaveothervariants forexample carreira etal back newell etal huandramanan show landmarks downinfor ability however graves recentlyintroduce themodelaugmentsthe networkwitha sigmoidalhaltingunit ateachtimestep figurnov etal extend to haltingunits over spatialpositions neumann etal extendthesimilar theyachieveas anadaptivecomponent jernite etal learnascheduler allthese but ofhaltingunits asfarasweknow odena etal istheattempt inforcementlearning modulesonaper inputba sis however nist andcifar ba etal augmentram withtheﬂend of dt ram however foreachtarget of sequenceﬂsymbolaf fectsthedynamics inthiswork weconductextensiveex recognition recognition fine tudiedinrecentyears recognitionap part thegroupat lems classvariationas wellassubtleinter classvariation the data trainingimages earcnn itis grainedrecognition inthispaper weshowthatthevisual attentionmodel iftrainedcarefully canstillachievecom of the artmethods model structures andparameter givenaninput tationalstructure isp sj whenthemodelspaceof ralnetwork duringtraining thelossis inputx isl sj thegradientof is log log sj inforcealgorithm figure model the duringexperiments gradientofthe over henceto trainthemodel thenap log ram pomdp ateach timestep the oftheenvironment inram partiallyobserved the thetargetoflearning moreformally ramtheinputimageas ateachtimestep aroundlocation timestep witharecur rentneuralnetwork themodelthencomputes twobranches which figure rentattentionmodel isadded toeachtimestep representsﬂcontinueﬂ greensolidcircle anda representsﬂstopﬂ redsolidcircle parameterizedby theoth eristhenetwork whichcomputes thescore parameterizedby duringinfer ence cy figure dure attention dt wesimply fa gthatde results representing ﬁcontinueﬂand representingﬁstopﬂ itssamplingpolicyis duringinfer ence andstopping figure comparedtofig ure toeachtimestep figure illustrateshowdt imagerecognition ognize figure left weexpectdt ramstopatthe fewsteps figure right mativeregions training log sj figure where rentnetwork theattentionnetwork thestoppingnetwork comparedtoequation equation isanapproximation toreplacethe intheterm thistraining lossissimilarto using theestimator nt timestep sforthe thtrainingexample thediscountfactor controlsthetrade nt istherewardat thstep duringexperiments weuseadelayedreward weset nt ift andr nt onlyif argmax unlikeoriginalram dt coresatanytimestep thelossof istheaveragecross entropylossover train ingsamplesand timesteps notethat dependson times duringexperiments dataset classes train testbbox mnist cub stanfordcars table cub andstan curriculumlearning duringexperiments weadopt first residu alnetworks pre trainedonimagenet wethen this second wetraintheram finally weinitializedt gorithm experiments dataset datasets mnist cub andstanford cars table mnistcontains imageswith digitalnumber however imagesinmnist ralimages therefore recognitiondataset cub con sistsof imageswith birdcategories stanford cars includes imagesof carclasses both cub whichwedo notuseinouralgorithm hencevisualattention mnist pixelresolution image steps ateachtimestep wecropa patchfromthe the hencethemodelusually theattentionnetwork ionnetworkandstop theout and re spectively recurrentnetwork computingthediscrete probability icywithaedvariance therecurrentstate vectorhas dimensions andmo mentumof ifthe agentcorrectlyand otherwise therewardsfor allothertimestepsare onecanreferto formore trainingdetails cub andstanfordcars weusethesame resizingto wethencropa imagepatch whichisakeydif ferencefrommnist atthestep weusea crop thisguaranteesthe stepramand stepdt ram alnetwork pre trainedon imagenet asthebaselinenetwork weusetheﬁpool therecurrent the attentionnetwork sgd withmomentumof for epochs thelearning rateissetto every epochs thebatchsizeis whichisthemaxi mumwecanusefor resolution fordiagnostic weusebatchsizeof duringtesting fromeachnetwork part we computationaltime torch structure gpu ona resolutionimageis ms stepramis ms sinceitrunsresnet on extra images comparisonwith mnist wetraintwodt discountfactors wetraindt ram withasmallerdis countfactor anddt ram withalargerdiscount factor henceonecanexpectdt ram stepsthandt ram table mnist stepserror fc layers hiddenseach convolutional layers ram steps ram steps ram steps ram steps dt ram steps dt ram steps table alltheram resultsarefrom figure dt sonmnist steps dt forexample dt ram gets of stepswhileramwith stepsgets error sim ilarly dt ram gets errorwithanaverageof stepswhileramwith stepshas error figure thetwodt we acleartrade dt ram achieveshigher errorthandt ram cub table summarizesthere sults further byutilizingattribute surprisingly networkwith layersalreadyhit surpassingmost oftheexistingworks tion ram reaches cub accuracy accw box zhangetal bransonetal simonetal krauseetal linetal jaderberg etal kong etal liuetal liuetal resnet ram steps dt ram steps table dataset stanfordcars accuracy accw box chaietal gosselinetal girshicketal linetal wang etal liuetal krauseetal resnet ram steps dt ram steps table ualnetby leadingtoanewstate of the artoncub dt byachieving thesamestate of the artperformance withlessnumberof stepsv steps stanfordcars fordcarsdataset table showstheresults surprisingly work withoutany can provesto accuracy of the compared to dt stepsonaverage wealso resolutionresnet ram resnet ram table ablationstudy ramon recognition weworkwithcub datasetsinceits however duetothelarge therefore insteadofusing resnet with state of the artresults weuseresnet with as thebaselinemodel table com entnetworkdepths ingeneral ahigherimageresolu tionhelpsrecognition forex ample giventhesameresnet model resolu tionwith cropsgets accuracy betterthan from with crops thisisproba adeeperresid for example resnet obtains accuracycomparedto resnet withonly duringexperiments wealso trainaresnet on cropsandget recog nitionaccuracy ramv dt ram table showshowthenumber dataset starting fromresnet whichisalsothe stepram themodel after steps ramnolongerimproves dt ram however itonlyuses stepsonaveragethan steps off figure thedt frommnist ningofthetime steps orintheendofthetime steps stopat steps learnedpolicyv fixedpolicy onemaysuspectthat whetherwe forexample onecan model stepsaccuracy resnet ram steps ram steps ram steps ram steps ram steps dt ram maxsteps table comparisontoramoncub notethatthe figure ram modeloncub dataset unlikemnist thedistribution threshold stepsaccuracy dt ram maxsteps table steps suchthatthe abovethethreshold network wecom ram table showsthecomparison wethatalthough stepswith accuracy dt ramstillworksslight lybetter stepswith accuracy curriculumlearning table comparestheresults ifwe outcurriculum timesteps thisisbecausesim step steps steps steps steps steps figure ramoncub testingset to each thereisnoboundingbox fromstep tostep matching step steps steps figure ramonstanfordcar testingset weonlymanagetotraina stepmodelwith resolution steps oc table longersequences stable thetestingperfor from to table comparesthetest notethatthe originalrammodel timestep nist timestepincreases oncontrary adding qualitativeresults ofdt ramoncub steps oi table setinfigure andfigure respectively fromstep to step andrecognitiondif weapplyitonthe forrecognition willbeconducted references behavior neuralmechanismsof top downandbottom visionresearch robertdesimone biologicalsciences radoslawmartincichy dimitriospantazis andaudeoli va natureneuroscience volodymyrmnih nicolasheess alexgraves etal re in advancesinneural pages jimmyba volodymyrmnih andkoraykavukcuoglu mul arxivpreprint arxiv pierresermanet andreafrome andestebanreal at arxivpreprintarx iv xiaoliu jiangwang shileiwen erruiding andyuan qinglin attribute guidedatten arxivpreprint arxiv aneurodynamicalcorti visionresearch alexgraves networks arxivpreprintarxiv richardssutton davidamcallester satinderpsingh yishaymansour etal in nips volume pages yoshuabengio er omelouradour ronancollobert andja sonweston curriculumlearning in proceedingsofthe th pages acm yannlecun eonbottou yoshuabengio andpatrick haffner gradient nition proceedingsoftheieee catherinewah stevebranson peterwelinder pietroper ona andsergebelongie thecaltech ucsdbirds dataset jonathankrause michaelstark jiadeng andlifei fei in putervisionworkshops pages laurentitti christofkoch andernstniebur amodel ofsaliency gence visualattention johnktsotsos scanmculhane winkyyankeiwai yuzhonglai nealdavis andfernandomodeling intelligence alexkrizhevsky ilyasutskever andgeoffreyehinton works in tems pages verydeepconvo arxivpreprintarxiv christianszegedy weiliu yangqingjia pierresermanet scottreed dragomiranguelov dumitruerhan vincen tvanhoucke andandrewrabinovich goingdeeperwith convolutions in pages kaiminghe xiangyuzhang shaoqingren andjiansun in proceed recognition pages dzmitrybahdanau kyunghyuncho andyoshuabengio translate arxivpreprintarxiv kelvinxu jimmyba ryankiros kyunghyuncho aaronccourville ruslansalakhutdinov richardszemel andyoshuabengio show attendandtell neuralim in icml vol ume pages ask attendandanswer ex ploringquestion answering in pages springer kanchen jiangwang liang chiehchen haoyuangao weixu andramnevatia abc cnn anattentionbased arxivpreprintarxiv akirafukui donghukpark daylenyang annarohrbach trevordarrell andmarcusrohrbach multimodalcom sualgrounding arxivpreprintarxiv zichaoyang xiaodonghe jianfenggao lideng and alexsmola answering in pages serenayeung olgarussakovsky gregmori andlifei fei end to glimpsesinvideos in pages reinforcementlearn ing anintroduction volume mitpresscambridge amirrzamir te linwu linsun williamshen jiten dramalik andsilviosavarese feedbacknetworks arxivpreprintarxiv marijnfstollenga jonathanmasci faustinogomez and urgenschmidhuber in advancesin pages chunshuicao xianmingliu yiyang yinanyu jiang wang zileiwang yongzhenhuang liangwang chang huang weixu etal lookandthinktwice capturing top ralnetworks in pages qianwang jiaxingzhang sensong andzhengzhang at feedback in systems pages joaocarreira pulkitagrawal katerinafragkiadaki andji tendramalik feedback in pages alejandronewell kaiyuyang andjiadeng stackedhour in europeancon pages springer bottom upandtop down in proceed recognition pages michaelfigurnov maxwelldcollins yukunzhu lizhang jonathanhuang dmitryvetrov andruslan salakhutdinov residualnetworks arxivpreprintarxiv markneumann pontusstenetorp andsebastianriedel arxivpreprintarxiv yacinejernite edouardgrave armandjoulin andtomas mikolov arxivpreprintarxiv augustusodena dieterichlawson andchristopherolah learning arxivpreprintarxiv learningmultiple lukasbossard matthieuguillaumin andlucvangool food forests in pages springer thomasberg jiongxinliu seungwoolee michellel alexander davidwjacobs andpeternbelhumeur bird snap large pages yincui fengzhou yuanqinglin andsergebelongie fine in proceed recognition pages shaolihuang zhexu dachengtao andyazhang part in pro patternrecognition pages jonathankrause hailinjin jianchaoyang andlifei fei fine in pro patternrecognition pages adityakhosla bangpeng yao andfei feili noveldatasetforimage categorization stanforddogs in proc cvprworkshop onfine fgvc volume jiongxinliu angjookanazawa davidjacobs andpeter belhumeur pages springer maria automat incomputervision graphics imageprocessing icvgip pages ieee yanggao oscarbeijbom ningzhang andtrevordarrell in pages low rankbilinearpool ingfor arxivpreprintarx iv tsung yulin aruniroychowdhury andsubhransumaji in putervision pages poof part basedone vs facev cation in proceedingsoftheieee pages stevebranson grantvanhorn sergebelongie andpietro perona arxivpreprintarxiv efstratiosgavves basurafernando ceesgmsnoek arnoldwmsmeulders andtinnetuytelaars fine grained in proceedingsoftheieee pages jonathankrause benjaminsapp andrewhoward howard zhou alexandertoshev tomduerig jamesphilbin and lifei fei recognition in putervision pages springer maxjaderberg karensimonyan andrewzisserman etal in pages jiadeng weidong richardsocher li jiali kaili andlifei fei imagenet alarge scalehierarchicalim agedatabase in cvpr ieeeconferenceon pages ieee ronancollobert koraykavukcuoglu andcl ementfarabet torch amatlab in biglearn nipsworkshop numberepfl conf xiaoliu tianxia jiangwang yiyang fengzhou and yuanqinglin fine efpartattention arxivpreprintarxiv ningzhang jeffdonahue rossgirshick andtrevordar rell part basedr pages springer tions networks in enceoncomputervision pages yuningchai victorlempitsky andandrewzisserman in pages philippe henrigosselin nailamurray herv ej egou and florentperronnin grained rossgirshick jeffdonahue trevordarrell andjitendra malik in pages yamingwang jonghyunchoi vladmorariu andlarrys davis grainedin pages 
agreement yongcheng shiqishen zhongjunhe weihe huawu maosongsun yangliu tsinghuauniversity beijing china tsinghuauniversity beijing china baiduinc beijing china abstract to endneuralmachine translation however unidi rectionalattention we proposeagreement tionalattention basedend to endneuralmachine translation to targetand target to thesametrainingdata experimentsonchinese englishandenglish thatagreement basedjointtraining introduction end to nmt isanewlypro kalchbrennerand blunsom cho etal sutskever etal bahdanau etal translation smt brown etal koehn etal chiang nmtbuildsonan encoder decoder frame work languagesentence intoacontinuous spacerepresentation fromwhichthede languagesentence ed lengthvector bahadanauetal advocatethe useof attention innmt inggenerated inaddition bahdanau etal luong etal xu etal liuyang tsinghua edu cn however languages espe ciallyfordistantly relatedlanguages forexample luonget al reportthatattention berkeleyaligner liang etal intermsofalignment errorrate aer ontheenglish germandata onepossible basednmtcanonly non inthiswork based learning liang etal liang etal intoattention agesource to targetandtarget to this experimentsonchinese english andenglish pendenttraining background givenasource languagesentence thatcontains wordsandatarget languagesentence thatcontains words end to endneu bilityasasingle largeneuralnetwork where theencoder decoderframework kalchbrennerandblun som cho etal sutskever etal bahdanau etal rnn where thsourcewordand isanon linearfunction arxiv cs cl apr figure basednmt thede anditscorresponding targetword givenasourcesentence abidirectional forexample bahdanauetal seefigure bahdanauetal ineq as where isanon linearfunction isthehiddenstatecor respondingtothe and thtargetword wereferto as alignmentmatrix inwhich anelement thecontributionofthe th sourceword togeneratingthe thtargetword exp exp where measureshowwell and are aligned parameterizedby basednmt fh ig the argmax log insteadof in bah danau etal by anddifferentiable although and alsodependon we state of the artofnmt figure showsthechinese to english upper andenglish to chinese bottom align boththetwo gold whilethechinese to english the english to fortunately thechinese to to ﬁbushﬂ therefore agreement basedjointtraining inthiswork basedlearn ing liang etal liang etal intoattention thecentralideaistoen couragethesource to targetandtarget to sourcemodelsto as showninfigure agreement tions moreformally wetrainboththesource to targetattention andthetarget to sourcemodel fh ig where and aremodelparametersin twodirections respectively givenby log log where isthesource to the thsentencepair isthetarget to sourcealign isalossfunction and isahyper forsimplicity weusethefollowing independenttraining jointtraining figure examplealignmentsof englishsentencepair the rowshowschinese to to chinesealignments wethatthetwo squareofaddition soa soa intuitively matrixcells squareofsubtraction sos sos ganchevetal multiplication mul theelement wisemultiplication mul log liang etal tion levinboim etal argmax log argmax log basednmt bahdanau etal toimplementagreement ingdata experiments setup englishandenglish forchinese english sistsof msentencepairswith mchinesewordsand menglishwords weusedthenist datasetasthe lection thenist and datasets wereusedastestsets inthenistchinese englishdatasets tions tobuildenglish we englishdatasets theen forenglish french consistsof msentencepairswith menglish wordsand mfrenchwords theconcatenationof news test andnews test wasusedasthevalida tionsetandnews test asthetestset eachenglishsen thefrench theenglish frenchdatasets of the artsmt andnmtsystems oses koehnandhoang aphrase basedsmt system rnn search bahdanau etal anattention basednmtsystem form oses corpustotraina loss bleu soa squareofaddition sos squareofsubtraction mul multiplication table to englishtranslation stolcke traininganddecoding forrnn search attention basednmtmodels kforalllanguages wefollowjeanetal toad ces lated ingsourceword whilejeanetal useabilingualdic the unknownwords search byreplacingin basedjointtraining the encoder remainunchanged thehyper parameter thatbalances forchinese englishand forenglish french the timeslongerthan we rnn search foroursystem section to englishtrans lation insensitivebleu asshownintable soa functions forexample while hardlyagree perfectly does therefore soa agreement sos isca probabilities however thelossfunction and ap parently itisunfavorablefor therefore sos butignores mul isableto soa and sos asaresult weuse mul inthefollowing experiments system training direction nist nist nist nist nist nist oses indep rnn search indep joint table resultsonthechinese oses isaphrase rnn search isanattention weintroduceagreement tionalattention basednmt nist aretestsets thebleuscoresarecase insensitive betterthanm oses sibetterthanm oses bet terthanrnn search betterthanrnn search withindependent training koehn training ee indep joint table resultsonthechinese signif icantlybetterthanrnn search resultsonchinese englishtranslation table to english andenglish to chinese translationtasks we thatrnn search oses exceptfor thec edirectiononthenist testset which basednmtondistantly related agreement cdirectiononthenist testset resultsonchinese englishalignment table englishwordalign menttask weusedthet singhua ligner evaluation dataset liuandsun andtestsetscontain manually alignedchinese english sentencepairs wefollowluongetal toﬁforce then weextractonlyone to wethatagreement basedjointtraining independenttraining figure showsexam basedjoint training however theerrorratesintable onthe thescoresfore cismuchlowerthanc ebecausebleu wordtypefreq indep joint toprepositionhigh andconjunctionhigh thearticlehigh yesterdaynounmedium activelyadverbmedium festivalnounmedium inspectsverblow noticingverblow table seeeq onchinese to englishtranslation samedataset accuracy denttraining forexample figure englishwords ﬁpresidentﬂ ﬁbushﬂ andﬁcondemnsﬂ inad dition similarly chinesewordﬁgongjiﬂ ﬁcondemnsﬂ ﬁsuicideﬂ ﬁbomingﬂ andﬁattackﬂ incontrast agreement forexample inthe thethirdchineseword likewise the cusesonﬁattackﬂ we the attentionentropy pairasfollows log givenaparallelcorpus fh ig the aver system training direction dev test oses indep rnn search indep joint table resultsontheenglish thebleuscoresarecase insensitive betterthan oses betterthanrnn search ageattentionentropy isas where onthe trainingcorpus table wordsonthechinese to we wordfrequencies appar ently independenttraining resultsonenglish to frenchtranslation table frenchtransla tiontask whilernn search withindependenttrain oses agreement relatedwork attention basednmtand agreement basedlearning attention bahdanauetal ing length inaddition theattentional luongetal theyshowthat attention attentionalmodels rnn search bahdanau etal wethatmodel attention els inthiswork weonlyapplyagreement basedjointlearn ingtornn search networkarchitectures agreement basedlearning liangetal introduceagreement basedlearning intowordalignment whichisalatentstruc tureinword brown etal manylanguages latent liang etal liuetal alignment metricalignments inattention basednmt thismakes attention basednmtmodels inattention allowformany to manysoftalignments wethatunidi turemapping based whichimproves conclusion tionalattention byencour mentmatrices dependenttraining inthefuture acknowledgements visitingbaidu pro gram cb cb thenationalnatu no talentplangrant tsinghuainitiative andagooglefaculty researchaward references bahdanau etal dzmitrybahdanau kyunghyun cho andyoshuabengio in proceedingsof iclr brown etal peterf brown stephena dellapietra vincentj dellapietra androbertl mercer tion parameterestimation chiang davidchiang ahierarchicalphrase based in proceedings ofacl cho etal kyunghyuncho bartvanmerrienboer caglargulcehre dzmitrybahdanau fethibougares holgerschwenk andyoshuabengio learningphrase machinetranslation in proceedingsofemnlp ganchev etal kuzmanganchev joaograc jen nifergillenwater andbentaskar posteriorregulariza thejournalof jean etal sebastienjean kyunghyuncho roland memisevic andyoshuabengio in proceedings ofacl nalkalchbrennerand philblunsom in proceedingsofemnlp koehnandhoang in proceedingsofemnlp koehn etal philippkoehn franzj och and danielmarcu statisticalphrase basedtranslation in pro ceedingsofhlt naacl koehn philippkoehn statisticaltests in proceedingsof emnlp levinboim etal tomerlevinboim ashish vaswani anddavidchiang modelinvertibility regularization paralleldata in proceedingsofnaacl liang etal percyliang bentaskar anddanklein alignmentbyagreement in proceedingsofnaacl liang etal percyliang danklein andmichaeli jordan agreement basedlearning in proceedingsof nips liuandsun yangliuandmaosongsun con localfea tures in proceedingsofaaai liu etal chunyangliu yangliu huanboluan maosongsun andhengyu in proceedingsofemnlp luong etal minh thangluong hieupham and christopherd manning attention in proceed ingsofemnlp luong etal minh thangluong ilyasutskever quocv le oriolvinyals andwojciechzaremba ad tion in proceedingsofacl stolcke andreasstolcke srilm anextensiblelan guagemodelingtoolkit in proceedingsoficslp sutskever etal ilyasutskever oriolvinyals and quocv le networks in proceedingsofnips xu etal kelvinxu jimmyleiba ryankiros kyunghyuncho aaroncourville ruslansalakhutdinov richards zemel andyoshuabengio show attendand tell in proceedingsoficml 
jianwang fengzhou shileiwen baiduresearch wangjian zhoufeng wenshilei liuxiao linyuanqing baidu derstandingofimage andakeyyetunder addressedprob betweenimages inthis paper whichtakesan metric contrastiveloss orrelativesim ilarity tripletloss ofimagepairs ourproposedmethod triangles first scaleinvari anceisintroduced second athird ordergeometric loss third introduction agoodsimilarity particularly orunknown however andcannotmodelhigh inaldatadimensions non suchasvisual productsearch facerecognition fea turematching þne zero shotlearning figure sne supercab right hatchback left classvariation theproposedangular invariantangleat thenegativepoint contrastiveloss siamesenetwork orthetriplet loss however suchasmulti tasklearning orhardnegativemin ing andthen pairloss batch neverthe less similarimages we onlyindistance first change betweendis similarclusters however itisinappropriate scalesofintra classvariation forinstance fig showsthe sne hatchback second samples optimizingdistance order solutionspace weproposeanovelangu orderrelationinside bycon ourmethodpushes ter our orderinformationfor augmentingpair matching andmarkovrandomþelds tothebest ofourknowledge inparticular thepro basedloss intwoaspects first comparedtodistance basedmetric invariant bynature forinstance thetwo tripletsshowninfig second angledeþnesthethird threepoints giventhesametriplet angularlossdescribes basedtriplet loss theexperimen state of the relatedwork standingprobleminma thesimplestformof ithasadeepcon pca setting thescopeofthispaper etal herewefocus contrastive usedincomputervision consistsof twoidenticalsub tivepairispenalized andlarger tion valuedprecisepair wise practice toaddressthisissue tripletembedding is and facerecognition pairandanegativepair and ithasbeenshownthat traininpractice cient forinstance facenet hardnegativeexample wang etal designedamore classandin class cui etal mans struction huang etal introducedaposition dependent deepmetricunit bustmanner morerecently yuan etal proposeda creasingcomplexities recently asimpleyet siþcationloss theimprove tion þne andproduct searchproblems however toþxthisis sue song etal batch sohn pairloss which inaddi eachmini batch optimizeclustering nca candirectly optimizeleave one outnearest whenappliedtomini batchtraining however ncaislim ation rippel etal space morerecently song etal pro nevertheless allabove andveryfew loss ourworkre andweshow proposedmethod inthissection weþrstreviewthe wethen gle finally onamini batch reviewoftripletloss ááá ofkclasses where rddenotesthefea ááá itslabel wesample amini batchoftriplets eachofwhich xa xp xn xa positive xpandnegative xnsamples whoselabelssatisfy ya yp yn negativepoint xnfromtheanchor xabyadistancemargin xp xa xp xa xn forinstance asshowninfig weexpecttheanchor xn is ltri xa xp xa xn wheretheoperator max denotesthehingefunc tion inor contrast figure theticexample tooptimizeeq ltri xn xa xn ltri xp xp xa ltri xa xn xp iftheconstraint eq isviolated orzerootherwise thereare first itisim samplingsize therefore second eq istoseparateclus however itisinappropriateto montheinter classgapas theintra world tasks third thegradient eq takesitspair butfails consider thenegativepoint xninfig foranexample itsgradient xa xn xaandpositivesample xpbelongto angularloss avarietyof techniques lastfewyears however lossdeþnition thepair hasrarelybeenchanged instead metriclearning wherethetriplet xa xp xn formsthetriangle apn ean xa xn epn xp xnandeap xa xprespectively theoriginal tripletconstraint eq penalizesalongeredge eancom paredtotheone eaponthebottom becausetheanchorand wecanderiveasym eap epn eanandepnhastobe thesmallestone min furthermore because nhastobelessthan where isapre deþnedparameter intuitively this whoseshortestedge eq thatisde thepro an gleisasimilarity transform invariantmetric proportional withaþxed margin eq alwaysholdsforanyre scalingofthelocal featuremap of incon trast intheoriginaltriplet constraint eq margin bycomparison setting however be considerthetriangle showninfig where byenforcingeq toreduce thenegativepoint xnwouldbepotentially draggedtowards xa toþxthisis sue were more stable negative theanchor xaandthepositive xp showninfig clecpassingthrough xaandxp centeredatthemiddle xc xa xp plane enc xn xcatxc the hyper plane cattwonodes xm structures mcnbyshiftingthe anchorxaandpositive given thenewtriangle were formulateeq toconstrainthean gle closedbytheedgeof figure where directlyminimizing nisunstableasit woulddrag xncloserto xa themorestable deþnedby re mcn pre deþneupperbound tan xm xc xn xc xa xp xn xc tan where xm xc which equalsto xa xp eq weseekfortheopti eq de scribes inanutshell lang xa xp tan xn xc loss langwithrespectto xa xpandxn whichare lang xa xa xp tan xa xp xn lang xp xp xa tan xa xp xn lang xn tan xa xp xn if islargerthan orzerootherwise asillustrated infig xnaway figure for eachnode itsrole anchor positiveornegative intriplet traditional tripletsampling pairsampling tokeepplotclean weonly fromxc xaandxp inaddition theanchor xaandthepositive xparedragged towardseachother whosegradients eq thegra dientsineq eq whenopti mizingamini wefound first weenhancethemini thefulluseofthebatch asillustratedinfig thecon batchasmulti this toallowjointcom wefollowthesam pairloss toconstructtu moreconcretely we þrstdraw differentclasses fromeachofwhichwe themainbene þtbehindn forinstance asshownin fig givenabatchwith nsamplesb xi yi thereareintotal ntuplets apairofanchor xa bandpositive xp bofthesame class and second adirectextensionofeq toconsidermorethan smoothob jectivefunction inspiredbyrecentwork we log exp exp max byassuming ineq wederive busingthefollowing log sum exp formulation lang xa log xn byn ya ypexp fa wherein fa thevalueof inasimilarspiritton pairloss fa tan xa xp txn tan xt axp orderrela studiedpair wisedis tance wecaneas asanexam ple pair loss lnpair ang lnpair lang wherelnpair denotestheoriginaln pairlossas lnpair xa log xn byn ya ypexp xt axn xt axp and isatrade offweightbetweenn pairandtheangular loss inallexperiments wealwaysset asitconsis experiments inthissection our of the artperfor benchmarkdatasets datasets foralldatasets cub datasethas speciesofbirdswith imagesincluded wheretheþrst species images species images areusedfortesting stanfordcar datasetiscomposedby cars imagesof classes weusetheþrst classes images classes images fortesting onlineproducts datasetcontains classes with productimagesintotal wheretheþrst classes images classes images areusedfortesting baselines method tripletloss tobefairincomparison gies themini batchof triplet asillustratedinfig inthesecondcaseof triplet ii ii weoptimizeeq usingthen pairsamplingasshown infig liftedstructure ls weadopttheopen source usedinthepaper pairloss nl weimplementn pairloss eq we portedinthepaper forourmethod angularloss al andn pair angularloss nl al that optimizeeq andeq respectively tobecomparable withpriorwork weemploythen pairsampling fig triplet ii andn pairloss we plicatethecomparison method evaluationmetrics we seenclasses forretrievaltask thisquantityisalso knownasrecall thedefactometric forimagere trievingevaluation weadoptthe codefrom meansalgorithm termsofthestandardf andnmimetrics see for trainingsetup thecaffepackage ments by beforefur therprocessing beforecomputingloss this googlenet pretrained timeslargerlearning dataset asithasfewerimages weuse sgdwith mini batchsize loss therefore baselines resultanalysis tables and thesetablesshowthat liftedstructure ls andn pairloss nl loss iandt ii inparticular pairachievesalarger al itisimportant al employsthe ii andn pair loss nl byintegrat ingwiththeoriginaln pairloss angularlossinnl fig comparesnl alwithn pairlossonthetask ofimageretrieval asitcanbeobserved theproposed nl classvarianceislarge forexample givenaqueryimage offiat convertible atthefourthrowoffig on therightside thetop imagesretrievedbynl alcon asthequery whilen inaddition fig putedbyourmethod nl al in dusingt sne despitethelargepose ity thatde eq wouldbeac tivated table andtable different lineproductdatasets respectively choosing forstanfordcarand methodclustering recall nmif ii ls nl al nl al table dataset methodclustering recall nmif ii ls nl al nl al table car dataset methodclustering recall nmif ii ls nl al nl al table ucts dataset al wefoundthat for with outintegratingwithnl whichis shownintable conclusion inthispaper metriclearning tivebasedondistance comparedtopair wisedistance nl al recall table forourmethodon stanfordcardataset methodnlnl al nl al recall methodnlal al recall table forourmethodon renderingthe mapinrealdata inaddition given thesametriplet toensurethatdis further more pairloss thesupe of the artworkis inthefuture tions first leveragesthethird it moresamples previouswork studiedthecaseof basedobjec tives second ornewclustering likeframeworks references bellandk bala acmtrans graph bhatia jain kar varma andp jain sparse labelclassiþcation in nips pages branson horn wah perona ands belongie ahybridhuman machinevi figure retrievalsbetweenn pair np andourmethod np al fromtoptobottom weplottwo examplesforthecub figure np al usingt sneonthecub dataset sionsystemforþne int comput vis bromley guyon lecun ackinger andr shah work in nips chechik sharma shalit ands bengio largescale journal chopra hadsell andy lecun learningasimilarity incvpr choy gwak savarese andm chandraker in nips cui zhou lin ands belongie fine grained in cvpr duchenne bach kweon andj ponce atensor ordergraphmatching ieeetrans patternanal mach intell fix gruber boros andr zabih agraphcut algorithmforhigher in iccv pages frome corrado shlens bengio dean ranzato andt mikolov devise adeepvisual in nips goldberger roweis hinton andr salakhutdinov in nips hsieh yang cui lin belongie and estrin in www huang loy andx tang localsimilarity aware deepfeatureembedding in nips pages egou douze andc schmid productquantiza ieeetrans patternanal mach intell jia shelhamer donahue karayev long girshick guadarrama andt darrell caffe convolu in acmmm pages kiapour han lazebnik berg andt berg wheretobuyit onlineshops in iccv krause stark deng andl fei fei dobjectrep resentationsforþne in iccvwork shopon kulis metriclearning asurvey foundationsandtrends inmachinelearning li su qi fish cohen or andl guibas agepuriþcation acmtrans graph rippel paluri dollar andl bourdev metriclearn in cvpr russakovsky deng su krause satheesh ma huang karpathy khosla bernstein berg andf li nitionchallenge salakhutdinovandg hinton learninganonlinearem in aistats schroff kalenichenko andj philbin facenet in cvpr sohn class pairlossobjective in nips song xiang jegelka ands savarese deepmetric in cvpr song jegelka rathod andk murphy learn ing corr abs sun chen wang andx tang deeplearningface veriþcation in nips szegedy liu jia sermanet reed anguelov erhan vanhoucke anda rabinovich in cvpr taigman yang ranzato andl wolf deepface closingthegaptohuman tion in cvpr ustinovaandv lempitsky withhistogramloss in nips pages vandermaaten acceleratingt sneusingtree basedalgorithms wang song leung rosenberg wang philbin chen andy wu learningþne grainedim in cvpr weinbergerandl saul journalofma weston bengio andn usunier wsabie scalingup in ijcai pages yi lei ands li personre identiþcation corr abs yuan yang andc zhang hard awaredeeplycas cadedembedding corr abs zhang zhou lin ands zhang embeddinglabel structuresforþne in cvpr 
scale youtube mvideounderstanding fuli chuanggan xiaoliu yunlongbian xianglong yandongli zhichaoli jiezhou shileiwen baiduidl tsinghuauniversity abstract youtube mvideounder rdplace becausethe challengeprovidespre level featuresformulti oursystemcon two streamsequencemodel fast networks tasks tobenoted ourfast layersachieves intermofgap onthekaggle publictestset introduction andmachinelearning therefore existing cnns and orrecurrentnetworks haveachievedstate of the artresults however due existingvideo scaledata whilelarge addressedproblem toremedythisissue googlere scalevideodataset named asyoutube whichcontainsover millionyoutube classes avideomayhave pervideois priortothis gan et al alsoinves andimages mdatasetis extractedvisual rawvideos therefore aggregatetheframe label recognition particularly modelingapproaches namelytwo streamsequencemodel fast networks proaches thestate of the section tion section followedbydis approach inthissection tweamsequencemodels rectionallstm andgru sincetheyhaveshown inordertobest modalclues weproposeseveral theoriginaltwo streamcnn frameworktrains and recently ma et al arxiv cs cv jul figure streamlstmmodel differentfromthem oneforrgbfeatures attentionlayersarein finally the connected labelclas exper two tivetwo recently onlarge scaleimage typically ones however the models first pre existingsmall thousandsvideos second thenewyoutube since large lengingproblem inthecompeti tion however weobserve this originalyoutube mtechniquereport weexploreanoveldeep lstm forwardcon figure nections tosequencemodels whichplaysanessen bidirectional lstms we forwardsequence model thefast twofeed layers eachfast previousfast anduse afully thefast forward sowecallthepathfast forwardconnections wewillin forwardsequence report lstmsand grus recentsequential showstrongpotentials li et al pro et al proposedatemporal videorecognition andachievedstate of the artresultson ucf andhmdb datasets inthecompetition videorecognition incontrastwith thatperformscon volutionsonframe level representations the formtheoriginalframe tratedinfigure concatenatedandzero edlengthdata where and indicates mini batchsize channelnumber andlengthofframes rep sectively ralresnet whichisastackof temporalresnetblocks trb tionallayers anda figure shortcutconnection weuse forallthe experiment inthissection wepresentthedataset experimentsetting dataset mdataset million youtubevideos tipletags inthecompetition pre ofthevideo ceptioncnnpre trainedontheimagenet followedby thepca compressionintoa dimensionalvector the trainedvgg network intheofsplit parts fortraining forvalidation and for testing inpractice weonlymaintain kvideosfromthe other set performances gap metricattop asusedin theyoutube mkagglecompetition table mtestset method gap video level vlad temporalcnn two streamlstm two streamgru fast forwardlstm fast forwardgru fast forwardlstm depth ensemble experimentresults table theyoutube mtestset forthevideo levelapproach we levelfeature vector weuse nor wethenfedtheserep scores fromtable our proposedtwo thefast forward lstmmodelwithdepth modelaround intermofgap differenttemporal our submissionensembles cellsanddepths conclusions inthiswork scale videorecognitiontask theen enablingourmethodto teamsinthechallenge competition fordownloadfrom https github com baidu youtube references abu el haija kothari lee natsev toderici varadarajan ands vijayanarasimhan youtube alarge scalevideobench mark arxivpreprintarxiv arandjelovicanda zisserman allaboutvlad in cvpr pages cho vanmerri enboer bahdanau and bengio lation encoder decoderapproaches arxivpreprint arxiv deng dong socher li li and fei fei imagenet alarge scalehierarchicalim agedatabase in cvpr gan sun duan andb gong webly in eccv pages gan wang yang yeung anda hauptmann devnet in cvpr pages gan yao yang yang andt mei you lead weexceed labor cvpr he zhang ren andj sun deepresid in proceedingsof recognition pages hochreiterandj schmidhuber longshort term memory neuralcomputation hochreiterandj schmidhuber longshort term memory neuralcomputation karpathy toderici shetty leung suk thankar andl fei fei large scalevideo in cvpr kuehne jhuang garrote poggio and serre hmdb motionrecognition in iccv pages li ma jiang li zhang liu cao kannan andz zhu deepspeaker an end to arxiv preprintarxiv ma chen kira andg alregib ts lstmandtemporal inception exploitingspatiotem arxivpreprint arxiv simonyananda zisserman two streamconvo in nips simonyananda zisserman verydeepconvo iclr soomro zamir andm shah ucf datasetof thewild arxivpreprintarxiv srivastava mansimov andr salakhutdinov lstms icml szegedy liu jia sermanet reed anguelov erhan vanhoucke anda rabi novich in cvpr pages tran bourdev fergus torresani and paluri iccv xu yang anda hauptmann adiscrimi cvpr zhou cao wang li andw xu forwardconnec arxivpreprint arxiv 
wenzhang lianghuang yangfeng leishen qunliu keylab beijing china zhangwen fengyang shenlei ict ac cn corvallis or usa baiduresearch sunnyvale ca usa liang huang sh gmail com huaweinoah sarklab hongkong china qun liu huawei com abstract itsuffersfrom slowtranslationspeed thedirectconse quenceisthatatrade offhastobemadebe thusits we applycubepruning apopulartechniqueto intoneural tion simi leading sideandless softmax targetvocabulary naivebeamsearchby ongpusand oncpus introduction nmt hasshown cently cho etal bahdanauetal gehringetal vaswanietal awidelyusedar basedencoder decoder framework choetal bahdanauetal pairs rnn hochreiterand schmidhuber byword aprobabil otherrnn atthetrainingtime thedecoderis while atinference evenwithbeamsearch nmtstillsuffersfrom slowtranslationspeed notongpus butoncpus whicharemorecom monpractice thereasonfortheinefy worsestill due targetwordscanonly begenerated sequentially ratherthaninparallel unks softmax operationwhen toaccelerate thetranslation mi etal or inthispaper basedandsyntax speedupthedecoding chiang huang andchiang informally cubepruning to applythisideatonmt however ismuchmore involved intheprocessofbeam search thethreedimen ulary denstates respectively togetherwith cubepruning related tothereason andless softmax operations relatedtothesecond reason when ity by ongpusand oncpus background ousnmtmodels we taketheattention basednmt bahdanauetal in thissection based theattention basednmtmodel theattention encoder tionmodule inthefollowingparts wewillintro assumethe and encoder the inthosesequences notethat isemployedto gru gru attention calledcontextvector nexttargetword atthe thstep togetthecon textvector andthe ij tanh then sequence ij exp ij exp ij decoder thedetailsarede scribedin bahdanauetal atthe thde codingstep thetargethiddenstate isgivenby overallthewords thecontext vector softmax where is usedtomap to cubepruning proposedby chiang basedonthe huangandchiang isactuallyanaccel algorithm beamsearch aheuristicdynamicpro exploresagraph fromcandidates forthesequence to sequence learningtask givenapre trainedmodel the ity graves boulanger lewandowskietal both sutskeveretal and bahdanau etal re markably huangandchiang successfully ingofsmt andtheyuti ofphrase based ochandney andsyntax based chiang galleyetal systems figure likelihood dateinthisexample gpu cpu calculationunits time percentage time percentage eq eq eq eq softmax table thecubeprun andstoringthem foreachdecodingstep those groupedtogether weillustratethe thcandidatewordinthe thbeamas bp where isthenegativelog likelihood nll accumulationinthe thbeam thbeam istheindexofthe ularyand bp thdecodingstep notethat foreachsourcesen tence indecoder existinginthebeam producestheprob choetal the softmax functionover computesprobabil itiesforallwordsin sothecalculationisex as such bahdanauetal andmanyothers onlyusedthetop vocabulary how ever tions timecostindecoding wedecodedthemt andrecorded and respectively thestatistical resultsintable while equalsto whichactu bos token intargetvocabulary the softmax oncpus costwascausedbythe softmax operationoverthe inordertoavoidthe time ing weintroduced self normalization denotedas sn intothetraining self normalization self normalization devlinetal wasde according toequation thecross entropy ce loss couldbewrittenas log log exp exp log exp where tion atthe thstep wemarkedthe softmax normalizer exp as followingtheworkof devlinetal we log log log log theobjectivefunction showninequation log isapproximated to equally make closeto onceitconverges wechosethevalueof empirically becausethe softmax normalizer isconvergedto ininfer ence wejustneedtoignore andpredictthetar thsteponlywith cubepruning table here algorithm equation and decoder cubepruning treatedasapruningal gorithm search keepingthe size thus generatecandidates assumethat restoresthesetofthe translations process beamsize timesforwardcalcula beamsize prob thepreviousbeam bahdanauetal while intermsofsome constraints viousbeamintoone equivalenceclass calleda sub cube then forthesub cube loosehiddenstate elementsinthesub cubeare columns and af termerging cubes thethirddimension westarttosearch cube cube and found onceaitemisselected theexacthidden throughallabovesteps thefrequencyoffor wegiveanexam giventhe th notethat identicalto bahdanauetal weonly used asthevocabularysize figure weonlydepictthe inthebracket assumethe beamsizeis elementsinthe th beamaremergedintotwo sub cubes and minimalelement ispoppedout meanwhile itsneighbor ispushedintotheheap minimalelement ispoppedout itsright neighbor andlower neighbor arepushedintotheheap minimalelement ispoppedout itsright neighbor anddown neighbor arepushedintothe heap minimalelement ispoppedout thenitsdown neighbor ispushedintotheheap th beam bestword th beam beam wegeneratethe th beam differentfrom thenaivebeamsearch wegroupitemsinthe cubes and in termofthetargetword asshowninpart offigure constructsthesub cube and areputto cube itemsin part cumulatednll foreachsub cube weusethe statevectorineachsub cubeastheapprox tionandthenextstate atbeginning eachupper cubeispushedinto aminimumheap fromtheheap nlloftheelement atthisrate ﬁdiffusionﬂinthesub cubeuntil elementsare popped nllstoconstructthe th beam notethatonce anelementispopped fromthestep infigure wecanseethat and and the th beam pruningalgorithm called ncp after somesim moresub cube cubeonlyac cordingtothetop statevector therowin thesub cubeinfigure andselectnextcandi datesafterranking beamsearch besides eachsub cubeonlyre thus itcouldsave unlikethenaive cubepruningalgorithm popseachitem insteadoftheexactone wedenotethiskindof acp experiments to english zh en translationtask datapreparation thechinese msentencepairs weusedthenist mt sen tences andthenist mt datasetasthe testdataset whichcontains sentences limitedupto tokens thenactually msen mchinesewords and menglishwords weextracted most iesforbothsides inalltheexperiments case insensitive gram bleu papinenietal wasemployed weusedthescript mteval pl system bahdanau etal consistingof equation isreplacedwiththefol lowingtwoequations gru gru besides tion isreplacedwith othercompo also were andnaive ductedastwobaselines trainingdetails specially from bahdanauetal wordembedding allhiddensizes ldc ldc ldc hansardspor tionofldc ldc andldc https github com moses smt mosesdecoder blob master scripts generic mteval pl https github com nyu dl dl mt tutorial blob master docs cgru pdf alsosetto allparametermatrices including biasmatrices distributionover parameterswereup datedbyusingmini scent sgd withbatchsizeof andthelearning zeiler with decayconstant entexplosion pascanuetal dropoutwas choetal searchdecoding training gpu ittook about zh entrainingdata for self normalization we empiricallyset as inequation searchstrategies tively entbeamsizes nbs sn sn nbs sn naivebeamsearchwith sn ncp sn cubepruningwithout sn ncp sn cubepruningwith sn acp sn sn acp sn sn rate denotedasamr givenatestdataset we countedthetotalword levelcandidates notedas andthetotalsub cubes notedas during thentheamrcanbe simplycomputedas themt following devlinetal wehadtried and forthevalueof wefoundthat pro ducedthebestresult figure amrcomparisononthemt test dataset decodingthemt testdatasetonasin axisrepresentsthe processandx unsurpris ingly server notshownhere sixmethods weusedthepre trainedmodel gtxtitanxgpuserver beamsizevaries from to foreach beamsize the mt showninfigure nbs arealways whetherthereis sn ornot wecould process wecan concludethat inanycaseofbeamsize theamr besides self self normalization intuitively the yimproves twoconditions withself normalizationandwith outself normalization figure demonstrates we conditions withoutself normalization figure speed points withself normalization figure showsthatthe spendaboutone beamsize concretely whenthebeamsizeis settobe acp sn is timesfasterthanthe baselineonthemt testdataset andbothper testdataset thetrendsofthe figure andthe gpus fromthefigure wecanalsoclearly seethat comparedwiththe nbs sn ncp sn acp sn pro however when normalization thepro about bleuscore theself normalization madethe acp byabout inwhichconditionthe nbs sn gotthebestbleuscore withbeamsize whilethe acp sn withbeamsize theresultscouldbeob servedinfigure independent itis performwellongpus codingoncpus thus theacceler normalizationcould decodingtime inthissection testdataset bleuvs decodingspeed withoutself normalization bleuvs decodingspeed withself normalization figure translations bleuvs decodingspeed withoutself normalization bleuvs decodingspeed withself normalization figure opteron tm oftranslations wecalculatedthetimes ferentbeamsizes and fromthefigure and wecould ongpus figure and on relatedwork recently yofthenmtdecoder someresearchers larysize jeanetal improvedthedecoding basedonthe workof jeanetal mietal intro ducedsentence levelandbatch levelvocabularies lary vocabulary inthisway theyonlylost bleu points wu etal and lengthnormalization cessby whenrunningoncpus hu etal which inspiredbytheworksof mietal and huangandchiang weconsiderprun pruningalgorithm ifweusephrase beamsizeswithoutself normalization beamsizeswithself normalization figure translations beamsizeswithoutself normalization beamsizeswithself normalization figure opteron tm oftranslations we sidecoverage how ever coding wecouldgroup targetwordtogether conclusions coderoftheattention basednmt foreachstep inbeamsearch class es leftcornerin evaluationsshowthat ourmethod alargeextent and ongpusandcpus respectively also besides self acknowledgements comments tions iis andisalsosupportedin china nos references dzmitrybahdanau kyunghyuncho andyoshuaben gio iclr nicolasboulanger lewandowski yoshuabengio and pascalvincent in ismir pages citeseer davidchiang ahierarchicalphrase based in pro ceedingsofthe pages as davidchiang hierarchicalphrase basedtrans lation kyunghyuncho bartvanmerrienboer dzmitrybah danau andyoshuabengio ontheproper encoderœdecoder approaches in proceedingsofssst eighthwork shoponsyntax caltranslation pages doha qatar asso kyunghyuncho bartvanmerrienboer caglargul cehre dzmitrybahdanau fethibougares holger schwenk andyoshuabengio learning in proceedingsof the emnlp pages doha qatar linguistics jacobdevlin rabihzbib zhongqianghuang thomas lamar richardschwartz andjohnmakhoul in proceedingsofthe tationallinguistics volume longpapers pages baltimore maryland associationfor michelgalley jonathangraehl kevinknight daniel marcu stevedeneefe weiwang andignacio thayer context in pro ceedingsofthe thannualmeet tics pages sydney australia association jonasgehring michaelauli davidgrangier and yanndauphin in proceed ingsofthe volume longpa pers pages vancouver canada associa jonasgehring michaelauli davidgrangier de nisyarats andyannn dauphin con in pro ceedingsofthe machinelearning volume of proceedingsof pages in sydney australia pmlr alexgraves arxivpreprint arxiv sepphochreiterandj urgenschmidhuber longshort termmemory neuralcomputation xiaoguanghu weili xianglan huawu and haifengwang proceedingsofmt summitxv page betterk bestparsing in parsing pages stroudsburg pa usa associa forestrescor ing els in proceedingsofthe thannualmeetingof pages prague czechrepublic associationfor ebastienjean kyunghyuncho rolandmemisevic andyoshuabengio onusingverylarge in proceedingsofthe rdannualmeetingofthe guageprocessing volume longpapers pages beijing china tionallinguistics recurrent sitionality in ality pages bulgaria association haitaomi baskaransankaran zhiguowang andabe ittycheriah in proceedingsof the pages austin texas haitaomi zhiguowang andabeittycheriah tion in proceedingsofthe thannualmeetingof vol ume shortpapers pages berlin ger many thealign lation kishorepapineni salimroukos toddward andwei jingzhu bleu in proceedingsof pages philadelphia pennsylvania usa linguistics razvanpascanu tomasmikolov andyoshuabengio networks in proceedingsofthe thinternational volume of pages atlanta georgia usa pmlr ilyasutskever oriolvinyals andquocvle works inz ghahramani welling cortes lawrence andk weinberger editors ad pages curranassociates inc ashishvaswani noamshazeer nikiparmar jakob uszkoreit llionjones aidanngomez ukasz kaiser andilliapolosukhin attentionisall youneed ini guyon luxburg bengio wallach fergus vishwanathan andr gar nett editors cessingsystems pages curranas sociates inc yonghuiwu mikeschuster zhifengchen quocv le mohammadnorouzi wolfgangmacherey maximkrikun yuancao qingao klaus macherey etal google sneuralma arxivpreprint arxiv matthewdzeiler adadelta anadaptivelearn ingratemethod arxivpreprintarxiv 
proceedingsofthe pages jeju republicofkorea july 
retrieval basedhuman ruiyan baiduinc no xibeiwangeastroad beijing china yanrui baidu com yipingsong baiduinc no xibeiwangeastroad beijing china songyiping baidu comhuawu baiduinc no xibeiwangeastroad beijing china wu_hua baidu comabstract incomputerscience informationretrieval intelli gence etc alongwith theprosperityofweb itcasts ow aretrieval basedconversa givenahumanissuedmes sage query inthispaper wepropose aretrieval deeplearning to respond bywebdata weincorporatethe inthe experiments tevidence of map ndcg informationsystems contentanalysis andindexing patternrecognition modelsš deeplearn ingkeywords learning to respond conversationsystem contextualmodeling deepneuralnetworks tiononthepage acmmustbehonored tocopyotherwise orre publish and orafee acm org sigir july pisa italy acm isbn doi http dx doi org introduction fimoviesforalongtime recently the tem isnolongeran illusionfaraway sationsontheweb whattorespondgiven almost anyinputs itislikelytobeagreat timingtobuilddata driven open infact inearlyyears researchershaveinves tigatedintotask which open createtherules prepare task forinstance orbusrouteinquiring origin andthenrespondac systemtopicscope whichis task oriented open domain so cial worldapplications creatinganopen problem itis whichcanbe incontrast insystemsbefore hand besides crafted therehasbeenanexplo bbs forums socialmedi facebook twitter cqa platforms baiduzhidao yahoo answers these http www facebook com http www twitter com http www zhidao baidu com https answers yahoo ofretrieval domainconversa tionresearch themeritisthat userinput thebigdataera however seemslikeadouble edgedsword ononeside asmentionedabove domain ontheotherside givenauser issuedquery besides theproposedmodel narios previousutterancesen therefore capturing inthispaper to respondﬂframe workforopen wecreateahuge anutterance namely aposting andits reply each hposting reply icanberegardedasa single turnconversation foragivenquery weapplytradition alkeyword replies we oneor thus adeepneuralnetwork dnn eachcandidatereply postisrelatedtoa reformulated query and reformulated query wemergethe inthisway multi contexts candidatepostingsand orreplies whichisanovelinsight especiallymulti turn conversations weareawareofback more over groundinformation byanovelformula withcontext andtheoriginalone thednnranker to rankﬂ schema queryversus context posting reply weuseabi aconvolutional sentence query reply query posting andquery context inaway inparticular webuild al most werun oursystemoutperforms standardandstate of the map ndcgandmrrmetrics tosumup to matichuman ralnetworks dnns context inacontinu turns theproposedquery formation reformulated queries re formulated query incorporatingmulti dimensionofranking evidences query reply query posting andquery context westartbyreview ingrelatedwork insection insections and to respondarchitecture wedevise section relatedwork conversationsystems these rule or tohandcraftrules theconversation tolearnandinfer however later theneedforahuge system deep questionand answeringsystems inthisway berofquestion answerpairs leuski etal buildsystemstos question question answerpairs question thenum tosomeextent expanded butarestill nowadays resources cqa tabecomeavailable based ta higashinaka etal erationwiththesearch basedmethods ritter etal havein smt techniques aswellas in theapproach notretrieved fromarepository unlikepreviouswork deeplearning to respond weformulatethe deepneuralnetworks inrecentyears deepneuralnetworks dnns alsoknownas deeplearning theycanextrac multiplelayersofnon lineartransformation innlpmodels howev er indexestodnns adense low dimensional real valuedvector calledan embedding anonymous tionalneuralnetworks cnns rnns incnns wehaveaed hiddenstates aniterativefashion socher etal leveragesentence mouetal proposesyntax howev er andhencerecur weprefer structure discourse unitrecognition etc sentence levelmodels cnns rnns thentwosentences informationis cosinemea sure orinner product hu etal developword by word matchingapproaches twosentences veryrecently rocktäschel etal proposecontext wherethesentence sinfor suchcontext matching prohibitingpre asentence tionalintensive ourscenario forefyconsideration weleveragevectorcon catenation theproposedlearning to wehave aftercomputing post context ourdnn mulatedqueries taskmodeling inthissection learning to respond weillustratethetask searchandre trieval dnn basedrankingwith table postingandthe associatedreplies ply reply andreply sationdata andob tainhpost replyipairs westoretwo posting reply pairsinthe hposting reply iandhposting reply posting reply reply youwillberecovered don tworry table part indicatesarealhuman denotedby com puter denotedby conversationscenario whilepart ii in isthe currentuser issuedquery queriesaslisted note reply isassociatedwitha posting ii human computerconversation taskformulation userquery contextinformation fc really reformulatedqueries yeah top rankedresponse reply scoring andrankedlistfusion wegooverthroughthe datacollection peopleinter whichprovides fo rums cqabases etc tothepublic wetreateachutterance in multi turnconversations reply pair ourdatabaseiscontext free forapostingwithmulti plyreplies hp ipairs table showsthepre appliedto arealhuman inthe sampleshownintable callyunique message various repliesasagroupof hposting reply ipairs thedatarepositoryis computer conversations searchandretrieval theuser issuesaquery intable terms usingtraditional tf idf weightingschema ontheconversation data base line based onthelight notethatwetreat virtualdocument which inthisway the candidatereply namely along withitsantecedent posting namely multi turn conversations asillustrated intable undersuchconditions mationtouse sationtask inparticular sidesas contexts denotedas fcig ccanbeusedtoenhance we callthisa process moreover the andhencewehavesever eachreformulated queryisdenotedas qi dnn basedscoring ranking andrankedlistfusion weap dnn tion inparticular wedesignabi ory lstm levelsemantic sofaquery hp aswellascontext sothatweknowhowcan contexts query documentmatching query replymatching theadditional query posting matching aswellas query context matching intuitively they tables foreach reformu lated query fromquery queryandquery posting wefurthermergeallcan fusion query context contextscanbe table withdeeplearning to respondschema estoverall merged rankingscore šfromthepre constructed repository replies tionsofcontexts gradientscanbeback ranking sentencepairing there fore to respondschema generally butsome timesmaybenot whenmodelingaquery itisnon trivialtoex sations inthissection whileothersare not astheexampleintable illustrates thecontextut someofthe www baidu com table thecurrentquery fcigcontexts utterancesbefore ciisautterancein cq concatenateswithsome cif matchingmetricfor query replyg matchingmetricfor query posting matchingmetricfor query context hp iinputquery context argmaxrfrjq fhp igteranceﬁreally old age therefore we one by one equallyinformative givenacontextof nsentences asa combinatoryproblem exponentiallyas ngrowsup an atmost linearas ngrows andcombine mulations weobserve thatcontexts nomatterfromthe humansideorfromthe computerside therefore inthecontext nevertheless query context match ingfunction letusassumethereare sentencestothequery eachisdenotedas qi possible ncombinations nocontext qnocontext fq wholecontext qwholecontext fq cg add one weconcatenate one atatime qadd one fq cng drop out weconcatenate leave one oneatatime qdrop out fq cnc cncn combined qnocontext sqwholecontext sqadd onesqdrop out thequeryispreserved cnciﬂindicates ciisoutfrom arethatthecostised for qnocontext andqwholecontext orlinear forqadd oneandqdrop out alongas ngrows theintuitionfor add oneanddrop out strategiesisbasedona granularity ortoexemptone inthissection replyrank ingandmerging didatereplygiventhe reformulated query basedonthecandi subsection thenthe gating product mechanism subsection inthisway sentencepairmodeling asmentioned query reply query posting and query context matchings the betweenareplyandthe reformulated query alargerscore thequery response amorerelevantcon text alarger shouldleadstoamore rankingresult andthescoresfrom andg sum fusion thescoringfunction outputsascalarin appropriate while gateﬂ which range neverthe less networkarchitecture but inparticular thedeepstructurefor wordembeddings thus word embeddings based textprocessing real valuedvector thisprocess knownasvectorization capturessome underlyingmeanings givenenoughdata usage andcontext word ofaparticularword look uptable inourmodel dings andthentunedduring bi directionallstm weuseabi bi lstm re asreviewedinsection rnn ineachtimestep alongasequence givenasequence ofinputs input forget and outputgates denotedas it ft and otrespectively thevector givenaninput sentences fx tg where xtisthewordembedding atposition tinthesentence htforpositiont givenby itftotlt tanh wht et ht ft ht itlthst ot ht where eisaknownasa sigmoid logisticfunction thewordtothelast thevariantbi lstm backward htand ht respectively thwordinthesentence ht ht hti convolution cnn toex discrim workissimilarto showninfigure unlikernns cnns size concretely lstm for minbi lstmoutputvectors ht ht where tisacertainposition theconvolutional willgeneratea ﬂbetweenthe twovectors moreformally of tanhm xi inpractice wealsoaddascalarbias tion inthisway weobtainthevector ofisavector eachdimen volution infact figure multiplefeaturemaps fandb pooling concatenation andmatching lstmwith cnn we inparticular indicating comparedwithother word by wordmatching orattentionmethods layer fully connected feed forwardneuralnetwork alsoknownas multi layerperception mlp startingfromlower higher levelones finally tences asmentioned isin hencethescoringneu for weapplyasigmoid merging now wemergethe asdiscussedinsection ifaposting ismorerelatedtothe reformulated then wouldbe morereliable andh inparticular scoresfromd weightedbythesetwo gates randq isde asfollows integrationofa sum product process whichcombinesthe sumoperationsand product operations jqjxi xpf qi qi reply acan inourdatasettings however equation lossorcross entropyloss dnnnetwork givenatriple inthetrainingset weran tivesamples concretely wewouldlike tobeasleast plusamargin thus minimizexq max kk whereweaddan penaltywithcoef foralltheparameters networkfrommulti query reply query posting andquery context correspondingly training asourmodelis almost theparame gating matching inthisway rankingevidences canbeincorporat inequation and thatis ourgoals table datastatistics source posting reply vocabulary zhidao douban tieba weibo misc total aretocompute and forquery reply query posting andquery context inthissection evaluatetheeffective to respondschema and eval dataset asmentioned variousforums microblogwebsites baiduzhidao doubanforum baidutieba sinaweibo etc forwardin high quality conversationrecord intotal thedatabasecontains million hposting reply ipairs neuralnetworks forvalidation and fortesting it databaseforretrieval learningregime weran validationwasbased priatenessof each samplewasjudgedby theappropriateness ifany ﬂindicatesan inappropriateone experimentalsetups hyperparameters inourproposedmodel weused dimensionalwordembed dings ing wordsegmentation phras occurrences thebi directionallstmhas sion cnnis weused withamini batchsizeof foropti mization propagation initial http www zhidao baidu com http www douban com http www tieba baidu com http www weibo com learningratewassetto wasapplied we evaluationmetrics giventherankinglists for testqueries ingmetrics precision meanaverageprecision map ndcg isthe precisionatthe stposition replyre trieved besides wealsoprovidedthetop we thetop thetop formally ndcg jtjxq zkxi ri log kdenotesthetop kposi tionintherankinglist and fromaperfectranking thcandi appropriate inappropriate mapiscomputedby map jtjxq nqkxi and piistheprecisionat wealsohavethehu hencewein mrr evaluationcomputedas mrr jtjxq rank whererank didaterankinglist unlikemapandndcg whichexaminethe perhaps thebestresponse weincludeseveral thebaselines generation basedmeth odsand retrieval veryrecentstudies retrieval basedmethod generation basedconversation put smt smtisamachinetrans ifwetreatqueriesand table indicatesthatwe performanceof forgenerativemethods hencethe modelp mapndcg ndcg ndcg mrrsmt ritteretal lstm rnn sutskeveretal nrm shangetal randommatch okapibm deepmatch luandli lstm rnn palangietal arc huetal deepmatchw contextadaption lstm rnnw contextadaption arcw contextadaption deeplearning to respond dl table mapndcg ndcg ndcg mrrnocontext wholecontext add one drop out combined table mapndcg ndcg ndcg mrrquery replyw oquery context query postingw oquery context query replyw query context query postingw query context fullcombination based lstm rnn lstm work rnn lstm archi tecture weuse lstm forgen eration weuseanlstm query toavectorspace rnntode reply forretrievals weadoptthelstm spondingmachine nrm proposedin whichisanrnn localattentionschema retrieval basedconversation whichreturnthebest aparticularquery based weselect strongretrieval randommatch beawareitisnot the okapibm rankcandidatereplies foreachquery vantreplyusingbm model fromthecorpus deepmatch obtainedvialda arc deeplearning to respond dl weproposethedl context basedqueryreformu and ofdl isonsbetweendl overallperformance andourproposeddl map ndcgandmrr intable lbaselinemethods ourproposedmethoddl rshowsclearly onaverage dl achievesanaverage improvement averagedonallmet rics tion intable givenoneus erquery cg rr ingeneral scores whilelstm method smt queries butnotenough basedbaselines randommatch lines aswementioned results buts lightlyworse okapibm andsimple retrievalsystem theperformanceforbm isnotasgoodasthe otherdeeplearning whichisnotsurpris ing whilebm only levelretrieval thedeep gorithms rstilloutperformsthe themightbedue whiletheother textinformation intable tion arc andlstm rnnall getboosted it tionalscenarios ourproposeddl andweintegratemulti dimensionsof thecontextu ronmrr evaluation asmentionedbefore thebestresponse tillnow contex dimensionsofranking otherthanthepro wehavet woothercontributions contextinformation and integrationofmulti dimensionofrank ingevidences asmentionedinsection nocontext wholecontext add one drop out andcombined thewholecontext coarsegranularity whilethe add oneanddrop out strategiesare ina granularity rframeworkintable nocontext towholecontext isratherobvious scenarios add oneanddrop out wholecontext strate gy wholecontext strategyisacoarse relevantones formationisimportant drop out theadd onestrategy andinmostcases therearemore thecombinationof grained andcontextmodeling avoidingexplosive numberofcombinations componentsanalysis dimensions ofrankingevidences query reply query posting andquery context note ofa posting reply pair fromthe reply sideand orthe postingside query context query context part query replyandquery posting query context information theproposedframework mighthandlesingle whilegeneral lymulti theperfor each adifferentaspect conclusions inthispaper givenahuman issued messageasthequery deeplearning to respond schema thereare weproposeacon conversationtask weintegratemulti dences queries postings replies andcontexts weestablish andcomponents massiverepository million posting reply pairs andrunex rmodelwithseveral ourmethodconsistent ofp map ndcg andmrr furthermore wehaveinvestigat analysis ingeneral fulforconversations especiallymulti thisworkopenstosev forinstance we orientedformulations suchasdialogueacts conversationallogics etc acknowledgments ofchina no cb valuablecomments references bengio foundationsand bessho harada andy kuniyoshi dialogsystemusing real scalecorpus in sigdial pages cong wang lin song andy sun finding question in sigir pages graves mohamed andg hinton in proc acoustics speechand signalprocessing pages he gimpel andj lin multi in emnlp pages higashinaka imamura meguro miyazaki kobayashi sugiyama hirano makino andy matsuo towardsanopen processing in coling hu lu li andq chen in nips pages järvelinandj kekäläinen cumulatedgain basedevaluationof irtechniques acmtrans inf syst ji lu andh li textconversation corr abs kalchbrenner grefenstette andp blunsom aconvolutional arxivpreprint arxiv lee ai croft andd sheldon anoptimization in cikm pages leuski patel traum andb kennedy buildingeffective in sigdial pages leuskiandd traum npceditor creatingvirtualhuman aimagazine liandj xu foundationsand li galley brockett gao andb dolan diversity models arxivpreprintarxiv li mou yan andm zhang stalematebreaker aproactive content computer conversation in ijcai luandh li in nips pages manning raghavan andh schütze introductionto informationretrieval mikolov chen corrado andj dean efestimationof arxiv mou li zhang wang andz jin convolutionalneural inaaai pages mou peng li xu zhang andz jin discriminative basedconvolution in emnlp pages mou rui li xu zhang yan andz jin basedconvolution arxivpreprintarxiv nakano miyazaki yasuda sugiyama hirasawa dohsaka andk aikawa wit real in sigdial pages nouri artstein leuski andd traum augmenting answerpairs in aaaifallsymposium questiongeneration palangi deng shen gao he chen song and ward memorynetwork arxivpreprintarxiv ritter cherry andw dolan data drivenresponse in emnlp pages rocktäschel grefenstette hermann ko cisk and blunsom arxivpreprintarxiv severynanda moschitti in sigir pages shang lu andh li text conversation in acl ijcnlp pages socher pennington huang ng andc manning semi distributions in emnlp pages sugiyama meguro higashinaka andy minami open systemsusingweb in sigdial pages sutskever vinyals andq le withneuralnetworks in nips pages walker passonneau andj boland quantitativeand systems in acl pages wallace theanatomyofalice springer wang lu li ande chen short textconversations in emnlp pages williams raux ramachandran anda black thedialog in sigdial pages xu jia mou li chen lu andz jin improved augmentation arxivpreprintarxiv xu mou li chen peng andz jin classifying dependencypaths in emnlp yan poet in ijcai yan lapata andx li co ranking in acl pages yan li hsieh hu hu andt he socialized onsocialnetworks in www pages yan wan otterbacher kong li andy zhang in sigir pages yan yen li zhao andx hu tacklingtheachilles heelofsocialnetworks smoothing in www pages zhaiandd williams task orienteddialogues in acl pages zhang su xiong lu duan andj yao shallow recognition in emnlp pages 
proceedingsofcoling the technicalpapers pages osaka japan december 
weisong shiqizhao chaozhang huawu haifengwang lizhenliu hanshiwang beijing china zbaiduinc beijing china wsong lzliu hswang cnu edu cn zhaoshiqi zhangchao wu_hua wanghaifeng baidu com abstract webpagetitles technique the seed orclassname which alarge tailedandcomplex theirconnections ourmethodcan of the artapproaches artiialintelligence learning knowledgeac quisition artiialintelligence naturallanguage processing generalterms algorithms experimentation keywords url pattern templategeneration weaksupervision baidu mittee iw iw author www may florence italy acm http dx doi org introduction anamedentity entityforshort oftextualtokens acertainclass en ie languageprocessing nlp ir tasks forexample oftextisanentity besides insuch scenario tailedentities standinguserintent itischallenging andrestaurantnames bexible ties probleminerentways whilesemi weakly names andhtml tablesofwebpages however edgeorhumanlabor semi butdon themanualla because itisimpossi therefore searchenginesand ingwrappers besides methodsareproposed butexistingsystems whichare inthispaper figure manuallabororsophis ticatednlptechniques ourmethod parallelwebpage theterm hid den isolatedwebpagetitle such wecanseethatentities titles therefore slotsinthetemplates themainchallenges aretwo fold forthetchallenge generalized urlpatterns basedonthisstrategy webpagetitles extraction weapplytheideaof multi plesequencealignment msa toalignthesewebpage then weuti entities finally werankcan wearethetoexploit ner figure approach thesystemoutputin thecon insummary weproposeal tures ourmethod manyofwhichare long tailedentities andtheirconnections mainspecicentities inbothsettings ourmethod outperformsthestate of the artapproachesbasedon insec tion weproposeourmethod weshowourexperiments extractioninsection section proposedmethod motivation inwebpagetitles sites thewebpagetitles havesimilar entitiesplayaroleas becausewebpage ifaweb theentity forone entity titlecontainingit therefore standable waywithinawebsite asshowninfigure itiseasytosee moviename digit plotsummary imdb template thedlike moviename or digit issubstitutable notedasa slot the sloters ofentities motivatedbythis weaimto weexpect figure urlinstances pattern http www imdb com title tt nd url http www imdb com title tt url http www imdb com title tt terninawebsite theprocessis asfollows step segmentthenon domainpartofeachurl with time forexample arehttp www imdb com reg tt and http www imdb com title reg where reg is step generalized theurldatabase suchas http www imdb com title reg step theal thissegment becomes http www imdb com title tt nd wecangetcorre wewill tern thetextualstructures there therefore weadapt theneedleman wunschalgorithm forpair wisealign pairwisealignment wenetheelementsetas vsfg where and suppose giventwo wordsequences and analignmentcanbe representedasa dimensionalarray align ithatevery inserting aword intoonesequenceor deleting awordfromtheother iisthe substitutionmatrix inwhich represents we alignscore asthesum alignscore ixi align align therefore thetask asshowninequation istodan align argmax align ialignscore toachievethis matrix indetail isnedasfollows exactmatch and areexactmatchedif sothatidentical matchisawarded mismatch includetwotypes normalwordmismatch andgappenalty if and areerent normalwords ne if and dene forbidenmatch anormalword aisnotallowedto forwhichwedene andsoon limiters derentblocksintitles generally delimitersshouldn tbe tbealignedwithnor malwords wecomputeamatrix the entriesof alignments oncethematrix fiscomputed webacktrack algorithm alignments input sequences aand substitutionmatrix randgappenalty output scorematrix initializematrix length length witha lineargappenalty fori to length do fi id endfor forj to length do jd endfor computeoptimalscores fori to length do for to length do match fi ai delete fi insert fi fi max fmatch delete insert endfor endfor return templateinduction ment ingly quencewitha slot slot thetemplate slot or thetemplateitselfis slot itisdiscarded becausethis giventwosequences respectively platesare template slot template slot figure in lectedasthekeyslot sinceitmatchesusers search someofthe suchastheex ampleshowninfigure arewhatwereallywant plate whichistermed keyslot inwhatfollows wetake advantageofuserclick likelytomatchusers searchinterest inotherwords for titles supposeatemplate thas mslots onetitle xismatchedby tandthecorresponding slotersare ff where fi istheslotter ofthe ithslot siofthetemplate ifthetitle xisclicked atitle querypair forms querypairs grelated totemplate theweightofthe ithslot sitouserqueries couldbemeasuredas weight si jaccard fi where jaccard wordsets wechoose argmax iweight si asthekey slotoftemplate if weight soifonetemplate hasmorethanoneslot eralization fromthekeyslot templatering ties tothisend wetidentifyhigh condentvalidand toteroutsuch candidates where who ah andsomepunctuation ifacandidateen weureoutasetof acandidateentity theweb inthiswork timesby dittemplates finally if kinvalidentities tion forexample the title stirfriedbeefrecipe websitename template slot recipe websitename template slot websitename template slot beefrecipe websitename weviewit formally giventhetitles fx jxj gforoneurlpattern andatemplate sett ft jtj gassociatedwith wehavetoselect thebesttemplate xforeachtitle forentity extraction where txisasubsetof tthatcanmatch atglance ranking common first alignment first butmaypro duce super entities insomecases forexample applying template stirfriedbeef recipe asacandidateentity incontrast alignmentrst butsometimes itmayresultin sub entities suchas stirfried suchtem weproposean adaptiveranking adarank giventitle theprob abilityofchoosing representedas tjx xjt xjt xisgeneratedus ingtemplate here weapproximateitusing alrank where alrank istherankoftemplate tamong txac eratethetitle alargervalueof canbeseenasthe tbeingagoodtemplate we estimate alrank jxj where alrank inxthattemplate them thetemplate tforthetitle xas rank tjx alrank alrank insummary balancebetweenthem basedonequation weusethetop it inalgorithm algorithm input asetofwebpagetitles xofcertainurlpattern asetoftemplates output asetofentity templatepairs et initiate et for each xinxdo gettemplates txwhichcanmatch rank tfor extracttheslotler oft add to et endfor return et extraction ties wetdescribethedata finally wereporttheex experimentalsettings data awebpagedataset which inall therearemorethan userclickedurls inourwork selectingkeyslots whilethebaseline seesection querylogs first on billionweb pages then table statisticsofthedata webpagedataset volume webpages querylogs volume queries logs finally terns alignment patternsaswell them ourmethod doesn clopedias baidubaike hudongbaike andchinesewikipedia baseline asreportedinboth and basedondocuments inthiswork weaimtoshowthat entityextrac fromqueries notedasoeq weadaptoeqtochi nese weextractall frequencies weretainall grams iftheysatisfythe aloneconstraints representation ceedathreshold stand querybyitself andstand alonewaymustexceeda threshold referto formoredetails parameters duetothe largesizeofdataset theparameters forpairwisealignment wenethesub and the templateinduction fortemplateering wesetparameter kto and jto ˆis setto forthebaseline alonescore to whichisthesamein wewouldreporttheper http baike baidu com http www baike com http zh wikipedia org table method entity precision coverage oeq oeq ours withwhichoeqproduces larger to asareference experimentalresults overallperformance precisionandcoverage werandomlysampled andasked rect enti suchas low carboneconomy areallconsidered ascorrect inner coent is weadopt precision asanevaluation whichisde edas allextractedentities evaluatetherecall weevaluatethe coverage bycompar the whichhasmorethan millionentries wedidn the coverage table wecanseewhen ourmethodhashigher cision threshold oncoverage ourmethod outperformsoeqaswell proved topicanalysis oftopics tothisend tagsfrombaidubaike indetail baidubaikemaintains thetaxonomycon tains sub categories in thisexperiments categorytag generalcategorytoit inthisway wecollectedabout infigure categoryhas person per literaturework literature andorga nization org amongthe millionentities morethan million thedistribution aswell thecoverageranges figure figure ciesinquerylogs from onliterature to onsport across allcategories frequencyanalysis figure illustratesthe wecansee thatabout makeup ofallentities lengthanalysis worden thusweanalyzethe length thenumberofwords fig ure wecanseethat than from wesampled en andevaluatedthe precision ourmethodachievesa precision of while oeqgetsa precision of al theyarenotentities erroranalysis extractedbyourmethod figure intervals into types toogeneralclasses theextractedentities suchas containingentity attributeorintent suchas attractions inbeijing usergeneratederrors somewebpagetitles aregeneratedbyusers insomecases althoughthetitles theslotlersarenoisy edit discussion weshowthat long tailedentitieswith the for example asbooks incontrast aliasoffullnames tailed lesspopularentities basedonredundancy entitiesofsomee forexample wecan dnamesofrareplants theselong tailede forwhichweexpect ontheotherhand thecoverageonlong tailed ontheotherside climate the monconceptstogether werandomlysampled andjudgedwhether theresults showthat outof denstructures table method precision common rst alignment rst adarank ties turecanbedividedinto categories alltitlesarethe same fortunately ourex entityextraction amongthe generalizedurlpat whiletheothers aftertemplateering remain whichaccountfor ofall eachremainedgen average however terncancovernearly webpagetitles thenumberin creasesto ifwetaketop eachurlpattern textualstructures dataandmetrics tiontemplates spcally forthispurpose weran domlysampled templates tofacilitatethe annotation wealsoprovide template we use precision asanevaluationmetric whichisnedas selectedkeyslots precision of dataandmetrics werandomlysampled generalized section fromeach urlpattern wesampled byatleast templates titleswhichhave templatesonaverage foreachtitle ducedinsection commonrst alignment rstand adarank weuse precision results table showstheresults wecanseethat templates alignment commonrst allerrorsofalignment rstheuristiccome wordentities in theextractedentity actually theentity therefore thedis tantsupervision andweaksupervision labels tionthroughthisgraph inthispaper weapplythisdata asin afewseeds sofadomainisgiven the main thisprocessinvolves thefollowingsteps step extractalltemplates tthatcontainatleast oneoftheseeds step buildabipartitegraph ft lg where bytemplatesin ifanentity isextractedbya template thereisanedge betweenthem step titiesinthisgraph weuse tscore use escore anduse pri etotheex amineddomain wesetthe pri forseednodesas jsjand forallotherentities the tscore escore forall entitiesas thenineachiteration therelevancescores tivelyasfollows escore ri xtp ejt tscore tscore xep tje escore where ejt textractsan entity if textracts ejt entitiesextractedbyt other wisep ejt similarly tje issetto iftextracts otherwise tje isarealvaluebetween and thatcanextractit ednumberofiterations agatinginformation experimentalsettings baseline whichisthestate of the notedaswslog this dataandmetrics domainsthatarethe foreachdomain seedsare given were theparameter isset to times foreachdomain wechosethetopranked expanded we entitiesofeachdomain ascorrectmustsatisfy itisacorrectentity itbelongs totheexamineddomain weuse precision recall and asevaluation metrics wene and prp experimentalresults table inmostdomains precisionandrecall ofentitiesinqueries forexample manytemplatesfordo main university pekinguniversity ascandidateentities incon trast webpagetitles our inaddition tailedentitieswell forthedomain food wslogandour itindi alowoverlap suchas and pork becausetheseappear moreofteninqueries kungpaochicken higher incontrast entitieslike âtq finally ousinsomecases forexample thetemplateslike drug allergy and drug instruction areimpor drug however thesetemplates cosmetics aswell actually thisisbe forexample news papers table ofdomainentities domain wslog oursmetrics prf prf city country drug food location movie newspaper person university videogame average getherinsomewebsites domain newspaper isnotsogood extractingentitiesof relatedwork inthissection entityextraction titieswithindomain theer sourceswith weak supervision forexam ple etzionietal althoughthe systemdoesn ourworkisalsorelated in asemi webtables instead be sides whereasourmethodfo for example in first ginebasedontheseeds patterns titiesfromqueries finally theentitiesareranked openentityextraction oie attemptstoextract bankoetal proposeanopen tion andthenmake ties downeyetal entitiesintexts wordex pressionsasentities however or parameswaranetal pro grams butthe kislarge soit jainetal according itdoesn wefocusonthe thetitle insteadofus ingdocumentcontents spcally weidentifyandutilize collectively inthisway inaddition we hiddenstructures applications msa generallypro tein dna orrna tionary and motivated bythesework lelwebpagetitles inthisway in wecan buttheir plexmulti wordentities in related couldbeseen in clusteringisadopted aspects inpreviouswork andsearch while wedealwithplain conclusions the alargera tioofwhichislong tailedandcomplex throughthewhole process inaddition theextractedentity whichcan ingweaksupervision acknowledgments programofchina program grantno cb grant no grantno andthebeijingmunici grantno references arasuandh garcia molina extractingstructured datafromwebpages in sigmod pages acm banko cafarella soderland broadhead ando etzioni openinformation extractionfortheweb in ijcai volume pages barzilayandl lee viamultiple sequencealignment in emnlp pages linguistics barzilayandl lee learningtoparaphrase an sequence alignment in naacl volume pages bikel miller schwartz and weischedel nymble ahigh performancelearning namender in pages blanco dalvi anda machanavajjhala highly websites in www pages acm carletta tasks thekappastatistic carlson betteridge wang hruschkajr andt mitchell coupled semi in wsdm pages acm carrilloandd lipman themultiplesequence siamjournalon appliedmathematics chambersandd jurafsky template based in acl volume pages associationfor chieuandh ng information in coling volume pages collinsandy singer in pages citeseer dalvi cohen andj callan websets in wsdm pages acm downey broadhead ando etzioni locating in ijcai volume pages etzioni cafarella downey popescu shaked soderland weld anda yates unsupervisednamed anexperimentalstudy artiialintelligence etzioni fader christensen soderland and mausam thesecond generation in ijcai volume pages gatterbauer bohunsky herzog kr upl andb pollak towardsdomain independent in www pages acm grishmanandb sundheim message abriefhistory in coling volume pages guptaands sarawagi answeringtable web haveliwala kamvar andg jeh ananalytical jainandm pennacchiotti openentityextraction in coling pages stroudsburg pa usa associationfor mccallumandw li earlyresultsfornamed enhancedlexicons in naacl volume pages association needlemanandc wunsch ageneral journalof molecularbiology nie wu wen andw ma extracting objectsfromtheweb in icde pages ieee parameswaran garcia molina and rajaraman proceedings ofthevldbendowment pa sca weakly in cikm pages acm persing davis andv ng modelingorganization instudentessays in emnlp pages sekine sudo andc nobata extendednamed entityhierarchy in lrec shinyamaands sekine namedentitydiscovery in coling page talukdar reisinger pa sca ravichandran bhagat andf pereira weakly in emnlp pages linguistics wangandw cohen character level analysisofsemi expansion in emnlp volume pages zhang zhao andh wang bootstrapping large texthybrid patterns in ijcnlp pages 
convolutional recurrent neural networks for small footprint keyword spotting sercan arõk markus kliegl rewon child joel hestness andrew gibiansky chris fougner ryan prenger adam coates baidu silicon valley artificial intelligence lab bordeaux dr sunnyvale ca usa equal contribution sercanarik baidu com klieglmarkus baidu com abstract keyword spotting kws constitutes major component of human technology interfaces maximizing the detection accuracy at low false alarm fa rate while minimizing the footprint size latency and complexity are the goals for kws towards achieving them we study convolutional recurrent neural networks crnns inspired by large scale state of the art speech recognition systems we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long range context we analyze the effect of architecture parameters and propose training strategies to improve performance with only parameters our crnn model yields acceptably low latency and achieves accuracy at fa hour for db signal to noise ratio index terms keyword spotting speech re cognition convolutional neural networks recurrent neural networks introduction motivated by the most common way humans interact with each other conversational human technology interfaces are becoming increasingly popular in numerous applicati ons high performance speech to text conversion and text to speech conversion constitute two important aspects of such interfaces as most computational algor ithms are developed for text input and output another crucial aspect of conversational interfaces is keyword spotting kws also known as wakeword detection to enable transitioning between different computational states based on the voice input provided by the users kws systems aim to detect particular keyword from continuous stream of audio as their output determines different states of the device very high detection accuracy for very low false alarm fa rate is critical to enab le satisfactory user experience typical appl ications exist in environments with interference from background audio reverberation distortion and the sounds generated by the speaker of the device in which the kws is embedded kws system should demonstrate robust performance in this wide range of situations furthermore the computational complexity and model size are impo rtant concerns for kws systems they are typically embedded in consumer devices with limited memory and comput ational resources such as smartphones or smart home sensors there are already millions of devices with embedded kws systems traditional approaches for kws are based on hidden markov models with sequence search algorithms with the advances in deep learning and increase in the amount of available data state of the art kws has been replaced by deep learning based approaches due to their superior performance deep learning based kws systems commonly use deep neural etworks dnns combined with compression techniques or multi style training approaches potential drawback of dnns is that they ignore the structure and context of the input and an audio input can have strong dependencies in time or frequency dom ain with the goal of exploiting such local connectivity patterns by shared weights convolutional neural networks cnns were explored for kws potential drawback of cnns is that they cannot model the context over the entire frame without wide filters or great depth recurrent neural networks rnns were also studied for kws with connectionist temporal classification ctc loss unlike the aforementioned dnn and cnn models with cross entropy ce loss yet high accuracy at low fa rate could not be obtained given the ambitious targets of the applications of such systems similar to dnns potential limitation of rnns is that the modeling is done on the input features without learning the structure between successive time and frequency steps recently proposed convolutional recurrent neural etwork crnn architecture with ctc loss however despite the large model size similar to rnns high accuracy at low fa rate could not be obtained in th is paper we focus on developing prod uction quality kws system using crnns with ce loss for small footprint model applied for single keyword our goal is to combine the strengths of cnns and rnns with additional strategies applied during training to improve the overall performance while keeping small footprint size the rest of the paper is organized as follows in sec tion we describe the end to end architecture and training methodologies for small footprint kws in section we explain the experiments and the corresponding results in section we present our conclusions small footprint keyword spotting end to end architecture we focus on canonical crnn architecture inspired by the successful large scale speech recognition systems to adapt these architectures for small footprint kws the model size needs to be shrunk two to three orders of magnitude we will analyze the impact of different parameters on performance while shrinking the size of the model fig shows he crnn architecture with the corresponding parameters the raw ime domain input are converted to per channel energy normalized pcen mel spectrogram for succinct representation and efficient training other input representations we experimented with yielded worse performance for model architectures of comparable size the pcen features are given as inputs to the convolutional layer which employ filtering along both the time and frequency dimensions the outputs of the convolutional layer are fed to bidirectional recurrent layers which migh nclude gated recurrent units grus or long short term memory lstm units and process the entire frame outputs of the recurrent layers are given to the fully connected fc layer lastly softmax decoding is applied over two neurons to obtain corresponding scalar score we use rectified linear units as activation function in all layers figure end to end crnn architecture for kws end to end raining in speech recognition large scale architectures with recurrent layers typically use variants of ctc loss to decode the most probable output label aside from the modeling limitations due to conditional independence assumptions of targets ctc loss has high computational complexity and typically yields good performance only when the model capacity is sufficiently large to efficiently learn from large data set as we focus on small footprint arch itectures the loss function that is optimized during the training is chosen as the ce loss for the estimated and target binary labels indicating whether frame corresponds to keyword or not to train with ce loss unlike ctc precise alignment of the training samples is important we use deep speech large scale speech recognition model to obtain the estimated probability distributions of keyword characters for each time instance as the ctc decoding yields peaked distributions we further smooth the output over time and obtain smoothed character occupancy scores we then obtain the beginning and end times of the keywords using the heuristic algorithm shown in algorithm an extra short padding is added while chopping the keywords to cover edge cases the accuracy of alignments obtained were significantly beyond the time scale of human perception experiments and results dat and training we develop our kws system for the keyword òtalktypeó which can be pronounced as single word or two words we choose fr ame length of seconds which is ufficiently long to capture reasonable pronunciation of òtalktypeó using sampling rate of hz each frame contains raw time domain samples corresponding pcen mel spectrograms are obtained for ms stride and channels yielding an input dimensionality of the entire dat set consists of different samples collected from more than speakers the dataset is split into training development and test sets with ratio training samples are augmented by applying additive noise with power determined by signal to noise ratio snr sampled from db interval the additive noise is sampled from data set of representative background noise and speech with total length exceeding hours to provide robustness against al ignment errors training samples are also augmented by introducing random timing jitter we use the adam optimization algorithm for traini ng with batch size of the learning rate is initially chosen as and later dropped to our evaluation considers streaming scenario such that inference is performed for overlapping frames of duration the shift between the frames is chosen as ms which should ideally be much longer than the spectrogram stride and much shorter than the inference latency see section for more details the metrics we focus on are the false rejection rate frr and false alarms fa per hour typically fixing the latter at desired value such as fa hr noise is added to the development and test sets with magnitude depending on the snr value we note that the collected samples are already noisy so the actual snr is lower if defined precisely as the ratio of powers of the information bearing signal and the noise similar to our augmentation of the training sets negative samples and noise datasets are sampled from representative background noise and speech impact of the model architecture table shows the performance of various crnn architectures for the development et with db snr we note that all models were trained until convergence even though it requires very different number of epochs we observe the general trend that the larger model size typically yields better performance increasing the number of convolu tion filters or increasing the number of recurrent hidden units are the two effective approaches to improve the performance increasing the number of recurrent layers has limited impact and gru is preferred over lstm as better performance can be obtai ned for lower complexity algorithm sequential alignment of keyword samples require keyword characters smoothed character occupancy scores decay rate initialize for to for to right to left decoding for end for to left to right decoding da da for end end return ef table performance of different crnn architectures see fig for the description of the parameters the chosen set of parameters for the rest of the paper is colored and highlighted in bold it is desired to limit the model size given the resource constraints for inference latency memory and power consumption following we choose the size limit which is more than times smaller than the architecture with ctc loss in for the rest of the paper the default architecture is the set parameters highlighted in bold which also corresponds to fairly optimal point given the model size vs performance trade off we compare the performance with cnn architecture based on given the discrepancy in input dimensionality and training data we reoptimize the model hyperparameters for the best performance while upper bounding the number of parameters to for fair comparison for the same development set with db snr the best cnn architecture achieves frr at fa hour and frr at fa hour both metrics are highe compared to the frr values of the chosen crnn model with parameters interestingly the performance gap is lower for higher snr values we elaborate on this in section recall that the model is bidirectional and runs on overlapping second wi ndows at ms stride however thanks to the small model size and the large time stride of in the initial convolution layer we are able to do inference comfortably faster than real time the inference computational complexity of the chosen crnn based kws model with parameters is roughly floating point operations flops when implemented on processors of modern consumer devices without special functions to implement nonlinear operations even hen implemented on modern smartphones without any approximations and special function units our kws model can achieve an inference time much faster than the time scale for reactive time for humans with auditory stimuli which is ms impact of the amount of training data fig ure frr at fa hour vs number of unique training keywords for the test set with db snr convolutional recurrent fc total number of parameters frr for the noise development set with db snr nc lt lf st sf nr recurrent unit nf at fa hour at fa hour gru lstm gru gru gru gru gru gru gru gru gru lstm gru gru gru gru gru lstm gru gru lstm gru gru gru gru gru given the repre sentation capacity limit imposed by the architecture size increasing th amount of positive samples in the training data has limit ed effect on the performance fig shows the frr at fa hour for the test set with db snr vs the number of unique òtalktypeó samples used while training saturation of performance occurs faster than applications with similar type of data but with large scale models besides increasing the amount of the positive samples we observe performance improvement by increasing the diversity of relevant negative samples obtained by hard mining we mine negative samples by using the pre converged model on very large public videos dataset that are not used in training development or test sets then training is continued using the mined negative samples until convergence as shown in fig hard negative mining yields decrease in frr for the test set noise robustness fig ure frr vs fa per hour for the test set with various snr values for the test set with various snr values fig shows the frr vs fa per hour for higher snr lower frr is obtained and stable performance starts for lower fa rate note that the snr values in db of the augmented training samples are sampled from distribution with mean of db and deterioration in performance is observed beyond this value performance for lower snr values can be improved by augmenting with lower snr but this comes at the expense of decreased performance for higher snr which can be attributed to the limited learning capacity of the model we observe the benefit of recurrent layers specially for lower snr values the performance gap of crnn architectures with cnn architectures adapted from as explained in section reduces as the snr increases we hypothesize that the recurrent layers are better able to adapt to the noise signature of individual samples since each layer process es information from the entire frame cnns in contrast require wide filters and or great depth for this level of information propagation far field robustness our dataset already consists of samples recorded at varying distance values which should be representative for most applications such as smartphone kws syst ems yet ome applications such as smart home kws systems require high performance at far field conditions figure frr at fa hour vs additional distance for far field test sets with varying snr values solid baseline performance dashed with far field augmented training fig shows performance degradation with the additional distance far field test sets are constructed by augmenting the original test set with impulse responses corresponding to variety of configurations at the given distance considering different values for degrees of arrival etc significant deterioration in performance is observed especially in conjunction with higher noise as also explained in to provide robustness against this deterioration we consider training with far field augmented training samples using variety of impulse responses that are different than the ones in the test set this augmentation achieves significantly less degradation in performance for farther di stances yet it yields worse performance for the original data set due to the training testing mismatch conclusions we studied crnns for small footprint kws systems we presente the trade off between model size and performance and demonstrated the optimal choice of parameters given the tradeoff the capaci ty limitation of the model has various implications erformance gain is limited by merely increasing the number of positive samples yet hard negative mining improves the performance training sets should be carefully chosen to reflect the application environment such as the nois level or far field conditions overall at fa hour which is an acceptable value from user perspective our model achieves and accuracy for the test set with db db and db snr values respectively our numerical performance results may seem better tha other kws models in the literature however direct comparison is not meaningful because of the difference in the data sets and the actual keywords the inference task given that human performance is excellent in the kws task we still believe tha there is further room for improvement in terms of performance acknowledgements discussions with andrew ng sanjeev satheesh jiaji huang jue sun and bing jiang are gratefully acknowledged we thank hui song for the impulse response measurements used for far field augmentation references rohlicek ru ssell roukos and gish continuous hidden markov modeling for speaker independent wordspotting in ieee proceedings of the international conference on acoustics speech and signal processing pp chen parada nd heigold òsmall footprint keyword spotting using deep neural etworks in proceedings international conference on acoustics speech an signal processing pp tucker wu sun panchapagesan fu and vitaladevuni òmodel compression applied to small footprint keyword spotting in proceedings of interspeech pp vikas sindhwani tara sainath and sanjiv kumar òstructured transforms for small foo tprint deep learning in neural information processing systems pp prabhavalkar alvarez parada nakkiran and sainath òautomatic gain control and multi style training for robust small footprint keyword spotting with deep neural networks in ieee proceedings of the international conference on acoustics speech and signal processing pp panchapagesan sun khare matsoukas mandal hoffmeister and vitaladevuni òmulti task learning and weighted cross entropy for dnn based keyword spotting in proceedings of interspeech pp sainath and parada convolution al neural networks for small footprint keyword spotting in proceedings of interspeech pp wang getreuer hughes lyon and saurous òtrainable frontend for robust and far field keyword potting arxiv preprint arxiv hwang lee and sung òonline keyword spotting with character level recurrent neural network arxiv preprint arxiv fernandez graves and chmidhuber òan application of recurrent neural networks to discriminative keyword spotting in artificial neural networks springer pp lengerich and hannun òan end to end rchitecture for keyword spotting and voice activity detection arxiv preprint arx iv deng and platt ensemble deep learning for speech recognition in proceedings of interspeech sainath vinyals senior and sak òconvolutional long short term memory fully co nnected deep neural networks in ieee proceedings of the international conference on acoustics speech and signal processing pp amode et al deep speech end to end speech recognition in english and mandarin arxiv preprint arxiv cho van merrienboer bahdanau and bengio òon the properties of neural machine translation encoder decoder approaches arxiv preprint arxiv hochreiter and schmidhuber òlong short term memory neural computation vol no pp kingma an ba òadam method for stochastic optimization arxiv preprint arxiv shelton and kumar gp òcomparison between auditory and visual simple react ion times neuroscience and medicine vol no pp kumatani et al microphone array processing for distant speech recognition towards real world deployment in ieee asia pacific signal information processing association annual summit and conference 
pingli baiduresearch and rutgersuniversity pingli gmail com martinslawski mslawsk gmu edu abstract onelineofresearch products insuchsetting weshowthatevenunder to bitsperprojection ofpracticalinterest icml whichis introduction rps thecelebratedjohnson lindenstrauss jl lemma hadbeenlaidout inanutshell dimensional representation conveniently matrices time similarity search compressedsensing clustering matrixsketching amongothers locality sensitivehashing ortheenhancement ofprivacy theextremecaseofone incomputerscience maxcut problem andthesimhash one bit hasmeanwhile thepapers discussquantized independentofthe application providejl andconsiderthetrade offbetween thepaper studiesapproximatejl typeresults nips longbeach ca usa thepaper inthiscontext quantizeddataare precisiondata relativetothefull precisioncase to bitsperprojection moreover weshowthat based based schemein the collision notation forapositiveinteger welet for denotesthe th componentofavector thebracketsin supplement basicsetup let gˆ wethinkof beinglarge bymeansofalinearmap lj ax ax ˇk forall thisisimplied iˇh ax ax forall forthetimebeing oneforms beforeapplying inthiscase itsuftorecoverthe cosine similarities ij of theinputdata precisionrps withoutquantization let zeromean unitvariance andcorrelation letfurther andlet bethecounterpartin thenthecomponents of aredistributedi asin with and canbere castasestimatingthe correlationfromani sampleof or lin aspointedoutin estimator mle given mle argmax log theestimator mle however thiscanbeaddressed mle overasufgrid atprocessingtime computationof mle canthenbereducedtoa look upinapre computedtable oneobviousissueof lin anatural norm weevaluatethemean squarederror mse weconsider mse bias var bias where issomeestimator andthesubscript sample with itturnsoutthat norm and mle asymptotic msesthan lin forlarge valuesof itcanbeshownthat cf and bias lin var lin bias norm var norm bias mle var mle whilefor the asymptotic msesarethesame of norm and mle decayatrate as whereasthemseof lin growswith the lin and norm forselectedvaluesof mse lin mse norm inconclusion ifitispossibletopre reduction themlecanprovide norm tion paradigm quantization let with beasetof thresholds intervals andlet beasetof codes with representinginterval given and thescalarquantizer orquantizationmap isby sign where denotesthe thcomponentof thebitdepth log forsimplicity isaninteger thecase is well studied bin basedvs code basedapproaches let and intheprevioussection inthispaper wedistinguishbetween from the paradigm whichwerefertoas bin based estimation thecodes ﬁbinsﬂ thisisoppositetothe secondparadigm referredtoas code based codes asweelaboratebelow anadvantageofthebin onthe otherhand acode inthispaper lloyd max lm quantizer with respectively beinged andthecodes ofthequantizer basedapproach inour setting with of argmin problem fored andviceversa intheabsenceof appearsasa inthelimitofcosine similarity with replacedbyitssquare andtakingthe however hencewestickto inthesequel bin basedapproaches mle givenapair and max isstudiedindepthin with asin itisshownin thatthemlewith single bitquantization theoptimalchoiceof increaseswith thegivensetting in so evaluatedfrequently collision basedestimator thecollision isabin basedapproach asthemle thesimilarity isestimatedas col wherethemap isby increasingin comparedtothemle col events inparticularfor closeto however as cf figure on thepositiveside col canbe onagrid figure asymptoticmses of col tobedividedby for asymptotic relativeefmse col mse mle for where mle isthemlein boundonbias figure bias lin andtheboundoftheorem uniformupperboundson bias lin obtainedfromtheorem bysetting var lin tobedividedby code basedapproaches inthecode basedapproach precisiondata with replacedby perhapssurprisingly it and withafundamental gapcomparedtobin linearestimator weconsider lin wenotethat lin lin dependson lin lin in acrucialdifference betweenthecode basedandthebin forany based theexactbiasof lin independenceof and by where arenon negativeintegersand arebivariatenormal with wehave lin var lin we lin theorem wehave bias lin where asshowninfigure therateofdecayis moreover itcanbev var lin var lin wemayconcludethat dependingon themseof lin suppose and withfullprecision wehave mse lin fromfigure var lin var lin wethatthemsecan goupbyatmost ii suppose and figure ingeneral thedifferenceofthe msesfor ontheonehandand andlargevaluesof squared biasof lin in particular figure msesof lin forvarious and dotted thesolid red linesindicatethe correspondingmsesfor lin inthefull precisioncase normalizedestimator form norm itturnsoutthat thecounterpart norm reducingthebiasof lin inwhichcase bias norm byconstruction ingeneral proposition intermsofthecoef in as wehave bias norm var norm figure inparticular inbiascomparedto lin asfor while proposition isasymptotic cf supplement figure asymptotic bias norm relativeto bias lin var norm asymptotic tobe dividedby msesof lin vs themsesof coll comparison atthebitlevel coding basedestimationvs collision basedestimation thecollision mse col and mse norm atthebitlevel figure since col rps while norm usestwiceasmany bits col thevaluesof approaches fored as decreasesfrom to thesevalues increasefromabout to inconclusion col induplicatedetection ontheotherhand forgenerichigh dimensional data figure showsthatas israised col requires mse bycontrast increasing forthecoding wholerangeof decayof var norm var col theorem forany wehave var norm var col as therate isthesameasthemle in thefullprecisioncase cf isintrinsictocode based withquantized data getsreplacedby quantizationofnorms computethenorms and iˇ ij ij where ij and itmaybe aswell max where empiricalresults high theresultsof inthissection weverifythis arcene dexter farm and pems setup maxquantizer thenumberofrps isvariedaccordingto weconsider weestimatethe ij as ij where isaplaceholder basedestimator coll basedon norm for usingdata one bitquantization ishereincludedasa reference the ij libsvm totrainabinary libsvm isrunwith differentvaluesof itstuningparameter rangingfrom to results onefor eachtypeofplot andfourrows oneforeachdataset independent the foraselectedvalueof thefull theminimum testerrorsof coll and norm atthebitlevel with doubledfor coll inallplots increases thegapbetween and coll and norm conclusion inthispaper figure accuracyonthetestset optimizedover log scale independenceof log comparisonofthe norm respectively coll anexceptiononly theorem norm canbeextremelyeven regardingfuturework lindenstrauss transforms thosein off andthebitdepth inthepresent work thechoiceof acknowledgments bigdata nsf iii pinglialsothanks references achlioptas database johnson journal ailonandb chazelle in stoc pages ailonande liberty acm anderson wiley binghamandh mannila textdata in kdd pages boufounosandr baraniuk in boutsidis zouzias andp drineas meansclustering in advancesin nips pages candesandt tao near universalencoding strategies changandc lin libsvm systemsandtechnology http www csie ntu edu tw cjlin libsvm charikar in ontheoryofcomputing stoc pages dasgupta in focs pages dasgupta randomstructuresand algorithms datar immorlica indyk andv mirrokni locality stable distributions in scg pages fradkinandd madigan in conferenceon kdd pages gershoandr gray springer goemansandd williamson journaloftheacm indykandr motwani in stoc pages matousek lindenstrausslemma jacques aquantizedjohnson lindenstrausslemma thefindingofbuffon sneedle ieeetransac johnsonandj lindenstrauss contemporary mathematics pages kenthapadi korolova mironov andn mishra privacyviathejohnson journalofprivacyand kieffer function krahmerandr ward isometryproperty laskaandr baraniuk regimechange bit li rane andp boufounos augmentedreality in mmsp pages li hastie andk church in annual colt pages li mitzenmacher anda shrivastava in proceedingsofthe icml pages li mitzenmacher andm slawski linearestimationof cosinesimilarity in nips pages mahoney learning maillardandr munos compressedleast squaresregression in processingsystems nips pages raneandp boufounos privacy revealingthem rane boufounos anda vetro quantizedembeddings methodforcloud basedimageretrieval in pages vempala 
sdh jiegui rutgersuniversity piscataway nj usa guijiejie gmail com pingli baiduresearch bellevue wa usa pingli gmail com abstract learning cientstorageandre trievalofhigh videos anddoc uments inthispaper weproposealearning basedhashingalgo sdh ing sdh inr sdh correntropy squareregression lsr ness furthermore tionaltransformation rotation cingaccuracy imagedatasets mnist cifar andnus wide con rmthat keywords robustm estimator rotation acmreferenceformat jieguiandpingli sdh ing in kdd the edgediscovery datamining august london unitedkingdom acm newyork ny usa pages https doi org introduction inlarge datamining andmachine learning andindexede cientlyandaccurately asawell establishedand powerfullarge scaletechnique hashing hasshownpromisingper datamining machinelearning computervision informationre trieval hashinggen avideo similaritiesbetween theoriginalinstances ingeneral workforpersonalor forpro tiononthe rstpage theauthor mustbehonored tocopyother wise orrepublish requirespriorspeci permissionand orafee acm org kdd august london unitedkingdom author acmisbn https doi org conductedverye thus afterthe dataarehashed cientlyinlarge scaledatasets due tothe hashingmethodscan forinstance searchinge ciently imageclassi cation digit recognition andfaceindexing generallyspeaking edinto twomaincategories learning freehashingmethods and learning based alsoknownasdata dependent hashingmethods learning freehashingmethods learning freemethods ifwerestrictthedis orinteger outputs then popularlearning sign gaussian randomprojections ofthehashcodeis proportionaltothe similarityofthedata bit minwisehashing forbinarydata thecollisionprob cws thecollisionprobabil maxsimilarity apparently however otherwisetheperfor forexample whilerandomprojec tionmethods are verypopular requiring longhashingcode minwisehashing andvariants andconsistent lenttohigh formrandomprojection basedmethods intermsofaccuracyver susnumberofhashes inrealdata learning basedhashingmethods recently data dependent learning based becomepopular since potentially wouldbeabletoe cientlyande scaledata unlikelearning hashfunctions learning anenormousvolume sub categories unsupervisedhashing requiredinlearning forexample gongetal presentedanit erativequantization itq weissetal sh otherunsu pervisedlearning hashing agh imh with sne andscal sgh have alsobeenproposed usefulinformation keytoclassi cationmaybelost therefore manysemi supervised semi supervisedlearning basedhashing hashfunctionis examples forexample kulisanddarrell presentedabinary bre wangetal presentedasemi supervisedhashing ssh thevarianceofall unlabeledandlabeled traininginstancesand supervisedlearning basedhashing forexample rastegarietal svms tohashlearning liuetal presentedakernel ksh algorithmthat dissimilarand similarinstancepairs othersuperviseddata dependenthashing lda hash cisiontrees fasthash inouropinion ranking basedal gorithms hashingalgorithms multimodalhashing itincludescross modalhashingand multi sourcehashing incross modalhashing thequeryde for instance givenatextquery thetext inmulti sourcehashing itisassumedthatall thus bothcross modalhashingandmulti modalinformation however theyareutilizedindi erentsitu ationsandcross multi deeplearning basedhashing actionrecognition andimageclassi cation asfarasweknow semantichashing isthe rbm however training whichisin foundin ingeneral hashcodesconsistof and or and thediscrete optimizationproblems whicharegenerallynp hard tofacilitate then and nallyturnrealval orquanti zation estheoriginalcompli however theapproxi oftenoflowquality and reducesthee ectivenessofthe nalbinarycode itispossiblydue longbinarycodes shenetal sdh method tofully luoetal guietal sdhr lsr isawidely sismethod suchasclassi cation clustering di andmulti viewlearning however theor tothisend estimator andcanbede nedsuchaswelschm estimatorandcauchym estimator furthermore whichis xed toincreasethe exibility therotationmatrix exibleandmaybe moreaccurate viseddiscretehashing sdh section brie yin section presentsther sdh section finally section concludesthepaper hashing sdh webrie inthissection given seetable examples fxigni fbigni space wherethe throwvector biisthe bitshashcodesfor xi fyigni rnc where cde theterm yik isthe kthelementof yiand yik if xiisfromclass kand otherwise table notation symbol description rndthedatamatrix xi dthe thexample rnlthehashcodes bi lthehashcodefor xi the throwvectorof yi cthe yp rcctherotationmatrix es lthehashcodelength cthenumberofclasses f¹º fajgmi randomlyselected ¹º the sobjectivefunction g¹º therobustm estimator estimator rthehammingradius sdh nedas min wnõi kyibiwk kwk vnõi kbif¹xiºk ib gl whichcanbewritten equivalently as min wkybw kwk vkbf¹xºk gnl where kk wistheprojection matrix the rsttermof which thesecondtermof f¹º inthelast termof matethehashcode f¹xº ¹xºp where ¹xºisan ¹xº exp kxa exp kxamk theterms fajgmj arerandomlyselected manchorpointsfrom thetraininginstances and ter thematrix rmlprojects ¹xºontothelow dimensional space arewidelyadoptedin bre andkernel ksh asfollows isunknown nelizeddatainhigh dimensionalspace furthermore theusage kernels ssublineartime functions sdh thef steptosolve the steptosolve andtheb steptosolve step if bis xed ¹xºt ¹xº ¹xºtb step formsolution by xing btb bty step by proposedsolving bit sdh discretehashing inthissection correntropy outliers inthisstudy in speci cally min wõni g¹ybw ºi kwk vkbf¹xºk gnlwherewerecall ¹ybw ºiisthe throwof ¹ybw theterm g¹º in istherobustm estimatorandcanbede nedsuchas welschm estimatorandcauchym estimator inthisstudy wefo cusonthewelschm estimator g¹xº exp kxk where isthekernelwidth determining sdh hashing ofdi forexample forbinaryclas si cation thecodewordsforthe fig and respectively ifwerandomly rotate and thedistancebe tween and remainsthesame therefore weaddrotationto toobtainmore exibilities sdh sobjectivefunctionis min rõni g¹yr bw ºi kwk vkbf¹xºk gnl rr where risarotationmatrix figure randomrotationof and optimization with veunknownvariables step basedonhalf theproblem di exp ¹yr bw ºi ¹yr bw ºi¹yr bw ºit where cisthenumberof classesand ¹yr bw ºiisthe throwof ¹yr bw inthenextstep min rtr ¹yr bw ºtd¹yr bw kwk vkbf¹xºk fs gnl rr where tr ¹º isthetracenormand thdiagonalelementis di theproblem canbesolvedwith regardto and respectively by step for xed and derivativeof tozero therefore whasaclosed formsolution asfollowing btdb btdyr step for xed and werewrite as min ¹yr bw fs rr theorem ytdbw is uvt ris uv proof l¹r aº ¹yr bw tr arr ti whichequals l¹r aº tr rtytwtbtd¹yr bw tr arr ti tr ytdyrr tr wtbtdyr tr rtar const where matrix since rr rtr isequalto l¹r aº tr wtbtdyr tr rtar const l¹r aºwithrespectto rtozero we have l¹r aºr ytdbw ar therefore weget ytdbw since rr weobtain ytdbww tbtdya thus wehave ytdbw ytdbw bysubstituting in weobtain ytdbw ytdbw ytdbw uvtvut uvt uv step inordertosolve canberewrittenas min btr ¹yr bw ºtd¹yr bw vkbf¹xºk min btr wtbtdbw tr ¹btmº gnl where dyrw vf ¹xº likeinsdh wealsousethedis duetothe let btbethe mthcolumnof land thematrix of bexcluding bt wbethe mthrowof wand thematrixof wexcluding similarly let qthe mcolumnof mand thematrix mexcluding thenwehave tr wtbtdbw tr ¹w ºt¹b ºt wtbdb btw const tr wtbdb tw tr bdb wt since bdb íidib íidiwhere biisthe thelementof bdb tisaconstant similarly since btm ¹b ºtm ¹b ºtqbm bq wehave tr ¹btmº const tr ¹bq therefore isequalto min tr ¹bdb wtqºs thesolutionto is sign qdb wtt step thepseudocode ofr algorithm sdh inputs trainingdatamatrix fxigni labelmatrix fyigni hashcodelength parameter maximumiteration number output hashcodes fbigni gnlrandomlyselect minstances fajgmi obtainthe initialize basa gnlmatrixrandomly initialize dand rasidentitymatrixes use toinitialize use toinitialize repeat step use tosolve step bbased on step use tosolve step use tosolve step use tosolve until convergence experiments inthissection sdhalgorithm ghz gbram andcon andmat lab scaleimagedatasets mnist cifar andnus wide sdh sdhr ksh ssh fasthash agh andimh witht sne itq itqandsu pervisedversioncca itq cca http yann lecun com exdb mnist https www cs toronto edu kriz cifar html http lms comp nus edu sg research nus wide htm itq weutilizethepub morespeci cally forsdhandr sdh and vareempiricallyset to and respectively tisset to theparameter inr sdhisestimatedby foldcrossval idation foragh imh sdh andr sdh weuse randomly sampledanchorpoints ing map accuracy trainingtime recall measureofham mingradiustwo rtobe asin thef measure sde nitionis precision recall precision recall precisionat nsamples precision sample whichisde neighborsamongthetop nretrievedsamples weset nto as in groundtruthsare theexamplelabels mnisthas to pixels instancesandatrain intermsofprecision recall measure map and accuracy whilepca however sdh ofpca itq forexample sdh sf measure andmapare itq onthissituation moreover theprecision sample precision ofhammingradiustwo measure ofhammingradiustwo map figs respectively spacelimitations inmostcases moreover sample figure precision sample di table mberofhashingbitsis method precision recall measure map accuracy trainingtime sec sdh sdh sdhr bre ksh ssh cca itq fasthash pca itq agh imh figure datasetwithdi figure setwithdi resultsonthecifar dataset mtinyimagecollection there are imagesoncifar from classeswith instances figure datasetwithdi figure erentnumber ofhashingbits foreveryclass trainingsetwith inginstances arelistedin figure erent numberofhashingbits table precision recall measure map accuracy andtrainingtime are reported forssh matrixconstruction ofprecision recall measure map andaccu racy forinstance therecall ofr sdhis timeshigher thanthatofsdh furthermore it sdhandsdhare aboutoneminute whichisquitee cient incontrast ittakes fasthashandkshabout minutesandmorethan minutesto train respectively speci cally thetrainingofr sdhisabout timesand timesmoree respec tively onthissituation sdhr ssh cca itq pca itq agh andimharealsoverye cient however sdhgenerallyout performsthem theprecisionat instances precision sample precision bitsareplottedinfigs respectively gure in regardtoprecision and bitsis instances precision sample andaccuracy demonstrating thee ectivenessofr sdh resultsonthenus widedataset thenus widedatasethasabout imagescollectedfrom flickr nus widecontains with arede image weutilizetheprovided dimensionalbag of wordsfea tures asin the foreachclass forthislargedata base cientr sdh sdh andcca itq forbre mlhandksh imagesaresam pledfortraining since sdhrcannotsolvemulti figure precision sample onthecifar dataset to figure to figure accuracyonthecifar datasetwiththenum berofhashingbitsfrom to table datasetwhenth method precision recall measure map accuracy trainingtime sec sdh sdh sdhr bre ksh ssh cca itq fasthash pca itq agh imh table widedatabasewhent method precision recall measure map trainingtime sec sdh sdh bre ksh ssh cca itq fasthash pca itq agh imh single labellearning sdh demonstratingtheef labeldata conclusion inthispaper weproposeadata dependent learning based hash sdh sdh sdh thus furthermore sdhused whicho eradditional exibility as expected ther sdh mostcases real worldimageclassi ectivenessande ciencyofthe proposedmethod butthecosts arestillhighforlarge scalehigh dimensionaldata howtospeed there aresomepossibleways forinstance acknowledgement nsf bigdata andnsf iii references neuralcomputation andreiz broder in positano italy andreiz broder mosescharikar alanm frieze min in stoc dallas tx andreiz broder stevenc glassman marks manasse andgeo reyzweig in www santaclara ca discrimination mosess charikar rithms in stoc montreal canada veniceerinliong jiwenlu gangwang pierremoulin andjiezhou in conferenceoncomputer michelx goemansanddavidp williamson nitepro gramming journalofacm apro in yunchaogong svetlanalazebnik albertgordo andflorentperronnin it erativequantization scaleimageretrieval gence jiegui tongliangliu zhenansun dachengtao andtieniutan super andlearningsystems ranhe tieniutan liangwang andwei shizheng regularized in patternrecognition sergeyio weightedminhashandl sketching in icdm sydney au qing yuanjiangandwu junli scalablegraphhash ingwithfeaturetrans formation in cialintelligence tiveembeddings in cross viewsimilaritysearch in cialintel ligence pingli in kdd sydney australia pingli tures in kdd bitminwisehashing in www raleigh nc pingli michaelmitzenmacher codingfor randomprojections in icml pingli artbowen andcun huizhang in nips laketahoe nv pingli gennadysamorodnitsky andjohnhopcroft signcauchyprojec tionsandchi squarekernel in nips laketahoe nv guoshenglin chunhuashen qinfengshi antonvandenhengel anddavid suter dimensional data in guoshenglin chunhuashen andantonvandenhengel supervised haomiaoliu ruipingwang shiguangshan andxilinchen deepsuper in patternrecognition weifengliu puskalppokharel andjosécpríncipe correntropy prop ieeetransactionson signalprocessing weiliu junwang rongrongji yu gangjiang andshih fuchang su in recognition weiliu junwang sanjivkumar andshih fuchang hashingwith graphs in yadanluo yangyang fuminshen zihuang panzhou andhengtaoshen patternrecognition reyhinton sne nov markmanasse frankmcsherry andkunaltalwar consistentweighted sampling technicalreportmsr tr microsoftresearch feipingnie henghuang xiaocai andchrisding cientandrobust normsminimization in neuralinformationpro cessingsystems mohammadnorouzi davidjfleet hamming in vision mohammadrastegari jonghyunchoi shobeirfakhraei haldauméiii and larrysdavis predictabledual viewhashing in onmachinelearning reyhinton semantichashing interna fuminshen chunhuashen weiliu andhengtaoshen supervised discretehashing in fuminshen chunhuashen qinfengshi antonvandenhengel andzhenmin tang in jingkuansong yiyang zihuang hengtaoshen andrichanghong timelargescalenear in in christophstrecha alexbronstein michaelbronstein andpascalfua ldahash ieeetransactionson antoniotorralba robfergus andwilliamtfreeman milliontiny images ieee junwang sanjivkumar andshih fuchang semi supervisedhashingfor large scalesearch yairweiss antoniotorralba androbfergus spectralhashing in neural danzhang feiwang andluosi mationsources in 
deepvoice multi speakerneuraltext to speech sercanö sercanarik baidu com gregorydiamos gregdiamos baidu com andrewgibiansky gibianskyandrew baidu com johnmiller millerjohn baidu com kainanpeng pengkainan baidu com weiping pingwei baidu com jonathanraiman jonathanraiman baidu com yanqizhou zhouyanqi baidu com bordeauxdr sunnyvale ca abstract to speech tts withlow singlemodel asastartingpoint of the speakerneuraltts deepvoice andtacotron weintroducedeepvoice voice weimprovetacotron byintroducingapost anddemonstratea speaker speakertts datasets introduction speechsynthesis commonlyknownastext to speech tts technologyinterfaces accessibility media andentertainment singlespeakervoice modelparameters asaresult inthiswork neuralmulti speakersystems concretely wepresentdeepvoice ariketal weintroduceawavenet based oordetal spectrogram to audioneuralvocoder and useitwithtacotron wangetal listedalphabetically nips longbeach ca usa arxiv cs cl sep usingthesetwosingle wedemonstratemulti speakerneural andtacotron section section presentsdeepvoice andhighlights section section mos evaluationandsection speakerdeepvoice and section relatedwork inorder startingfrom single withregardstosingle ponents zenetal ronanki etal acousticmodeling zenandsak by oordetal mehrietal ourcontributions includingdeepvoice ariketal tacotron wangetal andchar wav soteloetal buildingsingle speakerttssystems speakerttssystem forinstance intraditionalhmm basedttssynthesis yamagishietal speakers data dnn basedsystems yangetal withi similarly fanetal linespectralpairs forfurther context wuetal basedmulti speakermodeling morerecently gans hsuetal speakertts inspeechrecognition abdel hamidandjiang vectors inaddition speaker whichleadstohigher datarequirements weproposeusing salimans etal basedmethods reynoldsetal lietal single speakerdeepvoice inthissection wepresentdeepvoice ariketal ariketal asdepictedinfig the animprovedsingle qualitymulti speaker model anddeepvoice deepvoice figure text secondpredict phonemedurations feed andfrequency voicednessandtime indeepvoice allmodelsaretrained anddeepvoice insection segmentationmodel similartodeepvoice ctc loss gravesetal pairs deepvoice layers deepvoice relu where istheoutputofthe thlayer istheconvolution isthebiasvector and incontrast deepvoice relu bn where bn isbatchnormalization ioffeandszegedy inaddition whichcan processing we durationmodel indeepvoice valuedduration scaledbuckets and wemodelthesequence byaconditionalrandom crf lampleetal duringinference backward algorithm frequencymodel per deepvoice frequency max where isthe audiosignal max isthemaximumvalueof and is one exceedsaed threshold audiosamplingrate milliseconds forexample ifaphonemelasts milliseconds theinput frames ifitlastslessthan milliseconds itis extendtoasingleframe gru layers choetal anafprojection hiddenstates predictions theprediction gru ismade withasingle thesecondprediction conv finally gru conv predictionvia where and are respectively forthespeakerthe modelistrainedon wethatpredicting vocalmodel thedeepvoice oordetal withatwo layer bidirectionalqrnn bradburyetal conditioningnetwork similartodeepvoice however weremovethe in addition insteadofgeneratinga multi low unlikepreviouswork ourapproachdoes notrelyonper speakerweight matrices or layers speaker low weuse rnn initialstates nonlinearitybiases each empirically thevocalmodel speakerembeddings transformthe andanonlinearity embeddings inputaugmentation featuregating while however haveontheactivations figure speaker segmentation duration and frequencymodel next multi speakerdeepvoice thedeepvoice yet theycanbeviewed segmentationmodel inmulti convolutionlayers insteadofeq wemultiplythebatch speakerembedding relu bn where isaspeakerembedding inaddition embedding similarly ratherthanhaving durationmodel themulti tation andanothersi frequencymodel themulti layers withasinglespeaker embedding as describedinsection speakerfrequency modelpredicta normalized frequency byaedlinear transformation forthespeaker malespeakers forinstance tendto haveamuchlowermean embeddings insteadofeq wecomputethe predictionas softsign softsign where isaspeakerembedding and tothe and and figure griforourspeaker vocalmodel themulti suggestedin oordetal networkaswell sounding yet weindeedobservethat multi speakertacotron wealsoextendtacotron wang etal asequence to sequencecharacter to waveformmodel whentrainingmulti speaker tacotronvariants wealso themodelsaremuchless thus wetrimallinitial preprocessing thus likedeepvoice character to spectrogrammodel thetacotroncharacter to bank highway gru cbhg encoder anattentionaldecoder andacbhgpost processingnetwork duetothecomplexity ofthearchitecture outputquality withoutaspeaker dependentcbhgencoder seeappendix forspeaker dependentattention visualizations weuseoneembedding secondembedding weuseone net gruhiddenstate basedattention mechanism model samp freq mos deepvoice khz deepvoice khz tacotron grif khz tacotron wavenet khz table meanopinionscore mos evaluationswith intervalsofdeepvoice deepvoice andtacotron the spectrogram to waveformmodel wangetal usesthegrifn spectrogramstotime we tacotron vocal model buttakeslinear scaledlog as input thecombinedtacotron wavenet results inthissection speakerandmulti single wetraindeepvoice deepvoice approximately hoursofsingle speakerdata anddeepvoice canbefoundintable withinappendix crowdmosframework ribeiroetal table theresults yieldgainsin qualityoverdeepvoice multi hoursofspeech whichcontains utteranceseach ofaudiobooks whichcontains speakerswith minutesofaudioeach foratotalof hours wealso seefig asanexampleand appendix formoredetails framework setbeingevaluated quality however the overall dataset multi speakermodel samp freq mos acc vctk deepvoice layerwavenet khz vctk deepvoice layerwavenet khz vctk deepvoice layerwavenet khz vctk deepvoice layerwavenet khz vctk tacotron grif khz vctk tacotron layerwavenet khz vctk groundtruthdata khz audiobooks deepvoice layerwavenet khz audiobooks tacotron grif khz audiobooks tacotron layerwavenet khz audiobooks groundtruthdata khz table speakermodels toobtainmos weuse discriminativemodels seeappendix fordetails onthesamples figure layervocalmodel and character to seeappendix fordetails amulti highmos toshow butalsogenerate distinguishable voices we accuracy samples seeappendix formoredetails tacotronwithwavenet thewavenet conclusion inthiswork weexplorehowentirely speakertext to speechvialow westartbypresenting deepvoice animprovedsingle speakermodel next speakerdeepvoice andmulti and inconclusion highqualitytext to futureworkmaytest howlittle andwhetherthe references abdel hamidandh jiang in icassp arik chrzanowski coates diamos gibiansky kang li miller raiman sengupta andm shoeybi deepvoice real timeneuraltext to speech in icml bradbury merity xiong andr socher quasi in iclr cho vanmerriënboer gulcehre bahdanau bougares schwenk andy bengio learning arxiv fan qian soong andl he multi basedtts synthesis in ieeeicassp graves fernández gomez andj schmidhuber in icml hsu hwang wu tsao andh wang arxiv ioffeandc szegedy batchnormalization shift arxivpreprintarxiv kingmaandj ba adam arxiv lample ballesteros kawakami subramanian andc dyer recognition in proc naacl hlt li ma jiang li zhang liu cao kannan andz zhu deepspeaker anend to end arxivpreprintarxiv mehri kumar gulrajani kumar jain sotelo courville andy bengio samplernn an unconditionalend to arxiv oord dieleman zen simonyan vinyals graves kalchbrenner senior and kavukcuoglu wavenet arxiv reynolds quatieri andr dunn ribeiro florêncio zhang andm seltzer crowdmos scorestudies in ieeeicassp ronanki watts king andg henter median non parametricapproach arxiv salimans goodfellow zaremba cheung radford andx chen gans in nips sotelo mehri kumar santos kastner courville andy bengio char wav end to end speechsynthesis in iclr workshopsubmission wang skerry ryan stanton wu weiss jaitly yang xiao chen bengio etal tacotron towardsend to endspeechsynthesis in interspeech wu swietojanski veaux renals ands king basedspeech synthesis in interspeech yamagishi nose zen ling toda tokuda king ands renals robustspeaker adaptive hmm basedtext to speechsynthesis speech yang wu andl xie onthetrainingofdnn in signal apsipa asia zenandh sak forlow in ieeeicassp zen agiomyrgiannakis egberts henderson andp szczepaniak fast compact andhighquality lstm arxiv appendices atrainingdeepvoice figure forconvenience infig similartothein ariketal to phonememodel asin ariketal similarto ariketal counterpartsintable models model evaluationmetric deepvoice deepvoice segmentation phonemepairerrorrate duration meanabsoluteerror ms ms frequency meanabsoluteerror hz hz table duration anddeepvoice ariketal to weadded totheoverallloss function was decayedbyafactorof every iterations kingma andba with and convolutional layers and outputchannels ineachlayer thesizeis whereheight isin isintimeframes ariketal similarly any wangetal single speaker vctk audiobooks segmentation numberofmfccs convolutionallayers recurrentlayers bi gru wide wide wide learningrate silencethreshold gaussianwidth ms ms ms batchsize speakerembeddingsize duration fully connected units units units recurrentlayers bi gru wide wide wide outputbuckets learningrate batchsize speakerembeddingsize frequency hiddenlayers bi gru wide wide wide outputdimension convolutionwidths learningrate batchsize speakerembeddingsize vocal layers learningrate batchsize speakerembeddingsize character to spectrogram enc cbhgbanksize enc cbhgchannels enc cbhgrecurrentsize enc cbhghighwaylayers enc cbhgmaxpoolwidth enc cbhgproj sizes enc cbhgproj width decoderlayers attentionsize attentionstatesize decoderprenetsizes post cbhgbanksize post cbhgchannels post cbhgconv widths post cbhgrecurrentsize post cbhghighwaylayers post cbhgmaxpoolwidth reductionfactor ctclosscoef learningrate œn batchsize speakerembeddingsize table speakermodels speakermodelsintable wereferthereadersto ariketal model hardware timeperiteration numberofiterations totaltime segmentation titanxgpu ms hours duration teslak gpu ms hours frequency teslak gpu ms hours vocal layer titanxgpus ms hours vocal layer titanxgpus ms hours vocal layer titanxgpus ms hours vocal layer titanxgpus ms hours character to spectrogram titanxgpu ms hours table speakerdeepvoice inthissection outputs modelactivations speaker figure time andvoiced ness or speaker isa year isa year old ariketal as showninfig forexample female speaker figure speaker isa year old speaker isa year and speaker isa year sixspoonsof fig it figure layervocalmodel shownwith sionalspace fig andfig to wecan besides weobserveapparent inthetwo dimensionalspace especiallygreat https en wikipedia org wiki regional_accents_of_ english figure to spectrogrammodel figure tocomputemulti speakeraccuracy deepspeaker lietal thestate of the weusemel mfccs then weemploytwo poolinglayer we thenmean entropy loss nonlinearity inthissection onlythe resultsforthemodels and arepresentedintable accuracy param intable audioresamplingfreq khz khz khz khz numberofmfccs hoplength convolutionlayers maxpoolwidth stride fullyconnectedsize dropoutprobability learningrate table param intable audioresamplingfreq khz khz khz khz numberofmfccs hoplength convolutionlayers maxpoolwidth stride fullyconnectedsize dropoutprobability learningrate table dataset multi speakermodel intable vctk deepvoice layerwavenet vctk deepvoice layerwavenet vctk deepvoice layerwavenet vctk deepvoice layerwavenet vctk tacotron grif vctk tacotron layerwavenet vctk groundtruthdata table andd the dataset multi speakermodel intable audiobooks deepvoice layerwavenet audiobooks tacotron grif audiobooks tacotron layerwavenet audiobooks groundtruthdata table andd the 
proceedingsofthe pages berlin germany august 
haonanyu jiangwang zhihenghuang yiyang weixu purdueuniversity facebook haonanu gmail comzhiheng fb com baiduresearch wangjiang yangyi wei xug baidu rentneuralnetworks rnns problem the itexploitsboth temporal andspatial theparagraph generator and scalebenchmark datasets youtubeclipsandtacos multilevel theexper formsthecurrentstate of the artmethodswithbleu scores and respectively introduction inthispaper ing thecontentofavideo oritcouldbeas thisability machineintelligence automaticvideosub titling blindnavigation etc figure showssomeexample basedsystemondescrib inavery limitedsetting kojima etal ofspeech followingtheirwork severalsucceedingap proaches appliedsimilarrule basedsys with adhoc rules they amongthem the mostcomplexrule basedsystem supportsavocabulary of lexicalentries including verbsand nouns theproblemscales eitherinafully or weakly supervisedfashion thestatistical thetrain youtubeclips andtacos multilevel which videos asaresult morechallenging scaledatasets sincethen centlineofwork which es rnn applyingrnn tion nmt munity text vggnet andc average pooling tionmodel adogisplayinginabowl thepersontookoutapot figure whereonlyone multilevel methods forashortvideoclip somegraphical modelmethods suchasrohrbach etal areableto fromperfect us forexample insteadofsaying toes toesintothepot thepersoniscooking ing weproposea hierarchical theidea graph thesentences instead thegenerationof forexample inavideo ofcookingdishes asentence thanthesentence theperson turnedonthestove afterthesentence thepersontookout towardsthisend ourhierar asentence bothofwhichusere atthelowlevel the weexploit bothtemporal andspatial atthehighlevel thepara and generator seesection figure illustratesourover allframework datasets youtubeclips andtacos multilevel state of the artmethods toourknowledge thisisthe relatedwork themethodsfornmt lowtheencoder decoderparadigm anencodermapsthe sourcesentencetoaed beddingspace on topofthisparadigm bahdanauetal li etal andlin etal archyofadocument theattemptofvisual to agecaptioning whichcanbetreatedas asaresult image tures butnotaction motionfeatures theamountofdata dozensof times instance to sequence tioner sequence to sequence someothermethods such asparkandkim images sentencetransitions paragraphgenerator theveryearlyvideocap tioningmethod frames agecaptioning however usuallyap to avoidthisissue usingeitherarecur rentencoder oranattentionmodel our etal in videofeatures the attentionweights section sentencegenerator rnn forlanguagemodel ing amultimodallayer fromdifferentsources and anattentionmodel for thepara inter sentencedependency com andoutputsanewini thernnsexploited gatedrecurrentunit gru term memory lstm architecture inthefollowing we orthe gatedrnn gatedrecurrentunit asimplernn threelayers theinputlayer thehiddenlayer andthe outputlayer hiddenstate output wherew andb learned and areelement long toaddressthisissue the gru past insidetheunit lineargates let denotetheelement wise thegrucomputesthehid denstate as resetgate updategate hiddenstate where areelement wisesigmoidfunctions thereset gate the updategate theparameterscanbe bptt sentencegenerator tratedinfigure timestepwhenaone hotinput of encoding where isthevocabularysize the hotvectortoadenserep withanembeddingtable ofwhicheachrowisa therecurrentlayer dimensionsandacts etyofimage itupdatesitshid andencodesthe havebeenfedin ofthis relu asonebranch ourattention alignmentmethodthat de sentence generator paragraph generator di de di figure bluedenotestheoutput and the for simplicity chinetranslation nmt captioningbyyao etal etal isthattheirmodel weadditionallyin onsomedatasets tacos multilevel inthis case whole frame seesection fordetails let fv km wherem isthevideolengthand isthenumberofpatches oneachframe km suchthat km todoso wecomputean attentionscore foreachframe conditioningonthe previoushiddenstate wherew and and issetto theelement stanh func tion tanh where andthehidden stateh canrangefrom to scalar oneforeachfeature afterthis wesetupasequen exp km exp finally eraging km theaboveprocessisaso itallows attentionprocess inourexperiments we theotherforaction motion section theweightedsumsofthe dimensions supposewehavetwo videofeaturechannels andu respectively the togetherwiththe hiddenstate oftherecurrentlayeri intoa dimen where issettotheelement wisestanhfunction toreduce ovweadddropout withadroprateof to thislayer asoftmaxlayer seefigure bothwiththeelement thehid withthe wordembeddinglayer dataset dependent etal weset ithas rametersharing asthestepof thesentencegenerator layer notethatduringtest while inthetraining theannotatedsentence paragraphgenerator sentenceatatime however precedingsentences anembeddingaver agelayer seefigure embeddingvector qaembedding dividualwordsand orsymbols sentence decoder framework innmt afterthat theaveragedembedding and di mensions wetreat tencerepresentation gatedrnn seefigure areasynchronous therecurrentlayerii processed beenfedin finally sentenceembedding forthenextsentence rameters word given intheparagraph allthepreviouswords inthesamesentence and thelikelihood wholeparagraph paragraph as ppl log wheret the abovecostisinfactthe perplexity oftheparagraphgiven thevideo finally setisas ppl ppl wherey set toreduceovl andl regularizationterms weusebackpropa gationthroughtime bptt sgd totheoptimum forbetterconvergence wedividethe wesetasmall learningrate weperformthegen widthof begin of sentence symbol bos whichistreated asa assume thatatanytimestep thereareatmost lt wordsequences quence foreachofthe wordsequences givenitslast wordasinput thenextword log andthesequencecost thenfromallthe wordse quences wepickthetop ofthenew wordsequences anyonethatisacom pletesentence thelastword istheeos end of sentence symbol eos tree thereareless thanj sentencesinthepoolor itssequence sentencesinthepool inthe secondcase movedfromthepool alsoofthenew wordsequences anyonethathasa sentencesinthepool thereasonisthatex intheend processing andevaluation afterthis candidatesen tences thesentence candidatesentences end of paragraph eos finally oflists eachlistwith sentences inourexperiments weset features widthof is seconds intel core tm ghz experiments youtubeclips andtacos multilevel youtubeclips shortvideo clips secondsonaverage thevideoclipsareopen domain ple animals actions scenarios landscapes etc eachvideo entturkers thereare sentencesintotal withabout eachsentenceonaver agecontainsabout words uniquelex icalentries guadarramaetal where and videosare itshouldbenoted clip theyare parallel tent terval asa result proach tacos multilevel longvideos minutesonaverage inanindoorenviron ment thevideosareclosed domain containingdifferent actors activities tipleturkers gleshortsentence thereare distinctintervalsand sentencesintotal withabout intervalsand sentencespervideo anddifferent theper sonﬂ words ularyof uniquelexicalentries weadoptthetrainand etal where and note poraldependencies turkeronthatvideo followingdonahue etal and rohrbachetal andgeneration forourapproach weusethepre trainedvggnet ontheimagenetdataset forboth datasets prominent entireframe sentencegenerator insection fortacos multilevel anddiftobelocalized tosolvethisproblem both donahueetal androhrbach etal designedaspe cializedhanddetector interactingobjects case proposals weuseopticalflow toroughlydetect wethenextract imagepatchesofsize alongthelowerpartof theboxborder finally we mclstm yt vt mm vdn ta lstm rnn vgg rnn rnn ours table whereb andcareshort forbleu meteor andciderrespectively patchfeatures when tor inpractice wethatasmallvalueof weusethepre trainedc onthesports mdataset for youtubeclips thec aed frames thuswhen dfeaturepool we setk anddivide by section forthe tacos multileveldataset notworkwell tories fishervector fortheattentionmodel weset andm bleu meteor andcider becausethe captioningmethods ricshavebeenreported thetacos multileveldataset inthepreviouswork generally thehigherthemetric scoresare humanjudgment chenetal results wecompareourapproach rnn onyoutube clipswithsixstate of the artmethods lstm yt vt mm vdn ta andlstm atedforeachvideo toevaluatetheimpor baselinemethods rnn vggandh rnn thefor mccrf crf lrcn rnn vgg rnn dt rnn sent rnn cat rnn ours table resultsontacos multilevel whereb andcare shortforbleu meteor andciderrespectively frameworkunchanged intable inallthethreemet rics of the art method lstm are inthebleu score and intheme teorscore sincelstm dfeatures moreover althoughta tion thevideofeatures instead indicat captioningtask multilevelwith threestate of the artmethods crf crf andlrcn likeabove rnn vggandh rnn wealsoaddanothertwo baselinemethodsrnn sentandrnn catthathavenohi erarchy butnotthe paragraphgenerator rnn the sentence asaresult independently rnn then thisconcatenation weusernn send andrnn structure theresultsontacos of the artmethods in lrcn with animprovementof inthebleu rnn sent rnn rnn sent rnn figure redindicatesincor sentand greenshowstheones generatedbyourh in theexample walktothesink aftertheevent opentherefrigerator inthesecondexample rnn sentgeneratesthe event takethecuttingboard incontrast ourhi score tector ourmethodisalso althoughrnn cat levelconcatenation architecture again fig ure approachontacos multilevel rnngen catingeneral weper tacos multilevel wediscard testvideointervals tencegeneratedbyrnn catandh rnn thisresultsina totalnumberof videointervalsfor humanevaluation amt byrnn catandtheotherbyh rnn sidebyside foreach videointerval theturkeralsohasa goodorbad intheend weobtained selectionsforh rnnand selectionsforrnn cat withagapof selections thush rnnhasatleast improvementoverrnn cat rnnrnn catequallygoodorbad total itissubjectto severallimitations first mostofourfail urecasesontacos namesinthesentences cucumbervs carrot mangovs orange kiwivs avocado etc seefigure fora concreteexample slicedtheorange shouldreallybe slicedthemango sometimes withocclusion openproblem second from butnotalsoin thereverseway lastly our image namely thereisdiscrep oneusedbygeneration while thisproblemis apotentialcurefor tothetrain ingprocess an bleu usedattesttime conclusion rnnframeworkfor givenvideodata thestate of the artresultsontwolarge scaledatasets acknowledgments references bahdanau cho andy bengio neuralmachinetrans in interna banerjeeanda lavie meteor anautomaticmet judgments in and orsummarization pages june barbu bridge burchill coroian dick inson fidler michaux mussman siddharth salvi schmidt shangguan siskind wag goner wang wei yin andz zhang videoinsen tencesout in inintelligence pages bengio vinyals jaitly andn shazeer scheduled works in tems pages bordes chopra andj weston questionanswering in pages chenandw dolan in proceedingsofthe than tics acl portland or june chen fang lin vedantam gupta doll ar andc zitnick datacollec corr abs chenandc zitnick in proceedingsofthe tion cho vanmerrienboer ulc ehre bougares schwenk andy bengio usingrnnencoder tion in guageprocessing das xu doell andj corso athousandframes injustafewwords in proceedingsofthe tion pages donahue hendricks guadarrama rohrbach venugopalan saenko andt darrell long termrecur scription in elman cognitivesci ence farneb ack two nomialexpansion in proceedingsofthe thscandinavian pages guadarrama krishnamoorthy malkarnenkar venugopalan mooney andk saenko youtube text shotrecognition iniccv int conf oncomputervision december hanckmann schutte andg burghouts automated humanactions in pages hochreiterandj schmidhuber longshort termmemory neuralcomput nov jegou perronnin douze nchez perez andc schmid torsintocompactcodes ieeetrans patternanal mach intell sept kalchbrennerandp blunsom recurrentcontinuous translationmodels in pages karpathy toderici shetty leung sukthankar andl fei fei large scalevideowithconvo in cvpr khan zhang andy gotoh humanfocused videodescription in pages khan zhang andy gotoh towardscoher in pro visionworkshops pages kiros salakhutdinov andr zemel unifying visual guagemodels in kojima tamura andk fukunaga naturallanguage putervision krishnamoorthy malkarnenkar mooney saenko ands guadarrama generatingnatural language minedknowledge in pages lecun bottou orr andk uller efback prop in neuralnetworks tricksofthetrade page lee hakeem haering ands zhu save in pro pages li luong andd jurafsky in proceedings linguistics pages lin liu yang li zhou ands li hi pages sept mao xu yang wang huang anda yuille rnn iclr mao xu yang wang huang anda yuille learninglikeachild nairandg hinton linearunitsimprove in icml pages pan mei yao li andy rui jointlymodel corr abs papineni roukos ward andw jingzhu bleu in acl pages parkandg kim in pages ranzato chopra auli andw zaremba se corr abs rohrbach rohrbach qiu friedrich pinkal andb schiele coherentmulti in ternrecognition gcpr september rohrbach qiu titov thater pinkal and schiele descriptions in pages russakovsky deng su krause satheesh ma huang karpathy khosla bernstein berg andl fei fei nitionchallenge ijcv pages apr schusterandk paliwal ralnetworks nov simonyananda zisserman networksforlarge in srivastava hinton krizhevsky sutskever and salakhutdinov dropout ralnetworksfromov research sunandr nevatia usingrandomforestin proceedingsoftheeuro pages sutskever vinyals andq le sequencetosequence in tielemanandg hinton lecture rmsprop divide coursera tran bourdev fergus torresani and paluri in pro vision vedantam zitnick andd parikh cider consensus in proceed recognition pages venugopalan rohrbach donahue mooney darrell andk saenko sequencetosequence videoto text in oncomputervision pages venugopalan xu donahue rohrbach mooney andk saenko in proceedings pages vinyalsandq le in vinyals toshev bengio andd erhan showand tell in proceedingsofthe tion pages wang kl aser schmid andc liu action in ieeeconferenceon computervision patternrecognition pages june wen gasic mrksic su vandyke ands young basednaturallan in confer werbos whatdoesitdo andhowtodoit in proceedingsofieee volume pages xu venugopalan ramanishka rohrbach and saenko amulti network corr abs xu xiong chen andj corso jointlymodel languageinaframework in proceedingsofaaai yao torabi cho ballas pal larochelle anda courville structure in enceoncomputervision pages yuandj siskind tion in pages jan 
proceedingsofthe emnlp pages october doha qatar 
wenlinwang zhegan wenqiwang dinghanshen jiajihuang weiping sanjeevsatheesh lawrencecarin dukeuniversity purdueuniversity abstract languagemodel tcnlm anovelmethod global local word thetc of experts moe languagemodel whereeach expert isare currentneuralnetwork rnn thataccounts quence ciently plied dependent weightmatrices thedegreetowhicheach document respondingtopics based modelandothertopic guidedlanguagemod els further introduction nlp itplaysakeyrole rangingfromspeech recognition mikolovetal arisoyetal srirametal machinetranslation schwenk proceedingsofthe st aistats lan zarote spain pmlr volume copyright bythe author etal vaswanietal toimagecaption ing maoetal devlinetal training worderrorratesfor papinenietal hence learningapow nlp typically whichhasto rnn basedneural of the artperfor mance jozefowiczetal shazeeretal however tencelevel text long diengetal fortunately asemanticnature model by allocation lda method bleietal andits variants els wanetal caoetal miaoetal innlp shownpromise for example trainedtopicmodels mikolovandzweig with interest languagemodel mikolov andzweig lauetal orre scoringthe tor diengetal arxiv cs lg feb figure thenumberoftopics ofexistingapproaches tcnlm anewap asdepictedinfigure autoencoder kingmaandwelling framework quantheproba latentcode of expertsmodel hu etal inglanguagemodel expert acombinationofthese experts weightedbythetopic usageprobabilities ama to endbymaximizingthe setofexperiments both preliminaries wereviewrnn languagemodel apre denedvocabulary wedenote asthevocab ularysetand tobeasequenceofwords witheach tribution rnn givenalltheprevious words thefunction sicrnncell alongshort termmemory lstm cell oragated recurrentunit gru cell choetal the topicmodel latent dirichletallocation lda bleietal forex ample documentmodeling foreachtoken specif ically let and as dir discrete discrete where topic isthehyper prior and isthenumberofwordsin document canbeexpressedas languagemodel asillustratedin figure wang gan wang shen huang ping satheesh carin aneuraltopicmodel ntm and ii aneural languagemodel nlm long neuraltopicmodel let denotethebag of wordsrepresentation ofadocument with gers isthevocabularysize andeachelementof distinctfrom lda bleietal wepassa gaussian random miaoetal sp is discrete discrete where withmean andvariance ineachdimension tothetopicembedding hereas softmax where and aretrainablepa rameters is thesecondequationin by whichare isthenumber oftopicsand words allelementsof arenonnegative andthey sumtoone there kingmaandwelling bution asdiscussed insection diversityregularizer redundanceininferred models itisstraightfor followingxie etal miaoetal weapplyatopicdi sp arccos jj jj jj jj topicsis andthevariance is finally thetopic neurallanguagemodel weproposeamixture of experts moe language model whichconsistsasetof expertnetworks whichisthen generalizedtothelstm sp wetwo weighttensors and where isthe eachexpert corre and which denotesthe th slice of and respectively all put sep softmax where istheusageoftopic component of and isasigmoidfunction istheweightmatrix connectingthernn shiddenstate usedforcomput biastermsareomitted forsimplicity however thetrainingprocessis toremedy this experts asin beanensembleoftopic sp the softmax and motivatedbyganetal songetal wede compose and where isthenumberoffactors sp diag where and tocapture arethefactors thetopicdistribution rnnparametersas ceedingwords of languagemodels the share structure units sp positionallstmcellas ia ia fa fa oa oa ca ca tanh for we ourlstmunit from and further empiricalcomparison toverifythatour implementationasin modelinference theproposedmodel seefigure followsthevaria tionalautoencoder kingmaandwelling frame work whichtakesthebag of wordsasinputandem thisvectoris of wordsinput and isintractable we jordanetal we denote hence elbo as log kl jj neuraltopicmodel log neurallanguagemodel log rial inexperiments relatedwork topicmodel be yondlda bleietal textensions havebeenproposed relations bleiandy modelingtempo raldependencies bleiandy discover tehetal henaoetal zhou etal amongmanyothers recently neural build machines auto regressivemodels larochelleandlauly sig moidbeliefnetworks ganetal andvariational autoencoders miaoetal puetal wangetal chenetal therecentworkofmiao etal topicmodels their incontrast ourmodel wang gan wang shen huang ping satheesh carin dataset lmtm docs sents tokens docs sents tokens docs sents tokens apnews imdb bnc table umentsasbag of words sequencegeneration languagemodel mikolovetal thernn basedlanguagemodel rnnlm termtem ithasrecentlybeen tunedtraditionaln gram basedlanguagemodels jozefowiczetal broaddocumentcontext mikolovandzweig model importantworkin dieng etal lauetal ahnetal the intermsofthetopic model mikolovandzweig usesapre trained ldamodel diengetal usesavariationalau toencoder lauetal tictopics andahnetal utilizesthetopicas graph vinyalsandle mikolovandzweig andlauetal tionaltopicfeatures diengetal andahnetal astandardrnnlm allowingan tend to endtrainingprocess further the of experts moe modeldesign underourfactoriza tionmethod tly recently shazeeretal proposesamoemodel forlarge tfromours inwhicheachexpert standsforasmallfeed therefore it andcomputationalcost onasinglegpumachine moreover theyprovide andallexperts aretreatedequally however ganetal usesatwo steppipline learning amulti labelonagroupofimage tags onthem incomparison andfocusesonthe languagemodelingtask experiments datasets apnews imdb and bnc apnews newsarticlesfrom to imdb isasetof and bnc bncconsortium isthewrittenpor whichcontains excerptsfromjournals books letters essays mem oranda thesethree sp stanfordcorenlp manningetal welower caseallwordtokens andoutwordtokensthat occurlessthan times fortopicmodeling wead inthedocumentsand excludethetop documents all development andtestingsets isprovidedintable https www ap org en gb https github com jhlau topically driven language model https github com mimno mallet blob master stoplists en txt dataset lstm basic lstm lda lstm lclm topic rnntdlm tcnlm type apnews small large imdb small large bnc small large table apnews imdb and bnc takenfromlauetal setup forthentmpart weconsidera layer feed with relu nairandhinton thehyper parameter graph areusedto obtainthebag of to intermsofthenlmpart weconsider settings asmall layerlstmmodelwith hiddenunits and ii alarge layerlstmmodelwith hidden unitsineachlayer inaddition adaptive softmax graveetal isusedtospeedupthe trainingprocess duringtraining kingmaandba allthehyper weempirically datasets with coresand gbglobalmemory inordertodemonstrate wecomparetc basic lstm abaselinelstm basedlanguage model lda lstm atopic enrolledlstm based languagemodel wepretrainanlda model bleietal tolearn top icsfor apnews imdb and bnc givenadoc ument lclm wangandcho acontext based languagemodel thepreceding of words andanat nextword tdlm lauetal aconvolutionaltopic itstopicknowl topic rnn diengetal ajointlearn thetopicinfor topic rnn diengetal isimplementedbyour lau etal we allthetopic enrolled lstmmodel indicat tictopicinformation ii ourtcnlmperformsthe iii theim rectly iv themarginbetweenlda lstm topic topicmodelevaluation changetal new manetal mimnoetal followinglau etal malizedpmi npmi giventhetop wordsofa topic where arebasedonco inpractice wang gan wang shen huang ping satheesh carin apnews imdb bnc table apnews imdb and bnc topicmodel coherence apnewsimdbbnc lda ntm tdlm tdlm topic rnn topic rnn tcnlm tcnlm lda ntm tdlm tdlm topic rnn topic rnn tcnlm tcnlm lda ntm tdlm tdlm topic rnn topic rnn tcnlm tcnlm table on apnews imdb and bnc and indicate smallandlargemodel respectively takenfromlau etal topicwords atrainedmodel encescoresovertopics forcomparison weusethe lda lda bleietal isusedasabase linetopicmodel distributionsforlda lstm ntm posedincaoetal thedocument topic andtopic neuralnetworks tdlm lauetal thesamemodelas topic rnn diengetal thesame ourtcnlm sp weachieve apnews and imdb bnc ii one andthein iii additionally rnnindi guidance wepro videthetop wordsfor randomlychosentopics oneachdataset summarizedbyus asshownintable theseresults neurons we the demon moreinferred plementarymaterial sentencegeneration topics giventopic torbyusingonlythe thfactorof and then andgreedilysample table showsthe mostofthesen moreinterestingly ics an imal and lottory for apnews moreexamples itshows apnews army afemalesergeant animal medical market unk millionto unk lottory army terrorism thetaliban koreanwarinpakistan shistoriclifesince withtwoexampleof unk animal lottory imdb horror tevenazombie action buttheactionisn tverygood family unk and unk unk relationship unk andtheoldman unk children sforkids war unk ofthewar horror negative hildren unk bnc environment education unk politics business unk to unk inthequarter and unk onturnoverthatrose unk to unk facilities environment politics thecommission sreportonoct onjan deniedthegovernment sgrantto art crime aswellas table figure relatedsen tences togeneratesentences asin chine wetrainantmwith topicsandeachnlmof themoeisa layerlstmwith hiddenunits re boththenaivemoe basiclstm interestingly ourtcnlmout trivialmargin we structure theinher enttopic ovandyieldsttraining demonstrating guagemodel datasetbasic lstmnaivemoetcnlm apnews imdb bnc table apnews imdb and bnc conclusion guagemodel tcnlm the inadocument words of experts modeldesign further andhasthe ongiventopics wang gan wang shen huang ping satheesh carin references ahn choi arnamaa andy bengio arxivpreprint arxiv arisoy sainath kingsbury andb ram abhadran in naacl hltworkshop bleiandj dynamictopicmodels in icml bleiandj ofscience blei ng andm jordan latentdirich letallocation jmlr bncconsortium pus version bncxmledition dis universityof oxford url http www natcorp ox ac uk cao li liu li andh ji anovelneural in aaai chang gerrish wang boyd graber andd blei readingtealeaves howhumans interprettopicmodels in nips chen li chen wang pu and carin continuous models arxivpreprintarxiv cho vanmerrienboer gulcehre bah danau bougares schwenk andy ben gio encoder in emnlp devlin cheng fang gupta deng he zweig andm mitchell languagemodels forimagecaptioning arxivpreprintarxiv dieng wang gao andj pais ley topicrnn long arxivpreprint arxiv gan chen henao carlson andl carin eling in icml gan gan he pu tran gao carin andl deng arxivpreprint arxiv grave joulin cisse grangier and jegou arxivpreprintarxiv henao gan lu andl carin deeppoisson factormodeling in nips hintonandr salakhutdinov replicated softmax in nips hochreiterandj schmidhuber longshort term memory in neuralcomputation hu palreddy andw tompkins apatient pertsapproach engineering jordan ghahramani jaakkola and saul forgraphicalmodels machinelearning jozefowicz vinyals schuster shazeer andy wu eling arxivpreprintarxiv kingmaandj ba adam amethodforstochastic optimization arxivpreprintarxiv kingmaandm welling auto encodingvaria tionalbayes arxivpreprintarxiv larochelleands lauly topicmodel in nips lau newman andt baldwin machine readingtealeaves in eacl lau baldwin andt cohn topically arxivpreprint arxiv maas daly pham huang ng andc potts mentanalysis in acl manning surdeanu bauer finkel bethard andd mcclosky thestanfordcorenlp in acl mao xu yang wang huang and yuille rnn arxivpreprint arxiv miao yu andp blunsom neuralvariational in icml miao grefenstette andp blunsom discov inference arxivpreprintarxiv mikolovandg zweig slt mikolov at burget cernocky and khudanpur guagemodel in interspeech mimno wallach talley leenders anda mccallum intopicmodels in emnlp nairandg hinton linearunitsim in icml newman lau grieser andt bald win in naacl papineni roukos ward andw zhu bleu translation in acl pu gan henao yuan li stevens andl carin learningofimages labelsandcaptions in nips schwenk rousseau andm attik large pruned in naacl hlt workshop shazeer mirhoseini maziarz davis le hinton andj dean outrageously largeneuralnetworks thesparsely gatedmixture of expertslayer arxivpreprintarxiv song gan andl carin factoredtemporal in icml sriram jun satheesh anda coates cold fusion trainingseq guagemodels arxivpreprintarxiv teh jordan beal andd blei hierarchical dirichletprocesses in nips vaswani zhao fossum andd chiang de codingwithlarge provestranslation in emnlp vinyalsandq le arxivpreprintarxiv wan zhu andr fergus ahybridneural network latenttopicmodel in aistat wangandh cho larger contextlanguagemod acl wang pu verma fan zhang chen rai andl carin zero shotlearning viaclass arxiv preprintarxiv xie deng ande xing in kdd zhou cong andb chen thepoissongamma beliefnetwork in nips wang gan wang shen huang ping satheesh carin ference startfrom wehave log log log log log log log kl jj neuraltopicmodel log neurallanguagemodel distributions plotedinfigure areprovidedbelow apnews colombia operate gen townofpadilla orfarc operatesinthearea innorwayonoct thetalkstoendanearly decade onnov imdb time myfeel ingsremainunchanged thisisasoliddrama thatienjoyverymuch whatelementsthereare substanceofthe iswartimecict butinasetting ithasa solidcast alsoincludingmany britishtvregulars ifeelthis combatandair combat theseemlikeadapta evokingaretro feel thesets neverdetracting althoughthekil dimensional browconcepts forget thatit justwatchandenjoy bnc sliftwasa hamishlumsden themin istryofdefence london said there prduringanelection gazabarracksat which involved airmobilebrigade northyorkshirepolice thecountyemergency casualties fromlonglands college middlesbrough brief airwithan the st airmobileambulance whichservedinthegulf war tendsaburning piecesofaircraft barracks ripon civilianemer gencyservices theoverallco mrlumsden said figure examples infigure intable foreachdataset the areprovidedintable wang gan wang shen huang ping satheesh carin army afemalesergeant inworldwarii andatotalof milliononthe buthe forthesenate animal whichissoakedinayear medical asdoesinnationwide has days tosleepthere rightstronger market unk millionto unk up to centspershare unk billion lottory unk terrorism wednesdayhave unk law theeeoclawsuitsaysit unk foracheck unk art quentintarantino unk spanish cathyjohnson jane sshort theearlyshow addsclassics unk or unk next katiesaid he transportation millioninbanks unk ledtothescheme andtohavebeenmore than aformeru attorney thegovernment education thestate unk unk assemblymember unk playtheissue thestate unk studentsina unk child unk table apnews horror tevenazombie andheheadsinthe unk action buttheactionisn tverygood unk srubbish actionpackedneatly sand family unk and unk unk relationship unk andtheoldman unk unk desire andsubmittoher squestto unk theni maboutthelove herfriendaditya tatum unk marriesa yearoldgirl willbeableto children sforkids war unk ofthewar and unk thegoes farastothepolitical butthenewsthatwillbe unk detective hopefullythat sstarting unk sway moreprofessional unk whereitprevails unk allthat will unk thelaser negative it itwasprettybad mpsychotic smovieswithsomany unk ethic exquisite warmandsinisterjoys episode episodesasa unk won emmyseries early theshowisnotable moreofcourse notwiththeaudience watch table imdb environment theec slong education unk sroundof theyear with unk andanew unk unk modernenterprise politics thattherepublic unk unk intheaprilelections seepp unk business unk to unk inthequarter and unk onturnoverthatrose unk to unk unk unk duringturnoverup unk at unk duefollowingof facilities unk andthe unk withitsownviewsand unk sports the unk whohada unk winoverthe unk wasa unk goal unk throughinregular unk tointheseason botham art radiocodesaidtheband salbum newest armycluband unk album theyhavea unk ofthealbum run fora unk theorchestra whichincludesthe unk whichisandtheband slife nearlyallthisyear salbum sasell outtour award thelabel unk withjust aspokeswomanfor unk said femalespeakerthey expression roirbakstaredathim andthesmilehovering aunt makinghercry itwasfair crime unk acrossbristolroad whereshewashitaway table bnc apnews army terrorism thetaliban koreanwarinpakistan shistoriclifesince withtwo exampleof unk atthesamelevel the unk animal lottory whenthenumbersbegan theu tsellinganywolfhunts imdb horror negative sannoyingtodeath what snotthatbad hildren whenatrivial whinygirl unk troy unk unk captain playedbyhurley unk mortalbugs bnc environment politics unk unk tonnesofnuclearpower attheunplantin unk michael unk of unk hasgivena right seuropeanparliament buttobepassedby unk theandfourthstates thecommission sreportonoct onjan deniedthegovernment sgrantto art crime aswellas byalandonovan afterthetalks unk splay unk table 
proceedingsofnaacl hlt pages atlanta georgia june 
learninglikeachild junhuamao weixu yiyang jiangwang zhihenghuang alanl yuille losangeles baiduresearch mjhustc ucla edu wei xu yangyi wangjiang huangzhihengg baidu com yuille stat ucla edu abstractinthispaper sualconcepts usinglin ourmethodisableto cepts on inparticular wepro whichnotonly butalsomakes cepts inaddition projectpage intheexperiments weshowthatourmethod theprojectpageis www stat ucla edu ÿjunhua mao projects child_learning html introduction recognizing when wewereveryyoung tionsofourparents concepts inparticular learnedwords ortheirproperties researchedforover posed figure ingfromsentences nvcs task westartwithamodel model base thistask moreover especiallywhenthe however ingormappings betweensinglewords novelobjectcategory andimages weareunawareofany todoeffortlessly wecallthisthe novelvisualconcept nvcs task seefigure inthispaper thenvcstask we inparticular we itisplayedbyteams seefigure model ourmethodrequiresa basemodel forimagecaption we choosethem rnnmodel oftheart asourbasemodel inourmethod structureofm wenotethatthese captioningtask focusofthispaper inparticular weightsharing tws strategy motivatedbyauto encoders whichreduces byafactorofonehalf thenumberof thisallowsusto embeddingandmulti modallayers withoutovthedata yieldingaricher wetrainthis tencedescriptions forthenvcstask theweightsfor newconcepts weights secondly intuitively themodel whichis sentences thebase madeobjects ani mals andactivities thems cocodataset occurinms thesecon ceptsare quidditch rexandsamisen seesection the relatedwork deepneuralnetwork www stat ucla edu ÿjunhua mao projects child_learning html weareaddingmore tai ji huangmeiopera kiss rocketgun tem pura waterfall weddingdress andwindmill computervision fornaturallanguage recurrentneural networks rnns andlong shorttermmemories lstms achievethestate of the artperformancefor and speechrecognition forcomputervision deepconvo cnn outperformprevious cation anddetection thesuccessof imagecaptioningand sentence imageretrieval themeth odsofimage sentenceretrieval imagedescrip tiongeneration andvisualquestion answering veryre manyofthem adopt anrnn likelihood end to endway anexceptionis whichincorporates visualdetectors languagemodels andmultimodalsimi laritymodelsinahigh performingpipeline theevalua andedworddictionary andtraintheirmodel onalargedataset cnnframework zero shotandone shotlearning forzero shotlearning imagefeatures thedensewordvec fromco developed in addition adoptedauto proposeda egories examples however thebasemodelisbased onthem rnnmodel ure asasideeffect figure imageasinputs asinthem rnnmodel weaddastartsign start andanendsign end andu bestviewedincolor seesection secondly in byalong shorttermmemory lstm layer we strategyinsection themodelarchitecture asshowninfigure werepresentthisin dexasaone hotvector theoutputistheindex ofthenextword the languagecomponent modalcomponent itmapsthe mationinthelstmlayer cnn pre were dimensionallayer to ourmodel theactivationofthis dimensionallayer containsaone theweightsareshared acrossthesub asin them rnnmodel weaddastartsign start andan endsign end inthetestingstage forimagecaptioning weinputthestartsign start intothe modelandpickthe werepeattheprocess end tws fortheoriginalm rnnmodel mostoftheweights matrices andu rn wheren theweightmatrix betweentheone hotlayerand wheref isanelement wisenon linearfunction rn istheone notethat non zeroelementin inpractice wedonotneedto onecolumnof backwardpropagation theweightmatrix thesoftmaxlayer wherem isthesoftmaxnon linearfunction intuitively inequation istoencodetheone hotvector intoadensesemantic vector inequa tion backtoapseudoone hotvector withthehelpofthesoft maxfunction ofequation thedifferenceisthat isinthedense isinthedenseword semanticspace figure weonly updatethesub matrix thegreenpartsinthe inu one fornovelconcepts bestviewedincolor wedecompose intotwoparts manticspace tothepseudoone hotwordvector whichistheinverseop erationofequation thesub inatransposedman ner hereisan where equation wheref isaelement wisefunction if isanidentity mappingfunction intou andu inourexperiments wethatset tingf this dimension ersasshowninfigure inatransposedmanner tws word rnnmodel with wealso ingtask nvcs task thenwemeetwithim whatshouldwedo it istime trainthewhole inmanycases we tosolvetheseproblems more cally theweightmatrix where andu nassociatewith as showninfigure nisassociatedwith newwords suchascat kittenand pawing wethesub matrix andupdatethesub matrixu inequation thereisabiasterm intuitively each elementin thecorrespondingword similarto where andb spectively theestimationof isunreliable thenetworkwilltendto increasethevalueof data dur butthisisnotenough theweightmatrix playsasimilarroleto ability toavoidthisproblem intob asfollows afterthat weseteveryelementin tobetheaverage valueoftheelementsin and whenwetrainon thenewimages fixation bpf intheexperiments anduse adadelta nvcs task thesentences thelanguagepartof themodel thepartsofspeech italsohypothesizes figure forexample suppose alsosupposethereare thenalthoughthe itwillhypothe thevisionpartispre trainedontheimagenet tiontask with millionimagesand categories taskit self our datasets nc learningdatasets trainingim agesand validationimages withobjectinstancean notationsand to ﬁcatﬂ wealsocheckwhether asthe baseset wherewewilltrain validateandtestour basemodel novelconceptset ncset whichisusedtotrain validate visualconcepts newobj catandnewobj ﬁcatﬂandﬁmotor cycleﬂrespectively newobj cat newobj motor nc table figure concept nc dataset nc dataset tofurtherver weconstructanew ﬁquidditchﬂ are ﬁt rexﬂ adinosaur and ﬁsamisenﬂ aninstrument itcontains rexandsamisen butalso activityconcepts quidditch welabeled images foreachconceptwith age inthesamecategory ineachimage theaveragelength ofthesentencesisalso coco reasons firstly wherewepre secondly catandnewobj motor dataset nc infigure ing thenumberofimagesfor toinvestigatethe inthetesting stage ages originaltestset cattestingset imagesas novelconcept nc test setandtheaddedbase testingimagesas basetest set agoodnovelvisualcon the www stat ucla edu ÿjunhua mao projects child_learning html weareac novelconcepts meteorciderrouge lm rnn ours tws table rnn experiments evaluationmetrics visualconcepts bleuscores bleuscoreforn gramisdenotedasasb ninthe paper andmeteor inthenvcs task however therefore wealso calculatethe concepts forthecatdataset thereare newwords suchascat cats kitten andpawing theprecision andrecallr arecalcu latedasfollows gen ref gen gen ref ref wheres gen ref denotesrefer encesentences condition notethat andr thebasetestset alltest ahigh withalow thenewdata wecanalwaysget ifweoutputthe newwordeverytime whileahigh withalow indicatesweusethe asabalancedmea surementbetween andr best scoreis notethat ifeither orr comparedtometeorand bleu the sharing tws taskonthemscoco andcomparetom rnn whichdoesnotusetws wechoosethe parameters formance scoreof and respectivelyusing imagefeatureson whicharecomple mentarywithtws biasfixcentralizetws edbias deep nvcs fixedbias deep nvcs nobpf notws deep nvcs bpf notwspp deep nvcs bpf twsppp table performanceofdeep cat twsand weightsharing tws bpf newobj catdataset deep nvcsmodels termsof inequa tion atelayeractivation seeequation sothat willnot weachieve usingtws deep nvcs bpf twsv deep nvcs bpf notws andachieves increaseusingbpf deep nvcs bpf twsv edbias weusedeep nvcstorepresentdeep nvcs bpf tws resultsonand nvcsmodelscom catandnewobj motordatasetsintable fordeep nvcs weonlyusethe fordeep nvcs inc concepts model thebaseset nonovelconceptimages weimplementa baselinemodel model word vec wheretheweightsofnew words weightsof visedlearnedword embeddingsfromword vec we model retrain byretrain set basewhich thedeep nvcsmodelsperform deep nvcs word vecmodel the themodelwith ii the theweights inourexperiments ii sowe here evaluationmetrics fb meteorb meteorb meteornewobj cat model retrain model base model word vec deep nvcs deep nvcs inc newobj motor model retrain model base model word vec deep nvcs deep nvcs inc table resultsonthenewobj catandnewobj thedeepnvcsmodels model retrain but onlyneed ofthetime model baseandmodel nonovelconcepts and respectively model word vec vec deep deep nvcs incstandsforthedeep figure catandnewobj motor datasets thered blue thebasetestset base retrainfornctest to bestviewedincolor performanceofourdeep retrainbutneeds onlylessthan ofthetime themodellearnsthenew ouslearnedwords theperformanceofdeep nvcsisalsocomparable with nvcs inc in tuitively data however suchascatanddog itishelp shotscenarios werandomlysampled imagesfromthetrain ingsetofnewobj catandnewobj motor andtrainedour deep rangesfrom to timesandaverage weonlyshowthe resultsintermsof score meteor andb because ofspacelimitation theresultsofb andb andconsis indicatedbytheblue basetest red nctest ormagenta alltest dashedlinesinfigure few shotscenario baseis lowerbound retrainfor trainedwith and novelconceptsimages to trainingim figure intheseexamples cat motorcycle quidditch ages deep ingset inaddition usingabout trainingimages weob basemodel ourdeep retrain resultson thenc the ageset suchassamisenv guitar quidditchv foot ball secondly life cate gorieswherewepre thirdly baseset sentencesbetweennc andmscocoevenlarger the newobj catandnewobj motordataset seemodel base intable onnctest further more the indicatethatthemodel fromonly examples evaluationmetrics fb met fb met quidditcht rex model retrain model base deep nvcc deep nvcc inc samisenbasetest model retrain model base deep nvcc deep nvcc inc table datasets catkitten tabby puppy calico doll motorcyclemotorbike moped vehicle motor motorbikes quidditchsoccer football softball basketball frisbees rexgiraffe bull pony goat burger samisenguitar wii toothbrushes purse contents table embeddinglayer onthisdataset thebleuscores forthese qualitativeresults intable embeddinglayer learnedbyourdeep nvcsmodel wealsoshowsome andourdeep nvcsmodelinfigure conclusion inthispaper nvcs task inthistask methods afewimages concepts conceptimagesislarge we acknowledgement viewers process andaro cs references antol agrawal lu mitchell batra zitnick andd parikh vqa arxiv antol zitnick andd parikh zero ineccv pages bengio foundationsandtrends inmachinelearning bloom mitpress careyande bartlett chen fang lin vedantam gupta dollr andc zitnick arxivpreprint arxiv chenandc zitnick captiongeneration arxivpreprintarxiv cho vanmerrienboer gulcehre bougares schwenk and bengio decoderforsta arxivpreprintarxiv devlin gupta girshick mitchell andc zitnick ex arxivpreprint arxiv donahue hendricks guadarrama rohrbach venugopalan saenko andt darrell long arxivpreprintarxiv donahue jia vinyals hoffman zhang tzeng andt darrell decaf arxivpreprintarxiv elhoseiny saleh anda elgammal writeazero shotlearning in iccv pages elliottandf keller description in acl volume pages elman cognitivescience fang gupta iandola srivastava deng doll ar gao he mitchell platt etal arxivpreprintarxiv fei fei fergus andp perona one tpami frome corrado shlens bengio dean mikolov etal devise adeepvisual in nips pages gao mao zhou huang wang andw xu areyoutalkingtoa machine in nips girshick donahue darrell andj malik in cvpr guptaandp mannem in iconip heibeckande markman anexamination offastmapping childdevelopment pages hochreiterandj schmidhuber longshort termmemory neuralcomputa tion kalchbrennerandp blunsom in emnlp pages karpathyandl fei fei deepvisual imagedescriptions arxivpreprintarxiv kiros salakhutdinov andr zemel unifyingvisual semantic arxivpreprint arxiv klein lev sadeh andl wolf bridgaussian arxivpreprint arxiv krizhevsky sutskever andg hinton imagenetwith in nips pages kulkarni premraj dhar li choi berg andt berg babytalk in cvpr lake salakhutdinov gross andj tenenbaum oneshotlearning in cogsci volume lavieanda agarwal meteor in machinetranslation pages lazaridou bruni andm baroni isthisawampimuk cross modal in acl pages lebret pinheiro andr collobert viaalinearphrase basedapproach arxivpreprintarxiv lecun bottou orr andk uller efbackprop in neuralnetworks tricksofthetrade pages springer lin maire belongie hays perona ramanan doll ar andc zitnick microsoftcoco arxivpreprint arxiv ma lu shang andh li arxivpreprintarxiv malinowskiandm fritz amulti aboutreal in nips pages malinowskiandm fritz arxivpreprintarxiv mao xu yang wang huang anda yuille deepcaptioning rnn in iclr mao xu yang wang anda yuille explainimageswithmul in mikolov at burget cernock ands khudanpur recurrent in interspeech pages mikolov sutskever chen corrado andj dean distributed in nips pages mitchell han dodge mensch goyal berg yamaguchi berg stratos andh daum eiii midge in eacl ouyang luo zeng qiu tian li yang wang xiong qian etal deepid net multi arxivpreprintarxiv papineni roukos ward andw zhu bleu amethodforautomatic in acl pages russakovsky deng su krause satheesh ma huang karpathy khosla bernstein berg andl fei fei imagenet salakhutdinov tenenbaum anda torralba one shotlearningwitha sharmanska quadrianto andc lampert sentations in eccv pages simonyananda zisserman arxivpreprintarxiv socher ganjoo manning anda ng zero shotlearningthrough cross modaltransfer in nips pages socher le manning anda ng in tacl sutskever vinyals andq le neuralnetworks in nips pages swingley swordlearning tommasi orabona andb caputo tpami vedantam zitnick andd parikh cider consensus basedimage arxivpreprintarxiv vinyals toshev bengio andd erhan showandtell aneuralimage captiongenerator arxivpreprintarxiv weston bengio andn usunier learning torankwithjointword imageembeddings machinelearning xu ba kiros cho kyunghyun salakhutdinov zemel andy bengio show attendandtell visualattention arxivpreprintarxiv zeiler adadelta arxivpreprint arxiv zhu mao anda yuille expectationlosssvm svm algorithm in nips pages 
linear junekihong schoolofeecs corvallis or juneki hong liang huang sh gmail com lianghuang siliconvalleyailab baiduresearch sunnyvale ca abstract recently span however sternetal cubictime whichistooslowfor to wepro posealinear graph whichrunsintime nb where is thebeamsize wefurtherspeedthisup to nb log ing lines thislinear timeparserissubstan fordiscourseparsing andachievesthe highestf amongsinglemodelend to endsystems introduction span cross andhuang sternetal hasattracted simplicity constituencyparsers dyeretal liuand zhang durrettandklein whichuse thespan onlyus inputse quence andnotthe outputtree becauseofthis factorization gorithmsuchascky butexistingspan based ofsearch ontheonehand agreedyspanparser crossandhuang isfast linear time but largesearchspace andontheotherhand achart basedspanparser sternetal performs of the artaccu racy butincubictime whichistooslowfor to enddis courseparsing hernaultetal zhaoand huang detectionandparsing bj orkelundetal greedyandchart linear timespan following huang andsagae weperformleft to rightdy synchronous style with actions steps forasen tenceof words whilepreviousnon neuralwork huang andsagae miandhuang andthus ourstates areassimpleas where isthestepin dexand isthespan modeledusingbidirec this givesarunningtimeof withtheextra forstepindex nb atthecost ofexactsearchwhere isthebeamsize how ever onthepenntreebank mostsentencesare lessthan words andevenwithasmall beamsizeof the observed complexityof an nb parseris not exactlylinearin see experiments tosolvethisproblem weapply cubepruning chiang huangandchiang nb log which crementalparsing andachieves forthe time thecomplexityof nb log lin almost linearin beamsize treespans andemploymax violationupdate huang etal our andorders to enddiscourse parsing score end to endsystems structuredstack tomita whichre provinganew preliminaries span basedshift reduceparsing aspan basedshift crossandhuang maintainsastackof spans with ontopofthestack theparsercaneither shift onthe stack oritcan reduce tocombinethetoptwo spans and formingthelargerspan aftereachshift reduceaction thetop most null label parsinginitial isformed https github com junekihong beam span parser input state init goal shift reduce figure ourshift here isthestepindex and areandinside scores unlike huangandsagae and cross andhuang and arenotshift reduce scores instead theyarethe best labelscoresof theresultingspan max and max where isanonterminal symbol couldbe here bi lstmfeatures directional lstm crossandhuang sternetal thelstmproduces forwards and backwardsoutputs whichwecon and as thiseliminates andcan dynamicprogramming scoredecomposition like sternetal wealsodecomposethe scoreofatree max notethat isanonterminallabel aunarychain vp ornulllabel inashift reduce setting thereare steps shiftsand reduces fortheresultingspan theactualcodebaseof sternetal forces tobe however inour incrementalparser degradesaccuracy such labeled spans intree also isonlydependenton itself andnotdepend thusthemaxover label whichis anicepropertyofspan basedparsing crossand huang sternetal graph struct stackw obookkeeping sectionasashift reduceparser wemaintainastep index synchronous beamsearch seebelow figure showshow span ifthetopspan shifts itpro duces butifitreduces itneedstoknow whichis not thisproblem canbesolvedbygraph structurestack tomita huangandsagae whichmaintains foreachstate that is orﬁleftpoint ersﬂ to understandformula tioninfig cancom binewithas where atstep cancombinewithany for any atstep theorem thepredecessorstates areallinthesamestep proof byinduction thetimecom withtheextra duetostepindex action inexactsearch ateachtimestep wemaintainthe top parsingstates pruningofftherest thus ateverystep theword index beingalmost identicaltocky however with cubepruning however theorem canhaveupto predecessorstates ﬁleftpoint ersﬂ because areallinthe samestep with itemson abeamand actionstotake thisgivesusan overallcomplexityof nb eventhough is aconstant evenmodestvaluesof canmake actness search andretrievethetop thenexttime step weheapifythetop shift reducedstates to avoidinsertingall viousbeam shigh andwheneverwepopa weiteratedownits duplicatere thisprocess isheswhenwepop itemsfromtheheap the andpopping itemstakes log givingusanoverallim provedruntimeof nb log training sternetal shietal wewant higherthanany othertree byatleastamargin notethat forany and forany augmenteddecoding argmax argmax trainingsetisabout evenwithabeamsizeof weal readyhave ourruntime inpractice themaximum upper bound nevertheless orelseas increases andifthebeamsearch to bottom theneachstate sleft figure thedifferencesbe tersentencesoflength theregressioncurves havebeenempirically where istheloss augmentedscore if whichimplies argmax cross spanloss sternetal inthe predictedtree base notethat canbenull and denotes thegoldlabelforspan whichcouldalso be however asubspan duetobinarization oraninvalidspanin inthebaseline functionabove alently forexample aspan isnotpe vp new cross hasexactly spans but hasmuchfewerspans figure treebanktrainingset thelog logplotontheright ing where cross and or maxviolationupdates augmentedscores evenforpartialtrees theparsetrajectory ateachparsingtime step the violation estaugmented huangetal yuetal tiongivesusthemax steps givesusthemax violationloss experiments marcusetal andtheptb rstdiscourse treebank zhaoandhuang inbothcases and dropout hintonetal isemployedwith probability larization and rstrespec tively weuseadam kingmaandba with theweights training wasserandgoldberg theunknownwordsymbol unk withprobability unk with beingthenumberof developmentset baseline cross span time thisworkbeam thisworkbeam thisworkbeam thisworkbeam thisworkbeam chart table results per sentence stern etal withnull tobenonzero testset end to end singlemodel lrlpf socheretal durrettandklein crossandhuang liuandzhang dyeretal discrim sternetal sternetal cross span sternetal thisworkbeam thisworkbeam thisworkbeam reranking ensemble separatedecoding vinyalsetal ensem dyeretal gen rerank choeandcharniak rerank sternetal sep decoding friedetal ensem rerank table finalptbtestresults wecompareour modelswithother neural single modelend to endtrainedsystems occurrencesofword inthetrainingcorpus our neuralnetworklibrary neubigetal penntreebank treebank fortraining fordevelopment and fortest ing with way table overallspeeds whiletable comparesourtestre sults canbefast of the artperformances discourseparsing sequences lrlpf zhaoandhuang thisworkbeam thisworkbeam thisworkbeam thisworkbeam thisworkbeam table rstdis coursetreebank starred rowsindicatearunthat model segmentstructure nuclearity relation bachetal hernaultetal zhaoandhuang thisworkbeam thisworkbeam thisworkbeam table ptb rstdiscoursetreebank ajointdiscourse resentation zhaoandhuang wecompareourrun timesout of the boxinfigure withoutany pre processing to enddiscourse parsingsystems weadaptthe split zhaoandhuang wethat coursescores conclusions tainslineartime wealsousecubeprun nb log fortraining andachievestate of the modelend to endsystems acknowledgments tosecs and hedeservesco authorship and gestions iis anddarpan references ngoxuanbach nguyenleminh andakirashimazu in proceedingsofthe associationforcom pages andersbj orkelund agnieszkafale nska wolfgang seeker andjonaskuhn howtotraindepen in proceedingsofthe volume longpapers volume pages davidchiang hierarchicalphrase basedtrans lation parsing aslanguagemodeling in proceedingsofthecon processing pages span based labelsystem in proceed ingsofthe association austin texas pages https aclweb org anthology neuralcrfpars ing arxivpreprintarxiv chrisdyer adhigunakuncoro miguelballesteros andnoahasmith grammars arxivpreprintarxiv danielfried mitchellstern anddanklein im in proceedingsofthe hugohernault helmutprendinger davidaduverle mitsuruishizuka andtimpaek hilda adis cation dialogueanddiscourse geoffreyehinton nitishsrivastava alexkrizhevsky ilyasutskever arxivpreprint arxiv forestrescor ing in proceedingsofacl prague czechrep lianghuang suphanfayong andyangguo actsearch in proceedingsofnaacl http www isi edu lhuang perc inexact pdf dynamicpro grammingforlinear in proceedingsofacl uppsala sweden adam arxivpreprint arxiv sim corr abs http arxiv org abs shift reduce arxivpreprintarxiv mitchellpmarcus maryannmarcinkiewicz and beatricesantorini corpusofenglish thepenntreebank computa tionallinguistics shift reducecon postaglattice in proceedingsofnaacl grahamneubig chrisdyer yoavgoldberg austin matthews waleedammar antoniosanastasopou los miguelballesteros davidchiang daniel clothiaux trevorcohn kevinduh manaal faruqui cynthiagan dangarrette yangfengji lingpengkong adhigunakuncoro gauravku mar chaitanyamalaviya paulmichel yusuke oda matthewrichardson naomisaphra swabha swayamdipta andpengchengyin dynet arxivpreprint arxiv tianzeshi lianghuang andlillianlee fast er transition featureset in proceedingsofemnlp toap pear richardsocher johnbauer christopherdmanning andandrewyng parsingwithcomposi tionalvectorgrammars in proceedingsoftheas associa volume pages mitchellstern jacobandreas anddanklein aminimalspan in linguistics mitchellstern jacobandreas anddanklein aminimalspan parser codebase https github com mitchellstern minimal span parser mitchellstern danielfried anddanklein in guageprocessing pages masarutomita editor generalizedlrparsing oriolvinyals kaiser terrykoo slavpetrov ilyasutskever andgeoffreyhinton gram in advancesinneural pages hengyu lianghuang haitaomi andkaizhao max in proceedingsof emnlp kaizhaoandlianghuang jointsyntacto discoursetree bank in proceedingsofthe conferenceon ing pages 
xianglong chuanggan gerarddemelo xiaoliu yandongli fuli shileiwen tsinghuauniversity rutgersuniversity baiduidl longx ganc mails tsinghua edu cn gdm demelo org liuxiao liyandong lifu wenshilei baidu com abstract andmultimodal putvideo natureofthedata moreover timodalfusionmethods ucf activitynet kinetics and youtube andshowthatour especiallyon large scaledata ciencyandperformance mostremarkably ourbestsingle modelcanachieve intermsofthetop accuracyand intermsofthetop dationset andachieve intermsofgap onthe ofyoutube mtestset introduction talhumanabilities fromanearlyage infantsbegintorec mo tion aswellassound althoughvideoisavery learning thecurrentstate of the unlikeimagetion input andthus image motion forinstance twomusi tinctsounds inthiscase approachesbasedon cnns haveachieved state of the artresults however bycnns havenotbeen correspondingauthor copyright intelligence www aaai org allrightsreserved figure temporalsegments differenttimeperiods natureofvideos whichcanbeverylong recurrentnetworks rnns rangetem however existingend to scaledatasets itre forjointend to scaledatasets pre trainedmodel ortotrainsingle modalitycnnmod elsseparately this thisfacilitatestrans ferlearning toanother additionally data finally large scaledatasets suchasyoutube of tenalreadyprovidepre which generally thelocalfeaturesin describingthevideo basedonthese wecanusearecur wepro several multimodal featuresequences itisnon trivialtoconnectthem figure forthetopexample clearest intermsofaudio obvious forthebot tomexample whereasthegreatest jumpandlandingphase hence yet tion top orimageandsound bottom suchthatdifferent therefore overall paperasfollows timodalrnn basedarchitectures andthatourpro posedattention results tiondatasets longuntrimmedvideos andsingle labelaswellasmulti labelsettings andactivi tynetdatasets andyoutube mcompetitions forwhichthereleaseof relatedwork video krizhevsky sutskever andhinton szegedy etal simonyanandzisserman heetal cation karpathyetal ganetal simonyan andzisserman ganetal tranetal ganetal varol laptev andschmid car reiraandzisserman initially dcnnsweredirectly karpathyetal studied late early or slowfusion karpathyetal however simplepool framebaseline dcnnsandbetter accountforspatio temporalinformation theopticalw method zach pock andbischof wasproposedto simonyan etal respec tively simonyanandzisserman theyshowthatthe whichindicates however method tranetal dcnns drelieson temporalinformation varoletal rgbinputs varol laptev andschmid carreiraet al szegedyetal into dcnns carreiraandzisserman feicht enhoferetal thisspatio temporalinformation feichtenhofer pinz and zisserman theyalsocombinedtwo streamcnns withresnets heetal dcnns feichtenhofer pinz andwildes termpatternsin short ed lengthvideos butitremainsdiftodi rectlycapturelong length videos termmemory lstm ones termtemporalin proposedtwo streamlstmsforhigher accuracyvideoover longertimeperiods ngetal donahueetal devised anend to andcaptioning donahueetal srivastavaet al srivas tava mansimov andsalakhutdinov subsequently theyadaptedthispre trainedlstmtovideo tasks however rnn isfactory studiedinsufdepth attentionmechanisms formation drawingonthe reinforcealgorithm inparticular mnihetal applied mnihetal baetal presentedanattention basedmodeltothemost ages ba mnih andkavukcuoglu inpartic ular bahdanauetal chinetranslation bahdanau cho andbengio subsequently videotasks sharmaetal proposedasoft layeredrnnsto sharma kiros and salakhutdinov lietal proposedanend to endse lietal attentionlstm temporalregions however integratedwithrnns withbothvisual andsoundmodalities thevideosignal moreover canfur and hence us in thispaper visualfeatures rgbandwfea tures ages verticalvector simonyanandzisserman we trainedmodelfromim workframework wangetal aftertraining wecan figure onkeylessattention fora integrationpoint grationpoint timesfor differentmodalities concate andthenetwork extractframe inthevideo acousticfeatures ofmatrices overlapping msframes short msandthenaggre gated logarithm transformed into mel spacedfrequency binsfollowing hersheyetal thisyieldslog mel spectrogrampatches ages wecanextract frame segment levelfeatures tures first thousands challenging larlylong rangedependencies second thefeaturesofsuc anditisnotnecessary third alargenumber offrame segment weuse dadaptationmax poolingtotrans formalltheframe segment levelfeatures suchthateachsegment levelfeature thevideo numberoffeatures inthissection figure we andthende scribeattention keylessattention efandeffective quenceofinputvectors whichwecall an notationvectors anoutputvector annotationvectors theweightofeach iscomputedby exp exp where forconvenience wedenotethe soutputas keylessatt bahdanau cho and bengio notationvectors suchasthe oravectorrepre wangetal wereferto suchvectorsas keyvectors unlikesoftatten tionmechanisms ontheannotations werefertoitas keylessattention therefore onceforeachvideo rnn convolutionalop erationwith space attentionweights hence wepickthissimpleand keylessattention hochre iterandschmidhuber asshowninfigure given anlstmunit viarepeatedap tanh where denoteselement wisemultiplication ofhiddenstates whichwedenoteas and respectively ateachtimestep abidirectionallstmas where denotes resulting andbecausethelength ofthevideomaybelarge mensionalities wethuscomputeaed dimensionalglobal representation forthis mechanism keylessatt intuitively weapplysev fc layer bn ioffeandszegedy tocomputetheproba bilitiesforclasses forthe thvideo forthesingle labeldatasets ucf activitynet andkinetics andformulti label onyoutube weapplytwose quentialfclayerswith hiddenunits respec tively and tanh activation followedbyafc becausethepre extractedfea wehencedonot thvideoas aone hotvector setisdenotedas forsingle labelwecan log whileformulti inglossfunction log log multimodalfusion previously however inordertofullyex weneedtoaccount asinput thebest possibleresults weassumewearegiven differ entfeaturesequences lier samelength reddot tedline infigure featurefusion asimplefeature levelfusionisoneof stitchingtogether hence wecan byapplying andwecan quence lstmfusion levelfu eachin putfeaturesequence obtainsitsownhidden states byapplying afterthis wecan attentionfusion ods first foreachinputse attentionmodels then as thesubsequentlayers probabilityfusion finally inthisapproach experimentalresults inthissection datasets tiondatasets ucf soomro roshanzamir andshah isa trimmedvideodataset it contains framesperclip thesearelabeledwith actionclasses each followingthe overthreetraining testingsplits activitynet heilbronetal isanuntrimmedvideo dataset weusetheactivitynetv release whichconsists ofmorethan around kvideoswith annotationspervideo selected from classes and typically intheofsplit thedistribu tionamongtraining validation andtestdataisabout and ofthetotalvideos respectively becausethe kinetics carreiraandzisserman isatrimmed videodataset thedatasetcontains trainingvideos validationvideos and testvideos covering humanactionclasses eachcliplastsaround sandis onthevalidationsplit youtube abu el haijaetal ismassivelylarge itcontainsover billionvideo framesand millionvideos withmultipletags ondofthevideo trainedonimagenet denget al followedbypca dimensionalvector pre trainedvgg inspired simonyanandzisserman network intheofsplit validation andtestdataisabout and re spectively tionsetisoverlylarge wemaintain kvideosfromtheof othervideos we kvalidationsetandon theoftestset forucf andactivitynet weextractbothrgbandw featuresusingaresnet heetal model forki netics resnet szegedyetal withavgg simonyanandzisserman thenum forucf and wemax pooltheframe levelfeaturesto segment level featuresforucf andkinetics wherethelengthsof method accuracy idt fv wangandschmid idt hsv pengetal emv cnn zhangetal twostream simonyanandzisserman fstcn sunetal videolstm lietal tdd fv wang qiao andtang fusion feichtenhofer pinz andzisserman tsn seg wangetal st resnet idt feichtenhofer pinz andwildes actionvlad girdharetal ours rgbcnn rgbaverage rgblast rgbattention flowcnn flowattention featurefusion lstmfusion probilityfusion attentionfusion table meanaccuracyonucf videosareafewseconds and foractivitynet where differentfrom wang etal for youtube extracted levelfeatures wherethe activitynet andkineticsis whileforyoutube weuse tohandleitslonger videos tielemanandhin ton with alearningrateof inthissection ourproposedmodelwith keylessattention rgbattention ishencethemodelde scribedinsection flowcnn wealsocompare ourmethodwithtwornn basedmethods themethod rgblast andthenapplythe basedrecurrentmodel thesecondmethod rgbaverage rgbaverage moreover duringtraining wealsothat speed method map idt fv pengetal depth action zhuandnewsam twostream simonyanandzisserman tranetal ours rgbcnn rgbaverage rgblast rgbattention flowcnn flowattention featurefusion lstmfusion probilityfusion attentionfusion table butalsoefandef fective inthissection sionmethods probabilityfusion eachmodalityiscom sincethedifferent we this asshownintables theprobabilityfusion onkinetics the bestresultis andonyoutube thegap for probabilityfusionis wehypothesizethat onlargedatasets wehavemoredata leading tomorepronouncedgaps featurefusion mentmethod however inthiscase theattention entmodalities hence while thisburden maybetooheavy sion tweenmodalities quenceoffeatures suchasforyoutube inwhichthe segments themoreobvi method top top tranetal dresnet hara kataoka andsatoh two streami carreiraandzisserman ours rgbcnn rgbaverage rgblast rgbattention flowcnn flowattention audiocnn audioattention featurefusion lstmfusion probilityfusion attentionfusion table exceptfor thosemarkedwith quences teractions lstmfusion orinsomecases slightlybetter attentionfusion sion hence however the figure weobserve thatrgb opticalw wethattheat thefourdatasets basedre currentcomponents comparisonwithstate of the art finally of the art methods onucf andactivitynet wecompareitwith wang andschmid pengetal aswellasdeeplearn ingapproaches zhangetal simonyanandzisser man sunetal lietal wang qiao andtang feichtenhofer pinz andzisserman tranetal wangetal feichtenhofer pinz and wildes girdharetal method kvalid test vlad xu yang andhauptmann videolevel zhongetal lstm moe wangetal ours rgbattention audioattention featurefusion probilityfusion attentionfusion table youtube mgap onthe kvalidationand testset competitiveresults onkinetics sults tranetal hara kataoka andsatoh carreiraandzisserman notethatresultof carreira andzisserman for whichthetop lowerthanthe top onthevalidationset onyoutube wecompareit withvlad jegouetal xu yang andhauptmann zhongetal wangetal competitions table also mtestset ourmodelro entdatasets and conclusion ofvideos nism models and bestresults knowndatasets videos single labelandmulti labelsettings andsmall scaledatasets our tasks intermsoffuturework proachinend to end forfurthergains acknowledgments gerarddemelo references abu el haija kothari lee natsev toderici varadarajan andvijayanarasimhan youtube alarge scalevideobenchmark arxiv ba mnih andkavukcuoglu multipleobjectrecog arxiv bahdanau cho andbengio neural arxiv carreira andzisserman quovadis actionrecogni tion arxiv deng dong socher andli imagenet large in cvpr donahue hendricks rohrbach venugopalan guadarrama saenko anddarrell long termre tion in cvpr feichtenhofer pinz andwildes spatiotemporal in nips feichtenhofer pinz andzisserman convolutional two in cvpr gan wang yang yeung andhauptmann devnet in cvpr gan sun duan andgong webly supervised webvideoframes in eccv gan yao yang yang andmei youlead weexceed labor webvideosandimages in cvpr girdhar ramanan gupta sivic andrussell actionvlad learningspatio tionin cvpr hara kataoka andsatoh learningspatio temporalfeatureswith nition arxiv he zhang ren andsun deepresiduallearn in cvpr heilbron escorcia ghanem andniebles activitynet alarge derstanding in cvpr hershey chaudhuri ellis gemmeke jansen channingmoore plakal platt saurous sey bold slaney weiss andwilson cnn scaleaudio arxiv hochreiter andschmidhuber longshort termmem ory neuralcomputation ioffe andszegedy batchnormalization accelerat arxiv jegou douze schmid andperez aggregating in cvpr karpathy toderici shetty leung sukthankar andli large tionalneuralnetworks in cvpr krizhevsky sutskever andhinton imagenet in nips li gavves jain andsnoek video lstmconvolves arxiv mnih heess graves etal recurrentmodelsof visualattention in nips ng hausknecht vijayanarasimhan vinyals monga andtoderici beyondshortsnippets deep networksforvideoin cvpr peng wang wang andqiao bagofvisual comprehensive studyandgoodpractice cviu sharma kiros andsalakhutdinov actionrecog arxiv simonyan andzisserman networksforlarge arxiv simonyan andzisserman two streamconvolu nips soomro roshanzamir andshah ucf datasetof arxiv srivastava mansimov andsalakhutdinov unsu in icml sun jia yeung andshi human networks in iccv szegedy liu jia sermanet reed anguelov erhan vanhoucke andrabinovich goingdeeper withconvolutions in cvpr szegedy ioffe vanhoucke andalemi inception inception nectionsonlearning arxiv tieleman andhinton lecture rmsprop cours era tran bourdev fergus torresani andpaluri dconvolutionalnet works in iccv varol laptev andschmid long termtemporal pp wang andschmid trajectories in iccv wang xiong wang qiao lin tang and gool towardsgood in eccv wang cao demelo andliu relationclas viamulti levelattentioncnns in acl wang kuan ravaut manek song fang kim chen haro tuan zhu zeng cheung piliouras lin andchandrasekhar trulymulti modalyoutube mvideowithvideo audio andtext arxiv wang qiao andtang trajectory pooleddeep in cvpr xu yang andhauptmann adiscriminative in cvpr zach pock andbischof forrealtimetv opticalw dagm zhang wang wang qiao andwang real in cvpr zhong huang zhan zhang xiao wang andyang arxiv zhu andnewsam depth action exploringembedded depthforlarge in eccv 
proceedingsofcoling technicalpapers pages coling mumbai december 
ruiyan baiduinc no shangdi thstreet beijing china yanrui baidu com iane yen dept ofcomputerscience austin tx usa ianyen cs utexas edu cheng teli academiasinica no academiaroad taipei taiwan ctli citi sinica edu twshiqizhao baiduinc no shangdi thstreet beijing china zhaoshiqi baidu comxiaohuahu collegeofinfo sci andtech drexeluniversity philadelphia pa usa xh drexel edu abstract toshare generatedcontents thesocialnetworksbe comeﬁgiantsﬂ how ever extremedatasparsity individ ualpostingdocuments amicrobloglessthan characters scenarios inthispaperwepro weformulateaso cialinteractions an model twitter andweibo el intermsofperplexity ndcgandmapresults contentanalysis andindexingš textmining mittee iw iw author www may florence italy acm http dx doi org generalterms algorithms experimentation performance keywords propagation socialnetwork introduction onlinesocialnetworks mationviapeer to peerinteractions amongthepopularweb facebook encouragespho to video twit ter providessuccinct foursquare inthissense withincred iblylargenumber inbillions erationofuser generatedcontents infact peoplehaveconducted linkprediction andsummarization theirweakness extremesparsity accurate moreobserved model however duetothereal works characters pertweetontwitter inthiscase atweet whilegiven https www facebook com https www twitter com https www foursquare com limiteddatasampling themaximumlikelihood therefore andwe lan lem although semanticassociation and positionalproximity however forsocialnetworks thereismuchmoreinfor therefore alongthefactorgraph we our st intuitively andanalogouslyfor inotherwords thetermsthat havesimilar orsmoothed strongerso tributes and tobemorewe textquality social andthengroupedthemas factorfunctions inthispaper modelsmoothing slms weeval twitter andweibo oneinen https www weibo comsmoothingmethods inotherwords thesocial westartbyintro usingtextual wedescribetheexperi tion problemformulation inthissection smoothing slms viafactorgraphmodel givenaposting documentd tosmooth fd twit terorweibo etc ularuser forinstance socialnetworkssup portparticular relationships follower followeeontwitter also replying sharingandreposting documents documentcollection we thetextinformation thus inthispaper wemainlyem di anditsassociateduser ui wegivethefollow wehave ingusers vandedges andhave vu it ciated vd documentpairs where vd fdijdi dgisthepostingcollec tionwithasizeof jdj and indicating whichis ourgoaltoestimate vu vu fuijui jvuj vu vu vu vd edges ineu usually apostingdocument input andthesocialization augmentationnetworks thetargetdocument denotedas wjd basedonthe diwherefdijdi dg output wjd forevery originaldocument umentpairs userpairsanduser methodology inthissection putepropagation the textsandso cialinformation eralsub problems us erpairmeasurement and variablepairmea surement while socialcontexts wealsoanalyze thorship finally andthenapply figure proposedmodel ithasbeensuc suchassocial analysis andlinked datadisambiguation inthiswork weformulatetheso whichisshowninfigure let and eubethe usersocialties anda targetdocument tosmooth tributefactors whichareobservable variables fyigni attributefactor anddependencyfactor attributefactor representedas authors ingeneral pairsanduserpairs aswellasthe interactions authoritativeness andsoon tsection yi xi torepresentthe yigiven weintro figure tionfactorgraph the middlelayer betweendocumentpairs samelayer in thestep somelinksbetweendoc foreachfeature andformally fi yi xi zexpfxccfi yi xi wherexi cisthe fc thfeatureand dependencyfactor asproposed layer thesameuser isby documentdi dkfromthesameuser variable ykinthefactorgraph thefunctionisas yi zikexpfkgk yi whereg latedornot notethatif yi inotherwords thetwovariablesare notcorrelated andweuse yiisin hence for yk yi yihasdependencyon yk as gi yi yyk yig yi zexpfxyk yikgk yi yk again inthisway sameuserinthiswork modelsmoothing objectivefunction ingeneral inequation wethefeatures fc forallattributes where cisthecorre spondingweight inequation kindicatestheweights let weaconditional yjx yifi yi xi gi yi hence andalsofol wecanthefollowing log fyigni log yjx xixccfi yi xyk yikgk yi logz whichsumsupthecon ditionallikelihoodof poverallinstances isthecollectionof fg inderiving thelog requiresaloopysum wemay insection function inthispa per weinvestigate featuresorfactors besides somefeaturesthat wealsoutilizeseveral socialinteractions forinstance fol lowingbehaviors socialstatus andotherstatistics westartfrom thefeature attributes ditoshadeon referring toequation textsimilarity ing fsim dijjd jjjjdijj textquality tdi herebyweusetheout of vocabulary oov thelower oovratio againstthevocabulary fromtheofnewscorpora toasetofmisspellings andirregularsym bols foov jfwjw di oov gjjdij technically themeasurementof textquality isnotapairwise anddi fromditod postingpopularity replies andretweets di socialstatus sincemicro tothefollowers values theuserof di theuserof di thetwo usersimilarity stil duetotheasymmetry weusefunc tionf the fusim jf author author di jjf author author di interactionstrength lationship author torepostfrom author di userimpacts onsocialnetworks sportsstars polit icalcelebrities etc quotedand spreaded pacts scorestodenoteuser impacts withalargenumberof in links dependency re ferringtoequation wethefunction fortwocandi yi andlet yiandyk respectively thedepen asfollows gk yi if author di author dk fsim di if author di author dk didkjjdijjjjdkjj thebooleanindicator is when author di author dk and otherwise user modelinference ifwedonot variables viaatwo atallofthemodelparam eters giventheparameters we then given formance section outvalidation data unfortunately de besides ittakesalonger timetoconverge ues nomenonof repostings ontwitterandweibo foreachretweeted pairs as wecannow estimatetheparameter gtomaximizethe log log yjx approxi stochasticgradientde scent argmax algorithm input afactorgraph andthelearningrate output estimatedparameters saccordingto equations toget yjx for docomputegradientof usingsaccordingto equation ep yjyl sep supdateparameter withthelearningrate untilconvergence basedonequations yjx yjx zexpfxis yi zexpfsg wheres yi fi yi xi yi ands pis yi inthisway log yljx log xyjyl zexpfsg log xyjylexpfs logxyexpfs yjylisalabeling urationof yinferredfrom yl function wecalculatethe logpyjylexpfs logpyexpfs pyjylexps spyjylexpspyexps spyexps ep yjyl sep ep yjyl sandep thevalue rithms lbp algorithm gibbssampling onechallenge maycontaincycles itshouldbenoted forgraphicalmodels thegen ep yjyl ep ofaparameter theobjectivefunction onetime yi rate after theprobability yjx finally argmax yjylp yjx distributedlearning itis toaddressthischal lenge themap withineveryindividu alframework basedon thesubsetofvariables groupofvariablesas yi individual map oflargedatasets inthemapstage eachmachine calledaprocess figure the node mediatevalues inthereducestage andareducefunction thewholegraphisdi then fordetails pleasereferto wenowproposeaterm basedonpropagation edtocapturethe proximity modelsmoothing weassignﬁclose byﬂwordswith fromeachother inotherwords fromﬁnearbyﬂterms imity theoretically eachwordhasacertain non asif while inpracticalscenarios tremelylowcounts ingeneral consider elp andthebackground pb nation thetradeoffisbal granularity woccursatanin postingdocument withadis countedcount thelargerthe letd fw jd jgwherejd jisthelengthofthe document weuse indocument dbeforesmoothing if wdoesnotoccurin willbe wehavecalculatedthe valuesof fyigni fromthelastsection indicatingthe fromthe ditothedocument tosmooth thefunctionactually we usec xyi yyic evenif is maybegreaterthan fc wv extend edfromdocument thusthelanguage modelofthisnew wjd pw vc pw vc isthelength thefollowinglinkage weconstruct to andweibodatafrom to weuseroughly thedetails and ofreposting ﬂinweibo table user document link repostlangtwitter enweibo cnpre processing basically however thedataisnoisy byap oovratio ingpostingdocuments perform forchinesetexts andbuildthegraph we apple tech appleproducts sport americanfootball travel generalinterst mlb sport baseball fashion generalinterest tvshow voiceofchina food chinesefoods tech microblogservice tvdrama culture tech smartphone table weimplemen method smoothingbyse mantics for gorithms weinclude theplainsmoothingof additive alsoknownasadd smoothing wealsoimplementsever groundinformation jelinek mercer baseddocument model cbdm proposedin languagemodel delm in whiledelmnearest however levelsemantic similarity plm pro posedin whichisthestate of languagesmoothing modelsmoothing slms evaluationmetric directly the re trieval study tion languageitself henceweuselanguage perplexity toevaluatethe intrinsicevaluation theexperi wemanuallyselected topics foreachdataset basedonpopularity postings sports technology cultures andgeneralinterests thesetopicsareshown intable intoclusters perplexity as pow nxwi vlogp wi inthissense thebetterper extrinsicevaluation basedmeasurementson hashtagclusters andinformation recommendation mentalsetups fortheretrievaltask giventhe topicsmentionedabove tagsasfollows thegroundtruthlabels thehashtags are theonesorig notethat weapplytherecom talsetups forauser iorsusingthegraphco rankingalgorithm thelanguagemod andtherec testdata again weuse theper which alsothe forinstance itispossible wereturnthe queryoradesignated user relevant documentsorthe reposted documents ndcg ndcg nxjj zkxi ri log qforqueriesand uforusers kindicatesthetop kpositionsina rankedlist and user riisthejudgescore relevant reposted irrelevant unretweeted forthe thposting user topicen en en en en cn cn cn cn cn lm additive absolute jelinek mercer dirichlet plm cbdm delm slms table map map nxjj zkxi oruser ommended and piistheprecisionat user overallperformance ofperplexity ommendation table methods onaverage slmsachieves anaverage termsofndcgandmap andanaverage improvementin thead pected the achillesheel simpleintuition since avoidzerooccurrence dueto jelinek mercerand but positions the ment odsclusterdocuments terbackground however whileour ndcg ndcg ndcg maplm additive absolute jelinek mercer dirichlet plm cbdm delm slms table indicates ndcg ndcg ndcg maplm additive absolute jelinek mercer dirichlet plm cbdm delm slms table indicatesthatwe ships settings factorcontributions analysis parametersettings intheexperiments months welearnparameters gonthedatafromthe month andexamine there inequation tobalancetheorigi figure ndcg retrievalandrecom factor group inisolation figure ndcg retrievalandrecom mendationtasks factor group featuremodel asweopt weex to withastepsizeof we henceweset factorcontributions weconduct resultinfigure modelsmoothing weconsider textsimilarity ii textquality iii postingpopularity iv social status usersimilarity vi socialinteractions vii userimpacts and viii variabledependency besides wecombinefactors iii as textrelated onesand iv vii as socialrelated ones we hereforcomparison tobeweshowthe one out oneatatime fromfigure and there sultinfigure isperformedusingthe usingaleave one outmanner textsimilarity stillcontributesmost asto interaction it thelanguagemod wealsoexamine textrelatedfactors social dependencyfactor values ingeneral and convergenceproperty figure illustratesthe weseeon bothtestcases thelbp inlessthan iterations afteronly learningiterations the ble scalesocialnet works figure processing times however processing thedistribut timesormore inparticular computernodesusing thetwodatasets thatwhenthesize agoodparallelefy speedup inthethedashed theresultthat scaledatasets relatedwork centyears betterestimation etc retrievalmodels but givenlimiteddata sampling thezerocountproblem whichisnotreliable lan and therearesev theinforma bination thebasicidea modelsmoothing intheirwork thekeyideaisto and hencethe beyondthesemantic and structuralbasedlan thereisastudyin basi cally jelinek mercer dirichlet absolutediscounting etc onthetrackingtask whichisanovel insight languagemod conclusions tials totackle wepresentan thesocialin byutilizinga socialdimensions inthisway andsmooth inenglish andweibodataset inchi nese with improvementinterms ofperplexityand fur thermore ing ingeneral strongercontribution inthefuture socialnetworks face book foursquare acknowledgments references analytics twitterstudyœaugust bottou on carreira perpinanandg hinton oncontrastive divergencelearning in volume page casellaande george chen chen zheng jin yao andy yu in sigir pages deanands ghemawat mapreduce data pages hopcroft lou andj tang whowillfollowyouback in cikm pages huang yan kuo ands lin enriching information in acl pages järvelinandj kekäläinen cumulatedgain based acmtrans inf syst kindermann snell etal markovrandomand theirapplications volume americanmathematical societyprovidence ri kschischang frey andh loeliger factor graphsandthesum productalgorithm informationtheory ieeetransactionson kuo yan huang kung ands lin in kdd pages kwak lee park ands moon whatistwitter in www pages laffertyandc zhai query models in sigir pages lavrenkoandw croft models in sigir pages lin lin li wang chen andt li in cikm pages lin snow andw morgan topictrackingintweet streams in kdd pages liuandw croft cluster basedretrievalusing languagemodels in sigir pages lvandc zhai informationretrieval in sigir pages manning raghavan andh schütze introductionto informationretrieval volume mei zhang andc zhai ageneraloptimization structures in sigir pages ney essen andr kneser ontheestimation ofsmall one out patternanalysis ieeetransactionson page brin motwani andt winograd the pitler louis anda nenkova in acl pages ponteandw croft in sigir pages songandw croft informationretrieval in cikm pages tang lou andj kleinberg heterogenousnetworks in wsdm pages tang sun wang andz yang social analysisinlarge scalenetworks in kdd pages tang zhuang andj tang tiesinlargenetworks in ecml pkdd pages tao wang mei andc zhai languagemodel in hlt naacl pages teevan ramage andm morris twittersearch in wsdm pages wang li wang andj tang cross lingual in www pages wu sun andj tang in wsdm pages yan jiang lapata lin lv andx li semanticv positions in ijcnlp pages yan kong huang wan li andy zhang temporal summarization in emnlp pages yan lapata andx li graphco ranking in acl pages yan wan otterbacher kong li and zhang abalanced in sigir pages yang cai tang zhang su andj li social contextsummarization in sigir pages yedidia freeman weiss etal generalized beliefpropagation in nips volume pages zhaiandj lafferty in sigir pages zhaoandy yun informationretrieval in sigir pages zhao jiang weng he lim yan and li models in ecir pages zhao guo yan he andx li timeline in sigir pages 
proceedingsofthe pages copenhagen denmark september 
multi haoxiong zhongjunhe xiaoguanghuandhuawu baiduinc no shangdi thstreet beijing china xionghao hezhongjun huxiaoguang wu hua baidu com abstract attention basedencoder nmt whichtypically rnn tobuildtheblocks ingprocess ontheotherhand weof forexample we motivatedbythis demand weproposemulti channelencoder mce which position more ofencodingrnn mcetakes theoriginalwordembed and apartic ntm whileallthreeen em modelcanimproveby sourcenmtsystem dl mt onthewmt english frenchtask of the artdeepmodels introduction attention tion nmt translation smt sen nrichetal basedmodel overcanonicalencoder decodermodel sutskever vinyals andle insmt in otherwords attention inatypicalattention basednmtsystem abidirectional birnn schusterandpaliwal yieldingasequenceof copyright intelligence www aaai org allrightsreserved https github com nyu dl dl mt tutorial vectorsfromthernn context tence withthisdesign nism lstm hochreiter andschmidhuber gru choetal fortranslation itiscommonthat ture examples unit wemayagainwant weproposemulti channelen coder mce more inaddition mcetakestheorig tion graves wayne anddanihelka formorecomplex composition ferentfrequency learnedinanend to endfashion more wede inthispaper wetestourmodelsonthe nistchinese achiev bleuoverthe strongdl mtsystem furthermore experimentsonthe wmt english of the artmodels inthe nextsection ventionalattention basednmt afterthat wewillpresent mceinmoredetails afterthat arxiv cs cl dec figure right externalmemoryinthe attention inrecentyears moststate of the art basedencoder decoderarchitecture figure basedencoder decoderarchitecture whichconsistsofthree parts anencoder inmoredetails vector namedwordembedding abovetheembed dinglayer be dur thecontrollerwillcon currenthid denstate formally givenasourcesequence and theprobabilityof nextword iscalculatedas softmax and tanh where and respectively hotem beddingvector step whichiscomputedas here whichcanbe and isadistinctcontext vectorattimestep mechanism normally theinputannotations tx ij where istheannotationof froma birnnand tx the normalizedweight ij for iscalculatedas ij exp ij tx exp kj ij tanh where and allofthe ty log here multi channelencoder basednmtmodels neverthe less itisdif tion thus channeltoenhance figure illus whereanexter additionally thehiddenstateof ontheotherhand heetal moreover short externalmemoryinntm arnnis concretely inthernn ateachtimestep thecurrentstate isde pendingontheinput andthe laststate thehistoricalstates anon innmttask mostresearchers fol thevalueof couldbecalculatedas gru and tanh we us where however asequation and indicates thecurrent toricalstate inthissituation thernnhasdifinwell figure thernnreads pendentrelationships oursolutionistode velopmulti mostinspiredby graves wayne and danihelka memoriesfornmt mengetal wangetal fengetal wepresenta read write memorytoen figure ateachtime step attention ofthememory thegru gru besidesthe read operation wealsodesigna write op eration write operationisthat formally let time where isthenum berofmemorycellsand cell weinitializethe tor mengetal ateachtimestep wegeneratethe read mem ory asfollows where step and signedtothecellsin asdescribedingraves wayne anddanihelka wecanusecontext basedoraddress based inpractice we model thuswecomputethe and inaddition gru cells more itcanbeas where and istrainableparameter read memory weuseit tofetchthecontext inequation afterthat and gru finally thenewstate update where istheupdategate andisparameter izedwith meanwhile inourexperiments the weightsforreading andwriting attime areshared and withthesamestate andthememory itisworthnoting sourcesequence and backwardrnn respectively ofhiddenstates nism gatedannotation wedesignmul consequently we includ ing fromthehidden tively inthispaper motivated bythedesignofgru solution sinceinprior thebestdecision tionsautomatically formally in ntm andthehiddenstate inrnn inwhich canbeas rnn ntm where isthegatedunit calculatedas where and hiddenstateinrnn nations rnn emb ntm emb ntm rnn emb rnn ntm and rnn ntm where and aretrainablepa rameters experiments chinese inordertocompareour wmtenglish forchinese english task weapplycase insensitivenistbleu forenglish french mancewithmulti bleu pl datasets nistchinese english ablefornistopenmt task puscontains wechoosenist https github com moses smt mosesdecoder blob master scripts generic multi bleu perl ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc ldc hknews hkhansards nist andthenist nist nist nist nist and nist weuseasource wmt english french weusethefullwmt par europarlv commoncrawl un newscommentary gi gaword intotal itincludes millionsentencepairs the news test andnews test areconcatenatedasour developmentset andthenews test isthetestset our viousworksonnmt luong pham andmanning jeanetal asvocabulary weuse ksub wordtokens sennrich haddow andbirch basedonbyte longerthan modelsettings forthechinese englishtask werunwidelyusedopen sourcetoolkitdl andconvs onthe ourmodels beyondthat basednmtwritteninw asourbaselinesystem whichincludes dl mt anopensourcetoolkit rnn basedrnn writteninw ntm annotation emb notation ntm emb ntm rnn https github com tensorflow tensor tensor https github com facebookresearch fairseq parametersfordl mt dim optimizer adadelta dim word clip words learning rate decay parametersfort model transformer hparams set trans former base single gpu parametersforconvs model fconv nenclayer nlayer dropout optim nag lr clip momentum bptt nembed noutembed nhid https www tensorflow org rnn emb equation ntm rnn emb anopensourcetoolkit vaswanietal convs anopensourcetoolkit gehringetal multiple tion inmoredetails weuse getlanguages decoder have memorycells sameasthehiddensize thedimensionof is trainingdetails wuetal weinitial weapply gradientclipping aedconstant whichis inourcase followingtheworkof vaswanietal weusethe adamoptimizerwith and overthe courseoftraining totheformula lrate min step num num gpus wherewesetthe sincewetrain we ofgpususedinourmodel onchinese daytotrainthe basicmodelon nivdiap gpusandonenglish french taskittakesaround days additionally andlog andweuseabeamwidthof inalltheexperiments more over resultsonchinese to english table english evaluationtasks enough tem dl mt atwethatthereisaim mt comparedto thedl mt ourbasic rnn mentby bleupoints althoughour rnn isabasic attention basednmt techniques systems parameters nist nist nist nist nist avg dl mt rnn ntm emb ntm emb ntm rnn rnn emb ntm rnn emb convs table to sourcesystemdl mt opensourcetoolkitst tandconvs sforcomparison rememberthatbotht tandconvs layers systems voc en fr existingrnnsystems lstm layers luong pham andmanning lstm layers posunk luong pham andmanning deep att zhouetal deep att posunk zhouetal gnmtwpm wuetal deeplau posunktrainedon mdata wangetal gnmtwpm hyperlstm ha dai andle existingothersystems convs layers bpe gehringetal transformer base vaswanietal transformer big vaswanietal oursystem rnn bpe mce bpe table english to frenchtask bleuscores the rnn isourbasicrnnmodel andthe mce hiddenstatefromrnn externalmemoryinntm usingtheoutputof whatever wegivethecompari sonbetweenourbasic rnn andallim unsurprisingly the emb morein teresting embeddings suchas ntm emb and rnn emb receives thereason how everateachtimestep bythehistorystate ourmce ntm isalmost equaltothe rnn rnn emb isrel ntm emb oneexplanationis beddings basedcontent thememory thatmeans comparedtothernn theexter sition component lastly ourbest modelachieves linesystemand bleupointsoverthedl mt which notingthat thanthet figure resultsonenglish french theresultsonenglish table deepcnnmodeland deepattentionmodel forfaircomparison herewejust on theenglish apromisingfoundingis state of the artsystems els besides oursystemis verycompetitive notingthat vaswanietal where of layersand heads them transformer base systemismoredeeper clearly our trainingdata moreimportant ment signs analysis onthechinese werandomlyse lect ofthesentences inparticular sentenceslongerthan onthetestset shownin clearly inallcurves performancede however ourbest longersentences translationsample table ingcompositionlevels themodel huaren zhiye and gaoshengouzhouzongbu moreover gaojia and maixia fromthetable wendthatone channelmodels rnn and ntm failedtotranslatethe gaojia whilethemulti channelmodelsexcepts ntm rnn gaojia incorrect form ntm rnn is thusaffects inthispaper channelencoder attention toenabletheen besidethe hiddenstateofthernn anddesign ponentsautomatically mtby bleupoints frenchtrans literatures inthefuture ingcomponents suchascnn lecun bengio andothers andself attentivestructure linetal acknowledgements programofchina programno cb we daqizhengandthe references choetal cho vanmerri enboer gulcehre bahdanau bougares schwenk andbengio arxivpreprint arxiv fengetal feng zhang zhang wang andabel memory translation arxivpreprintarxiv gehringetal gehring auli grangier yarats anddauphin convolu source huarenzhiye gaojia maixia gaoshengouzhouzongbu reference chineseestates buys goldman europeanheadquarters athighprice rnn chinesehomebuyers purchase highpricesof ntm chinesehomebuyers buy ntm rnn chinese purchase pricesathighprice tobuy rnn emb china shomebuyers purchased athighprice ntm emb chinese bought athighprice ntm rnn emb chinesehomebuyers purchase athighprice table arxivpreprint arxiv graves wayne anddanihelka graves wayne anddanihelka neuralturingmachines arxiv preprintarxiv ha dai andle ha dai andle hypernetworks corr abs heetal he zhang ren andsun in proceed recognition hochreiter and schmidhuber longshort termmemory neural computation jeanetal jean cho memisevic andben gio arxivpreprintarxiv lecun bengio andothers lecun bengio etal speech andtimeseries networks linetal lin feng dossantos yu xiang zhou andbengio astructured self corr abs luong pham andmanning luong pham andmanning attention arxivpreprint arxiv mengetal meng lu tu li andliu adeepmemory to sequencelearning arxivpreprintarxiv mengetal meng lu li andliu arxivpreprintarxiv schusterandpaliwal schuster andpaliwal ieeetrans sennrichetal sennrich birch currey germann haddow micelibarone andwilliams in proceedingsofthesec volume shared taskpapers sennrich haddow andbirch sennrich haddow andbirch corr abs sutskever vinyals andle sutskever vinyals andle ralnetworks in systems vaswanietal vaswani shazeer parmar uszkoreit jones gomez kaiser and polosukhin corr abs wangetal wang lu li andliu memory lation arxivpreprintarxiv wangetal wang lu zhou andliu tiveunit arxivpreprintarxiv wuetal wu schuster chen le norouzi macherey krikun cao gao macherey klingner shah johnson liu kaiser gouws kato kudo kazawa stevens kurian patil wang young smith riesa rudnick vinyals corrado hughes anddean google sneuralmachine translationsystem machinetranslation corr abs zhouetal zhou cao wang li and xu forward arxivpreprint arxiv 
lock parse ecurrent eural etworks sharannarang sharan baidu com baiduresearch ericundersander undersandereric baidu com baiduresearch gregorydiamos gregdiamos baidu com baiduresearch bstract rnns areusedinstate of the artmodelsindomains machinetranslation andlanguagemodelling spar models endserverpro cessors thespeed we block sparsityinrnns pruning ofweightswithzeros usingthesetechniques block to withsmalllossinac curacy additionally we ourtechnique block sparsernnseliminate ntroduction amodeietal language modeling ozefowiczetal wuetal arearesultoflarge rnns thesemodelshavegrown sohavemodelsizes memoryintensive narangetal hanetal however theseapproachesin ducerandom speed asshownin narangetal narang diamos overheads pathsinmodern processors savingindicesofnon non block sparseformats data attheendoftraining thealgo rithmcreatesablock sparsernn weexaminetheefy arxiv cs lg nov creatingblock sparsernns recurrentunits grus choetal resultsin to densebaseline withour approach wecanalsoreduce parameter retuning parameters furthermore sinceourapproach doesnotrequirere trainingthemodel elated ork hanson pratt lecunetal and hassibietal bothusehessian hanetal and liuetal cnns whilemaintaininghigh accuracy yuetal narangetal and zhu gupta unlikeourtechnique random maoetal use yuetal propose wen etal lstm hochreiter schmidhuber yuan lin kim xing wenetal faraoneetal fanetal tothebestofour knowledge scaledatasets tization micikeviciusetal vanhouckeetal rastegarietal guptaetal andlow rankfactorization deniletal dentonetal implementation lock runing narangetal they intheir work theirpruning inordertoprune blocks blocktozeros figure foragiventhreshold theblock sparse weightmatrix progress anyblocks figure generatingblock table parametersforblock pruning hyper start itr ramp itr pruning startof oftotalepochs end itr rameters startof oftotalepochs start slope old seeequation ramp slope rampiteration to freq isupdated theendoftraining narangetal usesixhyper table forthesehyper parameters the start slope and ramp slope inordertodetermine start slope toachieve sparsity theyassign assuming is theyuseequation todetermine freq ramp itr start itr end itr ramp itr forblockpruning weneedtomodifythe start slope ablock start slope wecalculate start slope forweightpruning usingtheequation given forblockpruning to parametersis the pruninghyper recurrentweightlayer andlinear fullyconnectedlayer roup asso egularization theweightsinagroup modelintoblocks foreachblock normoftheblock training where isablockofweights isthe normoftheblock and isthetotalnumber ofblock ouruseof yuan lin as kn hardzeros thus structuredsparsity we thus pruning ends oftrainingepochs asdiscussedinsection weights known regularization inappendix weightpruning xperiments amodeietal ctc layer gravesetal thebaseline rnnmodel rnndense consistsof millionparameters threerecurrentlayers thebaselinegrumodel grudense consistsof millionparameters modelsconsistsof hoursofenglishspeech hours ofdata cer consisting of hoursofenglishdata block pruning bp grouplasso gl glp wepruneweights intherecurrentlayers biases batch besidespruninghyper parameters and nootherhyper themodelsare sgd withmomentum allmodelsaretrained for epochs insection blocks section section discussestheimpact lock parsity initially usingbpandglp weareabletoreduce asshownintable thesparsernn modelwith table blocks model inmillions rnndense rnndense rnnsparse bp rnnsparse glp rnnsparse bp grudense grudense grusparse glp grusparse bp of whilelosingonly of accuracy model secondly wetrainadensemodel with thesparsemodels table finally racy forrnnmodels and asshownintable thernnsparse isonly the and sparse and respectively similarly pruningthe grumodelwith modelby sizebyapproximately ourresultsalso roup asso ariants table forbothrnn modelswith and hiddennodes table blocks model inmillions rnnsparse gl rnnsparse glp rnnsparse gl rnnsparse glp orhigher weneedtoset for instance ofapproximately table block paramsrelative modelsize inmillions sparsitycerperf rnnsparse rnnsparse rnnsparse rnnsparse rnnsparse rnnsparse grusparse grusparse grusparse anyregularization grouplassocould inthe glpexperiments weightstozero ofsparsity lock ize ariation table and and aswell advantageofarraydata therefore achievelowersparsity erformance indexingover head ii andii data paths allofwhichare indexingoverheads zerovalue forexample thecompressed sparse row csr foreachnon zerovalue size using bitindicesincurs bitsofoverheadpernon kx bits asin micikeviciusetal thisisa overhead forexample usinga blocksizeof andusingablocksizeof reducesthe overheadtolessthan cacheslines dramrowbuffers bytesforcachelines kbforadramrow block sparseformatsstore arraydata paths fine data paths such asthe nvidia orthe units jouppietal theseunits forexample onthevoltav gpu theyenableupto speed upforrnn layermatrixmultiply speed upforgru layermatrixmultiply figure speed rnnmatrixsizes are with sparsityand batch size grumatrixsizesare with sparsityand batch size wp and blockpruning bp simddata paths hardwaredata pathsize orgreateronv figure showsthatblock largebatchsizes inthiscase thespeed upthan blocks furtherinvesti iscussion runing haracteristics infigure wp narangetal pruning iterations intheseexperiments weusethe max pruning sparsityjustbefore iterationswhichis figure the sparsemodeldoes worsethanthe sparse forthemodelwith sparsity only baseline however forthelayerresultsin neuronshavingall zerooutputweights additionally zerooutput weights varyinghyper to sparsity fortheseexperi ments insteadofthetestset therefore asshowninfigure than to beyond resultsin ormoreaccuracyloss withblocksparsity forblocksize figure figure glpandbp models figure blocks figure figure andwpfor model are cappedat figure model prunedusingbpandwp lossof orhigher similarly forblocksof have ormoreaccuracyloss thisindicatesthat parsityvs ayers figure allrecurrent parameters layer isthe wesee increasingsparsity additionally theblocksizebeyond onclusionand uture ork ingwecanbuildblock the block requirements block therebyallow mentefblock upduringdeployment cknowledgements mohammadshoeybi relatedtothiswork eferences darioamodei rishitaanubhai ericbattenberg carlcase jaredcasper bryancatanzaro jingdongchen mikechrzanowski adamcoates gregdiamos etal deepspeech end to englishandmandarin in proceedingsofthe pp kyunghyuncho bartvanmerri enboer caglargulcehre dzmitrybahdanau fethibougares holger schwenk andyoshuabengio machinetranslation arxivpreprintarxiv mishadenil babakshakibi laurentdinh marc aurelioranzato andnandodefreitas predictingparame tersindeeplearning corr abs url http arxiv org abs emilydenton wojciechzaremba joanbruna yannlecun androbfergus corr abs url http arxiv org abs qinweifan weiwu andjacekmzurada springerplus julianfaraone nicholasfraser giuliogamberdella michaelablott andphiliphwleong compressinglow arxivpreprint arxiv alexgraves santiagofern andez faustinogomez andj urgenschmidhuber in proceedingsofthe rd pp acm suyoggupta ankuragrawal andpritishnarayanan numericalprecision in proceedingsofthe icml pp songhan huizimao andwilliamjdally deepcompression pruning arxivpreprintarxiv stephenjos chaptercom propagation pp morgankaufmann publishersinc sanfrancisco ca usa isbn url http dl acm org citation cfm id babakhassibi davidgstork andgregoryjwolff in neuralnetworks pp ieee sepphochreiterandj urgenschmidhuber longshort termmemory neuralcomput november issn doi neco url http dx doi org neco normanp jouppi cliffyoung nishantpatil davidpatterson gauravagrawal raminderbajwa sarahbates sureshbhatia nanboden alborchers rickboyle pierre luccantin cliffordchao chrisclark jeremy coriell mikedaley mattdau jeffreydean bengelb rajendragottipati williamgulland roberthagmann richardc ho doughogberg johnhu roberthundt danhurt julian ibarz aaronjaffey alekjaworski alexanderkaplan harshitkhaitan andykoch naveenkumar steve lacy jameslaudon jameslaw diemthule chrisleary zhuyuanliu kylelucke alanlundin gordon mackean adrianamaggiore mairemahony kieranmiller rahulnagarajan ravinarayanaswami ray ni kathynix thomasnorrie markomernick narayanapenukonda andyphelps jonathanross amir salek emadsamadiani chrissevern gregorysizikov matthewsnelham jedsouter dansteinberg andy swing mercedestan gregorythorson botian horiatoma ericktuttle vijayvasudevan richard walter walterwang ericwilcox anddoehyunyoon in processingunit corr abs url http arxiv org abs rafalj ozefowicz oriolvinyals mikeschuster noamshazeer andyonghuiwu exploringthelimitsof languagemodeling corr abs url http arxiv org abs tree yannlecun johnsdenker saraasolla richardehoward andlawrencedjackel optimalbraindamage in nips volume pp baoyuanliu minwang hassanforoosh marshalltappen andmariannapensky sparseconvolutional neuralnetworks in pp huizimao songhan jeffpool wenshuoli xingyuliu yuwang andwilliamdally exploringthe micikevicius narang alben diamos elsen garcia ginsburg houston kuchaiev venkatesh andh wu arxive prints october deepbench https svail github io deepbench update accessed sharannarang gregorydiamos shubhosengupta anderichelsen networks arxivpreprintarxiv nvidia nvidiateslav gpuarchitecture technicalreport mohammadrastegari vicenteordonez josephredmon andalifarhadi xnor net imagenetclas pp ing cham isbn doi url https doi org vincentvanhoucke andrewsenior andmarkz mao in nips weiwen chunpengwu yandanwang yiranchen andhaili networks in pp weiwen yuxionghe samyamrajbhandari wenhanwang fangliu binhu yiranchen andhaili termmemory arxivpreprintarxiv yonghuiwu mikeschuster zhifengchen quocv le mohammadnorouzi wolfgangmacherey maxim krikun yuancao qingao klausmacherey jeffklingner apurvashah melvinjohnson xiaobingliu lukaszkaiser stephangouws yoshikiyokato takukudo hidetokazawa keithstevens georgekurian nishantpatil weiwang cliffyoung jasonsmith jasonriesa alexrudnick oriolvinyals gregcorrado macduffhughes andjeffreydean google corr abs url http arxiv org abs dongyu frankseide gangli andlideng in icassp pp ieee jiecaoyu andrewlukefahr davidpalframan ganeshdasika reetuparnadas andscottmahlke scalpel in proceedingsofthe thannualinter pp acm mingyuanandyilin mingyuanandyilin journalofthe seriesb toprune ornottoprune pression arxivpreprintarxiv and egularization weconsidered and regularizerstoinduce sparsityinthenetwork regularizationisas training where notethegradient sgn weexplore pruning theweightpruning wp algorithmfrom narangetal isusedalongwithregu larization eithertoguide wealsoexplore training fanetal uses thegradientfor regular izationis ourexpectationisthat forour and experiments weusethedeepspeech describedinsection hours hours table and hiddenunits model inmillions rnndense rnnsparse weightpruning rnnsparse rnnsparse withpruning rnnsparse withpruning withoutpruning combining the withpruningmodel comparingthetwo regularizers moresuitableasa regularizer orboth 
the approachesforlarge scalevideo yunlongbian chuanggan xiaoliu fuli xianglong yandongli hengqi jiezhou shileiwen yuanqinglin baiduidl tsinghuauniversity abstract stplace mostofexistingstate of the artvideorecog to endpipeline themerit network instead to endscores softmaxscores the shelfmachine termbeenig inthissubmission weexten particularly weinvestigatefour multi temporalxception network multi forward sequencemodel proachesinthelarge most remarkably ourbestsinglemulti networkcanachieve intermoftop accuracyand intermoftop introduction learning smartphones surveillancecameras etc havecreated ithasthere recentap correspondingauthor cnns and orrecurrentnetworks haveachievedstate of the artresults however duetothe scaledata whilelarge addressedproblem toremedythisissue googledeep scalevideodataset namedas kineticsdataset whichcontains kvideoclipsof humanactionclass egyofdevnetframework particularly welearn thebasicrgb usingthevideos shelftemporalmodels namelymulti temporal xceptionnetwork multi forwardsequencemodel modelingapproaches andleadtothestate of the ble section sec tion approaches section followed andthe temporalparts visualfeature asin arxiv cs cv aug figure multi tureextraction andfoundinception resnet outperformsothersin thergbmodelis initializedwithpre izedfromthergbmodel inspiredby the level training duringtesting eachframesinthevideo acousticfeature weuseconvnet basedaudiosystem ms frames formation tion image image model wetrainedtheacous off inthissection work wealsorefer forthedetails ofmulti forwardsequence model modeling forexample tures inorderto nition shiftingattention theout iscalculatedthrough tures satt where softmax wx islearnablevector and arelearnablescalars and isahyper bution invariance theshift tributions satt which wedescribenext multi we suchasappearance figure rgb motionw andaudiosignals althoughtheat be dimensionsandscales instead weproposemulti group multi first for eachfeatureset weapply tions andthenwecon catenatetheoutputs next finally fully connectedlayer has racysimultaneously recently to inthiscompetition we mance tureisshowninfigure zero valuedmultimodalfea seg mentsforeachvideo whichisconsist fi nally andfedintothefully connectedlayerfor experimentresults trainingvideos validationvideosand testingvideos each videoisinoneof categories table tiondataset fromtable model modality top accuracy top accuracy inception resnet rgb inception resnet flow vgg audio latefusion rgb flow audio multi streamsequencemodel rgb flow audio fast forwardlstm rgb flow audio rgb flow audio rgb flow audio ensemble rgb flow audio table thevideo quencemodels lstm ture conclusions inthiswork scalevideo recognitiontask theensembleofour mancefurther bereleasedsoon references cho vanmerri enboer bahdanau andy bengio encoder decoderapproaches arxivpreprintarxiv chollet xception bleconvolutions cvpr gan wang yang yeung anda haupt mann devnet in cvpr pages gan yao yang yang andt mei youlead weexceed labor cvpr gehring auli grangier yarats andy dauphin arxivpreprintarxiv hershey chaudhuri ellis gemmeke jansen moore plakal platt saurous seybold slaney weiss andk wilson cnn scaleaudioin arxiv preprintarxiv hochreiterandj schmidhuber longshort termmemory neuralcomputation kaiser gomez andf chollet depthwisesep arxiv preprintarxiv karpathy toderici shetty leung suk thankar andl fei fei large scalevideowith in cvpr kay carreira simonyan zhang hillier vijayanarasimhan viola green back nat sev etal arxiv preprintarxiv li gan liu bian long li li zhou ands wen scale youtube mvideounderstanding arxiv lin feng nogueiradossantos yu xiang zhou andy bengio astructuredself attentivesen tenceembedding arxive prints mar simonyananda zisserman two streamconvolutional in nips simonyananda zisserman verydeepconvolu corr abs srivastava mansimov andr salakhutdinov un icml szegedy ioffe vanhoucke anda alemi inception inception resnetandtheimpactof in arxivpreprint arxiv tran bourdev fergus torresani andm paluri iccv vaswani shazeer parmar uszkoreit jones gomez kaiser andi polosukhin attentionisall youneed arxive prints june wang xiong wang qiao lin tang andl gool towardsgood in eccv xie girshick doll ar tu andk he aggregated cvpr 
dureader real worldapplications weihe kailiu jingliu yajuanlyu shiqizhao xinyanxiao yuanliu yizhongwang huawu qiaoqiaoshe xuanliu tianwu haifengwang baiduinc beijing china hewei liukai liujing lvyajuan zhaoshiqi xiaoxinyan liuyuan wangyizhong wu hua sheqiaoqiao liuxuan wutian wanghaifeng baidu com abstract anew large scale open domainchinesema mrc dataset world mrc data sources dao questiontypes itprovidesrich especiallyyes researchcommunity scale itcontains kquestions kanswersand documents mrcdatasetsofar experimentsshow currentstate of the artbaselinesystems tomakeimprovements tohelpthe bothdureader andbaselinesystems havebeenpostedonline wealsoorganize sincethe releaseofthetask thereare introduction mrc rajpurkaretal zhidao https zhidao baidu com isthe cqa siteintheworld http ai baidu com broad download dataset dureader https github com baidu dureader nguyenetal inrecentyears asshownintable advancessuchasmatch lstm wangandjiang bidaf seoetal aoareader cui etal dcn xiongetal andr net wangetal thispaperhopesto dureader moretypesof table highlightsdureader sadvantages scale tables highlightdureader sadvantages ideally however manyex ouscompromisessuchas clozetask data thetaskis hermannetal cuietal hilletal multiple choiceexams richardsonetal multiple lai etal collectthemultiple choicequestions fromenglishexams crowdsourcing turkers aregivendocuments articlesfromthenews and orwikipedia trischleretal rajpurkaretal ko cisk yetal ms marco nguyen etal isbasedonbinglogs inenglish anddureader thispaper isbasedonthelogs ofbaidusearch inchinese besides question sources marco questiontypes arxiv cs cl jun datasetlang que docssourceofque cnn dm hermannetal en hlf rc cuietal zh ksyntheticclozefairy newsfillinword cbt hilletal en sbooksmulti choices race laietal en choices mctest richardsonetal en choices newsqa trischleretal en squad rajpurkaretal en crowdsourcedwiki spanofwords searchqa dunnetal en mqasitewebdoc spanofwords trivaqa joshietal en ktriviawebsiteswiki webdoc span substringofwords narrativeqa ko cisk yetal en kcrowdsourcedbook moviemanualsummary ms marco nguyenetal en userlogswebdoc manualsummary dureader thispaper zh muserlogswebdoc cqamanualsummary table datasources search baiduzhidao questiontypes and scale kquestions kanswersand documents each ion inparticular itannotatesyes noandopin user squestions sizedfacts wellasfacts methodsthatan spanselection entities butitisless appropriateforyes tions documentsources dureadercollectsdocu wellasbaiduzhidao allthecontentinbaidu makingitdifferent itisinterestingto search questionan sweringcommunity additionally previouswork rajpurkaretal orafewpassages nguyenetal to whiledureaderpro thatcontainsalot foreachquestionto generateanswers tion answers tion datascale tains kquestions mdocumentsandmore than khuman summarizedanswers tothe bestofourknowledge dureaderisthelargest pilotstudy logsofasearchengine apilotstudywasper questionqueries baidusearchlogs eachquestionwas manuallyannotatedas either fact or opinion and oneof entity description or yesno regardingto entity questions theanswersare whiletheanswersto description questionsare usuallymulti sentencesummaries the descrip tion questionscontainhow whyquestions com jects its demeritsofgoods etc asfor yesno questions af we fact or opinion afactoranopinion table givestheexamplesof importantissues table showsthatallsixtypes baidusearch focusonfact entityandfact descriptionquestions asshownintable fact factopinion entity iphone moviesof description àh yesno is table withglossesinenglish previousdatasets havefocusedonfact entityandfact description factopiniontotal entity description yesno total table selectionmethods forfact entityandfact description opinionsand yes about and ofthesample respectively ofthesam ple fact ofthesample fromthistable wecan seethatopinions arecommoninsearch logs yes with onehalfaboutfact selectionmethods forfact entityandfact description butitisprob es peciallyforyes this andleavelarge datacollection sampleof lectionof kquestions tuples where isaquestion isaquestiontype and isananswer weneedtocol lectasetofquestions fromsearchlogs accord ingtoourestimation thereareabout ques itwouldtaketoomuch time queryinsearchlogs hence werandomly anduseapre trained withrecallhigher than fromsearchlogs then since eachquery theexpertswill theaccu racyofworkers annotation judgedbyexperts is higherthan initially wehave mfrequentqueriessam pledfromsearchlogs theautomati callyselected kquestionqueries afterhuman annotation thereare kquestionqueriesleft eventually weuniformlysampled kquestions fromthe kquestionqueries by baidusearch andbaiduzhidao zhidaocontains user sincethetwo wede kuniquequestions intotwosubsets ducethetop asingleparagraph rajpurkaretal orafew factopiniontotal entity description yesno total table dureaderissimilarto butdifferentfrom thepi lotstudy table but notfromthepilotstudy toreducetheburdenon theannotators passages nguyenetal inthiscase para graphselection containinganswers texthasbeentokenized asmentionedabove questionintwopasses thepass entity descrip tion and yesno questions thesecondpassclassi questionsaseither fact or opinion table table butdif ble butnotfromtable wedon twanttobur frequentquestions tionsindureader thatsaid bothtablesagree aspointedout above entityandfact description whileleaveslargeop portunityonyes answerannotation crowd documents he answersinhis marizingthedocuments ifnoanswerscanbe theannotatorwas ifmorethanone http ai baidu com tech nlp lexical insomecases asingleanswer notethattheanswersto entity questionsand yesno theanswersto the entity seetheex ampleintable theboldwords green gray yellow pink question theanswerstothe yesno yes no or depend theboldwords yesanddepend aretheopiniontypesby thesecond noquestionisn sosimple includingnotonly yes and no butalso depends dependingoncontext supportingsentences qualitycontrol ofthisproject man hoursdistributedover about workersand experts when annotatinganswers swerquality theaccuracy ofworkers judgedbytheexperts basically therearethree theworkers theexpertswillre andthey answersarewrong theaccuracy judgedbythe experts groupsaccording the againbyotherexperts iftheaccuracyislower than theloop training question questiontype entity fact answer ÿÿÿ rrr ppp rrr äää rrr rrr ff ff ff ff ff ff ff green gray yellow pink pinkforbachelor oflaw pinkforbachelorofart document ff ff document question questiontype yesno opinion answer yes yes teethdo answer depend depend notalways forexample theone oftencauses document àh àh document table examplesfromdureader aswellas supportingsentences training thetraining and questions and doc uments and answers respec tively dureaderis relatively challenging figures dureader thenumberofanswers onemightthinkthat andonlyone an swer butfigure questionsin whilethe wherethereis andcon sequently answerset meanwhile wecanseethat of but inbaiduzhidao in thelatercase lenge theeditdistance onemightalsohavebeen tempted basedonpriorwork tostartwithaspan selectionmethod manyofwhich suchas squad rajpurkaretal newsqa trischleretal figure afewquestionshaveone andonlyone answer especiallyforzhidao andtriviaqa joshietal how ever since tomea surethedifference med be sourcedocuments alargemedmeansthatan figure relativelyfar ineditdistance fromsourcedoc uments comparedtomsmarco generateananswer figure compares dureaderandms marcointermsofmed tomsmarco thedocumentlength indureader ques tionstendtobeshort wordsonaverage com paredtoanswers words andanswerstend words onaverage marco nguyen etal asop posedtoparagraphs experiments inthissection of the artmod els furthermore ourdataset baselinesystems dureader containmulti paragraphsormulti passages while graph rajpurkaretal orafewpas sages nguyenetal toextractorgenerate answers nguyenetal of the art selction therewillbeefyissues toim select and applythestate of the artmrcmodelson paragraphselection inthispaper intrainingstage ifthepara atedanswer graphforeachdocument then mrcmodelsde intestingstage atedanswer question then answerspanselection of the artmodels match lstm match lstmisawidelyused studies wangandjiang toananswer inaparagraph ingofanattention finally anan intheparagraph bidaf and seoetal itusesbothcontext to questionattention andquestion to text afterthat theso systems bleu rouge bleu rouge bleu rouge selectedparagraph match lstm bidaf human table bleu rouge goldparagraph match lstm bidaf table werandomlyinitial foralllay ers kingmaandba learningrateof andabatchsizeof resultsandanalysis character levelbleu papinenietal and rouge lin the ble forcomparison wealsoevaluatethese wealsoassess swerastheprediction graphbaseline ofthesemodels however zhidaodata comprehendopen community incontrast theperformanceofhu ference sreadingskill asdescribedinsection themostrelevant testingstage linemrcmodels were evaluateoursystemson thegoldparagraphs theexperimentre comparingta ble withtable performance moreover directlyusingthegold lscore itmeetstheexception becauseeachgoldpara rouge though wethatthebaselinemod tobleu selecttheanswers applications orpassage incontrast dureaderprovidesthe researchinareal worldsetting ourdataset wecansee while yesno this however the answersto yesno whichcouldbeasimple yes or no insomecases questiontype bleu rouge bleu rouge bleu rouge match lstm bidaf human table however human factopinion bleu rouge bleu rouge opinion unaware opinion aware table performanceofopinion awaremodelon yesno questions opinion awareevaluation yesno ques tions wefoundthatit thesequestions forexam ple itﬂandﬂyoucan withthesemetrics however asdescribedinsection especiallywhen inreal worldset tings wedon yes or no totacklethis rallanguage we insuchcases ev yes no or depend bytheirlabels finally byus ingthisopinion amodel higherscore opinionpolarities weadda weslightly changethematch lstmmodel inwhichthe connectedlayer thisistrainedwiththe bels we also classifyingthe discussion thecurrentstate of the thereisconsider first suchasyes requiringmulti documentmrc newmethods cross sentence reasoning andmulti hopefully dureader second whichresultsingreat paredtogoldparagraph sperformance itisnec worldmrcproblem third thestate of the artmodelsformulate however humanbe howtosummarize forth itisfar provement forexample weannotateonlyopin iontagsforyes noquestions wewillalsoanno tions overall worldread wehopethatthe mrcresearch asharedtask theresearchcommunity weorganizeanonline competition onlinewebsite there forexample ateamobtained rouge lon ourdataset the with rouge andhumanperformance with rouge hasbeenreduced itis toclose conclusion readingcomprehension mrc dureaderhas datasources questiontypes fact opinion entity description yes no and scale sincethereleaseof thetask https ai baidu com broad leaderboard dataset dureader acknowledgements wewouldliketothankdr kennethwardchurch paper prof per commentsonthiswork references yimingcui zhipengchen siwei shijinwang tingliu andguopinghu attention over sion in proceedingsof thannualmeetingofthe pages yimingcui tingliu zhipengchen shijinwang and guopinghu consensusattention basedneu matthewdunn leventsagun mikehiggins ugur guney volkancirik andkyunghyuncho searchqa anewq arxivpreprint arxiv karlmoritzhermann tomaskocisky edward grefenstette lasseespeholt willkay mustafasu leyman andphilblunsom teachingma in advancesinneu pages felixhill antoinebordes sumitchopra andjason weston reading children tions arxivpreprintarxiv mandarjoshi eunsolchoi daniels weld andluke zettlemoyer triviaqa alargescaledistantly sion corr diederikp kingmaandjimmyba adam corr tom sko cisk jonathanschwarz philblunsom chrisdyer karlmoritzhermann abormelis thenarrativeqa arxivpreprint arxiv guokunlai qizhexie hanxiaoliu yimingyang andeduardhovy race large scalereading arxiv preprintarxiv chin yewlin rouge apackageforautomatic in textsummarization branchesout proceedingsoftheacl work shop pages tringuyen mirrosenberg xiasong jianfenggao saurabhtiwary ranganmajumder andlideng msmarco arxivpreprint arxiv kishorepapineni salimroukos toddward andwei jingzhu bleu in proceedingsof tationallinguistics pages pranavrajpurkar jianzhang konstantinlopyrev and percyliang squad questions arxivpreprint arxiv matthewrichardson christopherj burges and erinrenshaw mctest achallengedataset fortheopen text in proceedingsofthe conferenceonem pages minjoonseo aniruddhakembhavi alifarhadi and hannanehhajishirzi corr adamtrischler tongwang xingdiyuan justinhar ris alessandrosordoni philipbachman andka heersuleman newsqa amachinecompre hensiondataset in proceedingsofthe ndwork pages machinecom prehensionusingmatch lstmandanswerpointer in iclr pages wenhuiwang nanyang furuwei baobaochang andmingzhou gatedself matchingnet answering in proceedingsofthe thannual guistics volume longpapers pages caimingxiong victorzhong andrichardsocher answering in 
pages nagoya japan october 
proceedingsofthe pages lisbon portugal september 
proceedingsofthe emnlp pages october doha qatar 
towardsanonlinehuman ruiyan andtechnology icst pekinguniversity beijing china yanrui baidu com yipingsong departmentofcomputer scienceandtechnology pekinguniversity beijing china songyiping pku edu cnxiangyangzhou baiduinc no xibeiwangeastroad beijing china zhouxiangyang baidu com huawu baiduinc no xibeiwangeastroad beijing china wu_hua baidu comabstract puterscience tionretrieval anddatamanagement etc andto maintainarelevant meaningful the empoweredbydata drivenapproaches wecannoworganizethe inthispaper weintro giv andef areal wehave twoscenariosofsingle turnandmulti turnconversations inour system formation forsingle turn andacontext awareranking formulti turn getherinoptimization weinvestigate posedmethods ofp map andndcg keywords human computerconversation bigdata rankoptimization tiononthepage acmmustbehonored tocopyotherwise orre publish and orafee acm org cikm october indianapolis in usa acm isbn doi http dx doi org introduction itisbelievedtobechal meaningfulandcon retrieval ment researchfordecades ratherthaninsci stocutting andindustry velopmentofhuman owingtothe wearelikelytolearn whattoreplygiven almost data forreal timeservices drivenback groundforourstudy where ﬁpracticalﬂmeansboth effectiveness andef thesetwois human years orientedcon versationsystems inadomain routeenquiry thedomainofthesystem itisonlyrecently thatnon task orienteddialogue open domainconversation social andenter tainmentroles inthispaper we achatbot intheopendomain buildinganopen firstly sincepeople query replyw ocontexts query replyw contexts figure takeashortmulti turn turns conversationfor example context secondly thepossi conventionalhand humanutterances asmentioned temsinopen domainconversations web aretrieval andthenreturnsareply whichisa greatadvantage tothisend basedonretrieval and rankingbasedmethod ingeneral single turnand multi turnconversa tion asingle multi turn conversation versation amulti going formulti turnconversa tion information namelyﬁcontextsﬂ single tounderstand nornecessary turnscenarioinfigure wherecon basedontheobser vations single turnandmulti namely baseranking andcontext awareranking inthispaper weintroduceadata driven real timeconversa users andthe we cal tosummarize the effective computercon and turnandmulti turnconversationsce nariosinopendomain the issuefor shallowlearning anddeeplearning awareranking weinvestigatethe trade effectiveness andef ternativemethods comparedwith state of the artapproaches ing runningtime mance insection thenweintro includingpipelinecom ponents including experimentalsetups sions relatedwork these rule orto handcrafttherules theconversation tolearnandinfer however later theneedforabigamoun tem re deep questionand answeringsystem inthisway berofquestion answerpairs leuski etal buildsystemstos question question answerpairs question thenum either language textunderstanding computerconversation nowadays resources cqa tabecomesavailable onmicroblogdata higashinaka etal alsocombinetemplate ritter etal have smt techniques aswellas in theapproach notretrieved fromarepository figure includingbothoff lineandonlineprocess thecandidatesare ifany andcombinedboth recently basedconversationsys tems to sequencemanner nrm is us rnn thesingle turncon turnconversation atten conversations thesegeneration whatismore neu common placeresponses idon tknow regardlessoftheinput retrieval ji etal intro textconversation opedasalearning to these methodsareforsingle withnocontextin inourpreviouswork wehaveproposed adeeplearning to informationformulti turnconversations lategoodmodels investigatingtheis suesof effectiveness andef platform andsharethehands gel weprovidethereal timeservice systemframework searchandretrieval and roughly wehaveanoff lineprocess andanonlineprocess section table replies posting wishapairof glassesasmygift reply iwillofferoneforyou reply itcanberecovered relax table weseparatethe postandrepliesas hposting replyipairs inthisway weprovide conversations posting atanﬁoldﬂage wishapairof glassesasmygift reply iwillofferoneforyou posting atanﬁoldﬂage wishapairof glassesasmygift reply itcanberecovered relax off lineprocess datapreparation peopleinter inbaidu inc forbaidusearchengine versationdata websitesorforums postingmessagevis ibletothepublic replies inresponse totheirposting time wecollectthe hposting reply ipairs weshowsome oftheexamplesintable istypicallyunique toagivenmessage tions withdifferent wethenseparate theposting repliesintoagroupof posting reply pairs eachwith pletingahuman wethenpre processtheobtainedre in high qualitycon versationrecords incompletesessions rude orevendirtywords besides we out of whichwebelievethey http www baidu besides wealso afterdatacleaning whichsupportsregu larupdates weeklyormonthly resources by lineprocess hposting reply onlineprocess searchandretrieval thelight unlikethetraditional thecon henceweincorporate virtualdocument toretrieveactually consistsofapairof postingandreply eventhoughweonlyneed line ingprocedure orrepliesandthen tooptimizetheranking theranking narioofhuman theuser theusercontinuesanon goingcon versationand twoscenarios eithersingle turnconversation ormulti turncon correspondingly wedesigntworankings namelybaseranking forsingle turnsand context awareranking formulti turns weoughttouseasucces withallthesemetrics weareableto moreimportantly sinceweprovidereal itisnaturalthat ascontextgrows we almost thesame ratherthan wenextpro problemformulation computerconversa computer giventhemessage issuedfromhuman tothecomingmessage our wepropose table turnconversation andmulti single turnmulti turninputs qnqn coutputs rnrnobj func argmaxrnf rnjqn argmaxrnf rnjqn turnandmulti turnconversationus awareapproach andexpect wehavethe followingintable hposting reply ipairs fora single turnconversation given aparticularquery ina multi forthereply rninthe thturn asingle thquery qnwhileamulti fq ngiventhecontext rankings archive orrepliesand single turnwithoutcontexts andcontext awarerankingdur ingthisstep thecandidatereplies formation qasthecontexts contextselection firstly lectionissue session neither weherebyuse utilizedasﬁcontextsﬂ wedeployatexttiling like seg sﬂintheconversation withqasthecontexts baseranking replies orrepliesandthen givenaquery weobtain arankinglist ofthetop krankedreplies rq fr apermuta tion orranking each xiwhereeach xi rjxjhasjxjdimen sions arankingfunction rjxj thereplies and ri rj rjintherankinglist rijq rjjq sinceitisbasedons ingleturns theshallow deep throughthisprocess we thisrankinglistisre qonly weomitthesubscriptof qanduse rtorepresent tillnow andhence createthebaseranking however thecontext insensitiveinfor oofmulti awareranking context awareranking wealsohaveavector basedcontext and hencetherankings likewise deepranker awareranking weoptto then theconver torespond next rankers shallowrankerv deepranker inthissection replies shallowlearning fortheshallowranker awarerankingforthe retrievedreplies at queries replies andpostings term basedrepresentation mostintuitively werepresentthe inthisway thecontentof hˇ wjdj iwhereˇ wi weevaluatetheter tf idfmeasurement which termimportance twotermvectors topic basedrepresentation weapplythe todiscovertopics we weempiricallytraina wealsocalculate entity basedrepresentation ofterms inthisstudy wedistinguish persons locationsandor conversation baidusearchlogs measuredby betweentwoentity based forpointwisefea tures eitherbysimilarity metric query replymatchingscore cosinesimilarity and context replymatchingscore similarly wecalculatethematch calculatedby query postingmatchingscore hposting reply ipairs calculatedbythe matchingmetrics context postingmatchingscore again wecalculatethematch metrics smt ifwetreat queries wecantraina onthetrainingcorpora weonlycal culatebasedonterm basedrepresentation languagemodel onlargenewsdata websites averagetermweighting tf length tooshort valueto fluency havealargeco occurrencelikelihood wecalculatetheco occurrence probabilityforthebi averagevalueasthey thefeaturesare empiricallyhand incontrastto deeplearning onhand inrecentyears deepneuralnetworks dnns alsoknownas deeplearning have madeimprovement withbigdataavailable dnnsare ofnon lineartransformation asmentioned wehavetwoparts hposting reply tocom parewiththequeryand orthecontext putsascalarin infora the baserankingpart orthecontext thecontext awareranking part awareranking inparticular wordembeddings adiscretetoken thus wouldbelost wordembeddings neuralnetwork basedtextprocessing awordismappedtoalow dimensional real valuedvector thisprocess knownasvector ization givenenoughdata usage andcontext embeddingscan hot uptable inourmodel dings andthentunedduring bi directionallstm weuseabi memory bi lstm longthewordsequence rnn keeps ahiddenstatevector timestep sequence givenasequence ofinputs input forget and outputgates denotedas it ft and otrespectively thevector givenaninput sentences fx tg where xtisthewordembedding atposition tinthesentence htforpositiont givenby itftotlt tanh wht et ht ft ht itlthst ot ht where eisaknownasa sigmoid logisticfunction thewordtothelast thevariantbi lstm backward htand ht respectively thwordinthesentence ht ht hti convolution cnn thestructureof unlikernns cnnsonly sizem concretely lstm for minbi lstmoutputvectors ht ht where tisacertainposition theconvolutional willgeneratea ﬂbetweenthe twovectors moreformally of tanhm xi inpractice wealsoaddascalarbias tion inthisway weobtainthevector ofisavector eachdimen volution infact multiplefeaturemaps fandb pooling concatenation andmatching onthebasisofsen lstmwithcnn wecanmodelthe inparticular amaxpooling alongthesequence query orcon text andreply query orcontext we query context posting andreply ingmatrice which by wordmatching orattentionmethods layer fully connected feed forwardneuralnetwork alsoknownas multi layerperception mlp startingfromlower higher levelones finally tweenaquery orthecontext andareply thescoringneuron rankwithoptimization aware ranking oftherankedlists then outputtorespond aware rankings let bbethebaserankingand cbethecontext aware ranking andalsoesti matetherank ri ri weaimatopti jxi bijj ri ijjjj ri bijj jxi cijj ri ijjjj ri cijj ciisthecontext aware rankingscore whichwillbelater among thecomponent we use jj ri ijjjj ri bijj insteadof ri ri inorder ranks context awarerank ourgoalisto ri argmin tominimize ri bii ri ri cii ri ri let ri weget ri ri bi ci indicatingweonly usethebaseranking single turnwithoutcontexts ranking we bi ci inthisway additionalcontext awarerankingscore weplugequation into equation scoreof context awarerankingby ri ri togetherasabatch ratherthanasentence by sentencestyle which inthissection resource evaluatethe evaluatetheefy and investigatethetrade experimentalsetups dataset asmentioned variousforums microblogwebsites baiduzhidao doubanforum baidutieba sinaweibo etc intotal thedatabasecontains million hposting reply ipairs neuralnetworks forvalidation and fortesting it databaseforretrieval learningregime weran validationwasbased http www zhidao baidu com http www douban com http www tieba baidu com http www weibo com table datastatistics source posting reply vocabulary zhidao douban tieba weibo misc total priatenessof each samplewasjudgedby theappropriateness ifany ﬂindicatesan inappropriateone hyperparameters inourproposedmodel weused dimensionalwordembed dings ing wordsegmentation phras occurrences thebi directionallstmhas sion cnnis weused withamini batchsizeof foropti mization propagation initial learningratewassetto wasapplied we evaluationmetrics giventherankinglists for testqueries ingmetrics precision meanaverageprecision map ndcg istheprecision atthe stposition replyretrieved besides wealsoprovidedthetop usingndcgandmap weaimed formally ndcg jtjxq zkxi ri log kdenotesthetop kposi tionintherankinglist and fromaperfectranking thcandi appropriate inappropriate mapiscomputedby map jtjxq nqkxi and piistheprecisionat table for microsoftxiaoice andgenerativemethods wereportthetime costperquery effectivenessmetrics efy inmilliseconds systemp mapndcg ndcg trainingtestingsmt ritteretal ššš nrm shangetal ššš lstm rnn sutskeveretal ššš microsoftxiaoice ššššˇ randommatch šˇ okapibm šˇ deepmatch luandli lstm rnn palangietal arc cnn huetal dl yanetal rocfindeepmatch rocfinlstm rnn rocfinarc cnn rocf shallowranker rocf deepranker weincludeseveral thebaselines generation basedmeth odsand retrieval veryrecentstudies retrieval basedmethod generation basedconversation put smt smtisamachinetrans ifwetreatqueriesand based lstm rnn lstm work rnn lstm archi tecture weuse lstm forgen eration weuseanlstm query toavectorspace rnntode reply forretrievals weadoptthelstm spondingmachine nrm proposedin whichisanrnn localattentionschema retrieval basedconversation whichreturnthebest aparticularquery based weselect strongretrieval randommatch beawareitisnot the okapibm rankcandidatereplies foreachquery vantreplyusingbm model fromthecorpus deepmatch obtainedvialda arc cnn deeplearning to respond dl weproposethedl workin dl turn andmulti industryapplication microsoftxiaoice thereareseveral computer conversationaswell googlenow ap plesiriandhuaweinoah basedonchinesecorpus whichcould of the warefromindustry wecanonly obtainthetop rocf wepro viousturns context awareranking framework ranker results fromthere sults forthe effectiveness concern sal http www msxiaoice com rangeofqueries conversation besides scorefor thesemethods didatereplies ingeneral usingdeeplearning nrmandlstm rnn performbetter forretrievalmethods datereplies withdiversity astotheretrieval basedmethods randommatchis aswementioned itrandomizes tothatofbm slightlyworse okapibm representsthes tandard andsimple retrievalsystem theperformanceforbm whichisnotsurprising rwhichisproposed inourpreviouswork inthemeanwhile dl risthemost timeconsumingmethod algorithms thedl contextualmodeling notethat ourproposedrocf erbaselines slightlyweaker resultwith dl therocf duetotheconversa still rocf shallowrankerisnotas itis the theproposedrocfframe there rate metric which to videreal timeservice time isalsoamajorconcern in general eitherthegenera tivemethod wasseveralmag itisintuitive computedcalculations embedding convolution andpooling etc randommatch theproposedrocf thebesttrade calsystem eryday andxible conclusion inthispaper weproposeapractical whichmeans effective andef frameworkforhuman main timeservices giventheuser ry million hposting reply ipairs ance turnandmulti turnconversation aware ranking ifany performancecom backsinefy weprovidean tem yet rocf deepranker acknowledgments programofchina grantno aa andnationalba grantno cb we andthankthe references bengio foundations bessho harada andy kuniyoshi dialogsystemusing real scalecorpus in sigdial pages bickmoreandr picard establishingand maintaininglong termhuman acm trans comput hum interact june blei ng andm jordan latentdirichlet allocation mach learn res mar cong wang lin song andy sun finding question in sigir pages graves mohamed andg hinton speech in proc acoustics pages hearst multi text in acl pages hearst texttiling segmentingtextinto multi comput linguist mar higashinaka imamura meguro miyazaki kobayashi sugiyama hirano makino and matsuo in coling hofmann semanticanalysis machinelearning hu lu li andq chen convolutionalneural sentences in nips pages järvelinandj kekäläinen cumulatedgain based acmtrans inf syst oct jaynes physicalreview ji lu andh li corr abs kalchbrenner grefenstette andp blunsom arxivpreprintarxiv leuski patel traum andb kennedy building in sigdial pages leuskiandd traum npceditor creatingvirtualhuman aimagazine li galley brockett gao andb dolan diversity conversationmodels li mou yan andm zhang stalematebreaker proactivecontent human computerconversation in proceedingsofthe th ijcai pages luandh li texts in nips pages manning raghavan andh schütze introductionto informationretrieval volume mikolov chen corrado andj dean ef arxiv nakano miyazaki yasuda sugiyama hirasawa dohsaka andk aikawa wit atoolkitfor in sigdial pages nouri artstein leuski andd traum question answerpairs in aaaifallsymposium question generation palangi deng shen gao he chen song andr ward arxivpreprintarxiv ritter cherry andw dolan data drivenresponse in emnlp pages rocktäschel grefenstette hermann ko cisk andp blunsom attention arxivpreprintarxiv serban sordoni bengio courville and pineau buildingend to shang lu andh li short textconversation in acl ijcnlp pages song mou yan yi zhu hu and zhang embedding enhancedtexttiling in interspeech sordoni galley auli brockett ji mitchell nie gao andb dolan aneural in naacl pages sugiyama meguro higashinaka andy minami open in proc sigdial pages sutskever vinyals andq le sequencetosequence in nips pages walker passonneau andj boland quantitative dialoguesystems in acl pages wallace theanatomyofalice springer wang song el kishky roth zhang and han in kdd pages wang song li zhang andj han knowsim informationnetworks in icdm pages wang song li zhang andj han text kernels in aaai pages wang lu li ande chen adatasetforresearch onshort textconversations in emnlp pages williams raux ramachandran anda black the in sigdial pages yan kong huang wan li andy zhang temporal summarization in emnlp pages yan lapata andx li graphco ranking in acl pages yan song andh wu basedhuman computer conversationsystem in proceedingsofthe th sigir pages yan wan otterbacher kong li and zhang abalanced in proceedingsofthe retrieval sigir pages yao zweig andb peng arxivpreprint arxiv zhaiandd williams task orienteddialogues in acl pages zhang su xiong lu duan andj yao relationrecognition in emnlp pages zhou dong wu zhao yan yu liu andh tian multi human computerconversation in emnlp 
proceedingsofthe emnlp pages october doha qatar 
persistentrnns chip gregorydiamos gregdiamos baidu com shubhosengupta ssengupta baidu com bryancatanzaro bcatanzaro baidu com mikechrzanowski mikechrzanowski baidu com adamcoates adamcoates baidu com erichelsen erichelsen baidu com jesseengel jengel baidu com awnihannun awnihannun baidu com sanjeevsatheesh sanjeevsatheesth baidu com bordeauxdrive sunnyvale ca unitedstates abstract rnn ef ontogpus weshowhowitispossi throughputatlowmini batchsizesthandirect tiplications gpu ourinitial tflop satamini batchsizeof onannvidiatitanxgpu this providesa footprint xmore allowsusto gpus andal lowsustoefexploreend to endspeech layers introduction rnns havebeenshownto nition sainathetal ing gaoetal sutskeveretal inthispaper weusethe multi bulk synchronous parallel valiant mbsp proceedingsofthe rd learning newyork ny usa jmlr cpvolume copyright bytheauthor communication and wefo thesecosts batchsizes evenonmini batchsizesofjust examples weexploit printofournetworksby chipmemoryonthe hardwarethread we attackthecostofinter demon xcomparedtorelyingon weonlycon msoffuturecon text heetal srivastavaetal wepresentevidence relatedwork dis persistentrnns chip technologytrends deepneuralnets dnn detection krizhevskyetal simonyan zisser man szegedyetal heetal local ization heetal andposeestimation toshev szegedy dnns eling bengioetal mikolovetal senti mentanalysis socheretal iyyeretal le zuidema syntacticparsing collobert weston socheretal chen manning and machinetranslation bahdanauetal devlinetal sutskeveretal inspeechrecognition dnnshavebecomeainthe asrpipeline mohamedetal hintonetal dahletal jaitly vanhoucke seide etal acousticmodels abdel hamidetal sainathetal rnns typicallylstms arecommonlyusedin state of theartrecognizers gravesetal sak etal saketal sainathetal end to amodei etal levelper formance highperformance haveemerged chetluretal althoughtheyhavenot additionalworkhasfo vasilacheetal lavin gray finally dnnsoftenrelyondense andfrompriorworkthat processors dongarraetal gray anopen diamosetal andcorrespondingout putsequence eachinputsequence isatime seriesoflength whereeverytime sliceisa vectoroffeatures audiosamples theforwardintime thefunction where istheinput hiddenweightmatrix isthere isabiasterm inthestage the thelayerasadensetwo dimensionaldense vector dimensionalmatrix thisispos inthesecondstage sentedbyatwo dimensionalmatrix referredtoasthere currentweightmatrix inthiscase eachtimestepmust vectorproduct tionfunction step eachtimestep bulk synchrononus parallelma chinemodel multi bulk synchronous parallelmachinemodel themulti bulk synchronous parallel mbsp abstract machinemodel valiant tipleprocessorcores tationalresources nizationcosts they withanarbitrary numberoflevels ateachlevel chipmemory cache persistentrnns chip figure leftsquare arrows computeunits rightsquare synchronization andcomputation memorycapacity memorybandwidth andmemoryla tency smemory hierarchywithatuple where representsthe representsthe and represents anexampleof uptothehighest modulecon nectionsandinter themodule sneurons modulecon sneurons communication synchronization tleneck fromdram chipmemoryac tflop tflop satalayer sizeof mini batchsizeof batchsizeachieves over tflop sresultingina xspeedup approach tiguousrows asshowninfigure isters ory performsthe nonlinearity and thelatency sowebreakthe persistentrnns chip figure strategy eachofthe blockrow reading timestep andwriting withinansm eachof warpsprocessesa block row groupsof cessa tile reduce andbarrieroperations weuseamini batchsizeoffouror stashingtheweightson chip outofthis wededicate infigure afastglobalbarrier however chipmemory thiscauses proximately andthiscannot weaddressthisproblem asingletimestep weencounter cor respondingto timesteps forarnnlayerwith hiddenunits andamini batchsizeof thiscorresponds to gbofstorageperlayer thisismuchmorethan the inprac tice withgpuswith gbofdram wethatthis layers acommonso propagation throughtime sutskever bptt however wehave observeda makingothertech suchasreducingthe mini batchsize moreattractive grusandlstms inbothcases theprevioustimestep asbefore thecompo similarlythesecond nalsšreset update activation etc thismatrixcannow chipmemory experiments wealsoinclude onlyrecurrent networksonalarge similar tothedeepspeech hannunetal ds anddeep speech amodeietal ds systems thesevery tations ofpersistentrnn we smallmini tions persistentrnns chip figure batchsize figure ensitivityto ini atch ize figure mini batchsizes notethatafteramini batchsizeof four tflop butmatrix startoutmuchslower eventhen wethatlayer sizesaround tflop satamini batchsizeof performanceis units suggest ensitivityto imesteps figure we figure rnn totallayer batchof imately timesteps timesteps however timesteps ms trong caling figure rnnlayernetwork from to gpus with gpusand cpus pciev usingtwo switches tweencpus weuse weusesynchronous portmultiplegpus laptheall tionevaluation thealgorithmicmini batchsizeisedat forallex periments themini batchpergpuis whenrun on gpusand whenrunon gpus thegemm gpus but the and achieves tflop son gpus about ofpeak entiresystem notjustthernnkernels persistentrnns chip figure throughputofthatgpu memory bandwidth orloadimbalance rocessor esign pace offeringaviewintothe sizes tationsin bitpoint doublingthedramband width generally andrunthemathigher throughput changes movingto followedbymorememory persm reduc stacksofforward laryend to figure whichweuse arecurrentneu ralnetwork rnn tion gravesetal we useadatasetof were portworderrorrate wer ing weintegrate scribedin amodeietal mostidenticaltotheds network thereisone difference persistentrnns chip architecturedev wer rnn total noskip rnn total skip rnn total skip rnn total skip rnn total skip rnn total skip rnn total skip rnn total skip rnn total skip table tweeneverynrnnlayers eachwith units volutions amodeietal withaedcontextsizeof recognitiontasks methodology tum sutskeveretal therangeof utterances ifthenormofthegradi itisrescaledto pas canuetal aheld evaluation aftereachepoch we useamomentumof forallmodels ensitivityto esidual onnections table we models mod forthese networks andmod exceptforthe outlierof whichwecannotexplain thissuggeststhat ensitivityto epth table wethatdepth helpsuptoapoint about layers afterwhichperfor mancedegrades figure usedinthispaper allnetworksuse layersof din variantconvolutions the allofwhichmay deeper modules allnetworksuse tion ensitivityto arameters table wethatincreas layersand millionparameters ensitivityto ini atch ize table mini batchsize fortheseexperiments weperforma momentum andannealing wethat tionalwork inthiscase thenumberofepochs neededto to thissuggests run enabledata discussion ingeneral weboth persistentrnns chip arch actsdev wer rnn rnn rnn rnn rnn rnn table ousdepthsofrnn thusthenum forthearchitecture ﬁmrnnﬂimpliesmuni directionalrnnlayers architecture paramsdev wer rnn total rnn total rnn total rnn total rnn total rnn total table ousdepthsofrnn thus utiveuni network itseemsclearthatthe biggerdatasets thiswork butalsobythestrat mappingrnnsto narrowlayers and processors wehavefoundthembspab figure epochsforvarious mini batchsizes thearchitectureisa layermodelwith layersof invariantconvolution rnnlayers with layers rowconvolutionlayers wardlayer batch becomeslargerthan siderably tion mappedtohardware conclusion nvidiatitanxgpu achieving tflop satamini batchsizeof thisprovidesa memoryfootprint wefocus ms offuturecontext weexpectthesegainsto technique references abdel hamid ossama mohamed abdel rahman jang hui andpenn gerald hmmmodelforspeech recognition in icassp amodei dario anubhai rishita battenberg eric case persistentrnns chip carl casper jared catanzaro bryan chen jingdong chrzanowski mike coates adam diamos greg etal deepspeech end to andmandarin arxivpreprintarxiv bahdanau dzmitry cho kyunghyun andbengio yoshua toalignandtranslate corr abs url http arxiv org abs bengio yoshua ducharme rãl jean vincent pascal andjauvin christian model search chen danqiandmanning christopherd afastand in proceedingsofthe emnlp oc tober doha qatar ameetingofsig dat pp url http aclweb org anthology pdf chetlur sharan woolley cliff vandermersch philippe cohen jonathan tran john catanzaro bryan and shelhamer evan cudnn efprimitivesfordeep learning url http arxiv org abs collobert andweston aarchitecturefor multitasklearning in chinelearning icml dahl yu anddeng dependentdbn hmms in proc icassp dahl yu deng andacero context dependentpre ieeetransactionsonau dio speech devlin jacob zbib rabih huang zhongqiang lamar thomas schwartz richard andmakhoul john fast chinetranslation in proceedingsofthe ndannual tics volume pp diamos gregory sengupta shubho catanzaro bryan chrzanowski mike coates adam elsen erich engel jesse hannun awni andsatheesh sanjeev persis tentrnns https github com baidu research persistent rnn accessed dongarra jack gates mark haidar azzam kurzak jakub luszczek piotr tomov stanimire andya mazaki ichitaro withgpus pp gao haoyuan mao junhua zhou jie huang zhiheng wang lei andxu wei areyoutalkingtoama chine tionanswering corr abs url http arxiv org abs graves fernández gomez andschmidhuber in icml pp acm graves alex mohamed abdel rahman andhinton ge offrey networks in icassp gray scott url https github com nervanasystems maxas sak hasim senior andrew andbeaufays francoise longshort in inter speech hannun awni case carl casper jared catanzaro bryan diamos greg elsen erich prenger ryan satheesh sanjeev sengupta shubho coates adam andng andrewy deepspeech scalingup end to endspeechrecognition http arxiv org abs he zhang ren andsun deepresidual arxive prints decem ber hinton deng yu dahl mohamed jaitly senior vanhoucke nguyen sainath andkingsbury ieeesignal processingmagazine november iyyer mohit manjunatha varun boyd graber jordan andiii haldaume as url docs pdf krizhevsky alex sutskever ilya andhinton geoff im works in systems pp persistentrnns chip lavin andrewandgray scott corr abs url http arxiv org abs le phongandzuidema willem arxiv preprintarxiv mikolov tomas martin burget lukas cer nock jan andkhudanpur sanjeev recurrentneu in interspeech makuhari chiba japan september pp mohamed dahl andhinton acousticmod ieeetransactionson audio speech url http ieeexplore ieee org xpls abs_all jsp arnumber jaitly nguyen seniorandvanhoucke applica in interspeech pascanu razvan mikolov tomas andbengio yoshua abs http arxiv org abs sainath tara vinyals oriol senior andrew andsak hasim convolutional longshort termmemory fully in icassp sainath taran rahmanmohamed abdel kingsbury brian andramabhadran bhuvana deepconvolutional in icassp sak hasim vinyals oriol heigold georg senior an drew mcdermott erik monga rajat andmao mark in inter speech seide frank li gang andyu dong conversational dependentdeepneu ralnetworks in interspeech pp simonyan karenandzisserman andrew verydeepcon corr abs url http arxiv org abs socher richard lin cliffc ng andrewy andman ning christopherd in proceed ingsofthe learning icml socher richard perelygin alex wu jeany chuang ja son manning christopherd ng andrewy andpotts christopherpotts in emnlp srivastava rupeshkumar greff klaus andschmidhu ber jürgen highwaynetworks corr abs url http arxiv org abs sutskever martens dahl andhinton on learning in learning sutskever ilya sutskever ilya vinyals oriol andle quocv sequence in proc nips montreal ca url http arxiv org abs szegedy christian liu wei jia yangqing sermanet pierre reed scott anguelov dragomir erhan du mitru vanhoucke vincent andrabinovich andrew corr abs url http arxiv org abs toshev alexanderandszegedy christian deeppose hu corr abs url http arxiv org abs valiant leslieg corecomput ing in proceedingsofthe siumonalgorithms esa pp berlin heidel berg springer verlag isbn doi url http dx doi org vasilache nicolas johnson jeff mathieu michaël chin tala soumith piantino serkan andlecun yann fast agpuperformanceeval uation corr abs url http arxiv org abs 
dvideo xinxinzuo senwang jiangbinzheng ruigangyang universityofkentucky baidulexington ky usxi an chinabeijing china xinxin zuo sen wangg uky eduzhengjb nwpu edu cnryang cs uky edu enhancementfromanrgb dvideosequence thebasic sequence weusethe thekeytech tireimageset performsoptical inaddition with composition introduction suchashuman computerinteractions robotics andvideoanalysis among these nevertheless thecurrent lutionandaccuracy asaresult structuraldetails suchasde noiseandup sampling amongthese approaches mostofthese from shading sfs tech niques however incorporated whichisalsounknown inorderto handlevaryingalbedos previousshading based and eggproblem albedoregularizers however inthispaperwepropose notthecamera inthis way theobject intheimagesequence itresem wemovetheobject thiskindofcuehasbeen exploitedinmulti andshape fromvideo however theyhavetheenviron motion erydayenvironment giventhecapturedrgb dsequence wetrytoalign thergb then finally weformulatean expectation inthepres enceofsomenon thecolorinformation ourmethod tothebestofourknowl edge albedo orturntablecapture toachievethese wemaketwotech nicalcontributions pixelnormal relatedwork inthissection relatedtopics shadinginformation theshape from shading sfs problemhaslongbeen itaimsto fromasingleimage orpriorassumptions withtheinherentlyill posedproblem dcam eras solvedexactly surface wu androy varyingalbedoposes assumes yu dealswith timizinggeometry abetter is geometry dshapere viewstereo themajordif wisemanner therefore unlikesfs photometricstereoisa problem depthmap wu andzhou dermulti net besides recentlychatter jee intheseap proaches to agedecomposition introducedbybarrow istosep itis againanill everyobservation theretinextheory is mostlyhigh frequency many forexample tappenet al sets globalpriors suchasthesparsity arede veloped non allcases atexturedsurface ity forexample dimage cues leeetal dvideo anothersolution however withoutany lighting theproblemisstillill independently tionterms preliminarytheory dimensionalmodel underthisassumption theshad ingfunction an itmaynotbefeasi mated photometricstereo is mathematically tion jj ak jj instead wecapturedthergb suppose andwecan for example forpixel inthereferenceframe itscorrespon denceinframe isw isgeneratedas ar whichequalstopixel andn dinate pipeline first wefuseevery rgb dframesviakinect fusion togenerate keyframedepthmaps they arobust color sequencedepth sequencekey frame fusion rigid alignment local match refinement lighting estimation pixel wise normal and albedo recovery surface integration detailed surface lighting correspondence figure systempipeline ment finally giventhecom we bustemframework approach including lightinges timation theywillbe robustpixelmatching thekeyframedepthmaps areobtainedvia notedas frame ref ref otherframes rigidalignment first theseextrin andwegettherotation andtranslation matrixt howevermisalignments sequence consideringthatthe reference frame sampled frame optical flow overlay initial overlay our overlay figure isonesampledkeyframe image iswarpedto and showsthe placementopticalw however case there fore issues agedenotedas refandi refrespectively foreachpixel ini ref after iscomputedas ref ini refandq ini maynotbe therefore bestmatchingpixelin foreachpixel ini ref itanditsbestmatchin viancc however theinten arbitrarymovements andusethe foreachpixel itsap ch ˆch ch inwhich cn ch ch ch besides thecolorimage undertheguidanceof imagei refk refkwithi insteadofusing refdirectly since andi successfullyavoided ref ori refk wecanthe correspondingpixelin score respondence totacklethisproblem weonlykeepthepix dences thelargest thediffer iftheseprinciples aremaintained thres issettobe andthres is inthispaperforallthe experiments deformtheimage ini refk thedeformation functionisas where theinterpolationcoef issetaccordingtothe finally cn refk cn jj jj isthecon trolweightsettobe inthispaper sincewehavegood initials lightingestimation timation inthispaper elim with tothereferenceimage foreachpixel ini ref isdenotedas ref ar ar nq ar nq nq np ar np np therefore argmin ref jj ar ar ref jj thenormal theweightingterm erenceframe ing foreach frame albedo foreachpixel tionsas fi albedoˆandnormal lowingdescription as jj jj ar however theoutliers theywillaffectthe we bertianmodel anexpectation maximization em algo whileourfor mulationisinspiredby more fn tionedonparameters isgivenas exp jj jj model given parameters iscomputedineverye step exp jj kjj exp jj kjj next inthefollowingm step wemaximizethe complete datalog tionh obtainedfromthee step logp logp log exp jj jj log wesetthederiva tiveof pwithrespectto andˆequaltozero inthis way sp jj jj sincethefunction argmin jj ar jj malandalbedo and issetto be and paper finally tails asapreprocessingstep theobjectissegmented mationintograbcut framework wemanuallymasked cally frames iscomputedinpixel wisemanner therunningtimecould experimentalresults intheexperiments son syntheticdata firstgiven the dmodel therendered structuraldetails fig mapandsurface displayedasnormalmap theout column incomparison the thirdcolumn umn color shading normal smoothed normal our normal our albedo albedo error our normal error shading normal error figure istheren showsthenor and ourrecoveredalbedo and respectively em em error with em with em error figure workand isitserrormap thenormalanderrormap and respectively fig ofoutliers density aswecanseefromthe twocolumns figure and turtlerespectively and methodaredisplayedin and and finally and thenormalmapisgiven in and optimizationisnoisy themeanerroris degree while degree whichis realdataset sorofkinectv namely thebackpack turtle book etc fig fig and as onthefused referenceframe while fig and fig and showsourresults theun misalignment fig andfig displaysour for thebackpack rigiddeformation wehave ofourpixel wiserecoverymethod problem stratedinfig asdisplayedinfig thetextureshave ment incomparison theresultfromshading method fig figure resultsonbookmodel isthereferencecolor image playedin method inordertoshowtheper wehavealso of the artintrinsic theytakethergb dim asdisplayedinthe column chen themethodfromjeon column however incomparison with ourpixel weareableto conclusion inthispaper dvideo figure the thethird methodin finally sequence quence then dos finally wehavevalidatedour withsomestate of the decompositionmethods ments marilydiffusesurface asafuturework wecouldex ondepthenhancement dspace acknowledgments iis iip usarmyresearchgrant nf nsfc no ktzdgy ktzdgy no zy jiangbin correspondingauthors forthispaper references barronandj malik high in cvpr pages barronandj malik shape illumination andre fromshading barronandj malik singlergb dimage barrowandj tenenbaum recoveringintrinsic comput vis syst bi han andy yu anl tion acmtrans graph ohme haker martinetz ande barth shad surements broxandj malik de chatterjeeandv govindu photometricof depthmapsformulti albedoobjects in cvpr pages chenandv koltun in iccv pages esteban vogiatzis andr cipolla multiviewpho tometricstereo machineintelligence ferstl reinbacher ranftl rther and bischof in iccv pages finlaysonandr xu patternrecognition letters garces munoz lopez moreno andd diego in in volume pages han lee andi kweon singlergb iniccv pages haque chatterjee andv govindu highquality in cvpr pages horn shapefromshading amethodforobtaining phd thesis mit jeon cho tong ands lee intrinsicimagede normals in eccv pages kim torii andm okutomi iccvworkshops kong gehler andm black intrinsicvideo in eccv pages laffontandj bazin in iccv pages laffont bousseau paris durand andg dret takis acmtrans graph lakdawallaanda hertzmann shapefromvideo dense shape texture streams in inria landandj mccann josa lee zhao tong gong izadi lee tan ands lin fromimage depthvideo in eccv pages min lu andm do basedonweightedmode ieeetransactionson imageprocessing newcombe izadi hilliges molyneaux kim davison kohi shotton hodges anda fitzgib bon kinectfusion real tracking in ismar pages or el rosman wetzler kimmel anda bruckstein rgbd fusion real covery in cvpr pages park kim tai brown andi kweon high rgb dcameras ramamoorthiandp hanrahan anefrepresenta in siggraph pages acm rother kolmogorov anda blake grabcut interac acm shi inose matsushita tan yeung and ikeuchi in dv pages simakov frolova andr basri denseshaperecon unknownlight ing in iccv volume pages tappen freeman ande adelson recov ieeetpami ti yang davis andz pan simultaneoustime of incvpr pages wu liu dai andb wilburn fusingmultiview bratedillumination computergraphics wu varanasi liu seidel andc theobalt shading view in iccv pages wu zollh ofer niener stamminger izadi andc theobalt real timeshading basedfor consumerdepthcameras tog wuandc tang imization chineintelligence wu tang tang andt wong densephotometric stereo ieeetransactions yang ye li hou andy wang color guided depthrecoveryfromrgb gressivemodel yu yeung tai ands lin shading basedshape ofrgb dimages in cvpr pages zhang curless hertzmann ands seitz shape unifyingstructure frommotion photometricstereo andmultiviewstereo in iccv pages zhang ye yang matsushita wilburn and yu edge incvpr pages zhao tan dai shen wu ands lin closed straints intelligence zhouandv koltun drecon acmtransactions ongraphics tog zhou wu andp tan multi in iccv pages 
semi yongcheng weixu zhongjunhe weihe huawu maosongsun andyangliu tsinghuauniversity beijing china tsinghuauniversity beijing china baiduinc beijing china chengyong gmail comweixu tsinghua edu cn hezhongjun hewei wu hua baidu com sms liuyang tsinghua edu cn abstract whileend to tion nmt recently inquantity quality andcoverage espe ciallyforlow resourcelanguages itis toimprovenmt weproposeasemi parallelcorpora andunlabeled mono lingualcorpora data thecentralideais inganautoencoder inwhichthesource to targetandtarget to sourcetranslation respectively targetlanguage butalsoofthesource language state of the artsmtandnmtsystems introduction end to nmt largeneuralnetworkto languagesentenceinto atarget languagesentence hasattractedincreas kalchbren nerandblunsom sutskeveretal bahdanauetal smt brownetal koehnetal chi ang inglong rnns withthegating choetal sutskeveretal andattentionmecha nisms bahdanauetal however theyheavilyrely els probabilityofatarget source kalchbrennerand blunsom sutskeveretal bahdanau etal unfortunately parallelcorporaare incontrast target oftranslations therefore theunavailabilityof large scale high quality andwide coveragepar asaresult gulccehreetal proposetwomethods fusion coder cantimprovements alternatively sennrichetal propose the dummyinput then arxiv cs cl dec figure examplesof sourceautoencoderand our inasourceautoencoder the source to targetmodel highlightedingrey fromwhichthetarget to sourcemodel asaresult semi supervisedsetting inthesec ondapproach simi smt uefetal bertoldiandfederico sennrichetal reportthattheirap inthispaper weproposesemi supervised givenla beled parallelcorpora andunlabeled monolingualcorpora data ourapproachjointly trainssource to targetandtarget to sourcetrans lationmodels which intheautoencoder the source to targetandtarget to sourcemodelsserve respectively asthe weproposetosamplethe specif ically ourap end to endnmtsystems poracanbeused ourapproachcanbene butalsothe experimentsonchinese englishnistdatasets of the art smtandnmtsystems semi machinetranslation supervisedlearning givenaparallelcorpus fh ig log where canbeseen as labeled sentence givenasourcesentence as ismodeledbyasingle largeneu ralnetwork languagemodel innmt therefore par unfortunately rich languages reports therefore scale high quality andwide able nmt letusconsideran unsupervised set ting corpus ourideaistoleverage autoencoders vincentet al socheretal encoding anob usingatarget to decoding to target model forexample asshowninfigure atalkwithsharonﬂ atarget to sourcetranslation model encoder highlightedin grey then asource to decoder moreformally let and be source to target and target to source transla where and arecor anautoencoder encoder decoder where isa copyof tobereconstructed and isalatent sourcesentence werefertoeq asa targetautoencoder likewise language sourceautoencoder al vincentetal socher etal valuedvectors sentence encoder decoder pleaseseefigure forillustration semi supervisedlearning to target andtarget to sourcemodels itisnaturaltocom semi supervisedsetting formally givenaparallelcorpus fh ig language andamonolingualcor pusofsourcelanguage weintro duceournewsemi asfollows log source to targetlikelihood log target to sourcelikelihood log targetautoencoder log sourceautoencoder where and arehyper parametersforbalanc coders source to targetlikelihood target to sourcelikeli hood targetautoencoder andsourceautoencoder inthisway targetlanguages argmax log log log argmax log log log to targetandtarget to ing training weusemini trainourjointmodel foreachiteration be sidesthemini we batchesbyran then gradientsarecol lectedfromthesemini batchestoupdatemodel parameters withrespect tothesource to targetmodel isgivenby log log log canbecal culatedsimilarly unfortunately tialsearchspace forexample thederivativein chinese english sent parallel word vocab sent monolingual word vocab table gualcorpora thethirdtermineq isgivenby log alternatively fullspace ˆx toapproximateeq log inpractice weusethetop listofcandidate translationsof as as xj efby inpractice improvementsand seemstosufto tionquality experiments setup englishdataset asshownintable weusebothaparallel ingset msentencepairswith mchinesewords and menglishwords thevocabularysizes mand re spectively thechinese msentences with mwords msentenceswith mwords thevo and respectively figure effectofsamplesize onthechinese to englishvalidationset figure effectofsamplesize ontheenglish to chinesevalidationset forchinese to englishtranslation weusethe nist chinese dationsetforhyper modelselection thenist and eachchi for english to chinesetranslation weusethenist treatingthe lation insensitive bleu papinenietal ascalculatedbythe multi bleu perl script of the artsmtandnmtsystems oses koehnetal aphrase based smtsystem figure to englishvalidationset figure to chinesevalidationset rnn search bahdanauetal an attention basednmtsystem form oses thephrase linearmodels och weusethesrilmtoolkit stolcke totrain gramlanguagemodels forrnns earch traintheattention to wefollow luongetal toaddressrarewords ontopofrnns earch ourapproachiscapa basedneural thesamplesize isset to wesetthehyper parameter and pus and and forsourcemonolin thethresholdofgra dientclippingissetto theparametersof parallelcorpus effectofsamplesize we thetop efy seeeq figure tingsof overtime onlytheenglishmono bleuscores therearegapsbetween and however keepingincreas ing wethat asshowninfig ure english to chinesevalidationset therefore we set effectofoovratio givenaparallelcorpus tionquality toanswerthisquestion weinvesti gatetheeffectof oovratio ontranslationquality whichisas ratio where isatarget lingualcorpus isatarget languagewordin allelcorpus intuitively corpus iftheratiois allwordsinthemono figure thechinese to englishvalidationset onlyen weconstructedfour sentencepairs oovﬂmeanstheoovra tiois pus nogreater lingualcorpus higherbleuscores low oovcounterpartand figure english to chinesevalidationset onlyenglish corpusduringtraining wethatﬁ oovﬂ comparisonwithsmt table oses andourwork oses usedthemonolingual mchinese sentencesand menglishsentences we chinese to englishandenglish to chinesedirec tions rnn search outperformsm oses trainedalsoonlyonpar allelcorpus oses rnn search gualcorpora mchinesesen tenceswith mwordsand menglish englishsentenceswith mwords inother words oses our rnn search upto bleupoints inaddition im oses using upto bleupoints to search usingonly parallelcorpus upto bleupoints sug nmtusingsource intheenglish to chinesedirection weob addingchi system trainingdata direction nist nist nist nist nist ce oses rnn search table comparisonwithm oses andrnn search oses isaphrase translationsystem koehnetal rnn search isanattention system bahdanauetal ﬁceﬂdonateschinese ﬁcﬂdonateschinese monolingualcorpus thetrainingdataand meansnotincluded ﬁnist ﬂaretest sets thebleuscoresarecase insensitive betterthanmoses betterthanmoses betterthanrnn search betterthanrnn search method trainingdata direction nist nist nist nist nist ce sennrichetal thiswork table bothsennrichetal andourapproachbuild ontopofrnn search thebleuscoresarecase insensitive toenglish to wealsotriedtouse to butfailed therefore follows to targettranslation source to targettranslation buttheimprove lingualcorpus improvements were methodontopofrnn search asfollows trainthetarget to model ontheparallelcorpus fh ig thetrainedtarget to sourcemodel is corpus pus pus ti re trainthethesource to targetneuraltrans lationmodelon toobtainthemodel parameters monolingual hongsenshuo name reference hongsensaid ifany loggingcompanies daretodefythelaw thentheywill translation hunsensaid ifanyof thosecompanies daredefythelaw thentheywill havetheirownfate iteration hunsensaidifany treefellingcompany daredtobreakthelaw thenthey would killthemselves iteration hunsensaidifany loggingcompanies daretodefythelaw theywould iteration monolingual zebixuzai tianneizhixing reference butonce theverdictis days translation however intheanalysis days iteration however intheanalysis days iteration however once theverdictis days iteration table supervisedlearning table boththe lingualcorpora ourapproachachieves in bothchinese to englishandenglish to chinese directions upto and bleupoints terestimationforonce seestep above while to targetandtarget to tosomeextent sap proach pseudoparallelcorpus thelearnedmodelparam corpus table erations argmax relatedwork lationand semi supervisedlearning machinetranslation years corpora uefetal bertoldiandfed erico whichcan parallelcorpus vergence whileklementievetal pro zhang andzong niques ciphermentproblem raviandknight dou etal and sennrichetal ourapproachfocuseson themajorad semi supervisedlearning vincentet al socheretal ammaretal justtonameafew amongthem socheretal supervisedrecur thedif dai andle conclusion wehavepresentedasemi supervisedapproachto models to targetandtarget to encodersanddecoders experimentsonchinese leadstoimprovements inmonolingualcorpora etal itisalsonecessaryto to targetandtarget to sourcemodels tohelpthem acknowledgements itingbaidu program cb cb no talentplangrant programgrants andagooglefac ultyresearchaward wesincerelythankthe references waleedammar chrisdyer andnoahsmith in proceedingsofnips dzmitrybahdanau kyunghyuncho andyoshua bengio in proceed ingsoficlr domain monolingualresources in proceedingsofwmt peterf brown stephena dellapietra vincentj dellapietra androbertl mercer the pa rameterestimation davidchiang ahierarchicalphrase based in pro ceedingsofacl kyunhyuncho bartvanmerri enboer dzmitrybah danau andyoshuabengio ontheproperties encoder decoderap proaches in proceedingsofssst andrewm daiandquocv le semi in proceedingsof nips qingdou ashishvaswani andkevinknight beyondparalleldata in pro ceedingsofemnlp caglargulccehre orhanfirat kelvinxu kyunghyun cho lo barrault huei chilin fethibougares holgerschwenk andyoshuabengio on lation arxiv cs cl sepphochreiterandj urgenschmidhuber the pa rameterestimation sebastienjean kyunghyuncho rolandmemisevic andyoshuabengio onusingverylargetar in proceedingsofacl recur in proceedings ofemnlp alexandreklementiev annirvine chriscallison burch anddavidyarowsky towardstatisti in proceedingsofeacl philippkoehn franzj och anddanielmarcu statisticalphrase basedtranslation in proceedings ofnaacl philippkoehn hieuhoang alexandrabirch chris callison burch marcellofederico nicolabertoldi brookecowan wadeshen christinemoran richardzens chrisdyer ondrejbojar alexandra constantin andevanherbst moses open in proceedingsofacl demosession minh thangluong ilyasutskever quocv le oriol vinyals andwojciechzaremba addressing in proceedingsofacl franzoch in proceedingsofacl kishorepapineni salimroukos toddward andwei jingzhu bleu in proceedingsof acl decipheringfor eignlanguage in proceedingsofacl ricosennrich barryhaddow andalexandrabirch withmonolingualdata arxiv cs cl richardsocher jeffreypennington erichuang an drewng semi in proceedingsofemnlp andreasstolcke srilm amextensiblelan guagemodelingtoolkit in proceedingsoficslp ilyasutskever oriolvinyals andquocv le works in proceedingsofnips andanoop sarkar machinetranslation in proceedingsofacl pascalvincent hugolarochelle isabellelajoie yoshuabengio andpierre autoinemanzagol learning denoisingcriterion research learning aphrase in pro ceedingsofacl 
zhongqilu erhengzhong lilizhao evanweixiang weikepan qiangyang abstract ollaborative iltering cf aimstopredictusers ratingsonitems itempreferencedata inmanyreal worldapplications whichwould recently lemcouldbemitigated howeverformostcases partsofsource domaindataare notconsistent getdomain ing inthispaper consequently weembedthiscri selective knowledge transfer of the artmethods weshow keywords transferlearning crossdo mainrecommendation introduction movies tv books news images webpages etc thatare asastate of the arttech collaborative in thereal althoughtheitem berofitems thus sparseforeachuser services quality predictions inrecentyears andwiththe some technology zluab ezhong skyezhao wxiang weikep qyang cse ust hk works like transfer however trustthe whichis worldapplications especially forexample inalocal musicratingwebsite thetraditionalmusic website thoseuserswithgood obviously sdataassource cfproblems severalresearchworks like havebeenproposedto domainissparse user gettinga agoodmodel inotherwords domain someusers such asdomainexperts domains andfurther bythemodel predictionsforauser basedonthis observation asanimplementation we figure therow thesourcedomains areal thesecondrow thesourcedomains areal system whichprovidesmusic stlcf wherethosesourcedata first domain factors weproposeanovel criterion second weproposea selective workforcollaborative anextensionofthe sothatthesparseness third wheredif weproposean semanticanalysis real worldapplications preliminaries problemsettings isassociatedwith usersand itemsdenotedby and respectively inthistask withentries ui let ui where ui inthesystem can forexample orbinary values sourcedomains witheachsourcedomain contains usersand itemsdenotedby and eachsourcedomain andobserved links ui where ui we inthe wefocusonthecase sourcedomains is obtainedwith and beingindependent weformally aco pr ui pr pr pr ui pr pr pr ui pr ui inthefollowing tentsemanticanalysis gplsa wepresenta trans tg plsa asanexample afterthat stlcf stlcfiscompatible gplsa following for everyuser itempair with latenttopics sothatuser anditem arerendered userset andratingset inthesourcedomain we pr ui pr ui pr estimating ui byintroducing iz forthe meanand iz withthese we pr ui pr pr iz iz where pr and pr iz iz ui log pr ui where tlcf whenthetargetdata issparse gplsamay in tgplsa model again weuse knowledgecomefrom and forsimplicity andthismodel more over thereal worldsystems likedouban parameter sincetheitemsets and aredifferentor clearly the themorehelp weproposea weightedtlcfmodel wtgplsa tofurtheranalyzethis and oftheinstancesin then theobjective functionineq canbeextendedas ui log pr ui ui log pr ui either or we adopttheexpectation maximization em algorithm astan tothemaximum log detailsofderivations http www douban com which providesmusic targetinformation algorithm selectivetlcf input initialize initialize for iter to do step apply thatminimizeeq step and domains iter step and using eq step calculateweight foreachsource domain usingeq step choosemodelweight iter viaeq step viaeq step viaeq endfor output hypothesis selectivetlcf by factors exampleinfigure records spreferences becauseof errorandvariance tors thetargetdata ononehand wetakemore careofthosemis ontheother hand asshowninalgorithm ineachiteration weapply weaklearner andhypothesis iter thentoupdate domainlevel weight based and iter forbase modelisalsoupdated variances accordingly theweightsofmis predictedtarget the whichgiveslarger weprovidea absoluteerror mae error we ui ui ui nnz ui ui ui nnz where nnz and we withrespectto as aboveall scalar forpredictionis fromeachweaklearner the ui ui ui where subse quently learner andapositivescalar suchthat tation weomitthemodelindex anduse torepresent torepresent by and eq withrespectto jori forbrevity and here denotes fallsintothefault tolerablerange while denotestherestset bymakingthe equaltozero weget log ifweset log log finally theupdatingrulefor is inthesourcedomain wecan so basedboosting wefurther introduceare weightingfactor foreachsourcedomain soweformulatethe updatingrulefor tobe where jj jj where and table notation dataset datatype instancesno doubanmusic rating doubanbook rating doubanmovie rating rating wikipedia editinglog imdb hyperlink experiments weevaluate douban imdb andwikipedia usereditingrecords the millionratingswith valuesin usersonaround movies doubancontains movie withratingvalues alsoin inthegraph mostsimilarmovies the indicatorof follows theexperiments wesampledasubsetof users in doubandatasets weobtained ratingson music ratingson books and ratingson movies givenby users forboth we doubanmoviedatasets afterpre processing theimdb movies thewikipedia editinglogsby usersonthesame moviesasimdb dataset weusetheshorthand todenotethedatasets asfollows domaincollab thesam onepartconsists ofratingsgivenby movieswith density theremaining thesecondtaskisareal worldcross domainrecom mendation http www http www imdb com http en wikipedia org inthissetting weextract theratingsonthe sharedmoviesfromand doubanmovie thenweget ratingsfrom doubangivenby userswithdensity and ratingsfromgivenby userswithdensity doubanmovie inthistask domains doubanmoviedomain it hyperlinkgraph moviedataset forevaluation error rmse ontheheldout ofthetargetdata rmse ui ui ui where ui and ui respec tively and weimplementtwo stlcf isanstlcf stlcf ev notonly rorvariance pmf isare previ gplsa isaclassical non cmf ispro beingadoptedas cmf domainrecommen dationapproach tgplsa ferlearningmodel thetargetdomainmodel based experimentalresults table datasets source target non tl non selectivetl selectivetl sparseness sparseness gplsapmf tgplsacmf stlcf stlcf ev simulated to simulated tod tod wetesttheperfor the first weobservethatthenon transfermethods gplsaandpmf espe withthe helpofsourcedomains the non selective transferlearn liketgplsaandcmf ingpredictions stlcf andstlcf ev candoevenbetter the byperformingthe selective knowledgetransfer weusethe second comparingthetwonon selectivetlcfmeth weobserve tod andd tod say the againstthenon tod worldcases the third likeempiricalerror variance intable wecompare ourtwostlcfmethods stlcf andstlcf ev wecan thatonthetaskﬁd tod doubanbooktomovie stlcf ev butonthe taskﬁd simulated tod simulated theimprovementof stlcf ev isnotsoagainststlcf these for thetasksﬁd simulated tod simulated boththesource table tailusersonthed tod task stlcf whilestlcf ev does ratings non tl non selectivetl selectivetl per stlcf user gplsa tgplsacmf ev whilethetaskﬁd tod mendationsystem items the user therearechancestoget tentwithtargetdomain inthiscase itisimportanttocon comparingto stlcf stlcf ev ing resultsonlong tailusers tobetterunderstand we mentondoubandatasets theresultson first stlcf andstlcf ev such current table sourcedomain none target sparseness table douban sourcedomain none target sparseness tail users tailcasesatboth second stlcf ev works betterthanstlcf onthosenon long tailusers with morethan thisis transfer weapply stlcf ev oneiscomposedof theotheriscomposed ofdoubanmovie usereditingrecords theresultsareintable andtable respectively first we thereare theresultis for example novel inthiscase second whicharenot prediction comparing figure non transfer ourstlcfmethod inaddition sourcedomaind ord wethatthe despiteofthenoiseand heterogeneoussetting aswe on onehand aswecansee are thereforethe overalltransferofd ispenalized ontheother hand aboveall domain therearetwopa rametersinourstlcf since and are independent figure isoforder whichbalancesthe wethepredictionerror thresholdtobe intuning asshowninfigure from to thebestchoices of arefoundtobearound figure figure changeofthermsesand swhenmoreandmoreweak convergenceandovtest figure shows thermsesofstlcf ev fromthe ontheleft weobservethatstlcf ev convergeswellafter iterations also convergetoaround after iterationsaswell themodel whenwe themodeltends topicsistoolarge testingrmsesofthenon thenon ev overdifferent thedatasparsityfor the when isabout andfor tgplsa is largerthan decreasinguntil andevenwhen islargerthan theraiseofourstlcf ourstlcf relatedworks theproposed elective ransfer earningfor ollaborative iltering stlcf wesummarizethe to figure table selective non selective transfer stlcf rmgm cmf learning tif etc non transfer mmmf gplsa learning pmf etc andthetargetdomain proposed bayesiannetworks however mostof setofobserveddata inthispaper transferlearning panandyang surveyedthe oftransferlearning hofmann they whichdoesnot preferencesamongthe alignedusers li etal levelratingpatterns recently aboost the weightingsource basedboost ing moreover ebboost however sofar ourworkis selective knowledgetransfer wepropose thenovelfactor conclusions inthispaper weproposedtoperform selective knowledge theproposed weembed getdomain worlddatasets of the sitylevels furthermore our ov acknowledgments references burgardandd roth editors aaai sanfrancisco california usa august aaai press dai yang xue andy yu boostingfortransfer learning inghahramani pages eatonandm desjardins basedboosting inburgardand roth eaton desjardins andt lane modelingtransfer transfer in pages eldardiryandj neville across modelcollectiveensem bleinburgardandroth ghahramani editor machinelearning proceedingsofthe twenty icml ore gon usa june volume of acminterna acm hofmann in sigir pages hofmann ing acmtrans inf syst jin si zhai andj callan collaborative in pro ceedingsofcikm koren bell andc volinsky matrixfactorization computer li yang andx xue rate cross tion in proceedingsofthe onintelligence pages li yang andx xue tiveviaarating in interna volume mehtaandt hofmann in ki pages panandq yang october pan xiang andq yang transferlearningin in aaai paterek tionforcollaborative workshop pennock horvitz lawrence andc giles ahybrid memoryandmodel basedapproach in proc ofuai pages rendle acmtrans acmtist may salakhutdinovanda mnih ization in nips salakhutdinov mnih andg hinton restricted mani pages shi paiement grangier andp yu learning in sdm pages siam omnipress shivaswamyandt jebara ing proceedings track singhandg gordon in kdd pages xue dai yang andy yu topic bridgedplsa forcross domaintextins myaeng oard sebastiani chua andm leong editors sigir pages acm yaoandg doretto multiplesources in cvpr pages zhang cao andd yeung multi domaincollabora tivein uai pages 
proceedingsofthe andthe pages beijing china july 
cuephraseandblstm cnn liangwang sujianli xinyanxiao andyajuanlyu pekinguniversity moe china baiduinc beijing china xuzhou jiangsu china intfloat lisujian pku edu cn xiaoxinyan lvyajuan baidu com abstract fortopicsegmentation inthispaper termmemory blstm model cnn besides quality andourminedcue also keywords topicsegmentation neuralnetwork webdocuments sequencemin ing introduction nlp task whichaimstosegment previousworkmainly texttiling lexicalcohe segmentsislow therefore minetopicboundaries however anaccuratemodel andtopictiling onthecontrary model itsper inthispaper weconductex bothofthems itpreserves bothsides and achievesbestf score inour work forexample ﬂare phrasesautomatically wetreatcuephraseasa apriori algorithm this forthetime thepossibility relatedwork unsupervised and whilec ferentwords lsa andlatentdirichlet allocation lda taionsystem similartolda asvisiblevariables hiddenmarkovmodel hmm wereproposed ilaritybasedmodels able therein cohesion supportvectormachine svm domains trivial met ricssuchasprecision recallandf metric wasproposedby tocalculate weneedaslidingwin dowofedlength now however win dowdiff wd lstm rnn whichaimsto towoveralongdistance aswordsegmentation andpart of speechtagging etc models blstm unit inputgate forgetgate andoutputgate denotestimestep tanh tanh tanh tanh tanh here are isthe arecorrespondingbias tanh isusedasnon comparedtolstm ofasequence theoutputvector isconcatenationof thesetwolstm belowshowsblstm outputgate we tanh tanh fig blstm exampletext amongthoseequations denotesbackwardpass layer isaconcatenationof andtherefore forourtask softmax pred argmax argmax one inthispaper weadoptapopularcnn inother words eachword vector bymatrix vectorproduct isa is one hd toawindowof forexample afeature ofwords by isabiasterm isanon tanh this toproduce afeaturemap with max max parametersofcnnand paragraph modellearning andusecrossentropy lossfunction log log where ismodel probabilityoutput totrainournetwork weusemini sgd with adadelta convergenceproperty features whenwritingarticles suchasﬁ collect consuming moreover cuephrasesare domain inthispaper apriori algorithm algorithm input corpus disadocument minsup maxlen output function mine minsup maxlen counteachword count minsup fg for to maxlen do candidate gen for candidate in do for eachdocument do if is subsequence candidate then candidate count candidate candidate count minsup maxlen return function candidate gen len candidates fg for in do for in do if len len andco occurrence minsup then candidates candidates len len return candidates is subsequence candi date and co occurrence is subsequence candidate checkswhether given candidate ingstrategies inmatchingstrategy acan didatecuephrase matchesaparagraph when isaof similarly insuf matchingstrategy acandidatecuephrase matchesaparagraph when isasuf of co occurrence thatboth is subsequence and is subsequence evaluateto true can foreachpair oflength len cuephrasesequence and ifthe len sufof isequalof len of andtheirco minsup then and len candidate theco notnecessary inourexperiments candidatesfromover downtolessthan foreach candidate documentsthis candidate sequenceappears than minsup getintotheresultset however the seconds cuephrasesequence xxx xxx xxx xxx xxx xxx xxx xxx xxx xxx xxx xxx englishexplanation second xxx third xxx second xxx third xxx toolsxxx methodxxx precautionsxxx xxxintroduction xxxintroduction xxxintroduction table itisclearthatouralgo asexpected peopleoftenusenumber umentsonweb handcraftedcue comparedtotime efient otherfeatures besidescuephrase lexicalfeature modeldescribedabove part of speech pos feature we jieba the lengthfeature pre currentdocument positionfeature sparagraphor lastparagraph hyperlinkfeature perlink https github com fxsjy jieba textfontfeature forwebdocuments they tacticfeatures here experiments dataandsetup andconsistsof table shows numberofdocuments table validationset testset scoreonvalidationset then reportmodel keras weuse adadelta mini batchsizeis forcnn thenumberofissetto thewindow sizeissetto for dconvolution issetto word embeddingmatrix ascomparison crf similartoblstm seffectiveness weevaluateonmul tiplemetrics precision recall scoreand results table showsmodel unsupervisedanddoesn tneedanyfeature noticethatfor metric smallervalue https github com fchollet keras https github com tpeng python crfsuite score texttiling crf lstm cnn blstm cnn table beling modelsonothermetrics lstm allother cnn itsf scoreis lowerthanblstm cnn and metric higherthanblstm cnn mentation blstm highestrecall highest scoreandlowest featureset precisionrecallf score cuephrase table comparisonofblstm withblstm cnnmodel pleaserefertosection feature to umentsinalldomains notjustwebdocuments hyperlink feature boldanditalic text feature scoregoesupby and valuegoes downby table erroranalysis byanalyzingbadcases topicstructure task havealinearstructure however ture manysubtopics forexample computer differentsoftwares ics however and forexample one the lyingtopic ourmodelneedtohavea sglobalstructure structure inthispaper tionofwebdocuments learning directions matically forfuturework andotherlanguages also acknowledgements this andnational and thecorrespondence references agrawal srikant etal in proc th int conf verylargedatabases vldb vol pp beeferman berger lafferty machine learning carroll in gies the pp chen qiu zhu liu huang longshort in chiu nichols cnns arxivpreprint arxiv choi in proceedingsofthe pp choi wiemer hastings moore in inproceedingsofemnlp citeseer du buntine johnson in hlt naacl pp eisenstein barzilay in proceedingsof pp asso galley mckeown fosler lussier jing partyconversation in proceedingsofthe volume pp georgescul clark armstrong in pp hearst texttiling compu tationallinguistics hochreiter schmidhuber longshort termmemory neuralcomputation jameel lam in proceedingsofthe pp acm pevzner hearst mentation riedl biemann in pro ceedingsofthe pp associationfor riedl biemann riedl biemann topictiling in proceed ingsofacl pp linguistics wang qian soong he zhao part of longshort arxivpreprintarxiv yamron carp gillick lowe vanmulbregt ahiddenmarkovmodelap in acoustics proceedingsofthe vol pp ieee zeiler adadelta arxivpreprintarxiv 
proceedingsofthe emnlp pages october doha qatar 
ixed recision raining sharannarang gregorydiamos erichelsen baiduresearch sharan gdiamos baidu com pauliusmicikevicius jonahalben davidgarcia borisginsburg michaelhouston oleksiikuchaiev ganeshvenkatesh haowu nvidia pauliusm alben dagarcia bginsburg mhouston okuchaiev gavenkatesh skyw nvidia com bstract weintro precision ingpointnumbers parameters onrecentgpus speedsuparithmetic weights activations precisionformat precisionwe firstly precisionfor theforward andback propagation secondly weproposeloss scalingtopre thirdly weusehalf precisionarith precisionoutputs exceeding millionparameters modelarchitectures ntroduction heetal tolanguagemodeling jozefowiczetal tomachinetranslation wuetal andspeechrecognition amodeietal forexample theneural had unit gru modelsinamodeietal theserequirements performance speed of anyprogram arithmeticbandwidth memorybandwidth orlatency limiters values precisionmath forexample half to higher thanforsingle precision precision fp format inthispaper weaddress wetrainvari equalcontribution google com arxiv cs ai feb precisionformat fp sincefp formathasanarrower dynamicrangethanfp maintain loss andfp training trained forregression andgenerativetasks generation objectdetection languagemodeling machinetranslation andspeechrecognition the parameters elated ork cnns with reducedprecision courbariauxetal allotherten hubaraetal theactivations hubaraetal and bits rastegarietal binarizealltensors however alloftheseapproaches leadtonon tiontask russakovskyetal zhouetal quantizeweights activations andgradients models mishraetal improveonthetop however thegradientsare thewidenedbaseline guptaetal demonstratethat itisnotclearhow rnns heetal trainquantized variantsofthegru choetal lstm hochreiterand schmidhuber accuracy hubaraetal theyevaluatebinary ternary first fp in ourcase second nohyper parameters suchaslayerwidth areadjusted lastly modelstrained precisionbaselines we of the artmodels mplementation fp trainingsession single loss scaling andaccumulating fp productsintofp fp mastercopyofweights weights inordertomatch theaccuracyofthefp networks anfp ineachiterationanfp figure training figure whiletheneedforfp learningrate than becomeszeroinfp wecanseeinfigure bthatapproximately ofweightgradient usinga single in thiscase thiscanhappenwhen sincefp has bitsofmantissa shiftedby ormorepositionsto largerthan shiftedby ormorepositions thiswillcausethe effectforde normalizednumbers again infp mastercopyofweights hours ofspeechdatafor epochs asshownin wematchfp fp whileupdatingfp weights resultsin relativeaccuracyloss weightsby smaller propagationpass sinceactivations arealsostoredinhalf precisionformat ossscaling fp whilegradient negativeexponents forexample considerfigure duringfp liuetal notethatmuchof thefp by training dev curvesformandarin figure figure baseline fp pseudofp with fp mastercopy pseudofp withoutfp mastercopy figure weights the figure for example ofthevaluesareinthe range ofvaluesareinthe range and ofvaluesarezero gradientvaluesbelow butvaluesin the priortostartingback propagation bychainruleback propagation duringback weightgradi training it othergradient relatedcomputations ensuringthatnohyper parameters threshold weightdecay etc havetobeadjusted stantscalingfactor to icallyor back propagation forexample nextiteration rithmeticprecision vectordot products reductions andpoint wiseoperations fp vectordot value whichisconverted tofp somefp modelsdidnot multiply addoperation inputmatricesand orfp outputs nvidia largereductions suchreductions mostlycomeupinbatch tensorsfrommemory arememory point wiseoperations suchasnon wisematrixproducts arememory bandwidthlimited either fp orfp mathcanbeused esults models baseline fp single weightsandgradients mixedprecision mp fp weights activationsand anfp loss experimentswithfp arithmeticusedtensor forconvolutions fully connectedlayers and smaxwellorpascalgpu mixedprecision thataccumulatesfp productsintofp themixed section fp storageonly voltahard ware gpus accumulatingfp productsintofp onamemorywrite cnn sfor ilsvrcc lassification russakovskyetal usingmixed precision alexnet vgg googlenet inceptionv inceptionv andpre activationresnet accuracyofbaselinefp trainingsession usingidenticalhyper parameters jiaetal framework tousevoltatensorops exceptforresnet whichusedpytorch paszkeetal whenavailable dhasnotbeenpublished top baseline fp croptestinganda randomcroppingfrom images resnet table ilsvrc top accuracy model baseline mixedprecision reference alexnet krizhevskyetal vgg simonyanandzisserman googlenet inceptionv szegedyetal inceptionv ioffeandszegedy inceptionv szegedyetal resnet heetal loss updatedinfp asoutlinedinsection etection cnn network whereprob approaches faster rcnn renetal andmultibox ssd liuetal bothdetectors usedvgg networkasthebackbone girshick liu meanaverageprecision map testset faster rcnnwastrainedonvoc trainingset and data table model baseline mpwithoutloss scale mpwithloss scale fasterr cnn multiboxssd diverges ascanbeseenintable withoutloss scaling bylosing asdescribedinsection diverges asdescribedinsection loss scalingfactorof andmixed map peech ecognition modelforbothenglish andmandarindatasets dcon volutionlayers temporal ctc costlayer gravesetal ithasapproximately millionpa rameters hoursofenglishspeech millionparameters theman forthesemodels werunthe baselineandpseudofp experiments epochsusingnesterov sgd allhyper annealingschedule experiments table showstheresults table charactererrorrate cer english testset model dataset baseline mixedprecision english mandarin also thenumberoftime asshownintable pseudo fp resultsareroughly to precision figure lstmmodelwith attention ref ref andref trainingruns achine ranslation google themodelusedword vocabularies kand kentriesfor englishandfrench respectively or decoder each lstmcells trainonwmt dataset withthesamesettings forexample seethethreefp curvesinfigure whichshowsthe layer model mixed precisionwithloss scalingmatchedthefp results whilenoloss scalingresulted the anguage odeling designatedasbiglstm jozefowiczetal onthe billionworddataset dimensionalembedding kwords duringtraining negativesamples gpusis tomatchfp perplexitytraining thisnetworkwithfp requiresloss scaling asshowninfigure perplexitycurveforfp trainingdiverges comparedwiththefp training after kiterations scalingfactorof training dcgan results gans ing forimagetasks inourcase thegeneratorpredicts threechannelsof bitcolorvalueseach pixelim agesoffaces radfordetal andcelebfacesdataset liuetal figure figure fp training left andmixed precisiontraining right thegeneratorhad layersoffractionally stridedconvolutions withleakyreluactiva tions with tanh thediscriminatorhad convolutions and fully connectedlayers allused whichusedsigmoid adamoptimizerwas usedtotrainfor kiterations gansdonothaveawidely andmixed precision scalingtomatchfp results onclusionsand uture ork wehave parametertuning smallgradientvalues accuracyasfp baselinemodels onvoltagpusee xspeedupscomparedto fp speedupsarelower limited https github com baidu research deepbench experimentsinthis to speechsystems furthermore automatingloss loss whenanovwisdetected eferences amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos etal deepspeech end to mandarin in proceedingsofthe pages cho vanmerri enboer gulcehre bahdanau bougares schwenk andy bengio arxivpreprintarxiv courbariaux bengio andj david binaryconnect inc cortes lawrence lee sugiyama andr garnett editors pages curranassociates inc url http papers nips cc paper binaryconnect training deep neural networks with binary weights during propagations pdf girshick fasterr cnngithubrepository https github com rbgirshick py faster rcnn google twtutorial sequence to sequencemodels url https www tensorflow org tutorials seq seq graves fern andez gomez andj schmidhuber in proceedingsofthe rd pages acm gupta agrawal gopalakrishnan andp narayanan precision in proceedingsofthe icml pages hannun case casper catanzaro diamos elsen prenger satheesh sen gupta coates etal deepspeech scalingupend to endspeechrecognition arxivpreprint arxiv he zhang ren andj sun in proceedings pages he zhang ren andj sun in eccv he wen zhou wu yao zhou andy zou arxivpreprintarxiv hochreiterandj schmidhuber longshort termmemory neuralcomput nov issn doi neco url http dx doi org neco hubara courbariaux soudry el yaniv andy bengio in pages hubara courbariaux soudry el yaniv andy bengio quantizedneuralnet works arxivpreprint arxiv ioffeandc szegedy batchnormalization inf bachandd blei editors icml volume of pages jmlr org url http dblp uni trier de db conf icml icml html ioffes jia shelhamer donahue karayev long girshick guadarrama andt darrell caffe arxivpreprintarxiv jozefowicz vinyals schuster shazeer andy wu modeling url https arxiv org pdf pdf krizhevsky sutskever andg hinton inf pereira burges bottou andk wein berger editors pages curranassociates inc url http papers nips cc paper imagenet classification with deep convolutional neural networks pdf liu ssdgithubrepository https github com weiliu caffe tree ssd liu anguelov erhan szegedy ands reed ssd tor corr abs url http dblp uni trier de db journals corr corr html liuaesr liu luo wang andx tang in proceedingsof iccv mishra nurvitadhi cook andd marr wrpn widereduced precisionnetworks arxiv preprintarxiv year nvidia nvidiateslav gpuarchitecture https images nvidia com content volta architecture pdf volta architecture whitepaper pdf ott lin zhang liu andy bengio precision arxivpreprintarxiv paszke gross chintala chanan yang devito lin desmaison antiga anda lerer radford metz ands chintala corr abs url http dblp uni trier de db journals corr corr html radfordmc rastegari ordonez redmon anda farhadi xnor net imagenetusing pages cham isbn doi url https doi org ren he girshick andj sun fasterr cnn towardsreal in nips russakovsky deng su krause satheesh ma huang karpathy khosla bernstein berg andl fei fei lenge ijcv doi simonyananda zisserman scaleimagerecogni tion arxivpreprintarxiv szegedy liu jia sermanet reed anguelov erhan vanhoucke anda ra binovich in cvpr url http arxiv org abs szegedy vanhoucke ioffe shlens andz wojna in cvpr june wu schuster chen le norouzi macherey krikun cao gao macherey etal google arxivpreprintarxiv zhou ni zhou wen wu andy zou dorefa net corr abs url http arxiv org abs 
pages nagoya japan october 
multi transfer bentan erhengzhong evanweixiang qiangyang abstract transferlearning iarydomains tapplications textmining sentimentanal ysis etc inaddition inmanyreal worldapplications forexample whichincludethree views perspectives image voiceandsubtitles onemay borrowdatafromflickr last fmandgooglenews actually satewitheachother inthispaper wethistrans ferlearningproblemas astsources other giveoptimalresult thus collaboratively bylettingtviewsfrom training styleframework intdomains severalreal to againsttstate of the artbaselines keywords transferlearning multi viewlearn ing multipledatasources introduction inreal worldapplications thelackoflabeleddata accuratemodels lem domainmod elperformance ed rangingfromtext sentimentanaly sis eventrecognition tomultimediaanalysis source andsingle viewscenario infact manyreal worldappli btan ezhong wxiang qyang cse ust hk forexample invideo analysis suchasimages voice andsubtitles wheredatafrom voicefromlast fm andimagesfromflickr ongooglenews where newsgroups and reuters intherecentyears view mvtl setting orthemulti sourcesetting mstl infact in manyreal worldapplications multi viewinformationis forex ample invideoanalysis threetviews then timage text wethis problemas multiplesources andtl mvmsforshort anintuitive orallviewstogether stlrespectively unfortunately tsourceshave butions suchintuitiveso information forexample songsonlast fmandim videos ononehand ifweapplymstl tviews http news google com http www last fm http www flickr com http qwone com jason newsgroups http www daviddlewis com resources testcollections beremoved ontheotherhand ifmvtlisapplied dif recently co training beetoutilizemulti viewdata wherea thus eventhe theycancompen however applyingco targetdomains onaccountofthejoint distributionthe tiplesourcesintl mvms weextendco trainingac multi transfer multi lengesfromtwoaspects getinstances secondly itappliesadensi parametricmethod tomeasurethejoint fromtwodomains thisstrategyre weightsthein getdomain wewillshowthat ononehand theco ontheotherhand of the arttransfer problemformulation wetheproblem tl mvms asfol lows let where isthenumberofsources foreach wehave where denotesthe thsourcedomain let where wetheviewset sas where isthenumberofviews foreachsourcedomain itsviewsetisas table ofnotations notation notationdescription sourcedomains the thsourcedomain thetargetdomain theviewsetofthe thsourcedomain numberofinstancesin numberofinstancesin numberofviews jointdistributionof figure tl itsview setis let and denotethemarginal softhe and and betheparallelforthe targetdomain thegoaloftl mvmsistobuildmod elsfor withthehelpof weemphasizethatthis isageneralframework thebetweentl tra ttl multi viewlearning multi viewtransferlearning mvtl andmulti source transferlearning mstl tl mvms multi viewlearning and ttl and mvtl and mstl and clearly andtarget existingmulti basedonsourcedata inaddition ttlandmstldo notconsiderthemulti viewsetting andhencecannot finally on mvtl themulti transferalgorithm trainingframework forsimplicity wherethetarget multi transfer basedontheco trainingframe work ineachiteration webuildtwomodels and fromtwotviewsets and fromtwod tsourcedomains and respectively then analog toco training however duetothe domainmulti wochallenges getdataand forbuildingconsis tentmodels wepropose selectioncriterion domains then weintroduceanon parametricmethod instance butionshift ineachiteration multi ferent for mally let domainand sourceinstances then if weobtain lim instead instance itspredic ability thus weamarginaldistance measure under view targetview theindex alsoindicatesthe thsource domain however werewriteitas log log log weestimatetheratio viagaussian process gp gaussiandistribution sp undertheview tobenegative subsequently afterbuildingthegp model gp gp oftwodomainsarettoo the measure inco training thus weproposea harmonic functionbasedmethod whichissimilartothe onein let ij exp denotethe theharmonicmeasureof is ij where and is itsphysical underthecluster manifoldassumption theytendtohave thesamelabel weobtainthat iftheconditionaldis doesnothold thepredic tionsof sometargetinstances consequently iftheinstances thevalueofeq com biningeq andeq together weobtaina selectioncriterionas log ij where then ineach iteration weselect values duetothejoint distributionshift themodels onthetargetdomain inotherwords foragiven instance wesolvethisproblemby distributions weshow that afterweighting in modelbuilding weaimtobuildamodel inthesource domain min clearly duetothe irrelevanceto and term as followswhen dy dy toestimatetheratio weextendthemethodin let and let representthetarget dataundertheview viewineachiteration insource wemodel asakernelfunction where and andlabels we exp where isthekernelparameter theestimated parameters sothatthekullback leiblerdivergence from to canbeminimized kl log dxdy dxdy log dxdy wecanignoretheterm whichisindependentof theparameters inaddition weaddanormalizedterm fortheparameters since isa theobjectivebecomes max log algorithm multi transfer input sourcedomains unlabeledtargetdata output builtmodel setinstances weightsof and as and buildtwomodels and using and respectively let for to do performpredictionon using performpredictionon using selectinstancesfrom and as and usingeq re buildtwomodels and using and endfor return figure mainflowofmulti transfer where and thisisaconvex canbeobtained afterlearningthe wecanobtain theproposedframework theframeworkof multi andalgorith initially weightedas and respectively ineachitera tion tviewsets afterthat theyareutilizedtopre sets beledtargetdata using second thesetwo finally thepredictions thepredictions wenoticethat ifwemergetwo viewtransferlearn ingapproaches modelbuilding inaddition ifweignorethemulti approaches viewcompensa suppose andthetimecomplexity ofthebasemodelis ineachiteration multi transferneeds log to then itneeds instanceand toupdate insummary the experiment asyntheticexample webeginbyanalyzing multi whichhastwo tviews asshowninfigure clearly sourcedomains audiofeature main figure figure withthetargetdomain figure respectively inaddition instancesintdomains followtdistributions sp datafromdif undertviews firstofall asdif domain thetargetdata targetdomain thatistosay targetdata saudiofea tureisequaltozero dotlinesinfigure wenoticethat usingsingle stances however together theboundary thedashline candiscriminate thus weshouldexploit however wecannot directlyapplymulti co training forexample in figure boundary willassign withhigh likewise labelstopointsaround although whichac followedbypass weget figure audio feature arenewlyadded thesedatawillpushthe anddeterio ratetheperformance sourcedomain canbefoundinfigure onthe contrary theselection processofmulti itse forexample datainsourcedo main and intoaccount willselectpoints around and intargetdomain forsourcedomain figure likewise er fig ure provetheperformance afterre weightingnewtrain constructedbyco trainingandmulti transferareshown infigure transfer truthboundary thesolidline whileco experimentalsetting weevaluatetheperfor manceofmulti world textdatacollections newsgroupsandspamde tection of the artmeth ods latentmap co adaptation andco training learningparadigms mainundersingle view co adaptationisamulti view andco trainingisa representativemulti theper unlabeledtargetdata forco adaptation latentmap trainingand multi transfer svmandc areadopted datadescription dureisasfollows first aterm frequencyvector secondly toreducethenum beroffeatures ofthedocumentcoun finally ueintheexperiments the newsgroupsdata set suchas comp sci rec and talk categories suchas sci crypt and sci med weuse maincate goriestogenerate datasets ineachofwhichtwotop tiontasks foreachcat egory threeparts sourcedomain sd sourcedomain sd targetdomain td sd dataselectedbyct sd dataselectedbyct sd dataselectedbymt sd dataselectedbymt tdw boundaries figure ct forco training mt multi transfer boundaryct training boundarymt boundary constructedbymulti transfer table datasetdescription dataset sourcedomain sourcedomain targetdomain newsgroup rec vs comp autos misc baseball mac hockey windows rec vs tech autos guns motorcycles mideast hockey misc sci vs comp electronics graphics med misc space windows sci vs tech crypt guns electronics mideast med misc comp vs tech graphics guns misc mideast windows politics spamdetection filter user user user filter user user user filter user user user therefore purpose togeneratethemulti viewinmulti source notidentical tothisend eachof whichdiscussestsub categorytopics wecan each domainspe vocabularies domain and whicharerespec besides the tinctsub categorytopics thespamdetectiondata set isfromtaskaofecml pkdddiscoverychal lenge users eachofwhichhas emails theemailsofa userconsistof spamsand non spams inad dition thatistosay wellascommonones inourexperiment weusetwo targetdomain table to in alldatasets while performance figure co adaptation co trainingandmulti transfer ineachthe thanthatwithlowerone multi thebestaccuracy latentmapandco viewsetting sp adaptationdoes besides co transfer withinmulti transfer sincesvmisknown problem however whenusingc asbaselearner multi rec vs comp rec vs tech sci vs comp sci vs tech comp vs tech filter filter filter figure lm latentmap ca co adaptation cts co trainingwithsvm ctc co trainingwithc mts multi transferwithsvm mtc multi transferwithc training especially on ngsci vs compdataset the accuracyofmulti transferisover percenthigher ofmodelparameter asmentionedbe fore whichdirectly impacttheperformance theoneisthenum theselectionprocess while onunlabeleddata fromfigure wecanseethe so aresuitablechoices fromtheresult wecanalsosee training secondly datawithmosttlabels isalsoimportant forupdating value isused onthecontrary largevalue thisphenomenon canbeseeninfigure however addingalarge algorithm weadd therefore the afterwards asshowninfigure fromfigure converges intheexperiments wethatsetting maximumiterationas workswellinmostcases modelanalysis besidestheparameters base ontheaccuracy weconductextensive firstly thestrate forexample anal tontwodomains inthismethod weusethisstrategyon ng srec vs compdataset andgetitsaccuracyof whichlies transferwithsvmor asbaselearner also fromthefigure we secondly ofmulti transfer forinstance inbothsourcedomains andtargetdomains thentrainonthesein completeviews bychangingtheremoval ratio which weobtaintheper in thiswecanseethateven commonfeatures ineachviewareremoved singlecompletedomain ed thisallowsusto weobtainthe clearly in thissetting singlesource inaddition theofeachsourceis quitet viewknowledgecollabo finally twoviews ofmulti transferandco convergence figure parameteranalysison ng srec vs compdataset heterogeneous tsourceratios accuracyvs coverage singlesource figure modelanalysison ng srec vs compdataset weplotthemeans wheremulti adaptation edontextdatasets thereasonisthat twordsindatasets anyviewcanbereplaced byothertypeoffeature suchasimage relatedworks viewlearning generally the multi viewlearning inmanyreal worldapplica tions co training isarepresentingmulti viewlearningmethod in termintomulti viewsemi however mostexistingmulti forthesingle domain tll trans ferlearning tl instanceweighting andfeaturemap ping howev er view multi viewtransferlearning mvtl sev forexample theco usesthelabeled appliestheco forthetargetdomain co traininghasbeenex tendedtocross electionprocess recently amaximalmarginbased method view how ever multi mstl sourcetrans ferlearning forexample theworkin extends tradaboost presentsa sensus however viewsetting recently in whichaddressesamulti tasklearningproblem undermultipleviews itproposesagraph basedalgo ttasks however shiftamongdomains conclusion inthispaper ferlearningproblem wherethesourceandtar mains transferalgorithm works multi andmulti domain knowledgetransfer followingtheco trainingprocess ineachiteration besides weproposed distributionshift inad dition sourceinstance forthetargetdomain newsgroupandspamde tection state of the onaccuracy inourfuturework andonamore inaddition acknowledgement kongcergprojects andhongkong itfprojectghx references mikhailbelkin parthaniyogi andvikassindhwani mani foldregularization ingresearch labeleddatawithco training in pages ritachattopadhyay jiepingye sethuramanpan chanathan weifan andiandavidson multi sourcedo fatigue in proceedingsofthe thacmsigkddinterna pages chen weinberger andj blitzer co trainingfor domainadaptation in processingsystems wenyuandai qiangyang gui rongxue andyongyu in proceedingsofthe th pages lixinduan dongxu andshih fuchang exploitingweb amultiple in ieeeinternational cvpr multi taskmulti viewlearning in proceedingsofthe th pages kanamori suzuki andm sugiyama theoreticalanal sciences guangxiali stevenc hoi andkuiyuchang two view in proceedingsofthe pages pingluo fuzhenzhuang huixiong yuhongxiong and qinghe in proceedingsofthe th ment pages ing ing october shimodaira likelihoodfunction journalof gokhantur co adaptation adaptiveco trainingforsemi supervisedlearning in proceedingsofthe ieee speechandsignal processing pages christopherk bayesian ieeetransactionson sihongxie weifan jingpeng olivierverscheure and jiangtaoren in proceedingsofthe pages junyang rongyan andalexanderg hauptmann cross in pro ceedingsofthe pages boostingfortransfer in the rdieeeconference pages danzhang jingruihe yanliu luosi andrichard lawrence multi ginapproach in proceedingsofthe thacmsigkdd mining pages erhengzhong weifan jingpeng kunzhang jiangtao ren deepakturaga andolivierverscheure crossdomain in proceedings ofthe pages xiaojinzhu zoubinghahramani andjohnd semi functions in proceedingsofthe pages 
neuralarchitect amulti yanqizhou sunnyvale california zhouyanqi baidu com gregorydiamos sunnyvale california gregdiamos baidu com abstract nas sources hundredsofgpus rl sicparallelism moreover ciency for instance resource awaremulti insteadof weusenetworkembed basedontheembedding weintroduce multi com putationalresource there thatarepre neuralar chitectcan ciently withmulti wecan nde cientnetwork introduction beenproposed however whichmakes students forexample toexpeditetheprocess google uses gpus moreover sourceconstraints memoryre quirement fordi ciency asaresult those insizebutlesse cient forexample butthetotalnumber cantlyhigherthan densenet inthiswork asshowninfigure resource awaremulti figure high apolicynet thegoalis to ndresource objective guration neuralnetworks amulti and trainingtime erentneeds modelaccuracy tillconvergence methodology figure showshigh neuralarchi simulationnetwork actionssuchas insertalayer or scalealayer the figure networkembedding gurationintoa policynetwork the accuracy modelsize userspeci cation thecon nd memorysizeandgputime networkembedding gura figure showstheembedding network lookup vectors beenproduced networkembedding policynetwork opera network aninsertnetwork generateslayertype layersize thatlayer anexistinglayer ltersize anddropout rate figure insertconv insertsaconvolution insertadd figure insert conv and insertadd whichissimilarto residuallayer multi objectivereward weusea theperformance distribution leveragingtheembed dingnetwork foreachnet work andtrainingtime andthel itcanbe thetraining tensor ow runningonav aroo inemodel forthelattercase iftrainedjointly avaluenetwork theparameters lo gˇ theparameters descentusing ºº inthemulti wepenalizelargemod elsbyapplyingapiece forinstance wecanstartapplying mb references chrisanthafernando dylanbanarse malcolmreynolds fredericbesse marclanctot anddaanwierstra convolution byevolution di https arxiv org abs gaohuang zhuangliu kilianq weinberger https arxiv org abs ristomiikkulainen jasonliang elliotmeyerson adityarawal danfink olivier francon balaraju hormozshahrzad arshaknavruzyan nigeldu andbabak hodjat https arxiv org abs estebanreal sherrymoore andrewselle saurabhsaxena yutakaleonsue matsu jietan quocle andalexkurakin large classi ers https arxiv org abs barretzophandquocv le learning https arxiv org abs barretzoph vijayvasudevan jonathonshlens andquocv le learning https arxiv org abs 
listen interactandtalk learningtospeakvia interaction haichaozhang haonanyu andweixu baiduresearch sunnyvale ca zhanghaichao haonanyu xuwei baidu com abstract oneofthelong mostexistingwork collecteddataset withannotatedlabels moreover thisis wherelanguageis thispaperpresents whereanagent toachievethisgoal introduction andtherefore laborforannotating humansactupon skinner formechanicalactions suchasmovement whilefor language nodding kuhl weston for example kuhl skinner forlearningtospeak parter parent sentence thisisapple ﬂwhilepointing later whatisthis ﬂwhilepointing arxiv cs cl may ššœquestion answer feedbackššœ teacher whatisonthenorth learner on cabbageyeseast teacher onthenorthisavocado ššœstatement repeat feedbackššœ teacher onthewestisorange learner onthewestisapple teacher noorangeisonthewest šš learner statement feedbackššœ teacher learner cucumberisontheeast teacher cucumberisontheeast training ššcompositional generalizationšš teacher whatisontheeast learner avocadoisontheeast teacher teacher whereisavocado learner avocadoisontheeast teacher ššš knowledge transferringššš teacher whatisonthesouth learner onthesouthisorange teacher testing figure duringtraining question answer feedback statement repeat feedback and certain referredtoas inactivecombinations objects duringtraining forexample thecombinationof avocado east answersessions theobject orange neverappearsin question repeatsessions denotedas and inthe duringtesting teacher inactivecombinations objects avocado east and questionsabout orange and knowledge transferring section toanobject yes no withanodding smile kiss feedback fromababy sperspective fromparent interactive apropertywhich setting inspiredbybaby inthissetting instead thelearnerhasto actin ordertolearn conversationskills usinga languagemodel theagenttrainedby well trainedparrot stadieetal yes no ﬂandﬁ you ﬂmightneedtobe removed thiscannotbeachieved withimitationonly ontheotherhand inabilityofspeaking low tfullydeveloptheir language relatedimitation inthispaper weproposea learning reinforcement wepresentanovelhuman likeinteraction teacher innaturallanguage insection section based finally relatedwork forexample machinetranslation sutskeveretal imagecaption ing maoetal vinyalsetal antoletal anddialogue responsegeneration vinyalsandle wenetal fortraining datacontainingsource targetpairsisneeded collect ranzatoetal bahdanauetal lietal ormetriclearnedinan adversarialsetting yuetal lietal whichisnon differentiable leadingtotheusage differentfromthem foersteretal sukhbaataretal lazaridouetal mordatchandabbeel the processing mordatchandabbeel dif ferently generation speaking post processing strubetal dasetal these goal teacher space heetal weston lietal however intheseworks heetal weston lietal fromthecandidateset thusisessentiallya discretecontrol problem incontrast ourmodelachieves continuousspace interaction figure attimestep accordingto avisualimage whichcanbeaquestion whatisonthe east whereisapple astatement bananaisonthenorth oranemptysentence denoted asﬁ ssentence andthevisualcontent andproducesasentence response totheteacher andreward thesentence represents yesontheeastischerry noappleisontheeast and modelsthenon smile kiss hug whichalsoappears weusetheterm agent interchangeablywith learner thisapproachwon perspective stadieetal asmentionedearlier problemformulation foroneepisode giventhevisualinput andtextualinput theresponse ofthespeakingaction theagent interacts andreceivesthe feedbacks from teacherattimestep as and withes yes no addedwitha probabilityofhalf figure reward isascalar utterance thetaskofinteraction learningbyconversing sfeedbacks mathematically log imitation reinforce where generatedfromteacher isthe at timestep and to asforbothcomponents via interaction withtheteacher wetermedthistaskas interaction forthe imitationpart sverbalresponse forthereinforcepart itlearnsfromteacher srewardsignal imitation sbehaviors withincontext withoutanyexplicit labelingofground thewayof trainingisby predictingthefuture more isineffect thirdpersonimitation stadieetal conversingwithit reinforce they thisformof suttonandbarto last last encoding rnn action rnn shareparameter bos where is apple where is apple eos bos no west no west eos bos apple is on the south eos controller visual encoder visual encoder visual encoder spatialconvolution spatialsummation hadamardproduct mixaggregation att cnn att mask aggregated feature visualencoder last skipconnect controller figure networkstructure visual encodernetwork att withanattentionmap isappliedtothe attimestep theencoding rnntakesteacher ssentence whereisapple att asinputs rnn last tothe action rnnandaction rnn duringtraining controller sec approach yuetal serbanetal asshowninfigure attime step anencoding last toproducea controlvector asinputtotheaction totheteacher ssentence accordingtoboth and inadditiontobeing rnninthenextstep last forlearningfrom thus rnn theteacher onewaytolearnfrom wecan and currentimage as last last where last asthesummarizationof figure and thwordinthe th andwordsupto withinthe thsentence rnn thus last softmax att where and att att andvisualimage asinput redcubeinfigure bluecubeinfigure map rnnstate theinitialstateof theencoding last and ontheinput therefore rnnwithaction rnndirectly inputing rnnintoaction rnnastheinitialstate thelearnerwillhave however assentences canbesummarized asthelastrnnstate last last to network asfollows figure last last theagent sbehaviors controller last inpractice wealsoincorporatea gradient withinthecontroller residuecontrol otherwise therefore witha skipconnection asfollows where thereasonfor includes fromteacher weimplement astwofully gaussianpolicy ontheinputvector whichis rnn diag thusback tion suttonandbarto where isasub thevector gen figure forthereward ineqn weintroduce where parametervector asfollows where thevaluenetwork whoseparametervector mnih etal inpractice weaddasmallvalue to training sfeedback asatraining signal shownineqn for fromimitation module wehaveitsgradientas log suttonandbarto reinforcemodule log where isthetd erroras deeplearningplatform paddlepaddle duchietal with abatchsizeof andalearningrateof discountfactor experiencereplayis usedinpractice mnihetal experimentalresults fortrainingefciency askingaquestionasﬁ whatisonthesouth whereisapple ﬂand appleisontheeast ﬂandagents repeatthestatement sayingnothing fromteacher ifitbehavescorrectly generatesacorrect anda negativereward otherwise snon verbalfeedbacksuch as nodding xisontheeast ﬂorﬁ ontheeastisx ﬂand with yes no thereis sknowledge this inthissetting theteachergenerates reinforce srewardfeedback sutton andbarto imitation sbehavior sutskeveretal reinforce thisis and ontheotherhand the imitation reinforce duetothe speaking comparedapproaches similarbehaviors along what and where questions whenteachersays nothing https github com paddlepaddle paddle quantativeresutls table settings reinforce imitation proposed compositional gen knowledge transfer table out settings reinforce imitation proposed compositional gen knowledge transfer figure evaluationresults comparisonofthepro mixed held out denotes whatisonthenorth onthenorthisapple yesappleisonthenorth att map whatisontheeast ontheeastisavocado avocadoisontheeast att map whereisstrawberry att map ontheeastiscucumber att map figure exampleresults what questions where question teachersaysnothing foreachexample weshowthevisualimage the aswellasthe attentionmap att map generated overlaidontop right theattentionmapis renderedasaheatmap grid teacher learner positive negativerewards zero shotdialogue whilethis anapproach butdoesnothaveany referredtoas inactive objects whileintesting objects and locations aswellas novel object location forcertainobjects whileintesting learnedfromteacher mixed and held out andtable respectively mixedation held outation reinforce approach intheprevioussection the imitation reinforce mainlyduetoits notethattheheld outisasubsetofthe combinations case moredifheld conclusion experimentalresults scenarios asforfuturework knowledge yang andrychowiczetal another acknowledgements wethankxiaochenlian zhuoyuanchen references andrychowicz denil colmenarejo hoffman pfau schaul andn defreitas learning in nips antol agrawal lu mitchell batra zitnick andd parikh vqa visualquestion answering in iccv bahdanau brakel xu goyal lowe pineau courville andy bengio anactor critic in iclr das kottur moura lee andd batra corr abs duchi hazan andy singer optimization foerster assael defreitas ands whiteson agent in nips he chen he gao li deng andm ostendorf languageactionspace in acl kuhl natrevneurosci lazaridou peysakhovich andm baroni multi natural language in iclr li monroe ritter jurafsky galley andj gao generation in emnlp li miller chopra ranzato andj weston in iclr li monroe shi ritter andd jurafsky corr abs mao xu yang wang huang anda yuille networks rnn iclr mnih kavukcuoglu silver graves antonoglou wierstra andm riedmiller playingatari in mordatchandp abbeel agentpopulations corr abs petursdottirandj mellor policyinsightsfrom ranzato chopra auli andw zaremba in iclr serban sordoni bengio courville andj pineau buildingend to in aaai skinner verbalbehavior stadie abbeel andi sutskever third in iclr strub devries mary piot courville ando pietquin end to driven in ijcai sukhbaatar szlam andr fergus in nips sutskever vinyals andq le in nips suttonanda barto anintroduction mitpress vinyalsandq le corr abs vinyals toshev bengio andd erhan showandtell in cvpr wen gasic mrksic su vandyke ands young based in emnlp weston dialog in nips yang yu wang huang yang andw xu networks in cvpr yu zhang wang andy yu seqgan in aaai 
vol pp actioneditor holgerschwenk submissionbatch revisionbatch published distributedunderacc by license 
xploring parsityin ecurrent eural etworks sharannarang erichelsen gregdiamos shubhosengupta baiduresearch sharan gdiamos baidu com bstract rnn sohavemodelsizes of the artnetworks uateit ofthenetwork attheendoftraining thenetwork sizeisreducedby additionally inferencetimespeed benchmarksshowthatus andspeed upisaround to ntroduction graves jaitly amodeietal languagemodeling ozefowiczetal wuetal canbe forexample had andinlanguagemod elingthesizeofthenon embeddingparameters haveexplodedeven ozefowicz etal andchenetal in addition evenincases inmobiledevices hanetal sincemost thenthesmaller furthermore berealized resultinginasuper google com fb com arxiv cs lg nov enoughtoserveoneuser techniques whilethe increasingthreshold value arecurrentlayer separate perlayertype withthisapproach we gru choetal aswellasvanilla rnns pruningitdown furthermore suchasinhanetal weeksoftrainingtime soafurther elated ork neuralnetworks vanhouckeetal deniletal dentonetal jaderbergetal gong etal inthesamehashbucket chenetal onemethod hanson pratt yetanotherapproach lecunetal inthistechnique once aweighthasbeensetto optimalbrain trainingafterpruning hassibietal hanetal inthiscase pruning therehas speechrecognition asr luetal parametersofalstmby whileincurringa wer approximateahessian lecunetal hassibietal yetanotheradvantage thatneedre training hanetal table hyper hyper start itr ramp itr pruning startof oftotalepochs end itr rameters startof oftotalepochs start slope ramp slope pruning to freq isupdated noadditionalre training parameters forrecurrentnetworks luetal yuetal ataparticular epoch however mplementation atregular intervals thehyper parameterscontrol theduration weuseadifferentset ofhyper the we don propagationstep inthiscase forwardpassagain start itr ramp itr and end itr intable afterpickingthese ramp slope is start slope wecalculate using equation freq ramp itr start itr end itr ramp itr inordertodetermine inequation model thpercentileas pruning similarly wepruneallthe algorithm pruningalgorithm current itr while training do forall parameters do param param and mask if current itr start itr and current itr end itr then if current itr mod freq then if current itr ramp itr then current itr start itr freq else ramp itr start itr current itr ramp itr freq endif mask abs param endif endif endfor current itr endwhile xperiments setof hoursofmulti speakerdata state of the epochs besidesthehyper allotherhyper parameters idirectional rnn weusethedeepspeech asshownintable thismodelhas convolutionlayers followedby eachrecurrent linearlayerhas hiddenunits millionparameters for theseexperiments theforwardand theseexperimentsuse clippedunits relu min max inthesparserun th epoch wechosethesehyper attheendof pruning whichis cer onthe devsetisabout model table thussparsemodelis and hiddenunits figure parameters except parameters asweseeintable themodelwith hiddenunitsachievesa relativeimprovement whilethemodelwith hiddenunitshasa im provement thedense model thesparse modelisabout boththese table deepspeech architecturewith hiddenunits layeridtype params layer dconvolution layer dconvolution layer layer layer layer layer layer layer layer fullyconnected layer ctccost intheirapproach table millionto million withthernnsparse modelintable gradual pruningis to ofthemodelby to however itisbettertoprunea thanhardpruning table gru model unitscer paramsrelativeperf rnndensebaseline million rnndensesmall million rnndensemedium million rnnsparse million rnnsparsemedium million rnnsparsebig million grudense million grusparse million grusparsemedium million table unitsprunedepochcer paramsrelativeperf million million million million million figure dense andsparsetraining figure aincludes and hiddenunits comparedtothe densebaseline figure sparseanddense with parameters table layeridtype params layer dconvolution layer dconvolution layer gatedrecurrentlinear layer gatedrecurrentlinear layer gatedrecurrentlinear layer rowconvolution layer fullyconnected layer ctccost ated ecurrent nits thathave hiddenunitsinthegru layerandatotalof millionparameters fortheseexperiments figure the sparsegrumodelhasa asshownin table with millionparameters similar tothernnmodels hiddenunits thedatasetandthe sparsityof with millionparameters asshownintable themodelwith hidden unitsisonly inaddition fullyconnectedlayers worsethanthebaseline densemodel however connectedlayers table sec speedup rnn rnn rnn rnn gru gru gru erformance omputetime datasets we ageneralmatrix matrixmultiply gemm neuralnetworkmodel table ofhiddenunitsthatare sparse scudnn allexperiments arerunonaminibatchof andinthiscase vectorproduct spmv wecanachievespeed upsrangingfrom xto layer similarly forthegrumodels thespeed upsrangefrom xto however wenotice xspeedupthatwe seebaxter andliuetal thismeansthatthe additionally we sizes ompression thedeepspeech mbtoaround mb hiddenunits or mb hiddenunits thegrumodelcanbe compressedfrom mbto mb iscussion runing haracteristics figure thelayers istherecur tothelayers however experiments figure figure thesamehyper figure duringatrainingrun infigure trainedfor epochs iterations epochat iterations epochs halfthetotalepochs arecomplete at iterations weseethatnearly epochsarecompleteat around iterations inourexperiments we ersistent ernels diamosetal blockram chipmemory anvidiap hiddenunits withthesame datatype at sparsity and sparsity ap and additionally toindividualthreads performedonce and reusedmanytimes onclusionand uture ork thesesparse endserverfarmsdue evenwithexistingsub optimalsparse matrix upswiththesemodels regularization matrix cknowledgments eferences darioamodei rishitaanubhai ericbattenberg carlcase jaredcasper bryancatanzaro jing dongchen mikechrzanowski adamcoates gregdiamos etal deepspeech end to end arxivpreprintarxiv seanbaxter moderngpu url https nvlabs github io moderngpu segreduce html welinchen davidgrangier andmichaelauli languagemodels corr abs url http arxiv org abs wenlinchen jamest wilson stephentyree kilianq weinberger andyixinchen compressing corr abs url http arxiv org abs kyunghyuncho bartvanmerri enboer caglargulcehre dzmitrybahdanau fethibougares hol gerschwenk andyoshuabengio decoder arxivpreprintarxiv mishadenil babakshakibi laurentdinh marc aurelioranzato andnandodefreitas predicting corr abs url http arxiv org abs emilydenton wojciechzaremba joanbruna yannlecun androbfergus exploitinglinear corr abs url http arxiv org abs gregdiamos shubhosengupta bryancatanzaro mikechrzanowski adamcoates erichelsen jesseengel awnihannun andsanjeevsatheesh persistentrnns chip in proceedingsofthe pp yunchaogong liuliu mingyang andlubomird bourdev corr abs url http arxiv org abs towardsend to networks in icml volume pp songhan huizimao andwilliamjdally deepcompression withpruning corr abs awnihannun carlcase jaredcasper bryancatanzaro gregdiamos erichelsen ryanprenger sanjeevsatheesh shubhosengupta adamcoates etal deepspeech scalingupend to end speechrecognition arxivpreprintarxiv stephenjos chap propagation pp sanfrancisco ca usa isbn url http dl acm org citation cfm id babakhassibi davidgstork andgregoryjwolff pruning in neuralnetworks pp ieee maxjaderberg andreavedaldi andandrewzisserman corr abs url http arxiv org abs rafalj ozefowicz oriolvinyals mikeschuster noamshazeer andyonghuiwu exploringthe corr abs url http arxiv org abs yannlecun johnsdenker saraasolla richardehoward andlawrencedjackel optimal braindamage in nips volume pp xingliu mikhailsmelyanskiy edmondchow andpradeepdubey efsparsematrix vector multiplicationonx basedmany coreprocessors in proceedingsofthe thinternationalacm ics pp newyork ny usa acm isbn doi url http doi acm org zhiyunlu vikassindhwani andtaran sainath corr abs url http arxiv org abs vincentvanhoucke andrewsenior andmarkz mao cpus in nips yonghuiwu mikeschuster zhifengchen quocv le mohammadnorouzi wolfgangmacherey maximkrikun yuancao qingao klausmacherey jeffklingner apurvashah melvinjohn son xiaobingliu lukaszkaiser stephangouws yoshikiyokato takukudo hidetokazawa keithstevens georgekurian nishantpatil weiwang cliffyoung jasonsmith jasonriesa alexrudnick oriolvinyals gregcorrado macduffhughes andjeffreydean google sneural corr abs url http arxiv org abs dongyu frankseide gangli andlideng in icassp pp ieee 
