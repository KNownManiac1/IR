OcclusionAwareUnsupervisedLearningofOpticalFlow YangWangYiYangZhenhengYangLiangZhaoWeiXu BaiduResearch Sunnyvale,CA94089 f wangyang59,yangyi05,zhaoliang07,wei.xu g @baidu.comzhenheny@usc.edu Abstract Ithasbeenrecentlyshownthataconvolutionalneural networkcanlearnopticalestimationwithunsuper- visedlearning.However,theperformanceoftheunsuper- visedmethodsstillhasarelativelylargegapcomparedto itssupervisedcounterpart.Occlusionandlargemotionare someofthemajorfactorsthatlimitthecurrentunsuper- visedlearningofopticalmethods.Inthisworkwe introduceanewmethodwhichmodelsocclusionexplicitly andanewwarpingwaythatfacilitatesthelearningoflarge motion.OurmethodshowspromisingresultsonFlying Chairs,MPI-SintelandKITTIbenchmarkdatasets.Espe- ciallyonKITTIdatasetwhereabundantunlabeledsamples exist,ourunsupervisedmethodoutperformsitscounterpart trainedwithsupervisedlearning. 1.Introduction Videomotionprediction,ornamelyopticalw,isafun- damentalproblemincomputervision.Withtheaccurate opticalwprediction,onecouldestimatethe3Dstructure ofascene[19],segmentmovingobjectsbasedonmotion cues[38],trackobjectsinacomplicatedenvironment[12], andbuildimportantvisualcuesformanyhighlevelvision taskssuchasvideoactionrecognition[45]andvideoobject detection[62]. Traditionally,opticalwisformulatedasavariational optimizationproblemwiththegoalofpixelcor- respondencesbetweentwoconsecutivevideoframes[23]. Withtherecentdevelopmentofdeepconvolutionalneural networks(CNNs)[33],deeplearningbasedmethodshave beenadoptedtolearnopticalwestimation,wherethe networksareeithertrainedtocomputediscriminativeim- agefeaturesforpatchmatching[21]ordirectlyoutputthe densewinanend-to-endmanner[17].Onemajor advantageofthedeeplearningbasedmethodscomparedto classicalenergy-basedmethodsisthecomputationalspeed, wheremoststate-of-the-artenergy-basedmethodsrequire 1-50minutestoprocessapairofimages,whiledeepnets onlyneedaround100millisecondswithamodernGPU 1 . Sincemostdeepnetworksarebuilttopredictwusing twoconsecutiveframesandtrainedwithsupervisedlearn- ing[27],itwouldrequirealargeamountoftrainingdata toobtainreasonablyhighaccuracy[36].Unfortunately, mostlarge-scalewdatasetsarefromsyntheticmovies andground-truthmotionlabelsinrealworldvideosaregen- erallyhardtoannotate[30].Toovercomethisproblem,un- supervisedlearningframeworkisproposedtoutilizethere- sourcesofunlabeledvideos[31].Theoverallstrategybe- hindthoseunsupervisedmethodsisthatinsteadofdirectly trainingtheneuralnetswithground-truthw,theyusea photometriclossthatmeasuresthedifferencebetweenthe targetimageandthe(inversely)warpedsubsequentimage basedonthedensewpredictedfromthefullycon- volutionalnetworks.Thisallowsthenetworkstobetrained end-to-endwithalargeamountofunlabeledimagepairs, overcomingthelimitationfromthelackofground-truth wannotations. However,theperformanceoftheunsupervisedmethods stillhasarelativelylargegapcomparedtotheirsupervised counterparts[41].Tofurtherimproveunsupervisedw estimation,werealizethatocclusionandlargemotionare amongthemajorfactorsthatlimitthecurrentunsupervised learningmethods.Inthispaper,weproposeanewend-to- enddeepneuralarchitecturethatcarefullyaddressesthese issues. More,theoriginalbaselinenetworksesti- matemotionandattempttoreconstructeverypixelinthe targetimage.Duringreconstruction,therewillbeafraction ofpixelsinthetargetimagethathavenosourcepixelsdue toocclusion.Ifwedonotaddressthisissue,itcouldlimit theopticalwestimationaccuracysincethelossfunction wouldprefertocompensatetheoccludedregionsbymoving otherpixels.Forexample,inFig.1,wewouldliketoesti- matetheopticalwfromframe1toframe2,andrecon- structframe1bywarpingframe2withtheestimatedw. Letusfocusonthechairinthebottomleftcornerofthe 1 http://www.cvlibs.net/datasets/kitti/eval_ scene_flow.php?benchmark=flow 1 arXiv:1711.05890v1  [cs.CV]  16 Nov 2017Figure1:(a)Inputframe1.(b)Inputframe2.(c)Ground-truthopticalw.(d)Imagewarpedbyground-truthopticalw. (e)Forwardopticalwestimatedbyourmethod.(f)Imagewarpedbyourforwardopticalw.(g)Backwardopticalw estimatedbyourmethod.(h)Occlusionmapforinputframe1estimatedbyourbackwardopticalw.(i)Opticalwfrom [41].(j)Imagewarpedby[41]. image.Itmovesinthedown-leftdirection,andsomepart ofthebackgroundisoccludedbyit.Whenwewarpframe 2backtoframe1usingtheground-truthw(Fig.1c),the resultingimage(Fig.1d)hastwochairsinit.Thechairon thetop-rightistherealchair,whilethechaironthebottom- leftisduetotheoccludedpartofthebackground.Because theground-truthwofthebackgroundiszero,thechair inframe2iscarriedbacktoframe1tointheoccluded background.Therefore,frame2warpedbytheground-truth opticalwdoesnotfullyreconstructframe1.Fromthe otherperspective,ifweusephotometriclossoftheentire imagetoguidetheunsupervisedlearningofopticalw, theoccludedareawouldnotgetthecorrectw,whichis illustratedinFig.1i.Ithasanextrachairinthewtrying totheoccludedbackgroundwithnearbypixelsofsimi- larappearance,andthecorrespondingwarpedimageFig.1j hasonlyonechairinit. Toaddressthisissue,weexplicitlyallowthenetwork toexploittheocclusionpredictioncausedbymotionand incorporateitintothelossfunction.Moreconcretely,we estimatethebackwardopticalw(Fig.1g)anduseitto generatetheocclusionmapforthewarpedframe(Fig.1h). Thewhiteareaintheocclusionmapdenotestheareain frame1thatdoesnothaveacorrespondenceinframe2. Wetrainthenetworktoonlyreconstructthenon-occluded areaanddonotpenalizedifferencesintheoccludedarea, sothattheimagewarpedbyourestimatedforwardoptical w(Fig.1e)canhavetwochairsinit(Fig.1f)without incurringextralossforthenetwork. Ourworkdiffersfrompreviousunsupervisedlearning methodsinfouraspects.1)Weproposedanewend-to- endneuralnetworkthathandlesocclusion.Ourworkisso fartheonethataddressesocclusionexplicitlyindeep learningforopticalw.2)Wedevelopedanewwarp- ingmethodthatcanfacilitateunsupervisedlearningoflarge motion.3)WefurtherimprovedthepreviousFlowNetSby introducingextrawarpedinputsduringthedecoderphase. 4)Weintroducedhistogramequalizationandchannelrep- resentationthatareusefulforopticalwestimation.The lastthreecomponentsarecreatedtomainlytackletheissue oflargemotionestimation. Asaresult,ourmethodimprovestheun- supervisedlearningbasedopticalwestimationonmulti- plebenchmarkdatasetincludingFlyingChairs,MPI-Sintel andKITTI.Ourunsupervisednetworksevenoutperforms itssupervisedcounterpart[17]onKITTIbenchmark,where labeleddataislimitedcomparedtounlabeleddata. 2.RelatedWork Opticalwhasbeenintensivelystudiedinthepastfew decades[23,35,11,49,37].Duetopagelimitation,wewill reviewtheclassicalapproachesandtherecentdeep learningapproaches. Opticalwestimation. Opticalwestimationwas introducedasafundamentalcomputervisionproblemsince thepioneeringworks[23,35].Startingfromthen,the accuracyofopticalwestimationhasbeenimproving steadilyasevidencedbytheresultsonMiddlebury[8]and MPI-Sintel[15]benchmarkdataset.Mostclassicalopti- calwalgorithmsbelongtothevariantsoftheenergy minimizationproblemwiththebrightnessconstancyand spatialsmoothnessassumptions[13,42].Othertrendsin- cludeaestimationorahierarchicalframe- worktodealwithlargemotion[14,57,16,6],adesignof losspenaltytoimprovetherobustnesstolightingchange andmotionblur[61,46,22,56],andamoresophisticated 2 frameworktohandleocclusion[2,50]whichwewillde- scribeinmoredetailsinthenextsubsection. Anotherresearchdirectionaimsonimprovingthecom- putationalcomplexityforopticalwmethods[53,10,24], sincetheenergyminimizationproblemoverhighresolution imagesusuallyrequirealargecomputationalcomplexity. However,thespeedusuallycomeswiththecostoflosing accuracy.Sofar,mostclassicalmethodsstillrequireahuge amountoftimetoproducestate-of-the-artresults[58]. Occlusion-awareopticalwestimation. Sinceoc- clusionisaconsequenceofdepthandmotion,itisin- evitabletomodelocclusioninordertoaccuratelyestimate w.Mostexistingmethodsjointlyestimateopticalw andocclusion.Basedonthemethodology,wedividethem intothreemajorgroups.Thetgrouptreatsocclusion asoutliersandpredicttargetpixelsintheoccludedregions asaconstantvalueorthroughinterpolation[47,3,4,54]. Thesecondgroupdealswithocclusionbyexploitingthe symmetricpropertyofopticalwandignoringtheloss penaltyonpredictedoccludedregions[52,2,26].Thelast groupbuildsmoresophisticatedframeworkssuchasmod- elingdepthoralayeredrepresentationofobjectstoreason aboutocclusion[50,48,60,43].Ourmodelissimilarto thesecondgroup,suchthatwedonottakeaccountthedif- ferencewheretheocclusionhappensintothelossfunction. Tothebestofourknowledge,wearethetoincorporate suchkindofmethodwithaneuralnetworkinanend-to-end trainablefashion.Thishelpsourmodeltoobtainmorero- bustwestimationaroundtheocclusionboundary[28,9]. Deeplearningforopticalw. Thesuccessofdeep learninginnovatesnewopticalwmodels.[21]usesdeep netstoextractdiscriminativefeaturestocomputeoptical wthroughpatchmatching.[5]furtherextendsthepatch matchingbasedmethodsbyaddingadditionalsemanticin- formation.Later,[7]proposesarobustthresholdedhinge lossforSiamesenetworkstolearnCNN-basedpatchmatch- ingfeatures.[58]acceleratestheprocessingofpatchmatch- ingcostvolumeandobtainsopticalwresultswithhigh accuracyandfastspeed. Meanwhile,[17,27]proposeFlowNettodirectlycom- putedensewpredictiononeverypixelthroughfullycon- volutionalneuralnetworksandtrainthenetworkswithend- to-endsupervisedlearning.[40]demonstratesthatwith aspatialpyramidnetworkpredictingina fashion,asimpleandsmallnetworkcanworkquiteac- curatelyandefonwestimation.Later,[25] proposesamethodforjointlyestimatingopticalwand temporallyconsistentsemanticsegmentationwithCNN. Thedeeplearningbasedmethodsobtaincompetitiveac- curacyacrossmanybenchmarkopticalwdatasetsin- cludingMPI-Sintel[58]andKITTI[27]witharelatively fastercomputationalspeed.However,thesupervisedlearn- ingframeworklimitstheextensibilityoftheseworksdue Figure2:Ournetworkarchitecture.Itcontainstwocopies ofFlowNetS[17]withsharedparameterswhichestimates forwardandbackwardopticalwrespectively.Thefor- wardwarpingmodulegeneratesanocclusionmapfromthe backwardw.Thebackwardwarpingmodulegenerates thewarpedimagethatisusedtocompareagainsttheorig- inalframe1overthenon-occludedarea.Thereisalsoa smoothnesstermappliedtotheforwardopticalw. tothelackofground-truthwannotationinothervideo datasets. Unsupervisedlearningforopticalw. [39]intro- ducesanend-to-enddifferentiableneuralarchitecturethat allowsunsupervisedlearningforvideomotionprediction andreportspreliminaryresultsonaweakly-supervisedse- manticsegmentationtask.Later,[31,41,1]adoptasim- ilarunsupervisedlearningarchitecturewithamorede- tailedperformancestudyonmultipleopticalwbench- markdatasets.Acommonphilosophybehindthesemeth- odsisthatinsteadofdirectlysupervisingwithground-truth w,thesemethodsutilizetheSpatialTransformerNet- works[29]towarpthecurrentimagestoproduceatarget imagepredictionandusephotometriclosstoguideback- propagation[18].Thewholeframeworkcanbefurtherex- tendedtoestimatethedepth,cameramotionandoptical wsimultaneouslyinanend-to-endmanner[55].This overcomesthewannotationproblem,butthewes- timationaccuracyinpreviousworksstilllagsbehindthe supervisedlearningmethods.Inthispaper,weshowthat unsupervisedlearningcanobtaincompetitiveresultstosu- pervisedlearningmodels. 3.NetworkStructureandMethod Wegiveanoverviewofournetworkstructureand thendescribeeachofitscomponentsindetails. Overallstructure. Theschematicstructureofourneu- ralnetworkisdepictedinFig.2.Ournetworkcontains twocopiesofFlowNetSwithsharedparameters.Theup- perFlowNetStakestwostackedimages( I 1 and I 2 )asinput andoutputstheforwardopticalw( F 12 )from I 1 to I 2 . ThelowerFlowNetStakesthereversestackedimages( I 2 and I 1 )asinputandoutputsthebackwardw( F 21 )from 3 Figure3:Illustrationoftheforwardwarpingmodule demonstratinghowthe occlusionmap isgeneratedusing thebackwardopticalw.Hereweonlyhavehorizontal componentopticalw F x 12 and F x 21 where1denotesmov- ingright,-1denotemovingleftand0denotesstationary.In theocclusionmap,0denotesoccludedand1denotesnon- occluded. I 2 to I 1 . Theforwardw F 12 isusedtowarp I 2 toreconstruct e I 1 throughaSpatialTransformerNetworksimilarto[31]. Wecallthis backwardwarping ,sincethewarpingdirection isdifferentfromthewdirection.Thebackwardw F 21 isusedtogeneratetheocclusionmap( O )by forwardwarp- ing .Theocclusionmapindicatestheregionin I 1 thatis correspondinglyoccludedin I 2 ( i.e .regionin I 1 thatdoes nothaveacorrespondencein I 2 ). Thelossfortrainingournetworkcontainstwoparts:a photometricterm( L p )andasmoothnessterm( L s ).Forthe photometricterm,wecomparethewarpedimage e I 1 andthe originaltargetimage I 1 inthenon-occludedregion toob- tainthephotometricloss L p .Notethatthisisakeydiffer- encebetweenourmethodandpreviousunsupervisedlearn- ingmethods.Wealsoaddasmoothnessloss L s appliedto F 12 toencourageasmoothwsolution. Forwardwarpingandocclusionmap. Wemodelthe non-occludedregionin I 1 astherangeof F 21 [2],which canbecalculatedwiththefollowingequation, V ( x;y )= W X i =1 H X j =1 max(0 ; 1 j x  ( i + F x 21 ( i;j )) j )  max(0 ; 1 j y  ( j + F y 21 ( i;j )) j ) where V ( x;y ) isthe rangemap valueatlocation ( x;y ) . ( W;H ) aretheimagewidthandheight,and ( F x 21 ;F y 21 ) are thehorizontalandverticalcomponentsof F 21 . Since F 21 iscontinuous,thelocationofapixelafterbe- ingtranslatedbyanumbermightnotbeexactly onanimagegrid.Weusereversedbilinearsamplingto Figure4:Illustrationofthebackwardwarpingmodulewith anenlargedsearchspace.Thelargegreenboxontheright sideisazoomviewofthesmallgreenboxontheleftside. distributetheweightofthetranslatedpixeltoitsnearest neighbors.The occlusionmap O canbeobtainedbysim- plythresholdingtherangemap V atthevalueof1andthe regionwithvaluelessthan1isconsideredasoccluded. O ( x;y )=min(1 ;V ( x;y )) .Thewholeforwardwarping moduleisdifferentiableandcanbetrainedend-to-endwith therestofthenetwork. Inordertobetterillustratetheforwardwarpingmodule, weprovideatoyexampleinFig.3. I 1 and I 2 haveonly 4pixelseach,inwhichdifferentlettersrepresentdifferent pixelvalues.Thewandreversedwonlyhavehorizon- talcomponentwhichweshowas F x 12 and F x 21 .Themotion from I 1 to I 2 isthatpixelAmovestothepositionofB andcoversit,whilepixelEinthebackgroundappearsin I 2 .Tocalculatetheocclusionmap,wecreateanimage withonesandthentranslatethemaccordingto F 21 . Therefore,theoneatthetop-rightcorneristranslatedtothe top-leftcornerleavingthetop-rightcorneratthevalueof zero.Thetop-rightcorner(B)of I 1 isoccludedbypixel Aandcannotitscorrespondingpixelin I 2 whichis consistentwiththeformulationwediscussedabove. Backwardwarpingwithalargersearchspace. The backwardwarpingmoduleisusedtoreconstruct e I 1 from I 2 withforwardopticalw F 12 .Themethodadoptedhere issimilarto[31,41]exceptthatweincludealargersearch space.Theproblemwiththeoriginalwarpingmethodis thatthewarpedpixelvalueonlydependsonitsfournear- estneighbors,soifthetargetpositionisfarawayfromthe proposedposition,thenetworkwillnotgetmeaningfulgra- dientsignals.ForexampleinFig.4,aparticularpixellands inthepositionof ( x 2 ;y 2 ) proposedbytheestimatedopti- calw,anditsvalueisaweightedsumofitsfournearest neighbors.However,ifthetrueopticalwlandthepixel at (^ x; ^ y ) ,thenetworkwouldnotlearnthecorrectgradient direction,andthusstuckatalocalminimum.Thisprob- lemisparticularlysevereinthecaseoflargemotion.Al- 4 thoughonecoulduseamulti-scaleimagepyramidtotackle thelargemotionproblem,ifthemovingobjectissmallor hasasimilarcolortothebackground,themotionmightnot bevisibleinsmallscaleimages. Moreconcretely,whenweusetheestimatedopticalw F 12 towarp I 2 backtoreconstruct e I 1 atagridpoint ( x 1 ;y 1 ) , wetranslatethegridpoint ( x 1 ;y 1 ) in I 1 (theyellow square)to ( x 2 ;y 2 )=( x 1 + F x 12 ( x 1 ;y 1 ) ;y 1 + F y 12 ( x 1 ;y 1 )) in I 2 .Becausethepoint ( x 2 ;y 2 ) isnotonthegridpoint in I 2 ,weneedtodobilinearsamplingtoobtainitsvalue. Normally,thevalueat ( x 2 ;y 2 ) isaweightedsumofitsfour nearestneighbors(blackdotsinthezoomedviewonthe rightsideofFig.4).Weinsteadsearchanenlarged neighbor(e.g.thebluedotsattheoutercircleinFig.4) aroundthepoint ( x 2 ;y 2 ) .Forinstance,ifintheenlarged neighborofpoint ( x 2 ;y 2 ) ,thepointthathastheclosest valuetothetargetvalue I 1 ( x 1 ;y 1 ) is (^ x; ^ y ) ,weassignthe valueatthepoint ( x 2 ;y 2 ) tobeaweightedsumofvalues at (^ x; ^ y ) andthreeothersymmetricalpoints(pointslabeled withredcrossesinFig.4)withrespecttopoint ( x 2 ;y 2 ) .By doingthis,wecanprovidetheneuralnetworkwithgradient pointingtowardsthelocationof (^ x; ^ y ) . Lossterm. Thelossofournetworkcontainstwocom- ponents:aphotometricloss( L p )andasmoothnessloss ( L s ).WecomputethephotometriclossusingtheChar- bonnierpenaltyformula  s )= p s 2 +0 : 001 2 overthe non-occludedregionswithbothimagebrightnessandim- agegradient. L 1 p =  X i;j  e I 1 ( i;j )  I 1 ( i;j ))  O ( i;j )  =  X i;j O ( i;j )  L 2 p =  X i;j  r e I 1 ( i;j ) r I 1 ( i;j ))  O ( i;j )  =  X i;j O ( i;j )  where O istheocclusionmapintheabovesection, and i , j togetherindexesoverpixelcoordinates.Theloss isnormalizedbythetotalnon-occludedareasizetoprevent trivialsolutions. Forthesmoothnessloss,weadoptanedge-awareformu- lationsimilarto[20],becausemotionboundariesusually coincidewithimageboundaries.Sincetheoccludedarea doesnothaveaphotometricloss,theopticalwestima- tionintheoccludedareaissolelyguidedbythesmoothness loss.Byusinganedge-awaresmoothnesspenalty,theopti- calwintheoccludedareawouldbesimilartothevalues initsneighborthathastheclosestappearance.Weuseboth andsecond-orderderivativesoftheopticalw inthesmoothnesslossterm. L 1 s = X i;j X d 2 x;y   j @ d F 12 ( i;j ) j e   j @ d I 1 ( i;j ) j  L 2 s = X i;j X d 2 x;y   j @ 2 d F 12 ( i;j ) j e   j @ d I 1 ( i;j ) j  Figure5:OurtotheFlowNetSstructureatone ofthedecodingstage.Ontheleft,weshowtheoriginal FlowNetSstructure.Ontheright,weshowour tionoftheFlowNetSstructure.conv6andconv5 1arefea- turesextractedintheencodingphaseandnamedafter[17]. Image1 6andImage2 6areinputimagesdownsampled64 times.Thedecodingstagesatotherscalesareac- cordingly. where  controlstheweightofedges,and d indexesover partialderivativeon x and y directions.Thelossisa weightedsumoftheabovefourterms, L =  1 L 1 p +  2 L 2 p +  3 L 1 s +  4 L 2 s Flownetworkdetails. Ourinnerwnetworkis adoptedfromFlowNetS[17].SameasFlowNetS,weuse amulti-scaleschemetoguidetheunsupervisedlearning bydown-samplingimagestodifferentsmallerscales.The onlywemadetotheFlowNetSstructureisthat fromcoarsertoscaleduringthephase,we addtheimagewarpedbythecoarseropticalwestima- tionanditscorrespondingphotometricerrormapasextra inputstoestimatethescaleopticalwinafashion similartoFlowNet2[27].Bydoingthis,eachlayeronly needstoestimatetheresidualbetweenthecoarseand scale.ThedetailednetworkstructurecanbefoundinFig.5. Ouronlyincreasesthenumberofparameters by2%comparedtotheoriginalFlownetS,anditmoderately improvestheresultasseeninthelaterablationstudy. Preprocessing. Inordertohavebettercontrastformov- ingobjectsinthedown-sampledimages,wepreprocessthe imagepairsbyapplyinghistogramequalizationandaug- menttheRGBimagewithachannelrepresentation.The detailedchannelrepresentationcanbefoundin[44].We bothpreprocessingstepsimprovetheopticalw estimationresults. 5 Methods Chairs SintelCleanSintelFinal KITTI2012KITTI2015 test traintesttraintest traintesttraintest Supervise FlowNetS[17] 2.71 4.507.425.458.43 8.26ŒŒŒ FlowNetS+ft[17] Œ (3.66)6.96(4.44)7.76 7.529.1ŒŒ SpyNet[40] 2.63 4.126.695.578.43 9.12ŒŒŒ SpyNet+ft[40] Œ (3.17)6.64(4.32)8.36 8.2510.1ŒŒ FlowNet2[27] Œ 2.023.963.14 6.02 4.09 Œ 10.06 Œ FlowNet2+ft[27] Œ (1.45)4.16(2.01) 5.74 (1.28) 1.8 (2.3) 11.48% Unsupervise DSTFlow[41] 5.11 6.9310.407.8211.11 16.98Œ24.30Œ DSTFlow-best[41] 5.11 (6.16)10.41(6.81)11.27 10.4312.416.7939% BackToBasic[31] 5.3 ŒŒŒŒ 11.39.9ŒŒ Ours 3.30 5.23 8.02 6.349.08 12.95Œ21.30Œ Ours+ft-Sintel 3.76 (4.03) 7.95 (5.95)9.15 12.9Œ22.6Œ Ours-KITTI Œ 7.41Œ7.92Œ 3.554.28.8831.2% Table1:Quantitativeevaluationofourmethodondifferentbenchmarks.Thenumbersreportedhereareallaverageend- point-error(EPE)exceptforthelastcolumn(KITTI2015test)whichisthepercentageoferroneouspixels(Fl-all).Apixel isconsideredtobecorrectlyestimatedifthewend-pointerroris < 3pxor < 5%.Theupperpartofthetablecontains supervisedmethodsandlowerpartofthetablecontainsunsupervisedmethods.Forallmetrics,smallerisbetter.Thebest numberforeachcategoryishighlightedinbold.Thenumbersinparenthesesareresultsfromnetworktrainedonthesame setofdata,andhencearenotdirectlycomparabletootherresults. 4.ExperimentalResults Weevaluateourmethodsonstandardopticalwbench- markdatasetsincludingFlyingChairs,MPI-Sinteland KITTI,andcompareourresultstoexistingdeeplearning basedopticalwestimation(bothsupervisedandunsu- pervisedmethods).Weusethestandardtheendpointerror (EPE)measureastheevaluationmetric,whichistheaver- ageEuclideandistancebetweenthepredictedwandthe groundtruthwoverallpixels. 4.1.ImplementationDetails Ournetworkistrainedend-to-endusingAdamopti- mizer[32]with  1 =0 : 9 and  2 =0 : 999 .Thelearning rateissettobe 10  4 fortrainingfromscratchand 10  5 for TheexperimentsareperformedontwoTitan ZGPUswithabatchsizeof8or16dependingonthein- putimageresolution.Thetrainingconvergesafterroughly aday.Duringtraining,wetassignequalweightsto lossfromdifferentimagescalesandthenprogressivelyin- creasetheweightonthelargerscaleimageinawaysimilar to[36].Thehyper-parameters (  1 ; 2 ; 3 ; 4 ; ) aresetto be(1.0,1.0,10.0,0.0,10.0)forFlyingChairsandMPI- Sinteldatasets,and(0.03,3.0,0.0,10.0,10.0)forKITTI dataset.Hereweusedhigherweightsofimagegradientpho- tometriclossandsecond-ordersmoothnesslossforKITTI becausethedatahasmorelightningchangesanditsopti- calwhasmorecontinuouslyvaryingintrinsicstructure. Intermsofdataaugmentaion,weonlyusedhorizontal ping,verticalandimagepairorderswitching.Dur- ingtesting,ournetworkonlypredictsforwardw,thetotal computationaltimeonaFlyingChairsimagepairisroughly 90millisecondswithourTitianZGPUs.Addinganex- tra8millisecondsforhistogramequalization(anOpenCV CPUimplementation),thetotalpredictiontimeisaround 100milliseconds. 4.2.QuantitativeandQualitativeResults Table1summarizestheEPEofourmethodandpre- viousstate-of-the-artdeeplearningmethods,including FlowNet[17],SpyNet[40],FlowNet2[27],DSTFlow[41] andBackToBasic[31].BecauseDSTFlowreportedmul- tiplevariationsoftheirresults,wecitetheirbestnumber acrossalloftheirresultsinﬂDSTFlow-bestﬂhere. FlyingChairs. FlyingChairsisasyntheticdatasetcre- atedbysuperimposingimagesofchairsonbackgroundim- agesfromFlickr.Itwasoriginallycreatedfortraining FlowNetinasupervisedmanner[17].Weuseittotrainour networkwithoutusinganyground-truthw.Werandomly splitthedatasetinto95%trainingand5%testing.Welabel thismodelasﬂOursﬂinTable1.OurEPEis smallerthanthepreviousunsupervisedmethods( i.e .EPE decreasesfrom5.11to3.30)andisapproachingthelevelof itscorrespondingsupervisedlearningresult(2.71). MPI-Sintel. SinceMPI-Sintelisrelativelysmalland onlycontainsaroundathousandimagepairs,weusethe trainingdatafrombothcleanandpassto ournetworkpretrainedonFlyingChairsandtheresulting modelislabeledasﬂOurs+ft-Sintelﬂ.Comparedtoother unsupervisedmethods,weachieveamuchbetterperfor- mance(e.g.,EPEdecreasesfrom10.40to7.95onSintel 6 Figure6:QualitativeexamplesforSinteldataset.ThetopthreerowsarefromSintelCleanandthebottomthreerowsare fromSintelFinal. Figure7:QualitativeexamplesforKITTIdataset.ThetopthreerowsarefromKITTI2012andthebottomthreerowsare fromKITTI2015. Cleantest).Notethatdidnotimprovemuch here,largelyduetothesmallnumberoftrainingdata. Fig.6illustratesthequalitativeresultofourmethodonMPI- Sintel. KITTI. TheKITTIdatasetisrecordedunderreal-world drivingconditions,andithasmoreunlabeleddatathanla- beleddata.Unsupervisedlearningmethodswouldhavean advantageinthisscenariosincetheycanlearnfromthe largeamountofunlabeleddata.Thetrainingdataweuse hereissimilarto[41]whichconsiststhemulti-viewexten- sions(20framesforeachsequence)frombothKITTI2012 andKITTI2015.Duringtraining,weexcludetwoneigh- boringframesfromtheimagepairswithground-truthw andtestingpairstoavoidmixingtrainingandtestingdata ( i.e .notincludingframenumber9-12ineachmulti-view sequence).Wetrainthemodelfromscratchsincetheopti- calwinKITTIdatasethasitsowndomainspatialstruc- ture(differentfromFlyingChairs)andabundantdata.We labelthismodelasﬂOurs-KITTIﬂinTable1. Table1suggeststhatourmethodnotonly outperformsexistingunsupervisedlearningmethods( i.e . improvesEPEfrom9.9to4.2onKITTI2012test),but 7 occlusionenlargedcontrast ChairsSintelCleanSintelFinal handlingsearchFlowNetenhancement testtraintrain 5.116.937.82 X 4.516.807.32 XX 4.276.497.11 XXX 4.146.387.08 X 4.626.607.33 XX 4.046.097.04 XXX 3.765.706.54 XXXX 3.305.236.34 Table2:Ablationstudy Method SintelSintelKITTIKITTI CleanFinal20122015 Our 0.540.480.950.88 S2D[34] Œ 0.57 ŒŒ MODOF[59] Œ0.48ŒŒ Table3:Occlusionestimationevaluation.Thenumberswe presenthereismaximumF-measure.TheS2Dmethodis trainedwithground-truthocclusionlabels. alsooutperformsitssupervisedcounterpart(FlowNetS+ft) byalargemargin,althoughthereisstillagapcompared tothestate-of-the-artsupervisednetworkFlowNet2.Fig.7 illustratesthequalitativeresultsonKITTI.Ourmodelcor- rectlycapturestheoccludedareacausedbymovingoutof theframe.Ourwresultsarealsofreefromtheartifacts seeninDSTFlow(see[41]Figure4c)intheocclusionarea. OcclusionEstimation. Wealsoevaluateourocclusion estimationonMPI-SintelandKITTIdatasetwhichpro- videground-truthocclusionlabelsbetweentwoconsecutive frames.Amongtheliteratures,weonlylimitedreports onocclusionestimationaccuracy.Table3showstheocclu- sionestimationperformancebycalculatingthemaximum F-measureintroducedin[34].OnMPI-Sintel,ourmethod hasacomparableresultwithpreviousnon-neural-network basedmethods[34,59].OnKITTIweobtain0.95and 0.88forKITTI2012andKITTI2015respectively(wedid notpublishedocclusionestimationresultonKITTI). NotethatS2Dusedground-truthocclusionmapstodosu- pervisedtrainingoftheirocclusionmodel. 4.3.AblationStudy Weconductsystematicablationanalysisondifferent componentsaddedinourmethod.Table2showstheover- alleffectsofthemonFlyingChairsandMPI-Sintel.Our startingnetworkisaFlowNetSwithoutocclusionhandling, whichisthesameas[41]. Occlusionhandling. ThetoptworowsinTable2sug- gestthatbyonlyaddingocclusionhandlingtothebaseline network,themodelimprovesitsEPEfrom5.11to4.51on Flying-Chairsandfrom7.82to7.32onMPI-SintelFinal, whichis Enlargedsearch. Theeffectofenlargedsearchisalso ThebottomtworowsinTable2showthat addingenlargedsearch,theEPEimprovesfrom3.76to 3.30onFlying-Chairsandfrom6.54to6.34onMPI-Sintel Final. FlowNet. Asmalltothe FlowNetalsoimproves,assuggestedinthe5- throwinTable2.Byonlyaddinga2%moreparameters andcomputation,theEPEimprovesfrom5.11to4.62on Flying-Chairsandfrom7.82to7.33onMPI-SintelFinal. Contrastenhancement. Wethatcontrastenhance- mentisalsoasimplebutveryeffectivepreprocessingstep toimprovetheunsupervisedopticalwlearning.Bycom- paringthe4throwandlastrowinTable2,wethe EPEimprovesfrom4.14to3.30onFlying-Chairsand7.08 to6.34onMPI-SintelFinal. Combiningallcomponents. Wealsothatsome- timesonecomponentisnotbyitself,butthe overallmodelimprovesdramaticallywhenweaddallthe 4componentsintoourframework. Effectofdata. Wehavetriedtousemoredatafrom KITTIrawvideos(60,000samplescomparedto25,000 samplesusedinthepaper)totrainourmodel,butwedid notanyimprovement.Wehavealsotriedtoadoptthe networkstructurefromSpyNet[40]andPWC-net[51],and trainthemusingourunsupervisedmethod.Howeverwedid notgetbetterresulteither,whichsuggeststhatthelearning capabilityofourmodelisstillthelimitingfactor,although wehavepushedthisforwardbyalargemargin. 5.Conclusion Wepresentanewend-to-endunsupervisedlearning frameworkforopticalwprediction.Weshowthatwith modelingocclusionandlargemotion,ourunsupervisedap- proachyieldscompetitiveresultsonmultiplebenchmark datasets.Thisispromisingsinceitopensanewpathfor 8 trainingneuralnetworkstopredictopticalwwithavast amountofunlabeledvideosandapplythewestimation formorehigherlevelcomputervisiontasks. References [1] A.AhmadiandI.Patras.Unsupervisedconvolutionalneural networksformotionestimation.In ImageProcessing(ICIP), 2016IEEEInternationalConferenceon ,pages1629Œ1633. IEEE,2016. [2] L.Alvarez,R.Deriche,T.Papadopoulo,andJ.S ´ anchez. Symmetricaldenseopticalwestimationwithocclu- sionsdetection. InternationalJournalofComputerVision , 75(3):371Œ385,2007. [3] A.Ayvaci,M.Raptis,andS.Soatto.Occlusiondetectionand motionestimationwithconvexoptimization.In Advances inneuralinformationprocessingsystems ,pages100Œ108, 2010. [4] A.Ayvaci,M.Raptis,andS.Soatto.Sparseocclusionde- tectionwithopticalw. InternationalJournalofComputer Vision ,97(3):322Œ338,2012. [5] M.Bai,W.Luo,K.Kundu,andR.Urtasun.Exploitingse- manticinformationanddeepmatchingforopticalw.In EuropeanConferenceonComputerVision ,pages154Œ170. Springer,2016. [6] C.Bailer,B.Taetz,andD.Stricker.FlowDensecorre- spondenceforhighlyaccuratelargedisplacementopti- calwestimation.In ProceedingsoftheIEEEInternational ConferenceonComputerVision ,pages4015Œ4023,2015. [7] C.Bailer,K.Varanasi,andD.Stricker.Cnn-basedpatch matchingforopticalwwiththresholdedhingeembedding loss.In IEEEConferenceonComputerVisionandPattern Recognition(CVPR) ,2017. [8] S.Baker,D.Scharstein,J.Lewis,S.Roth,M.J.Black,and R.Szeliski.Adatabaseandevaluationmethodologyforopti- calw. InternationalJournalofComputerVision ,92(1):1Œ 31,2011. [9] C.Ballester,L.Garrido,V.Lazcano,andV.Caselles.A tv-l1opticalwmethodwithocclusiondetection. Pattern Recognition ,pages31Œ40,2012. [10] L.Bao,Q.Yang,andH.Jin.Fastedge-preservingpatch- matchforlargedisplacementopticalw.In Proceedings oftheIEEEConferenceonComputerVisionandPattern Recognition ,pages3534Œ3541,2014. [11] M.J.BlackandP.Anandan.Therobustestimationofmulti- plemotions:Parametricandpiecewise-smoothw Computervisionandimageunderstanding ,63(1):75Œ104, 1996. [12] J.-Y.Bouguet.Pyramidalimplementationoftheaflu- caskanadefeaturetrackerdescriptionofthealgorithm. Intel Corporation ,5(1-10):4,2001. [13] T.Brox,A.Bruhn,N.Papenberg,andJ.Weickert.Highac- curacyopticalwestimationbasedonatheoryforwarping. ComputerVision-ECCV2004 ,pages25Œ36,2004. [14] T.BroxandJ.Malik.Largedisplacementopticalw:de- scriptormatchinginvariationalmotionestimation. IEEE transactionsonpatternanalysisandmachineintelligence , 33(3):500Œ513,2011. [15] D.J.Butler,J.Wulff,G.B.Stanley,andM.J.Black.A naturalisticopensourcemovieforopticalwevaluation.In EuropeanConferenceonComputerVision ,pages611Œ625. Springer,2012. [16] Z.Chen,H.Jin,Z.Lin,S.Cohen,andY.Wu.Largedis- placementopticalwfromnearestneighborIn Pro- ceedingsoftheIEEEConferenceonComputerVisionand PatternRecognition ,pages2443Œ2450,2013. [17] A.Dosovitskiy,P.Fischer,E.Ilg,P.Hausser,C.Hazirbas, V.Golkov,P.vanderSmagt,D.Cremers,andT.Brox. Flownet:Learningopticalwwithconvolutionalnetworks. In ProceedingsoftheIEEEInternationalConferenceon ComputerVision ,pages2758Œ2766,2015. [18] C.Finn,I.Goodfellow,andS.Levine.Unsupervisedlearn- ingforphysicalinteractionthroughvideoprediction.In Ad- vancesinNeuralInformationProcessingSystems ,pages64Œ 72,2016. [19] D.ForsythandJ.Ponce. Computervision:amodernap- proach .UpperSaddleRiver,NJ;London:PrenticeHall, 2011. [20] C.Godard,O.MacAodha,andG.J.Brostow.Unsuper- visedmonoculardepthestimationwithleft-rightconsistency. arXivpreprintarXiv:1609.03677 ,2016. [21] F.G ¨ uneyandA.Geiger.Deepdiscretew.In AsianCon- ferenceonComputerVision ,pages207Œ224.Springer,2016. [22] D.Hafner,O.Demetz,andJ.Weickert.Whyisthecensus transformgoodforrobustopticwcomputation?In Inter- nationalConferenceonScaleSpaceandVariationalMeth- odsinComputerVision ,pages210Œ221.Springer,2013. [23] B.K.HornandB.G.Schunck.Determiningopticalw. intelligence ,17(1-3):185Œ203,1981. [24] Y.Hu,R.Song,andY.Li.Efpatch- matchforlargedisplacementopticalw.In Proceedings oftheIEEEConferenceonComputerVisionandPattern Recognition ,pages5704Œ5712,2016. [25] J.HurandS.Roth.Jointopticalwandtemporallycon- sistentsemanticsegmentation.In EuropeanConferenceon ComputerVision ,pages163Œ177.Springer,2016. [26] J.HurandS.Roth.w:Exploitingsymmetriesin jointopticalwandocclusionestimation. arXivpreprint arXiv:1708.05355 ,2017. [27] E.Ilg,N.Mayer,T.Saikia,M.Keuper,A.Dosovitskiy,and T.Brox.Flownet2.0:Evolutionofopticalwestimation withdeepnetworks. arXivpreprintarXiv:1612.01925 ,2016. [28] S.InceandJ.Konrad.Occlusion-awareopticalwestima- tion. IEEETransactionsonImageProcessing ,17(8):1443Œ 1451,2008. [29] M.Jaderberg,K.Simonyan,A.Zisserman,etal.Spatial transformernetworks.In AdvancesinNeuralInformation ProcessingSystems ,pages2017Œ2025,2015. [30] J.Janai,F.Gney,J.Wulff,M.Black,andA.Geiger.Slow w:Exploitinghigh-speedcamerasforaccurateanddiverse opticalwreferencedata.In ConferenceonComputerVi- sionandPatternRecognition(CVPR) ,2017. [31] J.Y.Jason,A.W.Harley,andK.G.Derpanis.Backtobasics: Unsupervisedlearningofopticalwviabrightnesscon- stancyandmotionsmoothness.In ComputerVisionŒECCV 2016Workshops ,pages3Œ10.Springer,2016. 9 [32] D.KingmaandJ.Ba.Adam:Amethodforstochasticopti- mization. arXivpreprintarXiv:1412.6980 ,2014. [33] A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenet withdeepconvolutionalneuralnetworks.In Advancesinneuralinformationprocessingsystems ,pages 1097Œ1105,2012. [34] M.Leordeanu,A.,andC.Sminchisescu.Locally afsparse-to-densematchingformotionandocclusiones- timation.In ProceedingsoftheIEEEInternationalConfer- enceonComputerVision ,pages1721Œ1728,2013. [35] B.D.Lucas,T.Kanade,etal.Aniterativeimageregistration techniquewithanapplicationtostereovision.1981. [36] N.Mayer,E.Ilg,P.Hausser,P.Fischer,D.Cremers, A.Dosovitskiy,andT.Brox.Alargedatasettotraincon- volutionalnetworksfordisparity,opticalw,andscene westimation.In ProceedingsoftheIEEEConference onComputerVisionandPatternRecognition ,pages4040Œ 4048,2016. [37] M.Menze,C.Heipke,andA.Geiger.Discreteoptimization foropticalw.In GermanConferenceonPatternRecogni- tion ,pages16Œ28.Springer,2015. [38] D.Pathak,R.Girshick,P.Doll ´ ar,T.Darrell,andB.Hari- haran.Learningfeaturesbywatchingobjectsmove. arXiv preprintarXiv:1612.06370 ,2016. [39] V.Patraucean,A.Handa,andR.Cipolla.Spatio-temporal videoautoencoderwithdifferentiablememory. arXiv preprintarXiv:1511.06309 ,2015. [40] A.RanjanandM.J.Black.Opticalwestimationusinga spatialpyramidnetwork. arXivpreprintarXiv:1611.00850 , 2016. [41] Z.Ren,J.Yan,B.Ni,B.Liu,X.Yang,andH.Zha.Unsu- perviseddeeplearningforopticalwestimation.In AAAI , pages1495Œ1501,2017. [42] J.Revaud,P.Weinzaepfel,Z.Harchaoui,andC.Schmid. w:Edge-preservinginterpolationofcorrespondences foropticalw.In ProceedingsoftheIEEEConference onComputerVisionandPatternRecognition ,pages1164Œ 1172,2015. [43] L.Sevilla-Lara,D.Sun,V.Jampani,andM.J.Black.Op- ticalwwithsemanticsegmentationandlocalizedlayers. In ProceedingsoftheIEEEConferenceonComputerVision andPatternRecognition ,pages3889Œ3898,2016. [44] L.Sevilla-Lara,D.Sun,E.G.Learned-Miller,andM.J. Black.Opticalwestimationwithchannelconstancy.In EuropeanConferenceonComputerVision ,pages423Œ438. Springer,2014. [45] K.SimonyanandA.Zisserman.Two-streamconvolutional networksforactionrecognitioninvideos.In Advances inneuralinformationprocessingsystems ,pages568Œ576, 2014. [46] F.Stein.Efcomputationofopticalwusingthecen- sustransform.In DAGM-symposium ,volume2004,pages 79Œ86.Springer,2004. [47] C.Strecha,R.Fransens,andL.J.VanGool.Aprobabilistic approachtolargedisplacementopticalwandocclusion detection.In ECCVWorkshopSMVP ,pages71Œ82.Springer, 2004. [48] D.Sun,C.Liu,andH.ster.Locallayeringforjointmotion estimationandocclusiondetection.In Proceedingsofthe IEEEConferenceonComputerVisionandPatternRecogni- tion ,pages1098Œ1105,2014. [49] D.Sun,S.Roth,andM.J.Black.Secretsofopticalw estimationandtheirprinciples.In ComputerVisionandPat- ternRecognition(CVPR),2010IEEEConferenceon ,pages 2432Œ2439.IEEE,2010. [50] D.Sun,E.B.Sudderth,andM.J.Black.Layeredimage motionwithexplicitocclusions,temporalconsistency,and depthordering.In AdvancesinNeuralInformationProcess- ingSystems ,pages2226Œ2234,2010. [51] D.Sun,X.Yang,M.-Y.Liu,andJ.Kautz.Pwc-net:Cnnsfor opticalwusingpyramid,warping,andcostvolume. arXiv preprintarXiv:1709.02371 ,2017. [52] J.Sun,Y.Li,S.B.Kang,andH.-Y.Shum.Symmetricstereo matchingforocclusionhandling.In ComputerVisionand PatternRecognition,2005.CVPR2005.IEEEComputerSo- cietyConferenceon ,volume2,pages399Œ406.IEEE,2005. [53] M.Tao,J.Bai,P.Kohli,andS.Paris.w:A non-iterative,sublinearopticalwalgorithm.In Computer GraphicsForum ,volume31,pages345Œ353.WileyOnline Library,2012. [54] M.Unger,M.Werlberger,T.Pock,andH.Bischof.Joint motionestimationandsegmentationofcomplexsceneswith labelcostsandocclusionmodeling.In ComputerVision andPatternRecognition(CVPR),2012IEEEConferenceon , pages1878Œ1885.IEEE,2012. [55] S.Vijayanarasimhan,S.Ricco,C.Schmid,R.Sukthankar, andK.Fragkiadaki.Sfm-net:Learningofstructureandmo- tionfromvideo. arXivpreprintarXiv:1704.07804 ,2017. [56] C.Vogel,S.Roth,andK.Schindler.Anevaluationofdata costsforopticalw.In GermanConferenceonPattern Recognition ,pages343Œ353.Springer,2013. [57] P.Weinzaepfel,J.Revaud,Z.Harchaoui,andC.Schmid. w:Largedisplacementopticalwwithdeepmatch- ing.In ProceedingsoftheIEEEInternationalConferenceon ComputerVision ,pages1385Œ1392,2013. [58] J.Xu,R.Ranftl,andV.Koltun.Accurateoptical wviadirectcostvolumeprocessing. arXivpreprint arXiv:1704.07325 ,2017. [59] L.Xu,J.Jia,andY.Matsushita.Motiondetailpreservingop- ticalwestimation. IEEETransactionsonPatternAnalysis andMachineIntelligence ,34(9):1744Œ1757,2012. [60] K.Yamaguchi,D.McAllester,andR.Urtasun.Efjoint segmentation,occlusionlabeling,stereoandwestimation. In EuropeanConferenceonComputerVision ,pages756Œ 771.Springer,2014. [61] R.ZabihandJ.WNon-parametriclocaltransforms forcomputingvisualcorrespondence.In Europeanconfer- enceoncomputervision ,pages151Œ158.Springer,1994. [62] X.Zhu,Y.Wang,J.Dai,L.Yuan,andY.Wei.Flow-guided featureaggregationforvideoobjectdetection. arXivpreprint arXiv:1703.10025 ,2017. 10  
BreakingtheBeamSearchCurse:AStudyof(Re-)ScoringMethodsand StoppingCriteriaforNeuralMachineTranslation YilinYang 1 LiangHuang 1 ; 2 MingboMa 1 ; 2 1 OregonStateUniversity Corvallis,OR,USA f yilinyang721,liang.huang.sh,cosmmb g @gmail.com 2 BaiduResearch Sunnyvale,CA,USA Abstract Beamsearchiswidelyusedinneuralma- chinetranslation,andusuallyimprovestrans- lationqualitycomparedtogreedysearch.It hasbeenwidelyobservedthat,however,beam sizeslargerthan5hurttranslationquality.We explainwhythishappens,andproposesev- eralmethodstoaddressthisproblem.Fur- thermore,wediscusstheoptimalstoppingcri- teriaforthesemethods.Resultsshowthat ourhyperparameter-freemethodsoutperform thewidely-usedhyperparameter-freeheuristic oflengthnormalizationby+2.0BLEU,and achievethebestresultsamongallmethodson Chinese-to-Englishtranslation. 1Introduction Inrecentyears,neuralmachinetranslation(NMT) hassurpassedtraditionalphrase-basedorsyntax- basedmachinetranslation,becomingthenewstate oftheartinMT( KalchbrennerandBlunsom , 2013 ; Sutskeveretal. , 2014 ; Bahdanauetal. , 2014 ).WhileNMTtrainingistypicallydone inaﬁlocalﬂfashionwhichdoesnotemployany search(barnotableexceptionssuchas Ranzato etal. ( 2016 ), Shenetal. ( 2016 ),and Wiseman andRush ( 2016 )),thedecodingphaseofallNMT systemsuniversallyadoptsbeamsearch,awidely usedheuristic,toimprovetranslationquality. Unlikephrase-basedMTsystemswhichenjoy theofverylargebeamsizes(intheor- derof100Œ500)( Koehnetal. , 2007 ),mostNMT systemschoosetinybeamsizesupto5;forexam- ple,Google'sGNMT( Wuetal. , 2016 )andFace- book'sConvS2S( Gehringetal. , 2017 )usebeam sizes3and5,respectively.Intuitively,thelarger thebeamsizeis,themorecandidatesitexplores, andthebetterthetranslationqualityshouldbe. Whilethisholdsforphrase-basedMT systems,surprisingly,itis not thecaseforNMT: manyresearchersobservethattranslationqual- itydegradeswithbeamsizesbeyond5or10( Tu etal. , 2017 ; KoehnandKnowles , 2017 ).Wecall thisphenomenonthe ﬁbeamsearchcurseﬂ ,which islistedasoneofthesixbiggestchallengesfor NMT( KoehnandKnowles , 2017 ). However,therehasnotbeenenoughattention onthisproblem. Huangetal. ( 2017 )hintthat lengthratioistheproblem,butdonotexplainwhy largerbeamsizescauseshorterlengthsandworse BLEU. Ottetal. ( 2018 )attributeittotwokinds ofﬁuncertaintiesﬂinthetrainingdata,namelythe copying ofsourcesentenceandthe non-literal translations.However,theproblemisonly foundinEuropeanlanguagedatasetsandthesec- ondproblemoccursinalldatasetsbutdoesnot seemtobotherpre-neuralMTsystems.Therefore, theirexplanationsarenotsatisfactory. Ontheotherhand,previousworkadoptsseveral heuristicstoaddressthisproblem,butwithvari- ouslimitations.Forexample,RNNSearch( Bah- danauetal. , 2014 )andConvS2Suse lengthnor- malization ,which(wewillshowinSec. 6 )seems tosomewhatalleviatetheproblem,butfarfrom beingperfect.Meanwhile, Heetal. ( 2016 )and Huangetal. ( 2017 )useword-reward,buttheir re- ward isahyper-parametertobetunedondevset. Ourcontributionsareasfollows:  Weexplainwhythebeamsearchcurseexists, supportedbyempiricalevidence(Sec. 3 ).  Wereviewexistingrescoringmethods,and thenproposeourstobreakthebeam searchcurse(Sec. 4 ).Weshowthatour hyperparameter-freemethodsoutperfromthe previoushyperparameter-freemethod(length normalization)by+2.0BLEU(Sec. 6 ).  Wealsodiscussthestoppingcriteriaforour rescoringmethods(Sec. 5 ).Experiments showthatwithoptimalstoppingalone,the translationqualityofthelengthnormaliza- tionmethodimprovesby+0.9BLEU. 2Preliminaries:NMTandBeamSearch Wereviewtheencoder-decoderarchitec- turewithattentionmechanism( Bahdanauetal. , 2014 ).AnRNNencodertakesaninputsequence x =( x 1 ;:::;x m ) ,andproducesasequenceof hiddenstates.Foreachtimestep,theRNNde- coderwillpredicttheprobabilityofnextoutput wordgiventhesourcesequenceandthepreviously generatedTherefore,whendoinggreedy search,attimestep i ,thedecoderwillchoosethe wordwithhighestprobabilityas y i .Thedecoder willcontinuegeneratinguntilitemits </eos> .Inthe end,thegeneratedhypothesisis y =( y 1 ;:::;y n ) with y n = </eos> ,withmodelscore S ( x ; y )= j y j X i =1 log p ( y i j x ;y 1 :: f i  1 g ) (1) Asgreedysearchonlyexploresasinglepath, wealwaysusebeamsearchtoimprovesearch quality.Let b denotethebeamsize,thenatstep i thebeam B i isan ordered listofsize b : B 0 =[ h <s> ;p ( <s> j x ) i ] B i = b top fh y 0  y i ;s  p ( y i j x ; y ) ijh y 0 ;s i2 B i  1 g Inthemostnaivecase,afterreachingthemaxi- mumlength(ahardlimit),weget N possiblecan- didatesequences f y 1 ;:::; y N g .The default strat- egychoosestheonewithhighestmodelscore.We willdiscussmoresophistcatedwaysofstopping andchoosingcandidatesinlatersections. 3BeamSearchCurse Themostpopulartranslationqualitymetric, BLEU( Papinenietal. , 2002 ),isas: BLEU= bp  exp  1 4 4 X n =1 log p n  (2) where bp =min f e 1  1 = lr ; 1 g (3) where lr = j y j = j y  j (4) Here p n arethe n -gramprecisions,and j y j and j y  j denotethehypothesisandreferencelengths, while bp isthe brevitypenalty (penalizingshort translations)and lr isthe lengthratio ( Shietal. , 2016 ; KoehnandKnowles , 2017 ),respectively. Figure1:Asbeamsizeincreasesbeyond3,BLEU scoreonthedevsetgraduallydrops.Alltermsarecal- culatedby multi-bleu.pl . Figure2:Searchingalgorithmwithlargerbeamsgen- erates </eos> earlier.Weusetheaveragesecond andthird </eos> positionsonthedevsetasanexample. Withbeamsizeincreasing, j y j decrases,which causesthelengthratiotodrop,asshowninFig. 1 . Thenthebrevitypenaltyterm,asafunctionofthe lengthratio,decreasesevenmoreseverely.Since bp isakeyfactorinBLEU,thisexplainswhythe beamsearchcursehappens. 1 Thereasonwhy j y j decreasesasbeamsizein- creasesisactuallytwofold: 1. Asbeamsizeincreases,themorecandidates itcouldexplore.Therefore,itbecomeseas- ierforthesearchalgorithmtothe </eos> symbol.Fig. 2 showsthatthe </eos> indices decreasesteadilywithlargerbeams. 2 2. Then,asshowninFig. 3 ,shortercandidates haveclearadvantages w.r.t. modelscore. 1 Thelengthratiois not justaboutBLEU:ifthehypothesis lengthisonly75%ofreferencelength,somethingthatshould havebeentranslatedmustbemissing;i.e.,badadequacy.We believethesameproblemstillexistsifweuseTERinstead. 2 Pre-neuralSMTmodels,beingprobabilistic,alsofavor shorttranslations(andderivations),whichisaddressedby word(andphrase)reward.Thecrucialdifferencebetween SMTandNMTisthattheformerstopswhencoveringthe wholeinput,whilethelatterstopsonemitting </eos> . Figure3:Candidatelengthsvs.modelscore.Thisscat- terplotisgeneratedfrom242candidateswhen translatedfromonesourcesequencewithbeamsize80. Hence,asbeamsizeincreases,thesearchalgo- rithmwillgenerateshortercandidates,andthen preferevenshorteronesamongthem. 4RescoringMethods Wereviewexistingmethodstocounterthe lengthproblemandthenproposenewonestoad- dresstheirlimitations.Inparticular,weproposeto predictthetargetlengthfromthesourcesentence, inordertochooseahypothesiswithproperlength. 4.1PreviousRescoringMethods RNNSearch( Bahdanauetal. , 2014 )intro- ducesthe lengthnormalization method,whose scoreissimplytheaveragemodelscore: ^ S length norm ( x ; y )= S ( x ; y ) j y j (5) Thisisthemostwidelyusedrescoringmethod sinceitishyperparameter-free. GNMT( Wuetal. , 2016 )incorporateslength andcoveragepenaltyintothelengthnormalization method,whilealsoaddingtwohyperparametersto adjusttheir(pleasecheckouttheirpa- perforexactformulas). BaiduNMT( Heetal. , 2016 )borrowsthe Word Reward methodfrompre-neuralMT,whichgives areward r toeverywordgenerated,where r isa hyperparametertunedonthedevset: ^ S WR ( x ; y )= S ( x ; y )+ r j y j (6) Basedontheabove, Huangetal. ( 2017 )propose avariantcalled BoundedWord-Reward which onlyrewardsuptoanﬁoptimalﬂlength.This lengthiscalculatedusingaedﬁgenerationra- tioﬂ gr ,whichistheratiobetweentargetand sourcesequencelength,namelytheaveragenum- beroftargetwordsgeneratedforeachsource word.Itgivesreward r toeachworduptoa boundedlength L ( x ; y )=min fj y j ; gr j x jg : ^ S BWR ( x ; y )= S ( x ; y )+ r  L ( x ; y ) (7) 4.2RescoringwithLengthPrediction Toremovetheedgenerationratio gr from BoundedWord-Reward,weusea2-layerMLP, whichtakesthemeanofsourcehiddenstatesas input,topredictthegenerationratio gr  ( x ) .Then wereplacetheedratio gr withit,andgetour predictedlength L pred ( x )= gr  ( x ) j x j . 4.2.1BoundedWord-Reward Withpredictedlength,thenewpredictedbound andscorewouldbe: L  ( x ; y )=min fj y j ;L pred ( x ) g (8) ^ S BWR  ( x ; y )= S ( x ; y )+ r  L  ( x ; y ) (9) Whilethepredictedlengthismoreaccurate,there isstillahyperparameter r (wordreward),sowe designtwomethodsbelowtoremoveit. 4.2.2BoundedAdaptive-Reward Wepropose BoundedAdaptive-Reward toauto- maticallycalculateproperrewardbasedonthe currentbeam.Withbeamsize b ,therewardfor timestep t istheaveragenegativelog-probability ofthewordsinthecurrentbeam. r t =  1 b b X i =1 log p (word i ) (10) Itsscoreisverysimilarto( 7 ): ^ S AdaR ( x ; y )= S ( x ; y )+ L  X t =1 r t (11) 4.2.3BP-Norm InspiredbytheBLEUscorewepropose BP-Norm methodasfollows: ^ S bp ( x ; y )=log bp + S ( x ; y ) j y j (12) bp isthesamebrevitypenaltytermasin( 3 ).Here, weregardourpredictedlengthasthereference length.Thebeautyofthismethodappearswhen wedropthelogarithmicsymbolin( 12 ): exp( ^ S bp ( x ; y ))= bp   j y j Y i =1 p ( y i j ::: )  1 j y j = bp  exp  1 j y j j y j X i =1 log p ( y i j ::: )  whichisinthesameformofBLEUscore( 3 ). 5StoppingCriteria Besidesrescoringmethods,thestoppingcriteria (whentostopbeamsearch)isalsoimportant,for bothefyandaccuracy. 5.1ConventionalStoppingCriteria Bydefault,OpenNMT-py( Kleinetal. , 2017 ) stopswhenthetopmostbeamcandidatestops,be- causetherewillnotbeanyfuturecandidateswith highermodelscores.However,thisisnotthe caseforotherrescoringmethods;e.g.,thescore oflengthnormalization( 5 )couldstillincrease. Anotherpopularstoppingcriteria,usedby RNNSearch( Bahdanauetal. , 2014 ),stopsthe beamsearchwhenexactly b candidates havebeenfound.Neithermethodisoptimal. 5.2OptimalStoppingCriteria ForBoundedWord-Reward, Huangetal. ( 2017 ) introducesaprovably-optimalstoppingcriterion thatcouldstopbothearlyandoptimally.Wealso introduceanoptimalstoppingcriterionforBP- Norm.Eachtimewegenerateacandi- date,weupdateourbestscore ^ S ? .Then,forthe topmostbeamcandidateoftimestep t ,wehave: ^ S bp = S t; 0 t +min f 1  L pred t ; 0 g S t; 0 R (13) where R isthemaximumgenerationlength.Since S t; 0 willdropaftertimestep t ,if S t; 0 R  ^ S ? ,we reachoptimality.Thisstoppingcriterioncould alsobeappliedtolengthnormalization( 5 ). Meawhile,forBoundedAdaptive-Reward,we canhaveasimilaroptimalstoppingcriterion:If thescoreoftopmostbeamcandidateattimestep t>L pred islowerthan ^ S ? ,wereachoptimality. Proof. Thepartof ^ S AdaR in( 11 )willdecrease aftertimestep t ,whilethesecondpartstaysthe samewhen t>L pred .Sothescoreinthefuture willmonotonicallydecrease. Figure4:TheBLEUscoresandlengthratios( lr = j y j = j y  j )ofvariousrescoringmethods. 6Experiments OurexperimentsareonChinese-to-Englishtrans- lationtask,basedontheOpenNMT-pycodebase. 3 Wetrainourmodelon2Msentences,andap- plyBPE( Sennrichetal. , 2015 )onbothsides, whichreducesChineseandEnglishvocabulary sizesdownto18kand10krespectively.Wethen excludepairswithmorethan50sourceortarget tokens.WevalidateonNIST06andtestonNIST 08(newswireportionsonlyforboth).Wereport case-insensitive,4referenceBLEUscores. Weuse2-layersbidirectionalLSTMsforthe encoder.Wetrainthemodelfor15epochs,and choosetheonewithlowestperplexityonthedev set.Batchsizeis64;bothwordembeddingand hiddenstatesizes500;anddropout0.3.Thetotal parametersizeis28.5M. 6.1ParameterTuningandResults Wecompareallrescoringmethodsmentioned above.Forthelengthnormalizationmethod,we alsoshowitsresultswith optimalstopping . ForBoundedWord-Rewardmethodwithand withoutourpredictedlength,wechoosethebest r onthedevsetseperately.Thelengthnormal- 3 https://github.com/OpenNMT/OpenNMT-py Figure5:BLEUscoresandlengthratiosonthedev setovervariousinputsentencelengths. izationusedby Wuetal. ( 2016 )hastwohyper- parameters,namely  forlengthpenaltyand  for coveragepenalty.Wejointlytunethemonthedev set,andchoosethebest(  =0.3,  =0.3). Figure 4 showourresultsonthedevset.We seethatourproposedmethodsgetthebestper- formanceonthedevset,andcontinuegrowing asbeamsizeincreases.Wealsoobservethatop- timalstoppingbooststheperformanceoflength normalizationmethodbyaround+0.9BLEU.In ourexperiments,weregardourpredictedlength asthe maximumgenerationlength in( 13 ).We furtherobservefromFig. 5 thatourmethodskeep thelengthratiocloseto1,andgreatlyimprovethe qualityonlongerinputsentences,whicharenoto- riouslyhardforNMT( Shenetal. , 2016 ). Table 1 collectsourresultsonbothdevandtest sets.Withoutlossofgenerality,weshowresults withbothsmallandlargebeamsizes,whichaver- ageover b =14,15,16and b =39,40,41,respectively. 6.2Discussion FromTable 1 ,wecouldobservethatwithour lengthpredictionmodel,Boundedword-reward methodgainsconsistentimprovement.Onthe otherhand,resultsfromlengthnormalization methodshowthatoptimalstoppingtechnique Smallbeam( b =14 ; 15 ; 16 ) devtest BLEUratioBLEUratio Moses( b =70)30.14-29.41- Default( b =5)36.450.8732.880.87 LengthNorm.37.730.8934.070.89 +optimalstopping  38.690.9235.000.92 Wuetal. ( 2016 )  =  =0.338.120.8934.260.89 Boundedword-r. r =1.339.220.9835.760.98 withpredictedlength Boundedword-r. r =1.4  39.53 0.9735.810.97 Boundedadaptive-reward  39.440.9835.750.98 BP-Norm  39.350.98 35.84 0.99 Largebeam( b =39 ; 40 ; 41 ) devtest BLEUratioBLEUratio Moses( b =70)30.14-29.41- Default( b =5)36.450.8732.880.87 LengthNorm.38.150.8834.260.88 +optimalstopping  39.070.9135.140.91 Wuetal. ( 2016 )  =  =0.338.400.8934.410.88 Boundedword-r. r =1.339.600.9835.980.98 withpredictedlength Boundedword-r. r =1.4  40.110.9836.130.97 Boundedadaptive-reward  40.14 0.98 36.23 0.98 BP-Norm  39.970.99 36.22 0.99 Table1:AverageBLEUscoresandlengthratiosover smallandlargebeams. ? indicatesourmethods. gainsimprovementbyaround+0.9 BLEU.Whilewithboth,ourproposedmethods beatallpreviousmethods,andgainimprovement overhyperparameter-freebaseline(i.e.lengthnor- malization)by+2.0BLEU. Amongourproposedmethods,Boundedword- rewardhasthereward r asanhyper-parameter, whiletheothertwomethodsgetridofthat. Amongthem,werecommendtheBP-Norm method,becauseitisthesimplestmethod,andyet worksequallywellwithothers. 7Conclusions Weexplainwhythebeamsearchcurseexists andthenformalizeallpreviousrescoringmethods. Beyondthat,wealsoproposeseveralnewmethods toaddressthisproblem.ResultsfromtheChinese- Englishtaskshowthatourhyperparameter-free methodsbeatthehyperparameter-freebaseline (lengthnormalization)by+2.0BLEU. Acknowledgements KentonLeesuggestedthelengthpredictionidea. ThisworkwaspartiallysupportedbyDARPA N66001-17-2-4030,andNSFIIS-1817231and IIS-1656051.Wethanktheanonymousreviewers forsuggestionsandJunekiHongforproofreading. References DzmitryBahdanau,KyunghyunCho,andYoshuaBen- gio.2014.Neuralmachinetranslationbyjointly learningtoalignandtranslate. arXivpreprint arXiv:1409.0473 . JonasGehring,MichaelAuli,DavidGrangier,Denis Yarats,andYannNDauphin.2017.Convolutional sequencetosequencelearning.In Proc.ofICML . WeiHe,ZhongjunHe,HuaWu,andHaifengWang. 2016.Improvedneuralmachinetranslationwithsmt features.In ProceedingsoftheThirtiethAAAICon- ferenceonIntelligence ,pages151Œ157. AAAIPress. LiangHuang,KaiZhao,andMingboMa.2017.When tooptimalbeamsearchforneuraltextgener- ation(modulobeamsize).In EMNLP . NalKalchbrennerandPhilBlunsom.2013.Recur- rentcontinuoustranslationmodels.In EMNLP ,vol- ume3,page413. G.Klein,Y.Kim,Y.Deng,J.Senellart,andA.M. Rush.2017.OpenNMT:Open-SourceToolkitfor NeuralMachineTranslation. ArXive-prints . PhilippKoehn,HieuHoang,AlexandraBirch,Chris Callison-Burch,MarcelloFederico,NicolaBertoldi, BrookeCowan,WadeShen,ChristineMoran, RichardZens,etal.2007.Moses:Opensource toolkitforstatisticalmachinetranslation.In Pro- ceedingsofthe45thannualmeetingoftheACLon interactiveposteranddemonstrationsessions ,pages 177Œ180.AssociationforComputationalLinguis- tics. PhilippKoehnandRebeccaKnowles.2017.Six challengesforneuralmachinetranslation. CoRR , abs/1706.03872. MyleOtt,MichaelAuli,DavidGrangier,and Marc'AurelioRanzato.2018.Analyzinguncer- taintyinneuralmachinetranslation. arXivpreprint arXiv:1803.00047 . KishorePapineni,SalimRoukos,ToddWard,andWei- JingZhu.2002.Bleu:amethodforautomaticeval- uationofmachinetranslation.In Proceedingsof ACL ,pages311Œ318,Philadephia,USA. Marc'AurelioRanzato,SumitChopra,MichaelAuli, andWojciechZaremba.2016.Sequenceleveltrain- ingwithrecurrentneuralnetworks. ICLR . RicoSennrich,BarryHaddow,andAlexandraBirch. 2015.Neuralmachinetranslationofrarewordswith subwordunits. arXivpreprintarXiv:1508.07909 . ShiqiShen,YongCheng,ZhongjunHe,WeiHe,Hua Wu,MaosongSun,andYangLiu.2016.Minimum risktrainingforneuralmachinetranslation.In Pro- ceedingsofACL . XingShi,KevinKnight,andDenizYuret.2016.Why neuraltranslationsaretherightlength.In Proceed- ingsofthe2016ConferenceonEmpiricalMethods inNaturalLanguageProcessing ,pages2278Œ2282. IlyaSutskever,OriolVinyals,andQuocVLe.2014. Sequencetosequencelearningwithneuralnet- works.In Advancesinneuralinformationprocess- ingsystems ,pages3104Œ3112. ZhaopengTu,YangLiu,LifengShang,XiaohuaLiu, andHangLi.2017.Neuralmachinetranslationwith reconstruction.In AAAI ,pages3097Œ3103. SamWisemanandAlexanderMRush.2016. Sequence-to-sequencelearningasbeam-searchop- timization.In ProceedingsofEMNLP . YonghuiWu,MikeSchuster,ZhifengChen,QuocV Le,MohammadNorouzi,WolfgangMacherey, MaximKrikun,YuanCao,QinGao,Klaus Macherey,etal.2016.Google'sneuralma- chinetranslationsystem:Bridgingthegapbetween humanandmachinetranslation. arXivpreprint arXiv:1609.08144 .  
Proceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics ,pages221Œ231 Vancouver,Canada,July30-August4,2017. c  2017AssociationforComputationalLinguistics https://doi.org/10.18653/v1/P17-1021 Proceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics ,pages221Œ231 Vancouver,Canada,July30-August4,2017. c  2017AssociationforComputationalLinguistics https://doi.org/10.18653/v1/P17-1021 221 222 223 224 225 226 227 228 229 230 231  
MappingClientMessagestoaDataModel withMixtureFeatureEmbeddingConvolutional NeuralNetwork DingchengLi  ,PeiniLiu y ,MingHuang z ,YuGu x ,YueZhang { , XiaodiLi k ,DanielDean x ,XiaoxiLiu y ,JingminXu y ,HuiLei x andYaopingRuan x  BigDataLab,BaiduInc,USA y IBMResearch,China z MayoClinic,MN,USA x WatsonHealthCloud,IBM,USA { SingaporeUniversityofTechnologyandDesign,Singapore k DonghuaUniversity,China ABSTRACT Datamappingamongdifferentdatastandardsinhealth institutesisoftenanecessitywhendataexchangesoccur amongdifferentinstitutes.However,nomatterrule-based approachesortraditionalmachinelearningmethods,noneof thesemethodshaveachievedsatisfactoryresultsyet.Inthis work,weproposeadeeplearningmethod,mixturefeature embeddingconvolutionalneuralnetwork(MfeCNN),tocon- vertthedatamappingtoamultipleproblem. Multi-modalfeatureswereextractedfromdifferentsemantic spacewithamedicalNLPpackageandpowerfulfeature embeddingsweregeneratedbyMfeCNN.Classesasmany astenwereclsimultaneouslybyafully-connected soft-maxlayerbasedonmulti-viewembedding.Experimental resultsshowthatourproposedMfeCNNachievedbestresults thantraditionalstate-of-the-artmachinelearningmodelsand alsomuchbetterresultsthantheconvolutionalneuralnetwork ofonlyusingbag-of-wordsasinputs. I.I NTRODUCTION Datamappingisoftenrequiredwhendataexchangehap- penedamongdifferentinstitutesusingdifferentdatastandards. Inthehealthforhistoricalreasons,hospitals,pharmaceu- ticalfactories,medicalinsurancecompanies,andotherhealth- relevantindustriesusuallyhaveformedtheirowntraditionsin composingdocuments.Word-classinstitutesincludingWorld HealthOrganization,NationalInstituteofHealthandHealth LevelSevenInternational(HL7)havemadegreatefforts indevelopinginternationalstandardsforelectronichealth informationthatsupportsclinicalpracticeandmanagement, anddeliveryandevaluationofhealthservices.Mostofhealth institutesadopttheinternationalstandardstoacertaindegree. However,inconsistenciesarestillquitepopularduetothe diverseinterpretationsofthesamestandardormissingand errorsmadebymedicalstaffs.Meanwhile,mostofdocuments areunstructuredorsemi-structuredwithlargevolumesoffree texts.Consequently,automaticmappingonedatasourceto anotherbecomesahotresearchtopicamongdatascience communityornaturallanguageprocessing(NLP)community. Datamappinganddatatransformationplayavitalrolein theinformationintegrationarea.Sincedecadesago,asthere- lationaldatabaseintegrationrequirementarose,lotsofresearch workshavebeenfocusingontheschemamappingproblem 1,2 . Theschemamappingprobleminvolvesautomaticdiscovery ofthemappingrelationshipbetweensourceandtargetdata models.Rahmetal 3 theseschemamappingmethods asschema-onlybased,instance/content-basedandcombination approaches;schema-onlybasedmethodsdiscoverthemapping relationshipbytheschemametadatainformation 4 ;instance- basedmappingsdiscovertheassociationrelationshipbythe realinstancesofdatasuchasthewordfrequencies,value patternsandranges 5 ;combinationapproaches 6 tacklethe mappingwithbothschemaandrealinstances. Recently,moreresearchersapplymachinelearningand sophisticatedstatisticaltechniquestodetermineinstancelevel matchingofschemaelements.Theapproachesintroduced byDoanetal 7 showthatnewmappingscanbelearned fromknownmappingstothetargetschema.Machinelearning algorithmshavebeenusedtotrainmodelsusingknown mappingandthemodelswereappliedtothenewschema elementstomapthemtothetargets 8 .Themethodsacquire probabilisticknowledgefromexamplesprovidedbydomain expertsinordertotrainthemodels.Thetrainedmodelsina domaincanbeappliedtonewschemamappingsinthesame domain 9 .Ourapproachismotivatedbytheseworkswhile wefocusparticularlyonintegratingclinicaldatarepresented bydifferentstandards,tryingtoassociatethesedatawitha datamodel,andintheendcontributingtoseamlessly exchangingtheclinicaldata. Inthiswork,weproposeasophisticatedmachinelearn- ingmodel,MixtureFeatureEmbeddingConvolutionalNeural Network(MfeCNN)totacklethetaskofdatamapping.The innovationofourapproachliesinapplyingdeeplearning methodtothedatamappingproblem.Ourdata,HL7messages fromhealthproviders,involvesemi-structureddata,which usesanon-XMLencodingsyntaxbasedonsegmentformat, supportedbymajormedicalinformationsystemvendorsinthe UnitedStates.ThestandardsofHL7v2allowsomecustom andquitemanycancontainfreetextcontents. Meanwhile,thedatasetweusedisveryunbalancedandone targetcategoryisverydominant.Thesecharacteristicsofdata leadstothecomplexmappingproblemandwearetryingto solvethedatamappingproblembytraininganadvancedmodel withmanyNLPfeatures. Basically,weconvertthedatamappingtasktoaclassi- task.Firstly,manyrelevantfeatureswereextracted withthird-partytoolsasmulti-modalinputs,includingbag- of-words(BOG),part-of-speech(POS),syntaxandconcepts 10 .Secondly,featureembeddingrepresentationswerelearned withaCNNmodeltogeneratefeaturetensors.Thirdly,these featuretensorswerefedintoamulti-viewbasedCNNmodelto predictthedatamappings.TheMfeCNNmodelwasevaluated ondatamappingfromHL7messagetoCommonSifand comparedwithbaselinemodelsbasedonSupport VectorMachine(SVM)aswellasotherdeeplearningmodels likebasicCNN.Theresultsshowthatourmodelyields betterperformancethanbaselinemodelsandindicatethatour approachisapromisingwaytoresolvetheautomaticdata mappingproblemandabletohandleunbalancedata.Although ourproposalistestedwiththemedicalthemethodology isgenericenoughtohandleanytasksofsimilarkinds. II.R ELEVANTWORKRELATEDTOOURMODEL AdeeplearningmethodMfeCNNisproposedhereto handlethemappingproblemofclinicaldata.Intheinputdata, freetextsthemselvescanberegardedasonetypeoffeatures, namelyBOG.Inaddition,weretrievemedicalcodesinthe HL7dataastermsandlanguage-relevantfeaturesincluding part-of-speech(POS)andsyntaxamongthosefreetexts. TraditionalmachinelearningmodelslikeSVMorConditional RandomField(CRF),justtreatthemasuniformedfeatures withoutdistinguishingtheirsemanticcategories.Allthose featuresareinfactfromdifferentsemanticspacesandthus canberegardedfromdifferentmodalsandcanbeviewed differently.Itisreasonabletoconstructamulti-modaland multi-viewmodelforthemappingtask.Beforedelvinginto ourMfeCNNapproach,wewillgiveaquicksurveyof previousworkrelatedtomulti-modalandmulti-viewmodels. Ngiametal 11 proposesanapplicationofdeepnetworksto learnfeaturesovermultiplemodalities.Theirdeepnetwork isbasedonsparserestrictedBoltzmannmachines(RBM). Theirsystemdemonstratesthecapacityofcross-modalfeature learning,wherebetterfeaturesforonemodalitysuchasvideo canbelearnedforothermodalitywhenmultiplemodalities (e.g.,audioandvideo)aregivenatfeaturelearningstage.This workgivesusinspirationsofmakinguseofdiversemodalsto learnrichfeaturesfromhealthdatasets.Multiplemodalities canbeextractedfromthosedatasets,includingwords,syntax andsemanticrolesofsentencesandterminologycodes. Anotherworkwhichbringsushunchesistwin-viewem- beddingforCNN 12 .Accordingtothemodel,variable X 1 mayhaveatwin-viewembedding(tv-embedding)withre- gardstoany X 2 ifthereexistsafunction g 1 suchthat P ( X 2 j X 1 )= g 1 ( f 1 ( X 1 ) ;X 2 ) ,where ( X 1 ;X 2 ) 2 X 1  X 2 .Thetv-embeddingcanbeexpressedasafunction f 1 . Thisproposalmakesitpossibleforcurrentdatatotv- embeddingfromunlabeleddataandaccordinglyenhancethe datarepresentation.Further,thelearnttv-embeddingcanbe integratedintosupervisedCNNwithacompoundsigmoid functionas ˙ ( W _ r l ( x )+ V _ u l ( x )+ b ) .Iftherearemultipletv- embeddings,asummationcanbeaddedtotheformerequation as ˙ ( W _ r l ( x )+ P k i =1 V ( i ) _ u ( i ) l ( x )+ b ) ,whichcanberegarded asmulti-viewembedding(mv-embedding). Ourmodelframeworksharesimilaritieswithbothabove models.Itisamultimodalmodelanditemploysmulti-view embeddingforfeatureintegration.Oneessentialcontribution comesfromhowweintegratefeatureswithmulti-viewembed- ding.FeaturesinMfeCNNarefromtotallydifferentsemantic spaceormodalitiesincludingwords,concepts,andsyntax. Differentintegrationfunctionsformulti-viewembeddingare deployedassigmoidfunctionsinMfeCNN. Usually,wordembeddingcanbeconstructedwithrecurrent neuralnetwork(RNN)andlong-short-termmemorynetwork (LSTM)tothesequentialrelationshipsamongwords. However,thosemodelsonlyallowforstrictlysequential informationpropositions.Inhumanlanguages,theorderand thedependenciesbetweenwordsandphrasesareoftenim- portant.Atree-LSTM 13 wasproposedasageneralization ofLSTMstotree-structurednetworktopologiestocatchthe dependenciesbetweenwordsandphrasesandprovidebetter semanticembeddingsofwords.Inthiswork,tree-LSTMis employedtogeneratecompoundfeatureembeddingstoenrich featuremodalitiesforMfe. III.N EURAL N ETWORK A RCHITECTURE A.MixtureFeatureEmbeddingConvolutionalNeuralNetwork Fig.1.GraphicViewofDeepLearningModelforDataMapping ThestructureofourMfeCNNisillustratedinFigure1.The MfeCNNmodelcontainsthreemainlayersforfeatureextrac- tion,mixturefeatureembedding,anddeepnetworklearning, respectively.Inthelayer,athird-partytoolcTAKES (clinicalTextAnalysisKnowledgeExtractionSystem) 14 was Fig.2.MixtureFeatureEmbeddingLayerinDetails employedtoextractmulti-modalfeaturesfromtheinputsen- tences.Themulti-modalfeaturesmainlycontainBag-of-words (BOW)modal,part-of-speech(POS)modal,conceptmodal andsyntaxmodalaswellassemanticrolemodalgeneratedby tree-LSTM.Extractedfeaturesformafeaturematrixforeach sentenceandarefedintoMixtureFeatureEmbeddingLayer withtensoroutputs.Thislayerincludessharedrepresentation modelformultimodalitiesandaconvolutionalneuralnetwork togetherwithamax-poolinglayertolearnfeatureembedding presentationsastensors.Themixturefeatureembeddingwas passedtothedeepnetworklayerformodellearning andprediction.Thedeepnetworkisaconvolutionalneural networkincludingaconvolutionlayandamaxpoolinglayer followedbyafully-connectedsoftmaxlayerbasedonmulti- viewembeddingfordatamappingprediction. Themainnoveltyinournetworkarchitectureistheinclu- sionofthemixturefeaturetensorgenerationlayerandmulti- viewbasedpredictionlayer.Themixturefeatureembedding layerallowsthenetworktoutilizerichexternalresources andgeneratemoreexpressiverepresentationsoffeatures.The predictionlayerwithmulti-viewembeddingcanincorporate themulti-modalfeatureembeddingsinamulti-viewstyleand hasthepotentialtoenhancethepredictionperformanceof datamapping.Thedetailsofthefeaturetensorgenerations andmulti-viewbasedpredictionaregiveninnextsubsections. B.ConvolutionalNeuralNetwork Convolutionalneuralnetwork(CNN)herehasbeende- ployedintwodifferentstagesofourmodeltraining.Firstly,a basicCNNmodelwasusedformixturefeatureembedding togenerateembeddingrepresentationoffeaturetensoras depictedinFigure2andthenamulti-viewbasedCNNmodel isemployedforthemodellearningofthedatamapping problemasillustratedinFigure1.BoththeCNNmodels areathree-layermodelandhaveaconvolutionallayer,a max-poolinglayerandafully-connectedsoftmaxprediction layer.ThemaindifferencebetweenthetwoCNNmodelsis thepredictionlayerandmulti-viewbasedCNNmodelhasa predictionlayerbasedonmulti-viewembedding. C.MixtureFeatureEmbedding Mixturefeatureembedding(Mfe)isthecombinationsof multi-modallearningandmulti-viewembeddingasshown inFigure2.Giventheinput X 1 ,multi-modalfeaturescan begeneratedbycTAKESandtheextractedsemanticmodals mainlyincludePOSmode,conceptmode,andsyntaxmode. ThePOSmodeaimsatcatchingthepropertyofeachword ininputsentences;theconceptmodeprovidesknowledgein domainslikemedicineandserveasagooddiscrimina- toroftargets;thesyntaxmodeconveysimportantdependency relationsbetweenwordsandthecompositionalityofasentence usuallyinvolvingphrasesactsasagoodindicatorofthe sentencenature.Duetotheofsyntaxmodein humanlanguages,furtherfeatureembeddingisprocessedby tree-LSTMtoproviderichfeaturesforsemantic-rolemode, whichwillbediscussedmoreinthefollowingsubsection.All thegeneratedfeaturesarefedintosharedrepresentation layerforcross-modallearningandthenpassedtoconvolu- tionallayer,max-poolinglayerandpredictionlayertolearn featurerepresentationsasfeaturetensorsfortheMfeCNN modellearning.Duringthemodellearning,Mfe,ontheother hand,stretchesitselftotransformmulti-modallearningintoa multi-viewembeddingproblem. g 1 ( f 1 ( x l ) ;x 2 )= ˙ u ( h 1  w u  r u l ( x 1 )+ b u h 1 + cx 1 ) (1) Byfollowingtheideaoftwo-viewembeddingdescribed inEquation1 12 ,Mfeattemptstomultipletwo-view embeddings(multi-viewembedding)forinputfeatures X 1 . Inaddition,thosemulti-viewembeddingsareobtainedfrom differentmodalsordifferentsemanticspacesandtheyare differentfromoriginaltwo-viewembeddings,whichallare fromwordlevels.Formulti-viewfeatureembeddingfrom multiplesemanticspaces,Equation1willbeexpandedinto, g 1 ( f 1 ( x l ) ; x )= ˙ u ( M X j =1 h  w u j  r u j ( x l )+ b u h + cx l ) (2) D.EnrichedFeatureswithTree-LSTM Inthetree-LSTM,twodifferentversionsareprovided, child-sumtree-LSTMand N -arytree-LSTMandbothvariants haverichnetworktopologiesandcanincorporateinformation frommultiplechildunits. N -arytree-LSTMtakestheorder andtheimportanceofthechildrenintoconsideration.Here weemploy N -arytree-LSTMtogenerateextrafeatureembed- dingssincethemodelcancatchthesubtleimportanceofeach wordfeaturesandthisshouldplaybigrolesforthetraining ofthemodel. Inourwork,constituentparserisutilizedtoparseasentence intobinaryconstituents.Forexample,asentencewithsubject, predicate,andobjectmaybeparsedasanounphrase(NP),the subjectandaverbalphrase(VP).TheVPisthenparsedinto averb(V)andanotherNP(object).Givenaconstituenttree, let C ( j ) denotethesetofchildrenofnode j .Theconstituent treehasatmost N branchingfactorsandforeachchild k ,a separateparametermatrixisintroducedtoallow N -arytree LSTMmodeltolearnmoreconditioningonthe statesofaunit'schildrenthanbothchild-sumtree-LSTMand theLSTM.Asentencelikeabovewillassigntheverbor VPthehighestweightsandthesubjectNPandtheobjectNP lowerweights. E.WorkablePipeline Bycombiningallcomponentsdiscussedabove,weobtained theend-to-enddatamappingpipelineasillustratedinFigure 3.Thepipelineisplayingtwostageroles,trainingstageand applyingstage.Inthetrainingstage,thecollectionreader componentreadsthetrainingdata,includingHL7message data,targetdataandmappingrelationship.Intheapplying stage,newHL7messagesgothroughthesamepipelineand arebyourmodelandeachofmessagecontents canbemappedtothetargetdataschemaelements. SourcedatainHL7messagesformatwillbepreprocessed byanIBMparsercalledDFDL(DataFormatDescription Language)parserandareconvertedtoamediatedformat,HL7 XML.ThemediatedformathelpsustodelimitateHL7content intosegments,andandmakeitpossibleto analyzeeachpartofthecontents.TheseparatedHL7contents areanalyzedbyannotatorsofcTAKESandtheCasConsumer componentreadstheanalysisoutputfromcTAKES.These resultscouldbeconsideredasNLPfeaturesofeachHL7 ThesefeaturesarefedintoMfeCNNandtrainedagainstthe targetdataschemaandestablishedmappings.Afterthemodels aretrainedandvalidated,testdatacangothroughthepipeline withsimilarpreprocessingandfeatureextraction. Fig.3.End-to-EndDataMappingPipelineofMfeCNN IV.E XPERIMENTS Inthissection,weevaluatetheeffectivenessofourMfeCNN modelondatamappingfromHL7messagetoCommonSif.We beginbydescribingthedatasetsusedforevaluation,followed bythedetaileddiscussionofexperimentalsettingsand evaluation.Intheevaluation,theresultsofMfeCNNandother modelsaregivenforcomparison. A.Datasets DataSetsDataVolumeMappings I2B21600056372 CustomerCDA240037230 CustomerHL7160018604 Total20000112207 TABLEI D ATASETSANDMAPPINGNUMBERS ClassIDClassName 0AL1.3.AllergenCodeMnemonicDescription 1DG1.4.DiagnosisDescription 2NK1.7.ContactRole 3NTE.3.Comment 4OBX.5.ObservationValue 5PID.11.PatientAddress 6PID.5.PatientName 7PV2.3.AdmitReason 8Remainder 9TXA.2.DocumentType TABLEII T ENTARGETDATASCHEMAELEMENTS TheinputdatastandardforourexperimentsisHL7mes- sagev2.Wehavetotally20000HL7documentswitheach containingonlyoneHL7message.Weareusing3datasets tocomposetheseHL7messagedata:I2B2datasets 15 which containspublicclinicalnotesforNLPresearchuse;sample CDAdatasetsobtainedfromahospitalwhich recordspatients'medicaltreatment;andsampleHL7message datasetsobtainedfromahealthcaresolutionproviderwhich containshospitalchargeanddischarge,medicaltreatmentand labtests.AllthedatasetsareconvertedtoconsistentHL7v2 messagesinadvancesothatwecanprocessdifferentformat datasetswiththesamepipeline. AlltheofHL7dataweremanuallyannotatedby domainexpertsusingourtargetdataschemaCommonSIF's elements,whichareconsideredasthegroundtruth.Table1 showsourdatasetsintheHL7v2formatwithannotated mappings.Weuseabout50%,20%,30%ofoveralldatasets fortraining,validation,andtestrespectively.Namely,inour experiment,weuse33662instancestoevaluateourmodel.In total,allinstancesaremappedto10classes(Seeclassnames listedinTables2)whichrepresent10kindsoftargetdata schemaelements. B.ExperimentalSettings Twokindsofbaselineswereruntoevaluatetheeffects ofmixturefeatureembedding.Sinceourmodelisbasedon convolutionalneuralnetwork,thebaselinedeploysabasic CNNmodelwiththeonlybagofwordsasinputsandthe resultsarelistedinTableIII.Besides,weusedSVMmodel totrainandtestdatamappingaswell,whichenablesus tomakecomparisonsontraditionalstate-of-the-artmachine learningmodel.HerebothSVMandCNNbaselinemodels useextractedfeaturesfromcTAKEsasinputsandresultsare showninTableIV.FortheSVMmodel,weusedlibsvm 16 librarytoimplementthefunction.BothCNN andMfeCNNwereimplementedwithw 17 withthe samenetworkasdescribedinSectionIII-B.We reportedstandardSupport,precision(Pre),recall(Rec)andF1 scoreforthemetrics,whichareas, Support = CorrectMappingsFound (3) Pre = Support=AllMappingsFound (4) Rec = Support=AllCorrectMappings (5) F 1=2  Pre  Rec= ( Pre + Rec ) (6) C.FinalResults ClassIDPre%Rec%F%Support 0465248566/1070 1453640356/988 242504521/42 3314235261/621 4703042217/722 52037251316/3556 667505712/24 756555572/131 867576113271/23282 95444481420/3227 all49.845.347.517512/33662 TABLEIII CNN RESULTSWITHTHEONLYBAGOFWORDS WecomparedtheMfeCNNresultswithtwobaselineap- proachesusingtraditionalSVMmodelandbasicCNNmodel withthesameForalldatasetsweuse: linearunits,windows(h)of3,4,5withdropoutrate(p) of0.5,l2constraint(s)of3,andmini-batchsizeof50.Feature embeddingdimensionsvaryaccordingtothepropertyandtotal vocabularyofeachfeature.Wordsthemselvesinvolve10000 uniquetokensand200asthefeaturemap.Conceptsinvolve 30000uniqueand400asthefeaturemapwhile syntaxandposhavemuchfewernumbers(totalabout100), thusembeddingmaponlyneeds20dimensions.Thesevalues forhyperparameterswerechosenviaagridsearchonthedev set. Table3reportedthebaselineresultsofourdatamapping predictionontestdataconductedbythebasicCNNmodelwith theonlybagofwordsasfeatures.Tables4and5showthe comparisonofmetricsofthreeapproacheswithallextracted featuresincluded.Alltheresultsaremuchbetterthanthatof basicCNNbaselineeventhoughthebaselineisemploying CNN,anadvanceddeeplearningframework. AlthoughSVM,CNN,andMfeCNNallleadtoverygood datamappingsprediction,MfeCNNachievesthebestperfor- manceoverallintheseclasses.Comparedwithdeeplearning model,SVMshowslowerrecallsaswellasF-scores.The possiblereasonisthatourSVMmodelisalinearmodel,which simplytransformstheinputtosomehighdimensionalspace torevealthedifferencesofclasses;wheredeeplearningmodel hasadeeparchitecturewithnonlinearmultiplelayerswhich combineandtransformfeaturelayerstolayers,thatcould helptoachievebetterresults.Comparedwith CNN,althoughsomerowlikeclass5(PID.11.PatientAddress) getslowermetricsduetosomedefaultvaluetexts,MefCNN getsmuchbetterresultsformostoftheclassesandthe averageF1-scoreisashighas86.2%.Namely,weachieve 22%improvementsthantheCNNresults.Inaddition,recently LecunetladevelopedaverydeepCNNmodelwith29 networklayerstoperformtopicfortentopics bygivenfreetextswithoutpreprocessingasinputsandthe predictionaccuracyachieves73.4% 18 . Theseresultsvalidatethatourmixturefeatureembedding convolutionalneuralnetworkapproachcanindeedmapthe customerHL7messagestocanonicaldatatypeseffectively andovercomedataunbalancetosomedegree.Comparison withtheresultsusingtraditionalSVMmodelandCNNmodels showsthatthecombinationofmixturefeatureembedding andconvolutionalneuralnetworkallowsthedevelopmentof sophisticateddeeplearningmodeltoachievetheexcellent mappingaccuracy. V.C ONCLUSIONAND F UTURE W ORK Inthiswork,weimplementedanovelandsophisticated deeplearningframeworkMfeCNNforclinicdatamapping. Withthisframework,weconvertedthedatamappingtaskto amulti-labelproblem.Innovatively,weincorpo- ratemultimodalitiesandmulti-viewembeddingintoCNNfor mixturefeaturetensorgenerationandprediction. AnopensourcetoolcTAKESwasutilizedtoperform deeplanguageanalysisforunstructuredfreetextssothat richlinguisticfeatureswereextracted.Inordertomakefull useofthosefeaturesinthemulti-modalsemanticspaces,we developedamixturefeatureembeddingconvolutionalneural networktodeploythosefeatures.Mixturefeatureembedding realizedamulti-modalandmulti-viewapproachtodigest featuresfromdifferentsemanticspaces.Thisisquitedifferent frompreviousapproachestodofeatureembedding,which usuallyfocusesonwordspaces.Incontrast,wecombined thefeatureembeddingofsyntaxspaceanddomainspace (medicalconcepts)aswellaswordspace.Ourexperimental resultsshowthatourapproachachievessatisfactoryresults.In addition,thecombinationofmixturefeatureembeddingand CNNplaysanimportantroleinachievingthehighresults. Infuturework,wewillfocusonmorefeaturesaswell asimprovingthemodelaswell.Wemayconsidertouse deepernetworkandextendMfeCNNtointegratingLSTMand reinforcementlearningsothatmoregenericmodelscanbe developedfordiversedatamappingtasks. A CKNOWLEDGMENTS ThisworkswassupportedbyBaiduBigDataLabandIBM WatsonHealthCloud R EFERENCES 1. J.Geller,Z.He,Y.Perl,C.P.Morrey,andJ.Xu,ﬁRule- basedsupportsystemformultipleumlssemantictype assignments,ﬂ Journalofbiomedicalinformatics ,vol.46, no.1,pp.97Œ110,2013. 2. H.Gu,Y.Chen,Z.He,M.Halper,andL.Chen, ﬁQualityassuranceofumlssemantictypeassignments usingsnomedcthierarchies,ﬂ MethodsofInformationin Medicine ,vol.55,no.2,pp.158Œ65,2016. 3. E.RahmandP.A.Bernstein,ﬁAsurveyofapproachesto automaticschemamatching,ﬂ theVLDBJournal ,vol.10, no.4,pp.334Œ350,2001. 4. J.Eisenstein,B.O'Connor,N.A.Smith,andE.P.Xing, ﬁAlatentvariablemodelforgeographiclexicalvariation,ﬂ ClassSVMCNNMfeCNN IDPre%Rec%F%Pre%Rec%F%Pre%Rec%F% 06288736563639868 81 1757145246487258 64 21002540597566 100100100 3486655318245 966678 4651726973248 100 37 54 55308264733 23 10037 6 100100100 975065 100100100 7401521702334 1004057 8878987969595 979998 9969796979797 100100100 MicroAve77.379.675.686.086.785.0 89.895.989.7 MacroAve67.852.959.469.059.363.9 88.683.986.2 TABLEIV M ULTI - CLASSCLASSIFICATIONRESULTSFOR SVM,CNN AND M FE CNN ClassIDSVMCNNMfeCNNGoldStandard 09426747271070 169454573988 211324242 3410509410621 4122231267722 51067167135563556 624122424 7203052131 820720221172304923282 93126313032273227 all26511288603192733662 TABLEV O VERALLPERFORMANCEOFTHETHREESYSTEMS .T HEVALUESARETHE NUMBEROFTRUEPOSITIVE . in Proceedingsofthe2010ConferenceonEmpirical MethodsinNaturalLanguageProcessing .Stroudsburg, PA,USA:AssociationforComputationalLinguistics, 2010. 5. L.L.Yan,R.J.Miller,L.M.Haas,andR.Fagin, ﬁData-drivenunderstandingandofschema mappings,ﬂin ACMSIGMODRecord ,vol.30,no.2. ACM,2001,pp.485Œ496. 6. C.Drumm,M.Schmitt,H.-H.Do,andE.Rahm,ﬁQuick- mig:automaticschemamatchingfordatamigration projects,ﬂin ProceedingsofthesixteenthACMconference onConferenceoninformationandknowledgemanage- ment .ACM,2007,pp.107Œ116. 7. A.Doan,P.Domingos,andA.Y.Halevy,ﬁReconciling schemasofdisparatedatasources:Amachine-learning approach,ﬂin ACMSigmodRecord ,vol.30,no.2.ACM, 2001,pp.509Œ520. 8. B.Alexe,B.TenCate,P.G.Kolaitis,andW.-C.Tan,ﬁDe- signingandschemamappingsviadataexamples,ﬂ in Proceedingsofthe2011ACMSIGMODInternational ConferenceonManagementofdata .ACM,2011,pp. 133Œ144. 9. J.Madhavan,P.A.Bernstein,A.Doan,andA.Halevy, ﬁCorpus-basedschemamatching,ﬂin DataEngineering, 2005.ICDE2005.Proceedings.21stInternationalCon- ferenceon .IEEE,2005,pp.57Œ68. 10. M.MiwaandM.Bansal,ﬁEnd-to-endrelationextraction usinglstmsonsequencesandtreestructures,ﬂ arXiv preprintarXiv:1601.00770 ,2016. 11. J.Ngiam,A.Khosla,M.Kim,J.Nam,H.Lee,and A.Y.Ng,ﬁMultimodaldeeplearning,ﬂin Proceedings ofthe28thinternationalconferenceonmachinelearning (ICML-11) ,2011,pp.689Œ696. 12. R.JohnsonandT.Zhang,ﬁSemi-supervisedconvolutional neuralnetworksfortextcategorizationviaregionem- bedding,ﬂin Advancesinneuralinformationprocessing systems ,2015,pp.919Œ927. 13. K.S.Tai,R.Socher,andC.D.Manning,ﬁIm- provedsemanticrepresentationsfromtree-structured longshort-termmemorynetworks,ﬂ arXivpreprint arXiv:1503.00075 ,2015. 14. G.K.Savova,J.J.Masanz,P.V.Ogren,J.Zheng, S.Sohn,K.C.Kipper-Schuler,andC.G.Chute,ﬁMayo clinicaltextanalysisandknowledgeextractionsystem (ctakes):architecture,componentevaluationandappli- cations,ﬂ JournaloftheAmericanMedicalInformatics Association ,vol.17,no.5,pp.507Œ513,2010. 15. S.N.Murphy,G.Weber,M.Mendis,V.Gainer,H.C. Chueh,S.Churchill,andI.Kohane,ﬁServingtheenter- priseandbeyondwithinformaticsforintegratingbiology andthebedside(i2b2),ﬂ JournaloftheAmericanMedical InformaticsAssociation ,vol.17,no.2,pp.124Œ130,2010. 16. C.-C.ChangandC.-J.Lin,ﬁLibsvm:alibraryforsupport vectormachines,ﬂ ACMTransactionsonIntelligentSys- temsandTechnology(TIST) ,vol.2,no.3,p.27,2011. 17. M.Abadi,A.Agarwal,P.Barham,E.Brevdo, Z.Chen,C.Citro,G.S.Corrado,A.Davis,J.Dean, M.Devin etal. ,ﬁTw:Large-scalemachinelearn- ingonheterogeneousdistributedsystems,ﬂ arXivpreprint arXiv:1603.04467 ,2016. 18. A.Conneau,H.Schwenk,L.Barrault,andY.Lecun, ﬁVerydeepconvolutionalnetworksfortextﬂ arXivpreprintarXiv:1606.01781 ,2016.  
DeepVoice2:Multi-SpeakerNeuralText-to-Speech SercanÖ.  sercanarik@baidu.com GregoryDiamos  gregdiamos@baidu.com AndrewGibiansky  gibianskyandrew@baidu.com JohnMiller  millerjohn@baidu.com KainanPeng  pengkainan@baidu.com WeiPing  pingwei01@baidu.com JonathanRaiman  jonathanraiman@baidu.com YanqiZhou  zhouyanqi@baidu.com BaiduSiliconValleyIntelligenceLab 1195BordeauxDr.Sunnyvale,CA94089 Abstract Weintroduceatechniqueforaugmentingneuraltext-to-speech(TTS)withlow- dimensionaltrainablespeakerembeddingstogeneratedifferentvoicesfroma singlemodel.Asastartingpoint,weshowimprovementsoverthetwostate-of- the-artapproachesforsingle-speakerneuralTTS:DeepVoice1andTacotron. WeintroduceDeepVoice2,whichisbasedonasimilarpipelinewithDeep Voice1,butconstructedwithhigherperformancebuildingblocksanddemonstrates aaudioqualityimprovementoverDeepVoice1.WeimproveTacotron byintroducingapost-processingneuralvocoder,anddemonstratea audioqualityimprovement.Wethendemonstrateourtechniqueformulti-speaker speechsynthesisforbothDeepVoice2andTacotronontwomulti-speakerTTS datasets.WeshowthatasingleneuralTTSsystemcanlearnhundredsofunique voicesfromlessthanhalfanhourofdataperspeaker,whileachievinghighaudio qualitysynthesisandpreservingthespeakeridentitiesalmostperfectly. 1Introduction speechsynthesis,commonlyknownastext-to-speech(TTS),hasavarietyofapplicationsin technologyinterfaces,accessibility,media,andentertainment.MostTTSsystemsarebuiltwitha singlespeakervoice,andmultiplespeakervoicesareprovidedbyhavingdistinctspeechdatabasesor modelparameters.Asaresult,developingaTTSsystemwithsupportformultiplevoicesrequires muchmoredataanddevelopmenteffortthanasystemwhichonlysupportsasinglevoice. Inthiswork,wedemonstratethatwecanbuildall-neuralmulti-speakerTTSsystemswhichsharethe vastmajorityofparametersbetweendifferentspeakers.Weshowthatnotonlycanasinglemodel generatespeechfrommultipledifferentvoices,butalsothatlessdataisrequiredper speakerthanwhentrainingsingle-speakersystems. Concretely,wemakethefollowingcontributions: 1. WepresentDeepVoice2,animprovedarchitecturebasedonDeepVoice1( Ariketal. , 2017 ). 2. WeintroduceaWaveNet-based( Oordetal. , 2016 )spectrogram-to-audioneuralvocoder,and useitwithTacotron( Wangetal. , 2017 )asareplacementforGrifaudiogeneration.  Listedalphabetically. 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. arXiv:1705.08947v2  [cs.CL]  20 Sep 20173. Usingthesetwosingle-speakermodelsasabaseline,wedemonstratemulti-speakerneural speechsynthesisbyintroducingtrainablespeakerembeddingsintoDeepVoice2andTacotron. Weorganizetherestofthispaperasfollows.Section 2 discussesrelatedworkandwhatmakesthe contributionsofthispaperdistinctfrompriorwork.Section 3 presentsDeepVoice2andhighlights thedifferencesfromDeepVoice1.Section 4 explainsourspeakerembeddingtechniqueforneural TTSmodelsandshowsmulti-speakervariantsoftheDeepVoice2andTacotronarchitectures. Section 5.1 theimprovementforsinglespeakerTTSthroughameanopinionscore(MOS) evaluationandSection 5.2 presentsthesynthesizedaudioqualityofmulti-speakerDeepVoice2and TacotronviabothMOSevaluationandamulti-speakerdiscriminatoraccuracymetric.Section 6 concludeswithadiscussionoftheresultsandpotentialfuturework. 2RelatedWork WediscusstherelatedworkrelevanttoeachofourclaimsinSection 1 inorder,startingfrom single-speakerneuralspeechsynthesisandmovingontomulti-speakerspeechsynthesisandmetrics forgenerativemodelquality. Withregardstosingle-speakerspeechsynthesis,deeplearninghasbeenusedforavarietyofsubcom- ponents,includingdurationprediction( Zenetal. , 2016 ),fundamentalfrequencyprediction( Ronanki etal. , 2016 ),acousticmodeling( ZenandSak , 2015 ),andmorerecentlyautoregressivesample-by- sampleaudiowaveformgeneration(e.g., Oordetal. , 2016 ; Mehrietal. , 2016 ).Ourcontributions builduponrecentworkinentirelyneuralTTSsystems,includingDeepVoice1( Ariketal. , 2017 ), Tacotron( Wangetal. , 2017 ),andChar2Wav( Soteloetal. , 2017 ).Whiletheseworksfocuson buildingsingle-speakerTTSsystems,ourpaperfocusesonextendingneuralTTSsystemstohandle multiplespeakerswithlessdataperspeaker. Ourworkisnotthetoattemptamulti-speakerTTSsystem.Forinstance,intraditionalHMM- basedTTSsynthesis(e.g., Yamagishietal. , 2009 ),anaveragevoicemodelistrainedusingmultiple speakers'data,whichisthenadaptedtodifferentspeakers.DNN-basedsystems(e.g., Yangetal. , 2016 )havealsobeenusedtobuildaveragevoicemodels,withi-vectorsrepresentingspeakersas additionalinputsandseparateoutputlayersforeachtargetspeaker.Similarly, Fanetal. ( 2015 ) usesasharedhiddenrepresentationamongdifferentspeakerswithspeaker-dependentoutputlayers predictingvocoderparameters(e.g.,linespectralpairs,aperiodicityparametersetc.).Forfurther context, Wuetal. ( 2015 )empiricallystudiesDNN-basedmulti-speakermodeling.Morerecently, speakeradaptationhasbeentackledwithgenerativeadversarialnetworks(GANs)( Hsuetal. , 2017 ). Weinsteadusetrainablespeakerembeddingsformulti-speakerTTS.Theapproachwasinvestigated inspeechrecognition( Abdel-HamidandJiang , 2013 ),butisanoveltechniqueinspeechsynthesis. Unlikepriorworkwhichdependsonedembeddings(e.g.i-vectors),thespeakerembeddingsused inthisworkaretrainedjointlywiththerestofthemodelfromscratch,andthuscandirectlylearn thefeaturesrelevanttothespeechsynthesistask.Inaddition,thisworkdoesnotrelyonper-speaker outputlayersoraveragevoicemodeling,whichleadstohigher-qualitysynthesizedsamplesandlower datarequirements(astherearefeweruniqueparametersperspeakertolearn). Inordertoevaluatethedistinctivenessofthegeneratedvoicesinanautomatedway,weproposeusing theaccuracyofaspeakerdiscriminator.SimilarmetricssuchasanﬁInceptionscoreﬂ havebeenusedforquantitativequalityevaluationsofGANsforimagesynthesis(e.g., Salimans etal. , 2016 ).SpeakerhasbeenstudiedwithbothtraditionalGMM-basedmethods(e.g., Reynoldsetal. , 2000 )andmorerecentlywithdeeplearningapproaches(e.g., Lietal. , 2017 ). 3Single-SpeakerDeepVoice2 Inthissection,wepresentDeepVoice2,aneuralTTSsystembasedonDeepVoice1( Ariketal. , 2017 ).WekeepthegeneralstructureoftheDeepVoice1( Ariketal. , 2017 ),asdepictedinFig. 1 (the correspondingtrainingpipelineisdepictedinAppendix A ).Ourprimarymotivationforpresenting animprovedsingle-speakermodelistouseitasthestartingpointforahigh-qualitymulti-speaker model. OnemajordifferencebetweenDeepVoice2andDeepVoice1istheseparationofthephoneme durationandfrequencymodels.DeepVoice1hasasinglemodeltojointlypredictphonemeduration 2 Figure1:Inferencesystemdiagram:text-phonemesdictionaryconversion,secondpredict phonemedurations,thirdupsampleandgenerate F 0 ,feed F 0 andphonemestovocalmodel. andfrequency(voicednessandtime-dependentfundamentalfrequency, F 0 ).InDeepVoice2, thephonemedurationsarepredictedandthenareusedasinputstothefrequencymodel. Inthesubsequentsubsections,wepresentthemodelsusedinDeepVoice2.Allmodelsaretrained separatelyusingthehyperparametersinAppendix B .Wewillprovideaquantitative comparisonofDeepVoice1andDeepVoice2inSection 5.1 . 3.1Segmentationmodel EstimationofphonemelocationsistreatedasanunsupervisedlearningprobleminDeepVoice 2,similartoDeepVoice1.Thesegmentationmodelisconvolutional-recurrentarchitecturewith connectionisttemporal(CTC)loss( Gravesetal. , 2006 )appliedtoclassifyphoneme pairs,whicharethenusedtoextracttheboundariesbetweenthem.Themajorarchitecturechangesin DeepVoice2aretheadditionofbatchnormalizationandresidualconnectionsintheconvolutional layers.,DeepVoice1'ssegmentationmodelcomputestheoutputofeachlayeras h ( l ) =relu  W ( l )  h ( l  1) + b ( l )  ; (1) where h ( l ) istheoutputofthe l -thlayer, W ( l ) istheconvolution b ( l ) isthebiasvector,and  istheconvolutionoperator.Incontrast,DeepVoice2'ssegmentationmodellayersinsteadcompute h ( l ) =relu  h ( l  1) +BN  W ( l )  h ( l  1)  ; (2) where BN isbatchnormalization( IoffeandSzegedy , 2015 ).Inaddition,wethatthesegmentation modeloftenmakesmistakesforboundariesbetweensilencephonemesandotherphonemes,whichcan reducesegmentationaccuracyonsomedatasets.Weintroduceasmallpost-processing steptocorrectthesemistakes:wheneverthesegmentationmodeldecodesasilenceboundary,we adjustthelocationoftheboundarywithasilencedetectionheuristic. 2 3.2DurationModel InDeepVoice2,insteadofpredictingacontinuous-valuedduration,weformulatedurationprediction asasequencelabelingproblem.Wediscretizethephonemedurationintolog-scaledbuckets,and assigneachinputphonemetothebucketlabelcorrespondingtoitsduration.Wemodelthesequence byaconditionalrandom(CRF)withpairwisepotentialsatoutputlayer( Lampleetal. , 2016 ). Duringinference,wedecodediscretizeddurationsfromtheCRFusingtheViterbiforward-backward algorithm.Wethatquantizingthedurationpredictionandintroducingthepairwisedependence impliedbytheCRFimprovessynthesisquality. 3.3FrequencyModel Afterdecodingfromthedurationmodel,thepredictedphonemedurationsareupsampledfroma per-phonemeinputfeaturestoaper-frameinputforthefrequencymodel. 3 DeepVoice2frequency 2 Wecomputethesmoothednormalizedaudiopoweras p [ n ]=( x [ n ] 2 =x max 2 )  g [ n ] ,where x [ n ] isthe audiosignal, g [ n ] istheimpulseresponseofaGaussian, x max isthemaximumvalueof x [ n ] and  is one-dimensionalconvolutionoperation.Weassignthesilencephonemeboundarieswhen p [ n ] exceedsaed threshold.TheoptimalparametervaluesfortheGaussianandthethresholddependonthedatasetand audiosamplingrate. 3 Eachframeisensuredtobe10milliseconds.Forexample,ifaphonemelasts20milliseconds,theinput featurescorrespondingtothatphonemewillberepeatedin2frames.Ifitlastslessthan10milliseconds,itis extendtoasingleframe. 3 modelconsistsofmultiplelayers:,bidirectionalgatedrecurrentunit(GRU)layers( Choetal. , 2014 )generatehiddenstatesfromtheinputfeatures.Fromthesehiddenstates,anafprojection followedbyasigmoidnonlinearityproducestheprobabilitythateachframeisvoiced.Hiddenstates arealsousedtomaketwoseparatenormalized F 0 predictions.Theprediction, f GRU ,ismade withasingle-layerbidirectionalGRUfollowedbyanafprojection.Thesecondprediction, f conv , ismadebyaddingupthecontributionsofmultipleconvolutionswithvaryingconvolutionwidths andasingleoutputchannel.Finally,thehiddenstateisusedwithanafprojectionandasigmoid nonlinearitytopredictamixtureratio ! ,whichisusedtoweighthetwonormalizedfrequency predictionsandcombinetheminto f = !  f GRU +(1  ! )  f conv : (3) Thenormalizedprediction f isthenconvertedtothetruefrequency F 0 predictionvia F 0 =  F 0 + ˙ F 0  f; (4) where  F 0 and ˙ F 0 are,respectively,themeanandstandarddeviationof F 0 forthespeakerthe modelistrainedon.Wethatpredicting F 0 withamixtureofconvolutionsandarecurrentlayer performsbetterthanpredictingwitheitheroneindividually.Weattributethistothehypothesisthat includingthewideconvolutionsreducestheburdenfortherecurrentlayerstomaintainstateovera largenumberofinputframes,whileprocessingtheentirecontextinformationef. 3.4VocalModel TheDeepVoice2vocalmodelisbasedonaWaveNetarchitecture( Oordetal. , 2016 )withatwo-layer bidirectionalQRNN( Bradburyetal. , 2017 )conditioningnetwork,similartoDeepVoice1.However, weremovethe 1  1 convolutionbetweenthegatedtanhnonlinearityandtheresidualconnection.In addition,weusethesameconditionerbiasforeverylayeroftheWaveNet,insteadofgeneratinga separatebiasforeverylayeraswasdoneinDeepVoice1. 4 4Multi-SpeakerModelswithTrainableSpeakerEmbeddings Inordertosynthesizespeechfrommultiplespeakers,weaugmenteachofourmodelswithasingle low-dimensionalspeakerembeddingvectorperspeaker.Unlikepreviouswork,ourapproachdoes notrelyonper-speakerweight matrices or layers .Speaker-dependentparametersarestoredinavery low-dimensionalvectorandthusthereisnear-completeweightsharingbetweenspeakers.Weuse speakerembeddingstoproducerecurrentneuralnetwork(RNN)initialstates,nonlinearitybiases, andmultiplicativegatingfactors,usedthroughoutthenetworks.Speakerembeddingsareinitialized randomlywithauniformdistributionover [  0 : 1 ; 0 : 1] andtrainedjointlyviabackpropagation;each modelhasitsownsetofspeakerembeddings. Toencourageeachspeaker'suniquevoicesignaturetothemodel,weincorporatethespeaker embeddingsintomultipleportionsofthemodel.Empirically,wethatsimplyprovidingthe speakerembeddingstotheinputlayersdoesnotworkaswellforanyofthepresentedmodelsbesides thevocalmodel,possiblyduetothehighdegreeofresidualconnectionspresentintheWaveNetand duetothedifcultyoflearninghigh-qualityspeakerembeddings.Weobservedthatseveralpatterns tendtoyieldhighperformance:  SpeakerEmbeddings: Foreveryusesiteinthemodelarchitecture,transformthe sharedspeakerembeddingtotheappropriatedimensionandformthroughanafprojection andanonlinearity.  RecurrentInitialization: Initializerecurrentlayerhiddenstateswithspeaker embeddings.  InputAugmentation: Concatenateaspeakerembeddingtotheinputatevery timestepofarecurrentlayer.  FeatureGating: Multiplylayeractivationselementwisewithasite-specispeakerembedding torenderadaptableinformationw. 5 4 Wethatthesechangesreducemodelsizebyafactorof ˘ 7andspeedupinferenceby ˘ 25%,while yieldingnoperceptualchangeinquality.However,wedonotfocusondemonstratingtheseclaimsinthispaper. 5 Wehypothesizethatfeaturegatingletsthemodellearntheunionofallnecessaryfeatureswhileallowing speakerembeddingstodeterminewhatfeaturesareusedforeachspeakerandhowmuchtheywill haveontheactivations. 4 (a) (b) (c) Figure2:Architectureforthemulti-speaker(a)segmentation,(b)duration,and(c)frequencymodel. Next,wedescribehowspeakerembeddingsareusedineacharchitecture. 4.1Multi-SpeakerDeepVoice2 TheDeepVoice2modelshaveseparatespeakerembeddingsforeachmodel.Yet,theycanbeviewed aschunksofalargerspeakerembedding,whicharetrainedindependently. 4.1.1SegmentationModel Inmulti-speakersegmentationmodel,weusefeaturegatingintheresidualconnectionsofthe convolutionlayers.InsteadofEq. ( 2 ) ,wemultiplythebatch-normalizedactivationsbya speakerembedding: h ( l ) =relu  h ( l  1) +BN  W  h ( l  1)   g s  ; (5) where g s isaspeakerembedding.Thesameembeddingissharedforall theconvolutionallayers.Inaddition,weinitializeeachoftherecurrentlayerswithasecondsite embedding.Similarly,eachlayersharesthesameembedding,ratherthanhaving aseparateembeddingperlayer. 4.1.2DurationModel Themulti-speakerdurationmodelusesspeaker-dependentrecurrentinitializationandinputaugmen- tation.AembeddingisusedtoinitializeRNNhiddenstates,andanothersi embeddingisprovidedasinputtotheRNNlayerbyconcatenatingittothefeaturevectors. 4.1.3FrequencyModel Themulti-speakerfrequencymodelusesrecurrentinitialization,whichinitializestherecurrent layers(exceptfortherecurrentoutputlayer)withasinglespeaker-embedding.As describedinSection 3.3 ,therecurrentandconvolutionaloutputlayersinthesingle-speakerfrequency modelpredicta normalized frequency,whichisthenconvertedintothetrue F 0 byaedlinear transformation.Thelineartransformationdependsonthemeanandstandarddeviationofobserved F 0 forthespeaker.Thesevaluesvarygreatlybetweenspeakers:malespeakers,forinstance,tendto haveamuchlowermean F 0 .Tobetteradapttothesevariations,wemakethemeanandstandard deviationtrainablemodelparametersandmultiplythembyscalingtermswhichdependonthespeaker embeddings.,insteadofEq.( 4 ),wecomputethe F 0 predictionas F 0 =  F 0   1+softsign  V  T g f  + ˙ F 0   1+softsign  V ˙ T g f   f; (6) where g f isaspeakerembedding,  F 0 and ˙ F 0 aretrainablescalarparametersinitialized tothe F 0 meanandstandarddeviationonthedataset,and V  and V ˙ aretrainableparametervectors. 5 Figure3:TacotronwithspeakerconditioningintheEncoderCBHGmoduleanddecoderwithtwo waystoconvertspectrogramtoaudio:Griforourspeaker-conditionedVocalmodel. 4.1.4VocalModel Themulti-speakervocalmodelusesonlyinputaugmentation,withthespeakerembedding concatenatedontoeachinputframeoftheconditioner.Thisdiffersfromtheglobalconditioning suggestedin Oordetal. ( 2016 )andallowsthespeakerembeddingtothelocalconditioning networkaswell. Withoutspeakerembeddings,thevocalmodelisstillabletogeneratesomewhatdistinct-sounding voicesbecauseofthedisctinctivefeaturesprovidedbythefrequencyanddurationmodels.Yet, havingspeakerembeddingsinthevocalmodelincreasestheaudioquality.Weindeedobservethat theembeddingsconvergetoameaningfullatentspace. 4.2Multi-SpeakerTacotron InadditiontoextendingDeepVoice2withspeakerembeddings,wealsoextendTacotron( Wang etal. , 2017 ),asequence-to-sequencecharacter-to-waveformmodel.Whentrainingmulti-speaker Tacotronvariants,wethatmodelperformanceishighlydependentonmodelhyperparameters, andthatsomemodelsoftenfailtolearnattentionmechanismsforasmallsubsetofspeakers.Wealso thatifthespeechineachaudioclipdoesnotstartatthesametimestep,themodelsaremuchless likelytoconvergetoameaningfulattentioncurveandrecognizablespeech;thus,wetrimallinitial andsilenceineachaudioclip.Duetothesensitivityofthemodeltohyperparametersanddata preprocessing,webelievethatadditionaltuningmaybenecessarytoobtainmaximalquality.Thus, ourworkfocusesondemonstratingthatTacotron,likeDeepVoice2,iscapableofhandlingmultiple speakersthroughspeakerembeddings,ratherthancomparingthequalityofthetwoarchitectures. 4.2.1Character-to-SpectrogramModel TheTacotroncharacter-to-spectrogramarchitectureconsistsofaconvolution-bank-highway-GRU (CBHG)encoder,anattentionaldecoder,andaCBHGpost-processingnetwork.Duetothecomplexity ofthearchitecture,weleaveoutacompletedescriptionandinsteadfocusonour WethatincorporatingspeakerembeddingsintotheCBHGpost-processingnetworkdegrades outputquality,whereasincorporatingspeakerembeddingsintothecharacterencoderisnecessary. Withoutaspeaker-dependentCBHGencoder,themodelisincapableoflearningitsattentionmech- anismandcannotgeneratemeaningfuloutput(seeAppendix D.2 forspeaker-dependentattention visualizations).Inordertoconditiontheencoderonthespeaker,weuseoneembedding asanextrainputtoeachhighwaylayerateachtimestepandinitializetheCBHGRNNstatewitha secondembedding. Wealsothataugmentingthedecoderwithspeakerembeddingsishelpful.Weuseone embeddingasanextrainputtothedecoderpre-net,oneextraembeddingastheinitial attentioncontextvectorfortheattentionalRNN,oneembeddingastheinitialdecoder GRUhiddenstate,andoneembeddingasabiastothetanhinthecontent-basedattention mechanism. 6 Model Samp.Freq. MOS DeepVoice1 16KHz 2 : 05  0 : 24 DeepVoice2 16KHz 2 : 96  0 : 38 Tacotron(Grif 24KHz 2 : 57  0 : 28 Tacotron(WaveNet) 24KHz 4 : 17  0 : 18 Table1:MeanOpinionScore(MOS)evaluationswith95%intervalsofDeepVoice1, DeepVoice2,andTacotron.UsingthecrowdMOStoolkit,batchesofsamplesfromthesemodels werepresentedtoratersonMechanicalTurk.Sincebatchescontainedsamplesfromallmodels,the experimentnaturallyinducesacomparisonbetweenthemodels. 4.2.2Spectrogram-to-WaveformModel TheoriginalTacotronimplementationin( Wangetal. , 2017 )usestheGrifn-Limalgorithmtoconvert spectrogramstotime-domainaudiowaveformsbyiterativelyestimatingtheunknownphases. 6 We observethatminornoiseintheinputspectrogramcausesnoticeableestimationerrorsintheGrif Limalgorithmandthegeneratedaudioqualityisdegraded.Toproducehigherqualityaudiousing Tacotron,insteadofusingGrifwetrainaWaveNet-basedneuralvocodertoconvertfrom linearspectrogramstoaudiowaveforms.ThemodelusedisequivalenttotheDeepVoice2vocal model,buttakeslinear-scaledlog-magnitudespectrogramsinsteadofphonemeidentityand F 0 as input.ThecombinedTacotron-WaveNetmodelisshowninFig. 3 .AswewillshowinSection 5.1 , WaveNet-basedneuralvocoderindeedimprovessingle-speakerTacotronaswell. 5Results Inthissection,wewillpresenttheresultsonbothsingle-speakerandmulti-speakerspeechsynthesis usingthedescribedarchitectures.AllmodelhyperparametersarepresentedinAppendix B . 5.1Single-SpeakerSpeechSynthesis WetrainDeepVoice1,DeepVoice2,andTacotrononaninternalEnglishspeechdatabasecontaining approximately20hoursofsingle-speakerdata.TheintermediateevaluationsofmodelsinDeepVoice 1andDeepVoice2canbefoundinTable 3 withinAppendix A .WerunanMOSevaluationusingthe crowdMOSframework( Ribeiroetal. , 2011 )tocomparethequalityofsamples(Table 1 ).Theresults showconclusivelythatthearchitectureimprovementsinDeepVoice2yieldgainsin qualityoverDeepVoice1.TheyalsodemonstratethatconvertingTacotron-generatedspectrograms toaudiousingWaveNetispreferabletousingtheiterativeGrifalgorithm. 5.2Multi-SpeakerSpeechSynthesis WetrainalltheaforementionedmodelsontheVCTKdatasetwith44hoursofspeech,whichcontains 108speakerswithapproximately400utteranceseach.Wealsotrainallmodelsonaninternaldataset ofaudiobooks,whichcontains477speakerswith30minutesofaudioeach(foratotalof ˘ 238 hours).Theconsistentsamplequalityobservedfromourmodelsindicatesthatourarchitecturescan easilylearnhundredsofdistinctvoiceswithavarietyofdifferentaccentsandcadences.Wealso observethatthelearnedembeddingslieinameaningfullatentspace(seeFig. 4 asanexampleand Appendix D formoredetails). Inordertoevaluatethequalityofthesynthesizedaudio,werunMOSevaluationsusingthecrowdMOS framework,andpresenttheresultsinTable 2 .Wepurposefullyincludegroundtruthsamplesinthe setbeingevaluated,becausetheaccentsindatasetsarelikelytobeunfamiliartoourNorthAmerican crowdsourcedratersandwillthusberatedpoorlyduetotheaccentratherthanduetothemodel quality.Byincludinggroundtruthsamples,weareabletocomparetheMOSofthemodelswith thegroundtruthMOSandthusevaluatethemodelqualityratherthanthedataquality;however,the resultingMOSmaybelower,duetotheimplicitcomparisonwiththegroundtruthsamples.Overall, weobservethattheDeepVoice2modelcanapproachanMOSvaluethatisclosetothegroundtruth, whenlowsamplingrateandcompanding/expandingtakenintoaccount. 6 Estimationoftheunknownphasesisdonebyrepeatedlyconvertingbetweenfrequencyandtimedomain representationsofthesignalusingtheshort-timeFouriertransformanditsinverse,substitutingthemagnitudeof eachfrequencycomponenttothepredictedmagnitudeateachstep. 7 Dataset Multi-SpeakerModel Samp.Freq. MOS Acc. VCTK DeepVoice2(20-layerWaveNet) 16KHz 2.87  0.13 99.9% VCTK DeepVoice2(40-layerWaveNet) 16KHz 3.21  0.13 100% VCTK DeepVoice2(60-layerWaveNet) 16KHz 3.42  0.12 99.7% VCTK DeepVoice2(80-layerWaveNet) 16KHz 3.53  0.12 99.9% VCTK Tacotron(Grif 24KHz 1.68  0.12 99.4% VCTK Tacotron(20-layerWaveNet) 24KHz 2.51  0.13 60.9% VCTK GroundTruthData 48KHz 4.65  0.06 99.7% Audiobooks DeepVoice2(80-layerWaveNet) 16KHz 2.97  0.17 97.4% Audiobooks Tacotron(Grif 24KHz 1.73  0.22 93.9% Audiobooks Tacotron(20-layerWaveNet) 24KHz 2.11  0.20 66.5% Audiobooks GroundTruthData 44.1KHz 4.63  0.04 98.8% Table2:MOSandaccuracyforallmulti-speakermodels.ToobtainMOS,weuse crowdMOStoolkitasdetailedinTable 1 .Wealsopresentaccuraciesofthespeaker discriminativemodels(seeAppendix E fordetails)onthesamples,showingthatthesynthesized voicesareasdistinguishableasgroundtruthaudio. (a) (b) Figure4:Principalcomponentsofthelearnedspeakerembeddingsforthe(a)80-layervocalmodel and(b)character-to-spectrogrammodelforVCTKdataset.SeeAppendix D.3 fordetails. Amulti-speakerTTSsystemwithhighsamplequalitybutindistinguishablevoiceswouldresultin highMOS,butfailtomeetthedesiredobjectiveofreproducingtheinputvoicesaccurately.Toshow thatourmodelsnotonlygeneratehighqualitysamples,butalsogenerate distinguishable voices,we alsomeasuretheaccuracyofaspeakerdiscriminativemodelonourgeneratedsamples. Thespeakerdiscriminativeisaconvolutionalnetworktrainedtoclassifyutterancestotheirspeakers, trainedonthesamedatasetastheTTSsystemsthemselves.Ifthevoiceswereindistinguishable (ortheaudioqualitywaslow),theaccuracywouldbemuchlowerforsynthesized samplesthanitisforthegroundtruthsamples.AswedemonstrateinTable 2 ,accuracy demonstratesthatsamplesgeneratedfromourmodelsareasdistinguishableasthegroundtruth samples(seeAppendix E formoredetails).Theaccuracyisonlylowerfor TacotronwithWaveNet,andwesuspectthatgenerationerrorsinthespectrogramareexacerbatedby theWaveNet,asitistrainedwithgroundtruthspectrograms. 6Conclusion Inthiswork,weexplorehowentirely-neuralspeechsynthesispipelinesmaybeextendedtomulti- speakertext-to-speechvialow-dimensionaltrainablespeakerembeddings.Westartbypresenting DeepVoice2,animprovedsingle-speakermodel.Next,wedemonstratetheapplicabilityofour techniquebytrainingbothmulti-speakerDeepVoice2andmulti-speakerTacotronmodels,and evaluatetheirqualitythroughMOS.Inconclusion,weuseourspeakerembeddingtechniquetocreate highqualitytext-to-speechsystemsandconclusivelyshowthatneuralspeechsynthesismodelscan learneffectivelyfromsmallamountsofdataspreadamonghundredsofdifferentspeakers. Theresultspresentedinthisworksuggestmanydirectionsforfutureresearch.Futureworkmaytest thelimitsofthistechniqueandexplorehowmanyspeakersthesemodelscangeneralizeto,howlittle dataistrulyrequiredperspeakerforhighqualitysynthesis,whethernewspeakerscanbeaddedtoa systembymodelparametersandsolelytrainingnewspeakerembeddings,andwhetherthe speakerembeddingscanbeusedasameaningfulvectorspace,asispossiblewithwordembeddings. 8 References O.Abdel-HamidandH.Jiang.FastspeakeradaptationofhybridNN/HMMmodelforspeechrecognitionbased ondiscriminativelearningofspeakercode.In ICASSP ,2013. S.O.Arik,M.Chrzanowski,A.Coates,G.Diamos,A.Gibiansky,Y.Kang,X.Li,J.Miller,J.Raiman, S.Sengupta,andM.Shoeybi.Deepvoice:Real-timeneuraltext-to-speech.In ICML ,2017. J.Bradbury,S.Merity,C.Xiong,andR.Socher.Quasi-recurrentneuralnetworks.In ICLR ,2017. K.Cho,B.VanMerriënboer,C.Gulcehre,D.Bahdanau,F.Bougares,H.Schwenk,andY.Bengio.Learning phraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation. arXiv:1406.1078 ,2014. Y.Fan,Y.Qian,F.K.Soong,andL.He.Multi-speakermodelingandspeakeradaptationforDNN-basedTTS synthesis.In IEEEICASSP ,2015. A.Graves,S.Fernández,F.Gomez,andJ.Schmidhuber.Connectionisttemporallabelling unsegmentedsequencedatawithrecurrentneuralnetworks.In ICML ,2006. C.-C.Hsu,H.-T.Hwang,Y.-C.Wu,Y.Tsao,andH.-M.Wang.Voiceconversionfromunalignedcorporausing variationalautoencodingwassersteingenerativeadversarialnetworks. arXiv:1704.00849 ,2017. S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariate shift. arXivpreprintarXiv:1502.03167 ,2015. D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization. arXiv:1412.6980 ,2014. G.Lample,M.Ballesteros,K.Kawakami,S.Subramanian,andC.Dyer.Neuralarchitecturesfornamedentity recognition.In Proc.NAACL-HLT ,2016. C.Li,X.Ma,B.Jiang,X.Li,X.Zhang,X.Liu,Y.Cao,A.Kannan,andZ.Zhu.Deepspeaker:anend-to-end neuralspeakerembeddingsystem. arXivpreprintarXiv:1705.02304 ,2017. S.Mehri,K.Kumar,I.Gulrajani,R.Kumar,S.Jain,J.Sotelo,A.Courville,andY.Bengio.SampleRNN:An unconditionalend-to-endneuralaudiogenerationmodel. arXiv:1612.07837 ,2016. A.v.d.Oord,S.Dieleman,H.Zen,K.Simonyan,O.Vinyals,A.Graves,N.Kalchbrenner,A.Senior,and K.Kavukcuoglu.Wavenet:Agenerativemodelforrawaudio. arXiv:1609.03499 ,2016. D.A.Reynolds,T.F.Quatieri,andR.B.Dunn.Speakervusingadaptedgaussianmixturemodels. Digitalsignalprocessing ,10(1-3):19Œ41,2000. F.Ribeiro,D.Florêncio,C.Zhang,andM.Seltzer.Crowdmos:Anapproachforcrowdsourcingmeanopinion scorestudies.In IEEEICASSP ,2011. S.Ronanki,O.Watts,S.King,andG.E.Henter.Median-basedgenerationofsyntheticspeechdurationsusinga non-parametricapproach. arXiv:1608.06134 ,2016. T.Salimans,I.Goodfellow,W.Zaremba,V.Cheung,A.Radford,andX.Chen.Improvedtechniquesfortraining gans.In NIPS ,2016. J.Sotelo,S.Mehri,K.Kumar,J.F.Santos,K.Kastner,A.Courville,andY.Bengio.Char2wav:End-to-end speechsynthesis.In ICLR2017workshopsubmission ,2017. Y.Wang,R.Skerry-Ryan,D.Stanton,Y.Wu,R.J.Weiss,N.Jaitly,Z.Yang,Y.Xiao,Z.Chen,S.Bengio,etal. Tacotron:Towardsend-to-endspeechsynthesis.In Interspeech ,2017. Z.Wu,P.Swietojanski,C.Veaux,S.Renals,andS.King.AstudyofspeakeradaptationforDNN-basedspeech synthesis.In Interspeech ,2015. J.Yamagishi,T.Nose,H.Zen,Z.-H.Ling,T.Toda,K.Tokuda,S.King,andS.Renals.Robustspeaker-adaptive hmm-basedtext-to-speechsynthesis. IEEETransactionsonAudio,Speech,andLanguageProcessing ,2009. S.Yang,Z.Wu,andL.Xie.OnthetrainingofDNN-basedaveragevoicemodelforspeechsynthesis.In Signal andInformationProcessingAssociationAnnualSummitandConference(APSIPA),Asia-P ,2016. H.ZenandH.Sak.Unidirectionallongshort-termmemoryrecurrentneuralnetworkwithrecurrentoutputlayer forlow-latencyspeechsynthesis.In IEEEICASSP ,2015. H.Zen,Y.Agiomyrgiannakis,N.Egberts,F.Henderson,andP.Szczepaniak.Fast,compact,andhighquality LSTM-RNNbasedstatisticalparametricspeechsynthesizersformobiledevices. arXiv:1606.06061 ,2016. 9 Appendices ATrainingDeepVoice2 Figure5:SystemdiagramfortrainingprocedureforDeepVoice2. Forconvenience,wedepictthetrainingprocedureforDeepVoice2inFig. 5 ,similartothein Ariketal. ( 2017 ).Thepronunciationdictionarycanbesubstitutedbyagrapheme-to-phonememodel asin Ariketal. ( 2017 ),toadaptforunseenwordsinthepronunciationdictionary. Forfrequencyextractionduringtraining,similarto Ariketal. ( 2017 ),weusethePraatsoftware, whichmaybesubstitutedbyanotherfundamentalfrequencyandvoicednessestimationalgorithm. TheintermediateevaluationmetricsforDeepVoice2modelsarecomparedtotheirDeepVoice1 counterpartsinTable 3 .Weobtainimprovementsintheseevaluationmetricsforallthe models,butwealsonotethattheoverallsynthesisqualitycannotbefullybythesemetrics. Model Evaluationmetric DeepVoice1 DeepVoice2 Segmentation Phonemepairerrorrate 7% 0.1% Duration Meanabsoluteerror 38.0ms 17.5ms Frequency Meanabsoluteerror 29.4Hz 24.5Hz Table3:Comparisonofsegmentation,duration,frequencymodelsforDeepVoice1andDeepVoice 2,bothtrainedwiththesamesingle-speakerdataandevaluatedasdescribedin Ariketal. ( 2017 ). BModelHyperparameters AllhyperparametersofthemodelsusedinthispaperareprovidedinTable 4 . Tospeedupthetrainingofcharacter-to-spectrogrammodelinTacotroninourexperiments,weadded apenaltytermintheformCTCloss(obtainedfromtheattentionhiddenstates)totheoverallloss function.Wedonothaveaclearconclusiveevidencethatitimprovestheoverallaudioqualitybutwe observedfasterconvergenceinsomecases. Learningrateispresentedasatriple ` Œ r Œ s ,whichmeansthattheinitiallearningrateof ` was decayedbyafactorof r every s iterations.AllmodelsusetheAdamoptimizationtechnique( Kingma andBa , 2014 )with  1 =0 : 9 ,  2 =0 : 99 ,and " =10  8 . Convolutionallayersarepresentedas l  ;o;h  w whichmeansthatthereare l convolutional layers,and o (outputchannels)ineachlayer.Thesizeis h  w ,whereheight h isin frequencybinsandwidth w isintimeframes. AnydetailsorhyperparametersforDeepVoice2areidenticaltothosefromthebest modelsusedintheoriginalimplementationofDeepVoice1( Ariketal. , 2017 ).Similarly,any detailsorhyperparametersforourTacotronimplementationareidenticaltothosefrom thebestmodelsusedintheoriginalimplementationofTacotron( Wangetal. , 2017 ). 10 Single-Speaker VCTK Audiobooks Segmentation NumberofMFCCs 40 40 40 Convolutionallayers 4  ,128 9  5  4  ,64 9  5  5  ,128 9  5  Recurrentlayers(Bi-GRU) 4  ,512-wide 4  ,1024-wide 4  ,1024-wide Dropoutkeepprobability 0.95 0.85 0.85 Learningrate 10  3 Œ0.95Œ400 2  10  4 Œ0.95Œ1000 2  10  4 Œ0.95Œ2000 Silencethreshold 0.1 0.1 0.1 Gaussianwidth 0.25ms 0.25ms 0.25ms Batchsize 8 8 8 Speakerembeddingsize N/A 16 32 Duration Fully-connected 2  ,256units 4  ,256units 4  ,256units Recurrentlayers(Bi-GRU) 4  ,256-wide 4  ,512-wide 4  ,512-wide Dropoutkeepprobability 0.8 0.85 0.85 Outputbuckets 100 250 300 Learningrate 3  10  4 Œ 0 : 9 Œ300 6  10  4 Œ 0 : 9 Œ400 3  10  4 Œ 0 : 9 Œ800 Batchsize 128 32 32 Speakerembeddingsize N/A 16 32 Frequency Hiddenlayers(Bi-GRU) 3  ,256-wide 3  ,512-wide 3  ,512-wide Outputdimension 32 32 64 Convolutionwidths 5,10,20 3,6,15,30 691835 Learningrate 10  3 Œ 0 : 9 Œ300 4  10  4 Œ 0 : 9 Œ300 4  10  4 Œ 0 : 9 Œ300 Batchsize 32 32 32 Speakerembeddingsize N/A 16 16 Vocal Layers 60 20/40/60/80 80 Learningrate 10  3 Œ 0 : 9886 Œ1000 10  3 Œ 0 : 9886 Œ1000 10  3 Œ 0 : 9886 Œ1000 Batchsize 8 8 8 Speakerembeddingsize N/A 16 16 Character-to-spectrogram Enc.-CBHGbanksize 16 16 16 Enc.-CBHGchannels 128 128 128 Enc.-CBHGrecurrentsize 128 128 128 Enc.-CBHGhighwaylayers 4 4 4 Enc.-CBHGmaxpoolwidth 2 2 2 Enc.-CBHGproj.sizes 128,128 128,128 128,128 Enc.-CBHGproj.width 3 3 3 Decoderlayers 3 3 3 Dropoutkeepprobability 0.5 0.8 0.8 Attentionsize 128 256 512 Attentionstatesize 256 256 256 Decoderprenetsizes 256,128 256,128 256,128,64 Post-CBHGbanksize 8 8 8 Post-CBHGchannels 128 512 512 Post-CBHGconv.widths 3 3 3 Post-CBHGrecurrentsize 128 256 256 Post-CBHGhighwaylayers 4 4 4 Post-CBHGmaxpoolwidth 2 2 2 Reductionfactor 4 4 4 CTClosscoef 0.01 0.01 0.01 Learningrate 10  3 Œ1ŒN/A 10  3 Œ 0 : 95 Œ3000 10  3 Œ 0 : 95 Œ3000 Batchsize 16 16 16 Speakerembeddingsize N/A 32 32 Table4:Modelhyperparametersforallmodelspresentedinthispaper. 11 CTrainingTimeforMulti-speakerModels Wepresentthedetailsoftrainingtimeformulti-speakermodelsinTable 5 .Weusethesameoptimized WaveNetinferencekernelsdescribedinDeepVoice1.Fordetailedanalysisandcorresponding techniquestooptimizetheinferencetime,wereferthereadersto( Ariketal. , 2017 ). Model Hardware Timeperiteration Numberofiterations Totaltime Segmentation 1TitanXGPU 1200ms 90k 30hours Duration 1TeslaK40GPU 320ms 60k 4.5hours Frequency 1TeslaK40GPU 1100ms 25k 7.5hours Vocal(20layer) 8TitanXGPUs 280ms 350k 27hours Vocal(40layer) 8TitanXGPUs 450ms 350k 44hours Vocal(60layer) 8TitanXGPUs 600ms 500k 83hours Vocal(80layer) 8TitanXGPUs 780ms 500k 108hours Character-to-spectrogram 1TitanXGPU 450ms 200k 25hours Table5:Trainingtimedetailsofmulti-speakerDeepVoice2andTacotronmodelsonVCTKdataset. DInterpretationofLearnedEmbeddings Inthissection,weexploretheconsequencesofspeaker-dependentmodelsonintermediatemodel outputs,modelactivations,andthedistributionsofthelearnedembeddings. D.1Speaker-DependentFundamentalFrequencyPr Figure6:Time-dependentfundamentalfrequency(collectivelyincludingthe F 0 andvoiced- ness)generatedbythemodelwhentheinputsandembeddingscorrespondtoSpeaker S 6 or S 24 . Speaker S 6 isa23-year-oldfemalewithaSouthernEnglandaccentandSpeaker S 24 isa24-year-old malewithanIndianaccent.Thepronouncedsentenceis"Sixspoonsoffreshsnowpeasvethick slabsofbluecheeseandmaybeasnackforherbrotherBob".Spikesinthegroundtrutharecausedby theestimationerrorsusingthePraatsoftware( Ariketal. , 2017 ),andweobservethatourfrequency modelcancompensatetheminsomecases,learningfromtheothercorrectgroundtruthsamples. 12 Todemonstratetheofthespeakerembeddings,weruninferenceforthefrequencymodel withthespeakerembeddingvectorscorrespondingtotheactualspeakerandadifferentspeaker.As showninFig. 6 ,whiletheinputphonemefeaturesaredominantindeterminingtheoverallshapeof thefundamentalfrequencytheactualvaluesarehighlyspeakerdependent.Forexample, whenthespeakerembeddingvectorofamaleissubstitutedwiththespeakerembeddingvectorofa female,theoutputfrequencylewouldcausegenerationofafemalevoicedespitealltheother inputfeaturescorrespondtothecorrectmalespeaker. D.2Speaker-DependentAttentionPlots Figure7:Attentionplotsforthreespeakersspeakingatdifferentspeeds.Speaker S 25 isa22year-old malewithaScottishaccent,Speaker S 4 isa23-year-oldfemalewithaSouthernEnglandaccent,and Speaker S 16 isa22-year-oldmalewithaLondonaccent.Thepronouncedsentenceis"Sixspoonsof freshsnowpeasvethickslabsofbluecheeseandmaybeasnackforherbrotherBob". Fig. 7 showsthelearnedattentionplotsforthreedifferentspeakerswhotalkatdifferentspeeds.It demonstratesthattheintheTacotronencoderarchitecturearehighlyeffectivemaking theattentionmodelspeakerdependentsuchthatdifferentportionsoftheinputtextcanbefocused dependingonthespeechfeaturesofthespeaker. D.3PrincipalComponentsoftheEmbeddings Figure8:Principalcomponentsofthelearnedembeddingsofthe80-layervocalmodel,shownwith thegendersandregionsofthespeakers. Weexplorethelatentspaceofthelearnedspeakerembeddingsbyvisualizingtheminalowerdimen- sionalspace.Fig. 8 andFig. 9 showthetwoprincipalcomponentsofthelearnedembeddings ofthevocalmodelandcharacter-to-spectrogrammodelrespectively.Althoughtheyareinitialized randomlyandcompletelytrainedbasedonalossfunctionrelatedtothegenerativequality,wecan observediscriminativepatternsinthelearnedembeddings.Genderofthespeakeristhemostapparent discriminativepatternintheselearnedembeddingsthatevenalinearontheshowntwo- dimensionalspacecanclassifythegenderwithaveryhighaccuracy.Besides,weobserveapparent discriminativepatternsfortheregionofthespeaker. 7 Inthetwo-dimensionalspace,especiallyGreat BritainandNorthAmericaregionsseemhighlyseparable. 7 Theregionsaredeterminedaccordingto https://en.wikipedia.org/wiki/Regional_accents_of_ English 13 Figure9:Principalcomponentsofthelearnedembeddingsofthecharacter-to-spectrogrammodel, shownwiththegendersandregionsofthespeakers. Figure10:Architectureforspeakerdiscriminator. ESpeakerDiscriminativeModel Tocomputemulti-speakeraccuracy,weuseaspeakerdiscriminativemodeltrainedon thegroundtruthdatasetofmultiplespeakers.Althoughusinganotherdiscriminatormodelsuchas DeepSpeaker( Lietal. , 2017 )orothermethodswouldalsosufwechoosetocreateourowndeep learningbaseddiscriminativemodel.Wenotethatouraccuracyresultsonthetestsetareonparwith thestate-of-the-artspeakermethodsintheliterature.OurarchitectureisdepictedinFig. 10 .Weusemel-frequencycepstralcoefcients(MFCCs)computedafterresamplingtheinputtoa constantsamplingfrequency.Then,weemploytwo-dimensionalconvolutionallayersconvolving overbothtimeandcepstralfrequencybands,witharelunonlinearityclippedtoamaximumofsix aftereachconvolutionallayer.Thelastconvolutionallayerisfollowedbymax-poolinglayer.We thenmean-poolovertimeforallutterancetimestepsandapplyafullyconnectedlayerwitharelu nonlinearityfollowedbyafullyconnectedoutputlayerwithasoftmaxnonlinearityandcross-entropy loss.Inordertoavoidovtothedataset,weapplydropoutaftereveryrelu nonlinearity. Inordertodemonstratethattheresultsarenotsensitivetothechoiceofthehyperpa- rametersofthediscriminativemodel,wedemonstratetheaccuracyforotherchoices inthissection.HyperparametersforallthediscriminatormodelsareavailableinTable 7 .Onlythe resultsforthemodels, D 3 and D 8 ,arepresentedinTable 2 ,astheyyieldedthehighestvalidationset accuracy. 14 Param. D1 D2 D3 (inTable 2 ) D4 Audioresamplingfreq. 16KHz 16KHz 16KHz 16KHz NumberofMFCCs 20 20 80 80 Hoplength 160 160 160 160 Convolutionlayers 5  ,32 2  10  5  ,32 9  5 5  ,32 2  20 5  ,32 9  5 Maxpoolwidth&stride 2 2 2 2 Fullyconnectedsize 16 16 32 32 Dropoutprobability 0.75 0.75 0.75 0.75 Learningrate 10  3 Œ0.95Œ1000 10  3 Œ0.95Œ1000 10  3 Œ0.95Œ1000 10  3 Œ0.95Œ1000 Table6:HyperparametersforspeakerdiscriminativemodelsforVCTKdataset. Param. D5 D6 D7 D8 (inTable 2 ) Audioresamplingfreq. 16KHz 16KHz 16KHz 16KHz NumberofMFCCs 20 20 80 80 Hoplength 160 160 160 160 Convolutionlayers 3  ,32 9  5  5  32 2  10 7  ,32 9  5 5  ,32 2  10 Maxpoolwidth&stride 2 2 2 2 Fullyconnectedsize 32 32 32 32 Dropoutprobability 0.75 0.75 0.75 0.75 Learningrate 10  3 Œ0.95Œ1000 10  3 Œ0.99Œ2000 10  3 Œ0.95Œ1000 10  3 Œ0.99Œ2000 Table7:HyperparametersforspeakerdiscriminativemodelsforAudiobookdataset. Dataset Multi-SpeakerModel D1 D2 D3 (inTable 2 ) D4 VCTK DeepVoice2(20-layerWaveNet) 97.87% 97.60% 99.92% 99.84% VCTK DeepVoice2(40-layerWaveNet) 98.56% 98.68% 100.00% 100.00% VCTK DeepVoice2(60-layerWaveNet) 98.56% 98.44% 99.68% 99.80% VCTK DeepVoice2(80-layerWaveNet) 99.06% 99.21% 99.96% 99.96% VCTK Tacotron(Grif 95.89% 96.24% 99.37% 99.60% VCTK Tacotron(20-layerWaveNet) 87.93% 85.19% 60.87% 65.43% VCTK GroundTruthData 98.07% 98.00% 99.74% 99.66% Table8:accuracyusingspeakerdiscriminativemodelsD1,D2,D3andD4.The correspondingmodelhyperparametersaregiveninTable 6 . Dataset Multi-SpeakerModel D5 D6 D7 D8 (inTable 2 ) Audiobooks DeepVoice2(80-layerWaveNet) 98.24% 95.66% 96.80% 97.42% Audiobooks Tacotron(Grif 96.93% 94.89% 92.24% 93.87% Audiobooks Tacotron(20-layerWaveNet) 83.36% 80.81% 60.00% 66.53% Audiobooks GroundTruthData 96.30% 97.49% 98.39% 98.80% Table9:accuracyusingspeakerdiscriminativemodelsD5,D6,D7andD8.The correspondingmodelhyperparametersaregiveninTable 7 . 15  
SparseAdditiveTextModelswithLowRank Background LeiShi Baidu.com,Inc. P.R.China shilei06@baidu.om Abstract Thesparseadditivemodelfortextmodelinginvolvesthesum -of-expcomputing, whosecostisconsumingforlargescales.Moreover,theassu mptionofequalback- groundacrossallclasses/topicsmaybetoostrong.Thispap erextendstopropose sparseadditivemodelwithlowrankbackground(SAM-LRB)an dobtainssim- pleyetefcientestimation.Particularly,employingadou blemajorizationbound, weapproximatelog-likelihoodintoaquadraticlower-boun dwithoutthelog-sum- expterms.Theconstraintsoflowrankandsparsityarethens implyembodiedby nuclearnormand ` 1 -normregularizers.Interestingly,wendthattheoptimiz a- tiontaskofSAM-LRBcanbetransformedintothesameformasi nRobustPCA. Consequently,parametersofsupervisedSAM-LRBcanbeefc ientlylearnedus- inganexistingalgorithmforRobustPCAbasedonaccelerate dproximalgradient. Besidesthesupervisedcase,weextendSAM-LRBtofavorunsu pervisedandmul- tifacetedscenarios.Experimentsonthreerealdatademons tratetheeffectiveness andefciencyofSAM-LRB,comparedwithafewstate-of-the- artmodels. 1Introduction  Generativemodelsoftexthavegainedlargepopularityinan alyzingalargecollectionofdocuments [3,4,17].ThistypeofmodelsoverwhelminglyrelyontheDir ichlet-Multinomialconjugatepair, perhapsmainlybecauseitsformulationandestimationisst raightforwardandefcient.However, theeaseofparameterestimationmaycomeatacost:unnecess arilyover-complicatedlatentstruc- turesandlackofrobustnesstolimitedtrainingdata.Sever aleffortsemergedtoseekalternative formulations,takingthecorrelatedtopicmodels[13,19]f orinstance. Recentlyin[10],theauthorslistedthreemainproblemswit hDirichlet-Multinomialgenerativemod- els,namelyinferencecost,overparameterization,andlac kofsparsity.Motivatedbythem,aSparse AdditiveGEnerativemodel(SAGE)wasproposedin[10]asana lternativechoiceofgenerativemod- el.Itscoreideaisthatthelexicaldistributioninlog-spa cecomesbyaddingthebackgrounddistribu- tionwithsparsedeviationvectors.Successfullyapplying SAGE,effort[14]discoversgeographical topicsinthetwitterstream,andpaper[25]detectscommuni tiesincomputationallinguistics. However,SAGEstillsuffersfromtwoproblems.First,theli kelihoodandestimationinvolvethe sum-of-exponentialcomputingduetothesoft-maxgenerati venature,anditwouldbetimeconsum- ingforlargescales.Second,SAGEassumesonesinglebackgr oundvectoracrossallclasses/topics, orequivalently,thereisonebackgroundvectorforeachcla ss/topicbutallbackgroundvectorsare constrainedtobeequal.Thisassumptionmightbetoostrong insomeapplications,e.g.,whenlots ofsynonymsvarytheirdistributionsacrossdifferentclas ses/topics. Motivatedtosolvethesecondproblem,weareproposetousea lowrankconstrainedbackground. However,directlyassigningthelowrankassumptiontothel og-spaceisdifcult.Weturntoap- proximatethedatalog-likelihoodofsparseadditivemodel byaquadraticlower-boundbasedonthe 1 doublemajorizationboundin[6],sothatthecostlylog-sum -exponentialcomputation,i.e.,therst problemofSAGE,isavoided.Wethenformulateandderivelea rningalgorithmtotheproposed SAM-LRBmodel.Maincontributionsofthispapercanbesumma rizedintofour-foldasbelow:  Proposetouselowrankbackgroundtoextendtheequallycons trainedsettinginSAGE.  Approximatethedatalog-likelihoodofsparseadditivemod elbyaquadraticlower-bound basedonthedoublemajorizationboundin[6],sothatthecos tlylog-sum-exponentialcom- putationisavoided.  FormulatetheconstrainedoptimizationproblemintoLagra ngianrelaxations,leadingtoa formexactlythesameasinRobustPCA[28].Consequently,SA M-LRBcanbeefciently learnedbyemployingtheacceleratedproximalgradientalg orithmforRobustPCA[20].  ExtendSAM-LRBtofavorsupervisedclassication,unsuper visedtopicmodelandmulti- facetedmodel;conductexperimentalcomparisonsonrealda tatovalidateSAM-LRB. 2SupervisedSparseAdditiveModelwithLowRankBackground  2.1SupervisedSparseAdditiveModel  SameasinSAGE[10],thecoreideaofourmodelisthatthelexi caldistributioninlog-spacecomes fromaddingthebackgrounddistributionwithadditionalve ctors.Particularly,wearegivendoc- uments D documentsover M words.Foreachdocument d 2 [1 ;D ] ,let y d 2 [1 ;K ] represent theclasslabelinthecurrentsupervisedscenario, c d 2 R M  + denotethevectoroftermcounts, and C d = P wc dw bethetotaltermcount.Weassumeeachclass k 2 [1 ;K ] hastwovectors b k ; s k 2 R M ,denotingthebackgroundandadditivedistributionsinlog -space,respectively.Then thegenerativedistributionforeachword w inadocument d withlabel y d isasoft-maxform: p ( w j y d )= p ( w j y d ; b y d; s y d)= exp( b y dw+ s y dw) P M  i=1 exp( b y di+ s y di) : (1) Given  = f B ; S g with B =[ b 1 ;:::; b K ] and S =[ s 1 ;:::; s K ] ,thelog-likelihoodofdata X is: L =log p ( Xj  )= K X k =1 X d : y d= k L ( d;k ) ; L ( d;k )= c >  d ( b k + s k )  C d log M X i=1 exp( b ki + s ki ) : (2) Similarly,atestingdocument d isclassiedintoclass ^ y ( d ) accordingto ^ y ( d )=argmax k L ( d;k ) . InSAGE[10],theauthorsfurtherassumedthatthebackgroun dvectorsacrossallclassesarethe same,i.e., b k = b for 8 k ,andeachadditivevector s k issparse.Althoughintuitive,thebackground equalityassumptionmaybetoostrongforrealapplications .Forinstance,toexpressasame/similar meaning,differentclassesofdocumentsmaychoosetousedi fferenttermsfromatupleofsynonyms. Inthiscase,SAGEwouldtendtoincludethesetermsasthespa rseadditivepart,insteadofasthe background.TakingFig.1asanillustrativeexample,thelo g-spacedistribution(left)isthesum ofthelow-rankbackground B (middle)andthesparse S (right).ApplyingSAGEtothistypeof data,theequalityconstrainedbackground B wouldfailtocapturethelow-rankstructure,and/orthe additivepart S wouldbenotsparse,sothattheremayberisksofover-tting orunder-tting. Moreover,sincethereexistssum-of-exponentialtermsinE q.(2)andthusalsoinitsderivatives,the computingcostbecomeshugewhenthevocabularysize M islarge.Asaresult,althoughperforming wellin[10,14,25],SAGEmightstillsufferfromproblemsof over-constrainandinefciency. Figure1:Lowrankbackground.  Lefttorightillustratesthelog-  spacedistr.,background B ,and sparse S ,resp.Rowsindex terms,andcolumnsforclasses. Figure2:Lower-bound'soptimization.Lefttoright  showsthetrajectoryoflower-bound,  ,and ˘ ,resp. 2 2.2SupervisedSparseAdditiveModelwithLowRankBackgroun d Motivatedtoavoidtheinefcientcomputingduetosum-of-e xp,weadoptthedoublemajorization lower-boundof L [6],sothatitiswellapproximatedandquadraticw.r.t. B and S .Furtherbasedon thislower-bound,weproceedtoassumethebackground B acrossclassesislow-rank,incontrastto theequalityconstraintinSAGE.Anoptimizationalgorithm isproposedbasedonproximalgradient. 2.2.1DoubleMajorizationQuadraticLowerBound  Intheliterature,therehavebeenseveralexistingefforts onefcientcomputingthesum-of-expter- minvolvedinsoft-max[5,15,6].Forinstance,basedonthec onvexityoflogarithm,onecan obtainabound  log P iexp( x i)  ˚ P iexp( x i)+log ˚ +1 forany ˚ 2 R + ,namelythe lb-log-cvx bound.Moreover,viaupper-boundingtheHessianmatrix,on ecanobtainthefol- lowinglocalquadraticapproximationforany 8 ˘ i2 R ,shortlynamedas lb-quad-loc :  log M X i=1 exp( x i)  1 M ( X ix i X i˘ i) 2  X i( x i ˘ i) 2  P i( x i ˘ i)exp( ˘ i) P iexp( ˘ i)  log X iexp( ˘ i) : In[6],Bouchardproposedthefollowingquadraticlower-bo undbydoublemajorization ( lb-quad-dm )anddemonstrateditsbetterapproximationcomparedwitht heprevioustwo:  log M X i=1 exp( x i)    1 2 M X i=1  x i   ˘ i+ f ( ˘ i)[( x i  ) 2  ˘ 2 i]+2log[exp( ˘ i)+1]  ; (3) with  2 R and ˘ 2 R M  + beingauxiliary(variational)variables,and f ( ˘ )= 1 2 ˘  exp( ˘ )  1 exp( ˘ )+1 .This boundiscloselyrelatedtotheboundproposedbyJaakkolaan dJordan[6]. EmployingEq.(3),weobtainalower-bound L lb L tothedatalog-likelihoodinEq.(2): L lb = K X k =1   ( b k + s k ) > Ak ( b k + s k )   > k ( b k + s k )   k  ; with  k = ~ C k (  k  1 2 M X i=1   k + ˘ ki + f ( ˘ ki )(  2 k  ˘ 2 ki )+2log(exp( ˘ ki )+1)  ) ; Ak = ~ C k diag[ f ( ˘ k )] ;  k = ~ C k ( 1 2   k f ( ˘ k ))  X d : y d= k c d ; ~ C k = X d : y d= k C d : (4) Foreachclass k ,thetwovariationalvariables,  k 2 R and ˘ k 2 R M  + ,canbeupdatediterativelyas belowforabetterapproximatedlower-bound.Therein, abs(  ) denotestheabsolutevalueoperator.  k = 1 P M  i=1 f ( ˘ ki ) " M 2  1+ M X i=1 ( b ki + s ki ) f ( ˘ ki ) # ; ˘ k =abs( b k + s k   k ) : (5) Oneexampleofthetrajectoriesduringoptimizingthislowe r-boundisillustratedinFig.2.Partic- ularly,theleftshowsthelower-boundconvergesquicklyto groundtruth,usuallywithin5rounds inourexperiences.Thevaluesofthethreelower-boundswit hrandomlysampledthevariation- alvariablesarealsosortedandplotted.Onecanndthat lb-quad-dm approximatesbetteror comparablywellevenwitharandominitialization.Pleases ee[6]formorecomparisons. 2.2.2SupervisedSAM-LRBModelandOptimizationbyProximalG radient Ratherthanoptimizingthedatalog-likelihoodinEq.(2)li keinSAGE,weturntooptimizeits lower-boundinEq.(4),whichisconvenientforfurtherassi gningthelow-rankconstrainton B and thesparsityconstrainton S .Concretely,ourtargetisformulatedasaconstrainedopti mizationtask: max B ; S L lb ; with L lb speciedinEq.(4), s : t : B =[ b 1 ;:::; b K ] islowrank ; S =[ s 1 ;:::; s K ] issparse : (6) Concerningthetwoconstraints,wecalltheaboveassupervi sed S parse A dditive Modelwith L ow- R ank B ackground,orsupervisedSAM-LRBforshort.Althoughbotho fthetwoassumptionscan 3 betackledviaformulatingafullygenerativemodel,assign ingappropriatepriors,anddelivering inferenceinaBayesianmannersimilarto[8],wedeterminet ochoosetheconstrainedoptimization formfornotonlyaclearerexpressionbutalsoasimplerande fcientalgorithm. Intheliterature,therehavebeenseveraleffortsconsider ingbothlowrankandsparseconstraints similartoEq.(6),mostofwhichtaketheuseofproximalgrad ient[2,7].Papers[20,28]studiedthe problemsunderthenameofRobustPrincipalComponentAnaly sis(RPCA),aimingtodecouplean observedmatrixasthesumofalowrankmatrixandasparsemat rix.CloselyrelatedtoRPCA, ourscenarioinEq.(6)canberegardedasaweightedRPCAform ulation,andtheweightsare controlledbyvariationalvariables.In[24],theauthorsp roposedanefcientalgorithmforproblems thatconstrainamatrixtobebothlowrankandsparsesimulta neously. Followingtheseexistingworks,weadoptthenuclearnormto implementthelowrankconstraint,and ` 1 -normforthesparsityconstraint,respectively.Lettingt hepartialderivativew.r.t.  k =( b k + s k ) of L lb equaltozero,themaximumof L lb canbeachievedat    k =  1 2 A 1 k  k .Since Ak is positivedeniteanddiagonal,theoptimalsolution    k iswell-posedandcanbeefcientlycomputed. Simultaneouslyconsideringtheequality  k =( b k + s k ) ,thelowrankon B andthesparsityon S , onecanrewrittenEq.(6)intothefollowingLagrangianform : min B ; S 1 2 jj    B  S jj 2  F+  ( jj B jj  +  j S j 1 ) ; with   =[    1 ;:::;    K ] ; (7) where jjjj F, jjjj  and jj 1 denotetheFrobeniusnorm,nuclearnormand ` 1 -norm,respectively. TheFrobeniusnormtermconcernstheaccuracyofdecoupling from   into B and S .Lagrange multipliers  and  controlthestrengthsoflowrankconstraintandsparsityco nstraint,respectively. Interestingly,Eq.(7)isexactlythesameastheobjectiveo fRPCA[20,28].Paper[20]proposedan algorithmforRPCAbasedon accelerated proximalgradient(APG-RPCA),showingitsadvantages ofefciencyandstabilityover(plain)proximalgradient. Wechooseit,i.e.,Algorithm2in[20],for seekingsolutionstoEq.(7).ThecomputationsinvolvedinA PG-RPCAincludeSVDdecomposition andabsolutevaluethresholding,andinterestedreadersar ereferredto[20]formoredetails.The augmentedLagrangianandalternatingdirectionmethods[9 ,29]couldbeconsideredasalternatives. Data :Termcountsandlabels f c d ;C d ;y d g D  d =1 of D docsand K classes,sparsethres.  ˇ 0 : 05 Result :Log-spacedistributions:low-rank B andsparse S Initialization :randomlyinitializeparameters f B ; S g ,andvariationalvariables f  k ; ˘ k g k ; while notconverge do if optimizevariationalvariables then iterativelyupdate f  k ; ˘ k g k accordingtoEq.(5); for k =1 ;:::;K do calculate Ak and  k byEq.(4),and    k =  1 2 A 1 k  k ; B ; S   APG-RPCA (   ; ) byAlgorithm2in[20],with   =[    1 ;:::;    K ] ; end Algorithm1: SupervisedSAM-LRBlearningalgorithm Consequently,thesupervisedSAM-LRBalgorithmisspecie dinAlgorithm1.Therein,onecan choosetoeitherxorupdatethevariationalvariables f  k ; ˘ k g k .Iftheyarexed,Algorithm1 hasonlyoneouteriterationwithnoneedtochecktheconverg ence.Comparedwiththesupervised SAGElearningalgorithminSec.3of[10],oursupervisedSAM -LRBalgorithmnotonlydoesnot needtocomputethesumofexponentialssothatcomputingcos tissaved,butalsoisoptimizedsim- plyandefcientlybyproximalgradientinsteadofusingNew tonupdatingasinSAGE.Moreover, addingLaplacian-Exponentialprioron S forsparseness,SAGEupdatestheconjugateposteriorsand needstoemployaﬁwarmstartﬂtechniquetoavoidbeingtrapp edinearlystageswithinappropriate initializations,whileincontrastSAM-LRBdoesnothaveth isrisk.Additionally,sincetheevolution fromSAGEtoSAM-LRBistwofolded,i.e.,thelowrankbackgro undassumptionandtheconvex relaxation,wendthatadoptingtheconvexrelaxationalso helpsSAGEduringoptimization. 3Extensions  Analogousto[10],ourSAM-LRBformulationcanbealsoexten dedtounsupervisedtopicmodeling scenariowithlatentvariables,andthescenariowithmulti facetedclasslabels. 4 3.1Extension1:UnsupervisedLatentVariableModel  WeconsiderhowtoincorporateSAM-LRBinalatentvariablem odelofunsupervisedtextmod- elling.Followingtopicmodels,thereisonelatentvectoro ftopicproportionsperdocumentand onelatentdiscretevariableperterm.Thatis,eachdocumen t d isendowedwithavectoroftopic proportions  d ˘ Dirichlet( ˆ ) ,andeachterm w inthisdocumentisassociatedwithalatenttopic label z ( d ) w˘ Multinomial(  d ) .Thentheprobabilitydistributionfor w is p ( w j z ( d ) w; B ; S ) / exp  b z ( d) w w+ s z ( d) w w ; (8) whichonlyreplacestheknownclasslabel y d inEq.(1)withtheunknowntopiclabel z ( d ) w. Wecancombinethemeaneldvariationalinferenceforlaten tDirichletallocation(LDA)[4]with thelower-boundtreatmentinEq.(4),leadingtothefollowi ngunsupervisedlower-bound L lb = K X k =1   ( b k + s k ) > Ak ( b k + s k )   > k ( b k + s k )   k  + X d [ h log p (  d j ˆ ) ih log Q (  d ) i ]+ X d X wh h log p ( z ( d ) wj  d ) ih log Q ( z ( d ) w) i i ; with  k = ~ C k (  k  1 2 M X i=1   k + ˘ ki + f ( ˘ ki )(  2 k  ˘ 2 ki )+2log(exp( ˘ ki )+1)  ) ; Ak = ~ C k diag[ f ( ˘ k )] ;  k = ~ C k ( 1 2   k f ( ˘ k ))  ~ c k ; (9) whereeach w -thitemin ~ c k is ~ c kw = P d Q ( k j d;w ) c dw ,i.e.theexpectedcountofterm w intopic k ,and ~ C k = P w~ c kw isthetopic'sexpectedtotalcountthroughoutallwords. ThisunsupervisedSAM-LRBmodelformulatesatopicmodelwi thlowrankbackgroundandsparse deviation,whichislearnedviaEMiterations.TheE-stepto updateposteriors Q (  d ) and Q ( z ( d ) w) is identicaltothestandardLDA.Once f Ak ;  k g arecomputedasabove,theM-steptoupdate f B ; S g andvariationalvariables f  k ; ˘ k g k remainsthesameasthesupervisedcaseinAlgorithm1. 3.2Extension2:MultifacetedModelling  WeconsiderhowSAM-LRBcanbeusedtocombinemultiplefacet s(multi-dimensionalclasslabel- s),i.e,combiningper-wordlatenttopicsanddocumentlabe lsandpursuingastructuralviewoflabels andtopics.Intheliterature,multifacetedgenerativemod elshavebeenstudiedin[1,21,23],andthey incorporatedlatentswitchingvariablesthatdeterminewh ethereachtermisgeneratedfromatopic orfromadocumentlabel.Topic-labelinteractionscanalso beincludedtocapturethedistributions ofwordsattheintersections.Howeverinthiskindofmodels ,thenumberofparametersbecomes verylargeforlargevocabularysize,manytopics,manylabe ls.In[10],SAGEneedsnoswitching variablesandshowsadvantageousofmodelsparsityonmulti facetedmodeling.Morerecently,paper [14]employsSAGEanddiscoversmeaningfulgeographicalto picsinthetwitterstreams. ApplyingSAM-LRBtothemultifacetedscenario,westillass umethemultifacetedvariationsare composedoflowrankbackgroundandsparsedeviation.Parti cularly,foreachtopic k 2 [1 ;K ] , wehavethetopicbackground b ( T ) k andsparsedeviation s ( T ) k ;foreachlabel j 2 [1 ;J ] ,wehave labelbackground b ( L ) j andsparsedeviation s ( L ) j ;foreachtopic-labelinteractionpair ( k;j ) ,we haveonlythesparsedeviation s ( I ) kj .Again,backgrounddistributions B ( T ) =[ b ( T ) 1 ;:::; b ( T ) K ] and B ( L ) =[ b ( L ) 1 ;:::; b ( L ) J ] areassumedoflowrankstocapturesingleview'sdistributi onsimilarity. Thenforasingleterm w giventhelatenttopic z ( d ) wandtheclasslabel y d ,itsgenerativeprobability isobtainedbysummingthebackgroundandsparsecomponents together: p ( w j z ( d ) w;y d ;  ) / exp  b ( T ) z ( d) w w+ s ( T ) z ( d) w w+ b ( L ) y dw+ s ( L ) y dw+ s ( I ) z ( d) w y dw ; (10) 5 withparameters  = f B ( T ) ; S ( T ) ; B ( L ) ; S ( L ) ; S ( I ) g .Thelog-likelihood'slower-boundinvolves thesumthroughalltopic-labelpairs: L lb = K X k =1 J X j =1    >  kj Akj  kj   > kj  kj   kj  + X d [ h log p (  d j ˆ ) ih log Q (  d ) i ]+ X d X wh h log p ( z ( d ) wj  d ) ih log Q ( z ( d ) w) i i ; with  kj , b ( T ) k + s ( T ) k + b ( L ) j + s ( L ) j + s ( I ) kj : (11) Inthequadraticform,thevaluesof Akj ,  kj and  kj aretrivialcombinationofEq.(4)andEq.(9), i.e.,weightedbyboththeobservedlabelsandposteriorsof latenttopics.Detailsareomittedhere duetospacelimit.ThesecondrowremainsthesameasinEq.(9 )andstandardLDA. Duringtheiterativeestimation,everyiterationincludes thefollowingsteps:  Estimatetheposteriors Q ( z ( d ) w) and Q (  d ) ;  With ( B ( T ) ; S ( T ) ; S ( I ) ) xed,solveaquadraticprogramover   ( L ) ,whichapproximates thesumof B ( L ) and S ( L ) .Put   ( L ) intoAlgorithm1toupdate B ( L ) and S ( L ) ;  With ( B ( L ) ; S ( L ) ; S ( I ) ) xed,solveaquadraticprogramover   ( T ) ,whichapproximates thesumof B ( T ) and S ( T ) .Put   ( T ) intoAlgorithm1toupdate B ( T ) and S ( T ) ;  With ( B ( T ) ; S ( T ) ; B ( L ) ; S ( L ) ) xed,update S ( I ) byproximalgradient. 4ExperimentalResults  InordertotestSAM-LRBindifferentscenarios,thissectio nconsidersexperimentsunderthree tasks,namelysuperviseddocumentclassication,unsuper visedtopicmodeling,andmulti-faceted modelingandclassication,respectively.  4.1DocumentClassication  WersttestourSAM-LRBmodelinthesuperviseddocumentmod elingscenarioandevaluate theclassicationaccuracy.Particularly,thesupervised SAM-LRBiscomparedwiththeDirichlet- MultinomialmodelandSAGE.TheprecisionoftheDirichletp riorinDirichlet-Multinomialmodel isupdatedbytheNewtonoptimization[22].NonparametricJ effreysprior[12]isadoptedinSAGE asaparameter-freesparseprior.Concerningthevariation alvariables f  i; ˘ ig iinthequadratic lower-boundofSAM-LRB,bothcasesofxingthemandupdatin gthemareconsidered. Weconsiderthebenchmark 20Newsgroups data 1 ,andaimtoclassifyunlabellednewsgrouppost- ingsinto20newsgroups.Nostopwordlteringisperformed, andwerandomlypickavocabulary of55,000terms.Inordertotesttherobustness,wevarythep roportionoftrainingdata.After5 independentrunsbyeachalgorithm,theclassicationaccu raciesontestingdataareplottedinFig.3 intermsofbox-plots,wherethelateralaxisvariesthetrai ningdataproportion. Figure3:Classicationaccuracyon 20Newsgroups data.Thepro- portionoftrainingdatavariesin f 10% ; 30% ; 50% g . 1 Following[10],weusethetraining/testingsetsfromhttp://people.csail.mit.edu /jrennie/20Newsgroups/ 6 Onecanndthat,SAGEoutperformsDirichlet-Multinomialm odelespeciallyincaseoflimited trainingdata,whichisconsistenttotheobservationsin[1 0].Moreover,withrandomandxed variationalvariables,theSAM-LRBmodelperformsfurther betteroratleastcomparablywell.If thevariationalvariablesareupdatedtotightenthelower- bound,theperformanceofSAM-LRBis substantiallythebest,witha10% ˘ 20%relativeimprovementoverSAGE.Table1alsoreportsthe averagecomputingtimeofSAGEandSAM-LRB.Wecanseethat,b yavoidingthelog-sum-exp calculation,SAM-LRB(xed)performsmorethan7timesfast erthanSAGE,whileSAM-LRB (optimized)paysforupdatingthevariationalvariables. Table1:Comparisononaveragetimecostsperiteration(inm inutes). methodSAGESAM-LRB(xed)SAM-LRB(optimized) timecost(minutes)3.80.63.3 4.2UnsupervisedTopicModeling  WenowapplyourunsupervisedSAM-LRBmodeltothebenchmark NIPS data 2 .Followingthe samepreprocessingandevaluationasin[10,26],wehaveatr ainingsetof1986documentswith 237,691terms,andatestingsetof498documentswith57,427 terms. Forconsistency,SAM-LRBisstillcomparedwithDirichlet- Multinomialmodel(variationalLDA modelwithsymmetricDirichletprior)andSAGE.Forallthes eunsupervisedmodels,thenumber oflatenttopicsisvariedfrom10to25andthento50.Afterun supervisedtraining,theperformance isevaluatedbyperplexity,thesmallerthebetter.Theperf ormancesof5independentrunsbyeach methodareillustratedinFig.4,againintermsofbox-plots . Figure4:Perplexityresultson NIPS data. Asshown,SAGEperformsworsethanLDAwhentherearefewnumb eroftopics,perhapsmainly duetoitsstrongequalityassumptiononbackground.Whereas ,SAM-LRBperformsbetterthan bothLDAandSAGEinmostcases.Withoneexceptionhappenswh enthetopicnumberequals50, SAM-LRB(xed)performsslightlyworsethanSAGE,mainlyca usedbyinappropriatexedvalues ofvariationalvariables.Ifupdatedinstead,SAM-LRB(opt imized)performspromisinglythebest. 4.3MultifacetedModeling  WethenproceedtotestthemultifacetedmodelingbySAM-LRB .Sameas[10],wechoosea publicly-availabledatasetofpoliticalblogsdescribing the2008U.S.presidentialelection 3 [11]. Outofthetotal6politicalblogs,threearefromtherightan dthreearefromleft.Thereare20,827 documentsandavocabularysizeof8284.Usingfourblogsfor training,ourtaskistopredictthe ideologicalperspectiveoftwounlabeledblogs.  Onthistask,AhmedandXingin[1]usedmultiviewLDAmodelto achieveaccuracywithin 65 : 0% ˘ 69 : 1% dependingondifferenttopicnumbersettings.Also,suppor tvectormachinepro- videsacomparableaccuracyof 69% ,whilesupervisedLDA[3]performsundesirablyonthistask . In[10],SAGEisrepeated5timesforeachofmultipletopicnu mbers,andachievesitsbestmedian 2 http://www.cs.nyu.edu/ ˘roweis/data.html 3 http://sailing.cs.cmu.edu/socialmedia/blog2008.html 7 result69.6%at K =30 .UsingSAM-LRB(optimized),themedianresultsoutof5runs foreach topicnumberareshowninTable2.Interestingly,SAM-LRBpr ovidesasimilarlystate-of-the-art result,whileachievingitat K =20 .ThedifferentpreferencesontopicnumbersbetweenSAGEan d SAM-LRBmaymainlycomefromtheirdifferentassumptionson backgroundlexicaldistributions. Table2:Classicationaccuracyon politicalblogs databySAM-LRB(optimized). #topic( K )1020304050 accuracy(%)medianoutof5runs67.3 69.8 69.168.368.1 5ConcludingRemarks  Thispaperstudiesthesparseadditivemodelfordocumentmo deling.Byemployingthedoublema- jorizationtechnique,weapproximatethelog-sum-exponen tialterminvolvedindatalog-likelihood intoaquadraticlower-bound.Withthehelpofthislower-bo und,weareabletoconvenientlyrelax theequalityconstraintonbackgroundlog-spacedistribut ionofSAGE[10],intoalow-rankcon- straint,leadingtoourSAM-LRBmodel.Then,aftertheconst rainedoptimizationistransformed intotheformofRPCA'sobjectivefunction,analgorithmbas edonacceleratedproximalgradient isadoptedduringlearningSAM-LRB.Themodelspecication andlearningalgorithmaresome- whatsimpleyeteffective.Besidesthesupervisedversion, extensionsofSAM-LRBtounsupervised andmultifacetedscenariosareinvestigated.Experimenta lresultsdemonstratetheeffectivenessand efciencyofSAM-LRBcomparedwithDirichlet-Multinomial andSAGE. Severalperspectivesmaydeserveinvestigationsinfuture .First,theacceleratedproximalgradient updatingneedstocomputeSVDdecompositions,whicharepro bablyconsumingforverylargescale data.Inthiscase,moreefcientoptimizationconsidering nuclearnormand ` 1 -normareexpected, withthesemideniterelaxationtechniquein[16]beingone possiblechoice.Second,thispaper usesaconstrainedoptimizationformulation,whileBayesi antacklingviaaddingconjugatepriorsto completethegenerativemodelsimilarto[8]isanalternati vechoice.Moreover,wemayalsoadopt nonconjugatepriorsandemploynonconjugatevariationali nferencein[27].Lastbutnottheleast, discriminativelearningwithlargemargins[18,30]mightb ealsoequippedforrobustclassication. Sincenonzeroelementsofsparse S inSAM-LRBcanbealsoregardedasselectedfeature,one maydesigntoincludethemintothediscriminativefeatures ,ratherthanonlytopicaldistributions [3].Additionally,theaugmentedLagrangianandalternati ngdirectionmethods[9,29]couldbealso consideredasalternativestotheproximalgradientoptimi zation. References [1]A.AhmedandE.P.Xing.Stayinginformed:supervisedand semi-supervisedmulti-view topicalanalysisofideologicalpespective.In Proc.EMNLP ,pages1140Œ1150,2010. [2]A.BeckandM.Teboulle.Afastiterativeshrinkage-thre sholdingalgorithmforlinearinverse problems. SIAMJournalonImagingSciences ,2(1):183Œ202,2009. [3]D.BleiandJ.McAuliffe.Supervisedtopicmodels.In AdvancesinNIPS ,pages121Œ128. 2008. [4]D.M.Blei,A.Y.Ng,andM.I.Jordan.LatentDirichletall ocation. JMLR ,3:993Œ1022,2003. [5]D.Bohning.Multinomiallogisticregressionalgorithm . AnnalsofInst.ofStat.Math. ,44:197Œ 200,1992. [6]G.Bouchard.Efcientboundsforthesoftmaxfunction,a pplicationstoinferenceinhybrid models.In WorkshopforApproximateBayesianInferenceinContinuous /HybridSystemsat NIPS'07 ,2007. [7]X.Chen,Q.Lin,S.Kim,J.G.Carbonell,andE.P.Xing.Smo othingproximalgradientmethod forgeneralstructuredsparseregression. TheAnnalsofAppliedStatistics ,6(2):719Œ752,2012. [8]X.Ding,L.He,andL.Carin.Bayesianrobustprincipalco mponentanalysis. IEEETrans. ImageProcessing ,20(12):3419Œ3430,2011. 8 [9]J.Eckstein.AugmentedLagrangianandalternatingdire ctionmethodsforconvexoptimization: Atutorialandsomeillustrativecomputationalresults.Te chnicalreport,RUTCORResearch ReportRRR32-2012,2012. [10]J.Eisenstein,A.Ahmed,andE.P.Xing.Sparseadditive generativemodelsoftext.In Proc. ICML ,2011. [11]J.EisensteinandE.P.Xing.TheCMU2008politicalblog corpus.Technicalreport,Carnegie MellonUniversity,SchoolofComputerScience,MachineLea rningDepartment,2010. [12]M.A.T.Figueiredo.AdaptivesparsenessusingJeffrey sprior.In AdvancesinNIPS ,pages 679Œ704.2002. [13]M.R.Gormley,M.Dredze,B.VanDurme,andJ.Eisner.Sha redcomponentstopicmodels. In Proc.NAACL-HLT ,pages783Œ792,2012. [14]L.Hong,A.Ahmed,S.Gurumurthy,A.J.Smola,andK.Tsio utsiouliklis.Discoveringgeo- graphicaltopicsinthetwitterstream.In Proc.12thWWW ,pages769Œ778,2012. [15]T.JaakkolaandM.I.Jordan.AvariationalapproachtoB ayesianlogisticregressionproblems andtheirextensions.In Proc.AISTATS ,1996. [16]M.JaggiandM.Sulovsk ˚ y.Asimplealgorithmfornuclearnormregularizedproblems .In Proc.ICML ,pages471Œ478,2010. [17]Y.JiangandA.Saxena.Discoveringdifferenttypesoft opics:Factoredtopicsmodels.In Proc. IJCAI ,2013. [18]A.Joulin,F.Bach,andJ.Ponce.Efcientoptimization fordiscriminativelatentclassmodels. In AdvancesinNIPS ,pages1045Œ1053.2010. [19]J.D.LaffertyandM.D.Blei.Correlatedtopicmodels.I n AdvancesinNIPS ,pages147Œ155, 2006. [20]Z.Lin,A.Ganesh,J.Wright,L.Wu,M.Chen,andY.Ma.Fast convexoptimizationalgorithms forexactrecoveryofacorruptedlow-rankmatrix.Technica lreport,UIUCTechnicalReport UILU-ENG-09-2214,August2009. [21]Q.Mei,X.Ling,M.Wondra,H.Su,andC.X.Zhai.Topicsen timentmixture:modelingfacets andopinionsinwebblogs.In Proc.WWW ,2007. [22]T.P.Minka.Estimatingadirichletdistribution.Tech nicalreport,MassachusettsInstituteof Technology,2003. [23]M.PaulandR.Girju.Atwo-dimensionaltopic-aspectmo delfordiscoveringmulti-faceted topics.In Proc.AAAI ,2010. [24]E.Richard,P.-A.Savalle,andN.Vayatis.Estimationo fsimultaneouslysparseandlowrank matrices.In Proc.ICML ,pages1351Œ1358,2012. [25]Y.S.N.A.SmithandD.A.Smith.Discoveringfactionsin thecomputationallinguistics community.In ACLWorkshoponRediscovering50YearsofDiscoveries ,2012. [26]C.WangandD.Blei.Decouplingsparsityandsmoothness inthediscretehierarchicaldirichlet process.In AdvancesinNIPS ,pages1982Œ1989.2009. [27]C.WangandD.M.Blei.Variationalinferenceinnonconj ugatemodels. ToappearinJMLR . [28]J.Wright,A.Ganesh,S.Rao,Y.Peng,andY.Ma.Robustpri ncipalcomponentanalysis:Exact recoveryofcorruptedlow-rankmatricesviaconvexoptimiz ation.In AdvancesinNIPS ,pages 2080Œ2088.2009. [29]J.YangandX.Yuan.LinearizedaugmentedLagrangianan dalternatingdirectionmethodsfor nuclearnormminimization. Math.Comp. ,82:301Œ329,2013. [30]J.Zhu,A.Ahmed,andE.P.Xing.MedLDA:maximummargins upervisedtopicmodels. JMLR ,13:2237Œ2278,2012. 9  
LocalizingbyDescribing:Attribute-Guided AttentionLocalizationforFine-GrainedRecognition XiaoLiu, JiangWang, ShileiWen,ErruiDing,YuanqingLin BaiduResearch {liuxiao12,wenshilei,dingerrui,linyuanqing }@baidu.comwangjiangb@gmail.com AbstractAkeychallengeinﬁne-grainedrecognitionishowtoﬁnd andrepresentdiscriminativelocalregions.Recentattention modelsarecapableoflearningdiscriminativeregionlocal- izersonlyfromcategorylabelswithreinforcementlearning.  However,notutilizinganyexplicitpartinformation,theyare notabletoaccuratelyﬁndmultipledistinctiveregions.Inthis work,weintroduceanattribute-guidedattentionlocalization  schemewherethelocalregionlocalizersarelearnedunder theguidanceofpartattributedescriptions.Bydesigninga novelrewardstrategy,weareabletolearntolocateregions  thatarespatiallyandsemanticallydistinctivewithreinforce- mentlearningalgorithm.Theattributelabelingrequirement oftheschemeismoreamenablethantheaccuratepartloca-  tionannotationrequiredbytraditionalpart-basedﬁne-grained recognitionmethods.ExperimentalresultsontheCUB-200- 2011dataset(Wahetal.2011)demonstratethesuperiority  oftheproposedschemeonbothﬁne-grainedrecognitionand attributerecognition. Introduction Humansheavilyrelyonsubtlelocalvisualcuestodistin- guishﬁne-grainedobjectcategories.ForexampleinFig-  ure(a),humanexpertsdifferentiateasummertanagerand  ascarlettanagerbythecolorofwingandtail.Inorderto buildhuman-levelﬁne-grainedrecognitionAIsystems,itis alsoessentialtolocatediscriminativeobjectpartsandlearn  localvisualrepresentationfortheseparts. State-of-the-artﬁne-grainedrecognitionmethods(Zhang etal.2014;Liu,Shen,andHengel2015;Wangetal.2014;  Krauseetal.2015b;2015a;Gaoetal.2015;Zhangetal.  2016a)eitherrelyonmanuallylabeledpartstotrainpartde-  tectorsinafullysupervisedmanner,oremployreinforce-  mentlearningorspatial-transformer-basedattentionmod-  els(Sermanet,Frome,andReal2015;Liuetal.2016b;  Zhaoetal.2016)tolocateobjectpartswithobjectcategory  annotationsinaweaklysupervisedmanner.However,both  typesofmethodshavemajorpracticallimitations.Fullysu-  pervisedmethodsneedtime-consuming,error-probingman-  ualobjectpartlabelingprocess,whileobjectlabelsalone  asasupervisionsignalisgenerallytooweaktoreliably Equalcontribution. Copyright c2017,AssociationfortheAdvancementofArtiﬁcial Intelligence(www.aaai.org).Allrightsreserved. locatemultiplediscriminativeobjectparts.Forexamplein  Figure(b),themethod(Liuetal.2016b)failstolocatethe  tailasattentionregions. Humanshavearemarkableabilitytolearntolocateob- jectpartsfrommultiplesourcesofinformation.Asidefrom  stronglylabeledobjectpartlocationandweaklylabeledob-  jectcategories,partdescriptions,suchas“redwing”,also  playsanimportantpartinthedevelopmentofobjectparts  locatingability.Intheirearlyages,childrenlearntorec-  ognizepartsandlocateobjectpartsbyreadingorlistening  topartdescriptions.Partdescriptionsdonotrequiretime-  consumingmanuallypartlocationlabeling,anditismuch  strongerthanobjectcategorylabels.Wecallit partattribute .Inspiredbythiscapability,weproposeapartattribute- guidedattentionlocalizationschemeforﬁne-grainedrecog-  nition.Usingpartattributesasaweaksupervisiontraining  signal,reinforcementlearningisabletolearnpart-speciﬁc  optimallocalizationstrategiesgiventhesameimageasen-  vironmentstate.Basedonthisintuition,itisreasonableto  expectthatdistinctivepartlocalizerscouldbelearnedas  strategiesoflookingforanddescribingtheappearancesof  differentparts.Intheproposedscheme,multiplefullycon-  volutionalattentionlocalizationnetworksaretrained.Each  networkpredictstheattributevaluesofapart.Wedesigna  novelrewardstrategyforlearningpartlocalizersandpart attributepredictors. Partattribute-guidedattentionlocalizationnetworkscan moreaccuratelylocateobjectparts(Figure(c)).Moreim- portantly,usingthepartlocationsandappearancefeatures frompart-attributeguidedattentionlocalizationnetworks  leadstoconsiderableperformanceimprovementonﬁne- grainedrecognition,asdemonstratedontheCUB-200-2011  dataset(Wahetal.2011).Moreover,partattributecanbeac-  quiredinlargescaleviaeitherhumanlabelingordatamin-  ingtechniques.Ithasbeensuccessfullyemployedforimage  recognition(ParikhandGrauman2011;Akataetal.2013; HwangandSigal2014),imageretrieval(Huangetal.2015),  facelocalization(Liuetal.2015)andimagegeneration(Yan  etal.2015). Attentionmodel(Mnihetal.2014;Ba,Mnih,and Kavukcuoglu2015)hasbeenwidelyusedinimagecap- tion(XuandSaenko2015;Xuetal.2015),imagerecog-  nition(Zhangetal.2016b;Seoetal.2016),actionrecog-  nition(Wangetal.2016),personre-identiﬁcation(Liuet Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)al.2016a)andimageparsing(Fanetal.2016).Toourbest knowledge,noonehasutilizedpartattributetoguidethe learningofvisualattentioninﬁne-grainedrecognition. Attribute-GuidedAttentionLocalization ThearchitectureoftheproposedschemeisshowninFig- ure2.Attributedescriptionsareonlyusedforlearningpart  localizersduringtraining.Theyarenotusedinthetesting  stage.Inthetrainingstage,afully-convolutionalattentionlo- calizationnetwork(Liuetal.2016b)islearnedforeach  part.Incontrastwith(Liuetal.2016b),thetaskofthe  fully-convolutionalattentionlocalizationnetworkistolearn  wheretolookforbetterpartattributepredictionratherthan  predictingthecategoryoftheentireobject. Fortesting,weextractfeaturesfrombothpartregionslo- catedbythenetworkandtheentireimageandconcatenate  themasajointrepresentation.Thejointrepresentationis  utilizedtopredicttheimagecategory. ProblemFormulation Given Ntrainingimages {x1,x2,...,xN},andtheirobject labels{l1,l2,...,lN}.Ourgoalistotrainamodelforclas- sifyingeachimage xiasitsground-truthlabel li.Fine-grainedobjectshave Pparts.Localizingbydescrib- ingproblemﬁndsapolicytolocatethepartsandtoclassify imagesbasedonthesepartlocationsandtheirfeatures.The  objectivecanbeformulatedas: L0=iL(G(xi,F1(xi),...,FP(xi)),li),(1)whereFp(·)isafunctionthatﬁndsthelocationofpart pandcropsitsimageregion.Thecropsizeofeachpartismanually  deﬁned.G(·)isadeepconvolutionalneuralnetworkclassi- ﬁerthatoutputstheprobabilityofeachcategorygiventhe  wholeimageandthecroppedimageregionsforalltheparts.  L(.)iscross-entropylossfunctionmeasuringthequalityof theclassiﬁcation. Preciselypredictingpartlocationsusingonlytheimage labelsisverychallenging.In localizingbydescribing ,we localizeobjectpartswiththehelpofvisualattributede-  scriptions.Thevisualattributeisasetofbinaryannotations  whosevaluecorrelateswithanaspectoftheobjectappear-  ance.Alocalattributeisgenerallyrelatedtotheappearance  ofanobjectpart.Theattributedescriptionofthe p-thpartis denotedas {Yp1,Yp2,...,YpN}.Eachpartdescription Ypiisabinaryvector: [ypi,1,ypi,2,...,ypi,Kp],whereeachelement ypi,kindicateswhetheranattributeexistsinthe i-thimage, andKpisthenumberofattributesforthe p-thpart.Weaim tolearnbetterpartlocalizers Fpusingpartattributeanno- tation.Anauxiliarypartattributeclassiﬁcationlossispro-  posedtofacilitatethelearningofpartlocalizers Lp=iL(Tp(Fp(xi)),Ypi),(2)whereTp(.)=[Tp,1(.),...,Tp,Kp(.)]isamulti-labelat- tributepredictionfunctionofthe p-thpart,andeach Tp,k(.)indicatesthepredictedprobabilityofthe k-thattributeof partp.L(.)isamulti-labelcross-entropyloss L(Xi,Ypi)=−k[ypi,klogTp,k(Fp(xi))+(1−ypi,k)log(1 −Tp,k(Fp(xi)))].(3)Theassumptionisthatthepartlocalizersthathelppre- dictpartattributesarealsobeneﬁcialtothepredictionofthe  wholeobject.WeuseEq.(3)asanauxiliarylosstolearn  localizerFp(·)thatoptimizesEq.(1). TrainingofLocalizers Givenimagesandattributedescriptions,wejointlylearna  partlocalizerandamulti-labelpredictorforeachpartsuch thatthepredictorusestheselectedlocalregionforattribute  prediction.Sincethelocalizationoperationisnon-differential,we employreinforcementlearningalgorithm(Williams1992) tolearnthepartlocalizersandmulti-labelpredictors.For  reinforcementlearningalgorithm,thepolicyfunctionisthe  partlocalizerfunction Fp(·);thestateisthecroppedlocal imagepatch Fp(xi)andthewholeimage xi;thereward functionmeasuresthequalityofthepartattribute. Theobjectivefunctionofthereinforcementlearningal- gorithmforpart pisJp(Fp,Tp)=iEFp(Rp,i)−(Tp(Fp(xi)),Ypi),(4)wherethereward EFp(Rp,i)=sp,ipFp(sp,i|xi)r(sp,i)d(sp,i)(5)istheexpectedrewardoftheselectedregionforthe p-thpartofthe i-thimage. sp,iindicatesaselectedregion, pFj(sp,i|xi)istheprobabilitythatthelocalizerselectthe region sp,i.r(sp,i)isarewardfunctiontoevaluatethecon- tributionoftheselectedregion sp,itoattributeprediction. Previousmethods(Sermanet,Frome,andReal2015; Liuetal.2016b)choosetherewardfunction r(sp,i)tobe 1onlywhentheimageiscorrectlyclassiﬁed.However,since  ouralgorithmpredictsmultipleattributevaluesforapart,  itistoostricttoenforcealltheattributesarecorrectlypre-  dicted.Therefore,weconsideranalternativerewardstrat-  egy.Aselectedregionhasreward1ifbothofthefollowing criteriaaresatisﬁed:1)itachieveslowerattributeclassiﬁ- cationlossthanmostotherregionsinthesameimage, i.e.,itspredictionlossrankstop- lowestamongthe Msampledregionsoftheimage.2)itachieveslowerattributeclassiﬁ-  cationlossthanmostotherregionsinthesamemini-batch,  i.e.,itspredictionlossislowerthanhalfoftheaverageloss  ofalltheregionsinthemini-batch. Following(Liuetal.2016b),welearnfullyconvolutional attentionlocalizationnetworksaspartlocalizers.Sinceboth  partsoftheobjectivefunctionEq.(4)aredifferentiable,RE-  INFORCEalgorithm(Williams1992)isappliedtocompute                          (a)(b)(c)Figure1:(a)Twotypesofbirdswithpart-attributedescriptions.Humanexpertsdifferentiateasummertanagerandascarlet tanagerbythecolorofwingandtail.(b)(Liuetal.2016b)failstolocatethetailpart.(c)Part-attribute-guidedattention  localizationnetworkscanlocateobjectpartsmoreaccurately.Thered,green,purpleandblueboundingboxeslocalizehead,  breast,wingandtail,respectively(bestviewedincolor).           has pattern plain –    has color red –   has color red –    shaped –         Figure2:Anoverviewarchitectureoftheproposedscheme.ThecorrespondenceofcolorandpartisthesameasFig.1.The upperpartshowstrainingstage,andthelowerpartshowsthetestingstage.Inthetrainingstage,multiplepartlocalizersare  trainedundertheguidanceofattributedescriptions.Inthetestingstage,featuresextractedfromtheselectedpartregionsand  theentireimagearecombinedintoajointrepresentationforcategoryprediction. thepolicygradienttooptimizetheobjectivefunction: FpEFp(Rp,i)(6)=sp,ipFp(sp,i|xi)Fplog[ pFp(sp,i|xj)r(sp,i)]d(sp,i)1MMm=1Fplog[ pFp(smp,i|xi)r(sm p,i)]wheresmi,jpFi(·|xj)isthelocalimageregionssampled accordingtolocalizerpolicy pFi(smi,j|xj).Mlocalregions ofthesameimagearesampledinamini-batch.Welistthe learningalgorithminAlgorithm1. TrainingofClassiﬁers Afterthelocalregionlocalizersaretrained,were-trained  theattributepredictionmodelsusingup-scaledlocalregions.  Whenre-trained,theattributepredictorstakesup-scaledlo-  calregionsfromthepartlocalizerstopredicttheattributes. Tocombineglobalandlocalinformation,weextractfea- turesfromallthepartregionsandtheentireimageandcon-  catenatethemtoformajointrepresentation.Thejointrep-  resentationisusedtopredictimagecategory.Indetails,we ﬁrsttrainclassiﬁerswitheachindividualpartregiontocap-  turetheappearancedetailsoflocalpartsforﬁne-grained  recognition.Aclassiﬁerutilizingtheentireimageisalso  trainedforglobalinformation.Wethenconcatenatefeatures extractedfromallthepartsandtheentireimageasajoint  representation,andweusealinearlayertocombinethefea- Algorithm1 Localizingbydescribingalgorithm: Input:trainingimages {x1,x2,...,xN},attributedescrip- tionsofeachpart {Yp1,Yp2,...,YpN}.Output:partlocalizationfunction F,multi-labelattribute predictionfunction T.1:for eachpart pdo2:InitializeFpandTp.3:repeat 4:Randomlysample Himages.5:for eachimage xido6:Samplesmp,ipFp(·|xi)7:endfor 8:for eachlocalregion sm p,ido9:Calculatelmp,i=L(Tp(smp,i),Ypi).10:endfor 11:CalculateˆL=lmp,i.12:for eachimage xido13:Sortlmp,iinascendingorder. 14:for eachlocalregion smp,ido15:iflmp,iisinthetop- positionofthesortedlist, andlmp,i<0.5ˆLthen16:Setreward r(sm p,i)=1.17:else18:Setreward r(smp,i)=0.19:endif 20:endfor 21:endfor 22:Calculatethegradientof Fpaccordingto(6). 23:Calculatethegradientof Tp.24:Updatetheparametersof FpandTp.25:untilconverge 26:endfor tures.Prediction Thepredictionprocessisillustratedinthelowerpartof Fig.2.Welocalizeandcropeachpartusingitslocalizer  Fp(xi),andeachpartregionisresizedtohighresolution forfeatureextraction.Featuresfromallpartregionsaswell  astheentireimageareconcatenatedasajointrepresenta-  tion.Alinearclassiﬁcationlayerisutilizedtomaketheﬁnal  categoryprediction. Wealsoattempttouseattributepredictionresultstohelp ﬁne-grainedobjectrecognition,ormodelthegeometricrela-  tionshipofthepartsusingrecurrentconvolutionaloperation.  However,weﬁndneitheroftheapproachesachievenotable  improvementsinourexperiments.Detailedexperimentalre-  sultsandsetupcanbefoundintheexperimentalsection. ExperimentsWeconductexperimentsontheCUB-200-2011datasets  (Wahetal.2011).Thedatasetcontains 11,788imagesof 200birdcategories,where 5,994imagesarefortraining, andtherest 5,794imagesarefortesting.Inadditiontothe categorylabel,15partlocations,312binaryattributesanda tightboundingboxofthebirdisprovidedforeachimage.  Examplesofimagesandattributesinthisdatasetareshown inFigure(a). ImplementationDetails Weevaluatetheproposedschemeintwoscenarios:“with BB”wheretheobjectboundingboxisutilizedduring  trainingandtesting,and“withoutBB”wheretheobject  bounding-boxisnotutilized. Wechoose4parts, i.e. “head”,“wing”,“breast”,and “tail”,totrainlocalregionlocalizers.Thecroppingsizeof  thesepartsarehalfoftheoriginalimage.Amongallthe312  attributes,ifapartnameappearsinanattribute,thenthe  attributeisconsidereddescribingthispart.Thenumberof  attributesdescribingthefourpartsare29,24,19,and40,  respectively. WeutilizeResNet-50(Heetal.2016)asthevisualrep- resentationforpartlocalizationandfeatureextraction.In  thetrainingstage,weutilizetheROI-pooledfeaturemaps  (inthe“withBB”setting,croptheimageregionwithinthe  boundingbox)tolearnmulti-labelattributepredictionsfor  eachpart.Theoutputofthe“res5c”layerofResNet-50is  employedastheinputofthefullyconvolutionalattentionlo-  calizationnetworks.TheattributepredictorsuseROI-pooled  featuremaps(Girshick2015)ofthefullyconvolutionalat-  tentionlocalizers. WetrainthemodelsusingStochasticGradientDescent (SGD)withmomentumof0.9,epochnumberof150,weight  decayof0.001,andamini-batchsizeof28onfourK40  GPUs.Oneepochmeansalltrainingsamplesarepassed  throughonce.Anadditionaldropoutlayerwithanratioof  0.5isaddedafter“res5c”,andthesizeof“fc15”ischanged  from1000to200. Theparametersbefore“res5c”areinitializedbythe model(Heetal.2016)pretrainedontheImageNetdataset  (Dengetal.2009),andparametersoffc15arerandomlyini-  tialized.Theinitiallearningrateissetat0.0001andreduced  twicewitharatioof0.1after50and100epoches.Thelearn-  ingrateofthelastlayer(“fc15”)is10timeslargerthanother  layers.Ourdataaugmentationissimilarto(Szegedyetal.2014), butwehavemoretypesofdataaugmentation.Atraining  imageisﬁrstrotatedwitharandomanglebetween −30and30.Acroppingisthenappliedontherotatedimage. Thesizeofthecroppedpatchischosenrandomlybetween  25%and100%ofthewholeimage,anditsaspectratiois chosenrandomlybetween 3/4and4/3.AlexNet-stylecolor augmentation(Krizhevsky,Sutskever,andHinton2012)is  alsoappliedfollowedbyrandomﬂip.Weﬁnallyresizethe  transformedcroppedpatchtoa 448×448imageastheinput oftheconvolutionalneuralnetwork. PartLocalizationResults WereportourpartlocalizationresultsinTable1.Percent  CorrectParts(PCP)isusedastheevaluationmetric.Apart  isdeterminedtobecorrectlylocalizedifthedifferenceof  itspredictedlocationandtheground-truthlocationiswithin  1.5timestheground-truthannotationsstandarddeviation. Methodheadbreastwingtailave(Shihetal.2015) 67.677.881.359.271.4(LiuandBelhumeur2013) 58.567.071.640.259.3(Liu,Li,andBelhumeur2014) 72.070.574.446.265.8Ours(withoutBB) 60.679.577.547.166.2Ours(withBB) 69.381.580.362.573.4Table1:Partlocalizationresults(measuredbyPCP)ontheCUB-200-2011dataset. Wecomparewithpreviousstate-of-the-artpartlocaliza- tionmethods(Shihetal.2015;LiuandBelhumeur2013; Liu,Li,andBelhumeur2014).Thestronglysupervised  method(Shihetal.2015),whichutilizesthepreciseloca-  tionofthepartsduringtraining,achievesthehighestaverage  PCP(71.4)inthe“withoutBB”scenario.Ourschemethat  doesnotuseanypartorobjectlocationsachievesthesecond  highestaveragePCP(66.2)andperformsthebestforlocal-  izing“breast”.Thepartsarelocalizedmuchmoreprecisely  (66.273.4)whenground-truthbirdboundingboxesis used.Figure3providesvisualizationsofourpartlocalization results.Ground-truthpartlocationsareshownashollowcir-  cles,predictedpartlocationsareshownassolidcirclesand theselectedpartregionsareshownasthumbnails. Itshouldbenotedthatdifferentfrom(Shihetal.2015), ourschemedoesnotdirectlyminimizethepartlocalization errorbutlearnspartlocalizersforattributeprediction.Thus,  aselectedregionthatisfarfrommanuallyannotationmight  betterpredictsomepartattributes.Forexample,ourpre-  dicted“head”positionsareusuallyinthecenterofthebird headsuchthatthecroppedlocalregiondoesnotloseany  information,whilethemanuallyannotated“head”positions normallyappearonthe“forehead”. AttributePredictionResults TheattributepredictionresultsmeasuredbyaverageArea  UndertheCurveofROC(AUC)arereportedinTable2.We  useAUCinsteadofaccuracybecausethedataofattributes arehighlyimbalanced:mostattributesareonlyactivatedin  averysmallnumberofimages,butsomeattributesappear veryfrequently.Foreachpart,wecalculatetheAUCofall  itsattributes,andreporttheaverageAUCofthemasthe  AUCofthepart. Directlyutilizingthefullimageasinputachieves 76.8,78.7,73.1,and 70.4averageAUCforparts“head”,“breast”, “wing”and“tail”,respectively.TheoverallaverageAUCof  thefourpartsis74.3.Usingthelocalizedlocalregionsre-  sultsinslightperformancedrop( 74.373.7)onoverall averageAUC.Thepredictionusingboththefullimageand  thelocalattentionregionsimprovestheoverallaverageAUC  resultto76.6.Boundingboxesofbirdsarenotusedforat-  tributeprediction. Fine-grainedRecognitionResults For“withoutBB”scenario,thebaselineResNet-50usingthe  wholeimageachieves 81.7%recognitionaccuracy.Adding featuresoftwoparts(“head”and“breast”)improvesthere-  sultto 84.7%andcombingfeaturesoffourpartsimproves MethodheadbreastwingtailtotalFullimage 76.878.773.170.474.3Attention77.878.371.968.873.7Image+Attention 80.380.775.572.176.6Table2:Attributepredictionresults(measuredbyaverage  AUC)ontheCUB-200-2011dataset. theresultto 85.1%.For“withBB”scenario,thebaseline achieves 82.3%accuracy.Combingfeaturesoftwopartsim- provestheresultto 84.9%,andcombingallthefeaturesim- provestheaccuracyto 85.3%.UsingfeaturesofGoogLeNet (IoffeandSzegedy2015)andVGGNet(SimonyanandZis- serman2014)achieves 85.1%and84.0%,respectively. Wecarryoutfurtherexperimentstoexploreusingtheat- tributesforbetterrecognition.Inthe“fullimage+attribute  value”experiment,weconcatenate112binaryattributela-  belswiththeoriginalvisualfeatureofthewholeimageto  predictthebirdcategory.Inthe“fullimage+attributefea-  ture”experiment,weconcatenatethevisualfeaturesofthe  5partattributepredictionmodels:oneistheoriginalfull  imagemodel,andtheotherfourmodelsareﬁne-tunedfor  predictingtheattributesofthefourparts. AsTable3shows,directlycombiningattributevalues doesnotimproverecognitionaccuracycomparedwiththe  baseline.Combingattributefeaturesleadstomarginalim-  provements( 81.7%82.5%and82.3%82.9%),be- causeweﬁndtheattributepredictionsareusuallynoisy  duetotheinherentdifﬁcultyofpredictingsomelocalat-  tributes.Bycombiningfeaturesfromthefullimagemodel,  fourpart-basedmodelsandtheattributepredictionmodels,  weachievethe 85.4%and85.5%forthe”withoutBB”and “withBB”scenarios,respectively.Wealsoexplorejointly  trainingthepartlocalizersusingarecurrentconvolutional  neuralnetworkmodelasgeometricregularizationofpart  locations(Tompsonetal.2014),butweﬁndtheaccuracy  improvementisnegligible( 85.1%85.2%withoutBB). Afterexaminingthedata,weﬁndbirdsposesaretoodiverse  tolearnaneffectivegeometricmodelfromlimitedamount  ofdata. Wecomparewithpreviousstate-of-the-artmethodson thisdatasetandsummarizetherecognitionresultsinTable  3.(Zhangetal.2014)trainpart-basedR-CNNmodeltode-  tecttheheadandbodyofthebird.Themethodrelieson  partlocationannotationduringtraining.Ourschemeoutper-  forms(Zhangetal.2014)andotherpart-basedmodels(Si-  monandRodner2015;Zhangetal.2015)byalargemargin  withoutrequiringstronglysupervisedpartlocationannota-        Figure3:Visualizationsofpartlocalizationresults.Ground-truthpartlocationsareshownashollowcircles;predictedpart locationsareshownassolidcircles;selectedpartregionsareshownasthumbnails.Differentpartsareshownindifferentcolors.  Noground-truthbirdboundingboxesorpartlocationsareusedduringtrainingortestingtimeintheproposedmethod. MethodAccwithoutBB(%) AccwithBB(%) (Lin,RoyChowdhury,andMaji2015) 84.185.1(Krauseetal.2015a) 82.082.8(Zhangetal.2014) 73.976.4(Liuetal.2016b) 82.084.3(Jaderbergetal.2015) 84.1-(Zhangetal.2015) 75.0-(SimonandRodner2015) 81.0-FullImage 81.782.3FullImage+2 ×parts84.784.9FullImage+4 ×parts85.185.3FullImage+attributevalue 81.782.2FullImage+attributefeature 82.582.9FullImage+4 ×parts+attributefeature 85.485.5Table3:RecognitionresultsontheCUB-200-2011datasetwithdifferentsettings. tion.(Lin,RoyChowdhury,andMaji2015)constructhigh dimensionalbilinearfeaturevectors,andachieve 84.1%and85.1%accuracyforthe”withoutBB”and“withBB”scenar- ios,respectively.(Krauseetal.2015a)learnandcombine multiplelatentpartsinaweaklysupervisedmanner,and  achieve 82.0%and82.8%accuracyforthe”withoutBB” and“withBB”scenarios,respectively.(Liuetal.2016b)uti-  lizethefullyconvolutionalattentionlocalizationnetworks  toselecttwolocalpartsformodelcombination.Theaccu-  racyis 82.0%withoutboundingboxand 84.3%withbound- ingbox,whileouraccuracyis 84.7%withoutboundingbox and84.9%withboundingboxbycombingfeaturesfrom twolocalparts(“head”and“breast”).Localizingdistinctive  partsleadstobetterrecognitionaccuracy.Similarly,(Jader-  bergetal.2015)combinefeaturesoffourlocalpartsand  achieveanaccuracyof 84.1%withoutusingboundingbox. Ourschemeusingthesamenumberofpartsoutperformsit  by1%(84.1%85.1%).RewardStrategyVisualization Therewardsduringreinforcementlearningalgorithmare  visualizedinFigure4.Fromlefttoright,weshowthe  heatmapsof1st,40-th,80-th,120-th,160-th,and200-thit-  erationsformulti-labelattributepredictionloss,rewardsand  theoutputprobabilityofthelocalizer.Ascanbeseen,after  aninitialdivergenceonlocalizerprobabilitymap,theout- putofthelocalizerconvergestothe“head”positionduring  trainingasexpected. Figure4:Ouralgorithmuses200iterationstolocate“head”  ofthebirdintheleftpart.Thetoprowintherightpartshows  themulti-labelattributepredictionlossatdifferentpositions,  andalighterpositionhashigherloss.Themiddlerowshows  therewardsatdifferentpositions.Thelocalizerisencour-  agedtofocusonthelightposition.Thebottomrowshows  theprobabilitymapofthelocalizer.Lighterpositionsindi-  catelargerprobabilityoflocalization. ConclusionInthispaper,wepresentanattentionlocalizationscheme  forﬁne-grainedrecognitionthatlearnspartlocalizersfrom  itsattributedescriptions.Anefﬁcientreinforcementlearn- ingschemeisproposedforthistask.Theproposedscheme consistsofarewardfunctionforencouragingdifferentpart localizerstocapturecomplementaryinformation.Itisalso highlycomputationallyefﬁcientwhenthenumberofat-  tributesandpartsislarge.Comprehensiveexperimentsshow  thatourschemeobtainsgoodpartlocalization,improvesat-  tributeprediction,anddemonstratessigniﬁcantperformance  improvementonﬁne-grainedrecognition.Inthefuture,we  willcontinueoureffortstoimprovethemodelsofgeometric  partlocationregularization. References Akata,Z.;Perronnin,F.;Harchaoui,Z.;andSchmid,C.2013. Label-embeddingforattribute-basedclassiﬁcation. Proc.ECCV .Ba,J.;Mnih,V.;andKavukcuoglu,K.2015.Multipleobjectrecog- nitionwithvisualattention. arXiv:1412.7755v2.Deng,J.;Dong,W.;Socher,R.;Li,L.;Li,K.;andFei-Fei,L.2009.  Imagenet:Alarge-scalehierarchicalimagedatabase. Proc.CVPR .Fan,H.;Mei,X.;Prokhorov,D.;andLing,H.2016.Hi-  erarchicalattentionnetworkforactionrecognitioninvideos. arXiv:1607.02537.Gao,Y.;Beijbom,O.;Zhang,N.;andDarrell,T.2015.ompact bilinearpooling. arXiv:1511.06062.Girshick,R.2015.Fastr-cnn. arXivpreprintarXiv:1504.08083 .He,K.;Zhang,X.;Ren,S.;andSun,J.2016.Deepresiduallearn-  ingforimagerecognition. Proc.CVPR .Huang,J.;Feris,R.S.;Chen,Q.;andYan,S.2015.Cross-domain  imageretrievalwithadualattribute-awarerankingnetwork. Proc. ICCV.Hwang,S.J.,andSigal,L.2014.Auniﬁedsemanticembedding: Relatingtaxonomiesandattributes. Proc.NIPS .Ioffe,S.,andSzegedy,C.2015.Batchnormalization:accelerating  deepnetworktrainingbyreducinginternalcovariateshift. Proc. ICML.Jaderberg,M.;Simonyan,K.;Zisserman,A.;andKavukcuoglu,K. 2015.Spatialtransformernetworks. Proc.NIPS .Krause,J.;Jin,H.;Yang,J.;andFei-Fei,L.2015a.Fine-grained  recognitionwithoutpartannotations. Proc.CVPR .Krause,J.;Sapp,B.;Howard,A.;Zhou,H.;Toshev,A.;Duerig,T.;  Philbin,J.;andFei-Fei,L.2015b.Theunreasonableeffectiveness ofnoisydataforﬁne-grainedrecognition. arXiv:1511.067891v1.Krizhevsky,A.;Sutskever,I.;andHinton,G.E.2012.Ima-  genetclassiﬁcationwithdeepconvolutionalneuralnetworks. Proc. NIPS.Lin,T.;RoyChowdhury,A.;andMaji,S.2015.Bilinearcnnmod- elsforﬁne-grainedvisualrecognition. Proc.ICCV .Liu,J.,andBelhumeur,P.2013.Birdpartlocalizationusing  exemplar-basedmodelswithenforcedposeandsubcategorycon- sistency. Proc.ICCV .Liu,Z.;Luo,P.;Wang,X.;andTang,X.2015.Deeplearningface attributesinthewild. Proc.ICCV .Liu,H.;Feng,J.;Qi,M.;Jiang,J.;andYan,S.2016a.End-  to-endcomparativeattentionnetworksforpersonre-identiﬁcation. 1606.04404.Liu,X.;Xia,T.;Wang,J.;andLin,Y.2016b.Fullyconvolutional attentionlocalizationnetworks:efﬁcientattentionlocalizationfor ﬁne-grainedrecognition. arXiv:1603.06765 .Liu,J.;Li,Y.;andBelhumeur,P.2014.Part-pairrepresentationfor partlocalization. Proc.ECCV .Liu,L.;Shen,C.;andHengel,A.2015.Thetreasurebeneath convolutionallayers:cross-convolutional-layerpoolingforimage classiﬁcation.Proc.CVPR .Mnih,V.;Heess,N.;Graves,A.;andKavukcuoglu,K.2014.Re- currentmodelsofvisualattention. Proc.NIPS .Parikh,D.,andGrauman,K.2011.Relativeattributes. Proc.ICCV .Seo,P.H.;Lin,Z.;Cohen,S.;Shen,X.;andHan,B.2016.Hierar- chicalattentionnetworks. arXiv:1606.02393.Sermanet,P.;Frome,A.;andReal,E.2015.Attentionforﬁne- grainedcategorization. arXiv:1412.7054v3.Shih,K.;Mallya,A.;Singh,S.;andHoiem,D.2015.Part- localizationusingmulti-proposalconsensusforﬁne-grainedcat-  egorization. Proc.BMVC .Simon,M.,andRodner,E.2015.Neuralactivationconstellations:  Unsupervisedpartmodeldiscoverywithconvolutionalnetworks. ICCV.Simonyan,K.,andZisserman,A.2014.Verydeepconvolutional networksforlarge-scaleimagerecognition. arXiv:1409.1556.Szegedy,C.;Liu,W.;Jia,Y.;Sermannet,P.;Reed,S.;Anguelov, D.;Erhan,D.;Vanhoucke,V.;andRabinovich,A.2014.Going deeperwithconvolutions. arXiv:1409.4842.Tompson,J.;Jain,A.;LeCun,Y.;andBregler,C.2014.Jointtrain- ingofaconvolutionalnetworkandagraphicalmodelforhuman poseestimation. Proc.NIPS .Wah,C.;Branson,S.;Welinder,P.;Perona,P.;andBelongie,S. 2011.Thecaltech-ucsdbirds-200-2011dataset. Wang,X.;Yang,T.;Chen,G.;andLin,Y.2014.Object-centric samplingforﬁne-grainedimageclassiﬁcation. arXiv:1412.3161.Wang,Y.;Wang,S.;Tang,J.;O’Hare,N.;Chang,Y.;andLi,B. 2016.Hierarchicalattentionnetworkforactionrecognitionin videos.arXiv:1607.06416.Williams,R.1992.Simplestatisticalgradient-followingalgo- rithmsforconnectionistreinforcementlearning. MachineLearn- ing.Xu,H.,andSaenko,K.2015.Ask,attendandanswer:Explor- ingquestion-guidedspatialattentionforvisualquestionanswering. arXiv:1511.05234.Xu,K.;Ba,J.;Kiros,R.;Cho,K.;Courville,A.;Salakhutdinov, R.;Zemel,R.;andBengio,Y.2015.Show,attendandtell:Neural imagecaptiongenerationwithvisualattention. arXiv:1502.03044.Yan,X.;Yang,J.;Sohn,K.;andLee,H.2015.At- tribute2image:Conditionalimagegenerationfromvisualat- tributes. arXiv:1512.00570.Zhang,N.;Donahue,J.;Girshick,R.;andDarrell,T.2014.Part- basedr-cnnsforﬁne-grainedcategorydetection. Proc.ECCV .Zhang,Y.;shenWei,X.;Wu,J.;Cai,J.;Lu,J.;Nguyen,V.-A.;and Do,M.N.2015.Weaklysupervisedﬁne-grainedimagecatego-  rization.arxiv:1504.04943.Zhang,H.;Xu,T.;Elhoseiny,M.;Huang,X.;Zhang,S.;Elgamnal,  A.;andMetaxas,D.2016a.Spda-cnn:Unifyingsemanticpart  detectionandabstractionforﬁne-grainedrecognition. Proc.CVPR .Zhang,J.;Lin,Z.;Brandt,J.;Shen,X.;andSclaroff,S.2016b.Top-  downneuralattentionbyexcitationbackprop. arXiv:1608.00507.Zhao,B.;Wu,X.;Feng,J.;Peng,Q.;andYan,S.2016.Diversi- ﬁedvisualattentionnetworksforﬁne-grainedobjectclassiﬁcation. arXiv:1606.08572. 
AJointLearningApproachtoIntelligentJobInterviewAssessment DazhongShen 1 ; 2 ,HengshuZhu 2 ;  ,ChenZhu 2 ,TongXu 1 ; 2 ,ChaoMa 2 ,HuiXiong 1 ; 2 ; 3 ; 4 ;  1 AnhuiProvinceKeyLabofBigDataAnalysisandApplication,UniversityofScienceandTechnologyofChina, 2 BaiduTalentIntelligenceCenter, 3 BusinessIntelligenceLab,BaiduResearch, 4 NationalEngineeringLaboratoryofDeepLearningTechnologyandApplication,China. sdz@mail.ustc.edu.cn, f zhuhengshu,zhuchen02,machao13 g @baidu.com,tongxu@ustc.edu.cn,xionghui@gmail.com Abstract Thejobinterviewisconsideredasoneofthemost essentialtasksintalentrecruitment,whichformsa bridgebetweencandidatesandemployersin therightpersonfortherightjob.Whilesubstantial effortshavebeenmadeonimprovingthejobinter- viewprocess,itisinevitabletohavebiasedorin- consistentinterviewassessmentduetothesubjec- tivenatureofthetraditionalinterviewprocess.To thisend,inthispaper,weproposeanovelapproach tointelligentjobinterviewassessmentbylearning thelarge-scalereal-worldinterviewdata. cally,wedevelopalatentvariablemodelnamed J oint L earning M odelon I nterview A ssessment (JLMIA)tojointlymodeljobdescription,candi- dateresumeandinterviewassessment.JLMIAcan effectivelylearntherepresentativeperspectivesof differentjobinterviewprocessesfromthesuccess- fuljobinterviewrecordsinhistory.Therefore,ava- rietyofapplicationsinjobinterviewscanbeen- abled,suchasperson-jobandinterviewques- tionrecommendation.Extensiveexperimentscon- ductedonreal-worlddataclearlyvalidatetheeffec- tivenessofJLMIA,whichcanleadtosubstantially lessbiasinjobinterviewsandprovideavaluable understandingofjobinterviewassessment. 1Introduction Asoneofthemostimportantfunctionsinhumanresource management,talentrecruitmentaimsonacquiringtheright talentsfororganizationsandalwayshasdirectimpacton businesssuccess.AsindicatedinanarticlefromForbes,US corporationsspendnearly72billiondollarseachyearon avarietyofrecruitingservices,andtheworldwideamount islikelythreetimesbigger [ Bersin,2013 ] Inparticular,job interview,whichisconsideredasoneofthemostusefultools andthetestinggroundforevaluatingpotentialemploy- eesinthehiringprocess,hasattractedmoreandmoreatten- tionsinhumanresourcemanagement.Whilesubstantialef- fortshavebeenmadeontheimprovementofjobinterview process,traditionalinterviewprocesshasasubstantialriskof biasduetothesubjectivenatureoftheprocess.Thissitua- tioncouldbeevenmoresevere,sincedifferentinterviewers  CorrespondingAuthor. mayhavedifferenttechnicalbackgroundsordifferentexperi- encelevelsinpersonalqualities.Thismayleadtoabiasedor incompleteassessmentofjobcandidate. Recently,theIntelligence(AI)trendhasmadeits waytotalentrecruitment,suchasjobrecommendation [ Ma- linowski etal. ,2006;Paparrizos etal. ,2011;Zhang etal. , 2014 ] ,talentmapping [ Xu etal. ,2016 ] ,andmarkettrend analysis [ Zhu etal. ,2016 ] .However,fewereffortshavebeen madeonenhancingthequalityandexperienceofjobinter- view.Acriticalchallengealongthislineishowtoreveal thelatentrelationshipsbetweenjobpositionandcandidate, andfurtherformperspectivesforeffectiveinterviewassess- ment.Intuitively,experiencedinterviewerscoulddiscoverthe topic-levelcorrelationbetweenjobdescriptionandresume, andthendesigntheinterviewdetailstomeasurethesuit- abilityofapplicants.Forexample,acandidateforﬁSoft- wareEngineerﬂ,whohasstrongacademicbackground,might beinterviewedwithquestionsnotonlyaboutﬁAlgorithmﬂ, ﬁProgrammingﬂ,butalsoﬁResearchﬂ.Meanwhile,compared withthetechnicalinterview,thevocabularyofcomprehensive interviewcouldbelargelydifferent. Tothisend,inthispaper,weproposeanovelapproach tointelligentjobinterviewassessmentbylearningthelarge- scalereal-worldinterviewdata.,wedevelop alatentvariablemodelnamed J oint L earning M odelon I nterview A ssessment(JLMIA)tojointlymodeljobdescrip- tion,candidateresumeandinterviewassessment.JLMIAcan effectivelylearntherepresentativeperspectivesofdifferent jobinterviewprocessesfromthesuccessfuljobinterview recordsinhistory.Also,twocategoriesofinterviews,the technical and comprehensive interviews,whicharehosted bytechnicalinterviewersandmanagerialinterviewersre- spectively,couldbewelldifferentiated.Furthermore,based onJLMIA,wealsoprovidesolutionsfortwoapplications, namelyperson-jobandinterviewquestionrecommenda- tion.Extensiveexperimentsconductedonreal-worlddata clearlyvalidatetheeffectivenessofJLMIA,whichcanlead tosubstantiallylessbiasinjobinterviewsandprovideavalu- ableunderstandingofjobinterviewassessment. 2ProblemStatement Formally,ourdatasetcontainstherecruitmentdocuments of j M j uniquejobs,i.e., S = f S m =( J m ;A m ) g j M j m =1 , where J m isthejobdescriptionofthe m -thjoband A m Figure1:ThegraphicalrepresentationofJLMIA. istheinterviewrecordsofthisjob., A m = f ( R md ;E md ) g j D m j d =1 contains j D m j interviews,where R md is theresumeofcandidatein d -thinterview,and E md isthecor- respondinginterviewassessment.Sinceallofthejobdescrip- tions,resumes,andinterviewassessmentsaretextualdata,we usebag-of-wordstorepresentthem,e.g., J m = f w J mj g N J m j =1 , similarto R md and E md . Ajobdescription J m containsdetailedjobrequirements, andaresume R md mainlyconsistsofthepastexperiences ofthiscandidatethatcanherabilities.Meanwhile,the evaluationaboutacandidateininterviewassessmentsbridges thegapbetweenjobrequirementsandherability.Andaccord- ingtothegoalofinterviews,interviewassessmentscanbe furtherdividedintotechnicalandcomprehensiveinterview. Asweknown,duringtheinterview,interviewerstendto askquestionsrelatedtotheworkexperiencesofcandidates. Thusthereoftenexitsstrongcorrelationbetweeninterview assessmentsandresumes.However,jobdescriptionisusu- allymoreabstractthanresumes,andcandidateswithdifferent backgroundsmaybesuitableforthesamejob.Thuswethink althoughthereexistscorrelationbetweenjobdescriptionsand resumes,thediversityofjobdescriptionsislessthanthatof resumes.Inaddition,itisobviousthatthefocusofinterviews isdifferentaccordingtheirgoals.Thusitisbettertomodelthe differencesbetweentechnicalandcomprehensiveinterview. Generally,themaintasksinthispapercanbesummarized as: Task1 ,howtodiscoverthestrongcorrelationbetweenre- sumesandinterviewassessments? Task2 ,howtomodelthe latentrelationshipsbetweenjobdescriptionsandresumes? Task3 ,howtodistinguishthedifferencesbetweendifferent interviewcategories? 3TechnicalDetailsofJLMIA Tosolvetheabovetasks,weproposeanoveljointlearning model,namelyJLMIA.Inthissection,wewillformallyintro- duceitstechnicaldetails. 3.1ModelFormulation Tomodelthelatentsemanticsinjobdescription,resume, andinterviewassessment,weassumethereexistlatenttop- ics,representedby ' J , ' R and ' E ,inallofthem.And ourtasksarefurthertransformedtomodeltherelationships amongtheselatenttopics.First,tomodelthestrongcorre- lationbetweenresume R md andinterviewassessment E md , wedirectlyassumetheysharethesamedistri- bution  A md overtopics.Second,forrevealingtherelation- shipsbetweenjobdescriptionsandresumesalongwiththe ALGORITHM1: TheGenerativeProcessofJLMIAfor ResumeandInterviewAssessment 1. Foreachtopic k ofcandidateinterviewrecord: (a) Draw ' R k fromtheDirichletprior Dir (  R ) . (b) Draw ' ET k and ' EC k fromtheDirichletprior Dir (  E ) . 2. Foreachjobdescription J m : (a) Sampletopicdistribution  J m ˘ Dir (  ) . 3. Foreachcandidateinterviewrecordpair ( R md ;E md ;I md ) : (a) Sampletopicdistribution  A md ˘ N ( h (  J m ;C ) ; 2 I ) (b) Forthe r -thword w R mdr inresume R md : i. Drawtopicassignment z R mdr ˘ Multi ( ˇ (  A md )) . ii. Drawword w R mdr ˘ Multi ( ' R z R mdr ) . (c) Forthe e -thword w E mde ininterviewassessment E md : i. Drawtopicassignment z E mde ˘ Multi ( ˇ (  A md )) . ii. Drawword w E mde ˘ Multi ( ' ET z E mde ) ( I mde == TI ). iii. Drawword w E mde ˘ Multi ( ' EC z E mde ) ( I mde == CI ). differencesbetweentheirdiversity,wegenerate  A md from thelogistic-normaldistributionwithmeanparameterrelated tothetopicdistributionofjobdescription  J m .Andtheto- picnumbersof ' J , ' R and ' E aresetas j k E j = j k R j = C j k J j = CK .Inotherwords,foreachtopicin ' J ,there are C topicsin ' R ( ' E )relatedtoit.Third,weusealabel I 2f TI;CI g (e.g., T echnical I nterviewor C omprehensive I nterview)toindicatethetypeofinterviewforeachinterview assessment,wheredifferenttypesofinterviewassessmentare generatedfromdifferenttopics ' E 2f ' ET ;' EC g .Tosim- plifyourmodel,wefollowtheideain [ WangandMcCallum, 2006 ] ,andsettheinterviewlabelforeachwordininterview assessmentinsteadoftheentireinterviewassessment. ThegraphicalmodelofJLMIAisshowninFigure1.Since thegenerativeprocessofjobdescriptionisthesameasLa- tentDirichletAllocation(LDA) [ Blei etal. ,2003 ] ,herewe onlylistthegenerativeprocessforresumeandinterviewas- sessment A = f A m g j M j m =1 ,showedinAlgorithm1,where h ( ;C ) ,inline 3 : ( a ) ,isavectorconcatenating C logvec- torsof  ,i.e., h (  J m ;C ) k = log J m;k 0 ;k 0 = kmodK; 1  k 0  K ,and ˇ (  ) ,inline 3 : ( b ) :i and 3 : ( c ) :i ,isthelogistic transformation,i.e., ˇ (  A md ) k = exp f  A md;k g CK P i =1 exp f  A md;i g . Duetothenon-conjugacyofthelogisticnormalandmulti- nomial,thelatentparametersposteriorisintractable.Thuswe proposeavariationalinferencealgorithmforJLMIA. 3.2VariationalInferenceforJLMIA Here,wedevelopavariationalinferencealgorithmfor JLMIAbasedonvariationalfamilies.Thebasic ideabehindvariationalinferenceistooptimizethefreepara- metersofadistributionoverthelatentvariables,sothatthe distributioniscloseinKullback-Liebler(KL)divergenceto trueposterior,whichcanbesubstituted.Inourmodel,let usdenotealllatentvariableparametersby  andallhyper- parametersby  .Followingthegenerativeprocess,thejoint distributioncanbefactoredas: p ( S;  j = p  j  j M j Y m =1 P ( S m j  ; (1) whereeachcomponentcanbecalculatedby: p ( S m j = p ( J m j z J m ;' J ) j D m j Y d =1 p ( R md j z R md ;' R ) p ( E md j z E md ;' E ;I md ) ; p  j = j M j Y m =1 j D m j Y d =1 p (  A md j  J m ; 2 ) N R md Y r =1 p ( z R mdr j  A md ) N E md Y e =1 p ( z E mde j  A md )  K Y k =1 p ( ' J k j  J ) CK Y k =1 p ( ' R k j  R ) p ( ' ET k ;' EC k j  E )  j M j Y m =1 p (  J m j  ) N J m Y j =1 p ( z J mj j  J m ) : Then,correspondingtothisjointdistribution,wepositthe fullyfactorizedvariationalfamiliesasfollowing,wherethe detaildescriptionofeachtermcanbefoundinAppendix: q = K Y k =1 q ( ' J k ) CK Y k =1 q ( ' R k ) q ( ' ET k ) q ( ' EC k ) M Y m =1 q (  J m ) N J m Y j =1 q ( z J mj )  M Y m =1 D m Y d =1 CK Y k =1 q (  A md;k ) N R md Y r =1 q ( z R mdr ) N E md Y e =1 q ( z E mde ) : (2) Accordingto [ Blei etal. ,2017 ] ,minimizingtheKLdiver- gencebetweenvariationaldistributionandtrueposterior,is equivalenttomaximizetheloglikelihoodboundofjobinter- viewrecords,whichistheevidencelowerbound( ELBO ): logp ( S j   E q [ logp ( S;  j + H ( q ) = E q [ logp  j + j M j X m =1 E q [ logp ( S m j + H ( q ) ; (3) wheretheexpectation E q [  ] istakenwithrespecttothevari- ationaldistributioninEquation2,and H ( q ) denotestheen- tropyofthatdistribution. Thelargestchallengetomaximize ELBO isthenon- conjugacyoflogisticnormalandmultinomial,whichleads tothedifincomputingtheexceptedlogprobability oftopicassignmentsindocumentsofeachcandidateinter- viewrecords.Similarto [ WangandBlei,2011 ] ,weintro- duceanewvariationalparameter  = f  m 1: j D m j g m =1: j M j topreservethelowerboundof ELBO .Herewetake the E q [ logp ( z R mdr j  A )] asanexampletoexplainit(the E q [ logp ( z E mde j  A )] canbecomputedinasimilarway): E q [ logp ( z R mdr j  A md )]= E q [  A md;z R mdr ]  E q [ log ( CK X k =1 exp f  A md;k g )]  E q [  A md;z R mdr ]    1 md ( CK X k =1 E q [ exp f  A md;k g ])+1  log (  md ) : (4) Formaximizingthe ELBO ,wedevelopanEM-styleal- gorithmwithcoordinateascentapproachtooptimizepara- meters,thedetailsofwhichcanbefoundinAppendix. 4Application Here,wewillintroducetwoapplicationsenabledbyJLMIA, i.e.,Person-JobFitandInterviewQuestionRecommendation. 4.1Person-JobFit Person-JobFitistheprocessofmatchingtherighttalentfor therightjob.Formally,givenajobdescription J g andare- sume R g ,theobjectiveistomeasuretheirmatchingdegree. ,weneedtoleverageJLMIAtoinferthela- tenttopicdistributionsof J g and R g respectively.However, ourmodelcannotinferthetopicdistributionforanindivid- ualresumeorjobdescription.Thusweconstructa S forre- sumes(jobdescriptions),whereallofotherdataaresetas empty,andinferthecorrespondingtopicdistribution. Afterthevariationalparametersoftopicassignmentof eachword, ˚ J g and ˚ R g ,arelearned,wecancomputethe document-topicdistributionby:  J g;k = 1 N J N J X n =1 ˚ J gn;k k =1 ;  ;K;  R g;k = 1 N R N R X n =1 ˚ R gn;k k =1 ;  ;CK: (5) Then,bycomputingthesimilaritybetween  J g and  R g ,we canmeasurethesuitabilitybetweenjobdescriptionandre- sume.Actually,anydistancecalculationformulationbetween twoprobabilitydistributionscanbeusedhereformeasuring thesimilarity,suchasCosinedistanceandKullback-Leibler divergence.Notethat,sincethedimensionof  J g and  R g may bedifferent,herewehave: ~  R g;k = X c 2 C k  R g;c k =1 ;  ;K; where C k isasetofmappingindexthat  R g;c ( c 2 C k )isgeneratedfrom  J g;k .Inparticular,thevectors  J g and  R g learnedbyJLMIAcanberegardedaslow-rankse- manticrepresentationsofjobdescriptionandresume.Thus, usingtheserepresentationsinsteadoforiginalbag-of-words asfeaturesfortraininga(e.g.,RandomForest)is anothersolutionforPerson-JobFit. 4.2InterviewQuestionRecommendation Duringtheinterview,interviewersneedtoasksomequestions toevaluatecandidates.However,duetothelimitedexpertre- sources,sometimestheinterviewersmaynothaveenough domainknowledgetopreparediscriminativequestionsfor systematicallyjudgingthecompetenciesofcandidates,espe- ciallyfromtheviewofPerson-JobFit.Thus,inthispaper,we proposeaneffectivealgorithmforrecommendinginterview questionsbasedonJLMIAandinterviewquestionsaccumu- latedinhistoricalassessments. Tobegivenaquestiondatabase Q = f q i g N i =1 ,the problemofinterviewquestionrecommendationisas retrievingasetofquestions X thatarerelatedtoagivenquery  (i.e.,jobrequirementitemorexperienceitemofcandidate). Similartotheprocessofcomputingthetopicdistributions  R g , wecancomputethetopicdistribution  Q i ofeachquestion q i 2 Q ,throughregardinginterviewquestionsasapartof interviewassessment.Let  Q i denotealatentrepresentation ofquestion q i andthelatentrepresentationofthegivenquery  as   g 2f  J g ; R g g . Torecommendhighqualityquestionstointerviewers,on theonehand,theselectedquestionset X ˆ Q , j X j = L shouldberelevanttothequery   g ,ontheotherhand,wehope toavoidmakingthequestionsin X toosimilartoeachother. Tobalancerelevanceanddiversityofselectedquestionset, weselectquestions X bymaximizingthefollowingobjective function: F  ;X )=  Rel  ;X ) Rel +(1   ) Div ( X ) Div =  P q j 2 X Sim (   g ; Q j ) Rel +(1   ) P q i 2 X P q j 2 X;q j 6 = q i Dis (  Q i ; Q j ) Div ; s:t:X ˆ Q; j X j = L; 0 << 1 ; (6) where, Rel  ;X ) and Div ( X ) measuretherelevanceand diversityabove, Sim (  ;  ) ischosenas Cosine (  ;  ) while Dis (  ;  ) issetas 1  Cosine (  ;  ) ,and Rel and Div arenor- malizationfactors,commonlychosenasthemaximumpossi- blevaluesof Rel  ;X ) and Div ( X ) respectively. Ingeneral,thecalculationofaddressing F ( X ) iscompu- tationallyprohibitive,sincewewillsuffertheassembleex- plosionproblemifwecalculateEquation6forallsubsets X .Fortunately,since F ( X ) inthispaperissubmod- ule [ Tang etal. ,2014 ] ,thesimplegreedyalgorithmcould achievea (1  1 =e ) approximationoftheoptimalsolution, 5ExperimentalResults Inthissection,wewillintroducetheperformanceofJLMIA basedonareal-worldinterviewdataset. 5.1ExperimentalSetup Thedatasetusedintheexperimentsisthehistoricalrecruit- mentdataprovidedbyahigh-techcompanyinChina,which containstotal14,702candidateinterviewrecords.Tobespe- withthehelpofseveralstafexperts,wemanually screenedrecordswithhighqualityinterviewassessmentwrit- tenbyseniorinterviewers,andremovedtherecordswhich lackdetailsinjobdescriptionorresume.Afterthat,the tereddatasetcontains4,816candidateinterviewrecordsre- latedto409jobpositions.InJLMIA,weempiricallyseted parameters f  2 ; J ; R  E g = f 0 : 01 ; 0.1 ; 0.1 ; 0.1 g .Note that,ourmodelistrainedwithoriginalChinesewords.And forfacilitatingdemonstration,allexperimentalresultswere translatedintoEnglish. 5.2EvaluationofTopicJointLearning Toevaluatetheeffectivenessofjointlearnedtopicsby JLMIA,wetrainedourmodelonallsuccessfuljobinter- viewrecords.Inparticular,wesettheparameters K =10 and C =2 .Table1showsonerandomlyselectedlatenttopicof jobdescriptionandcorrespondingtopicsofresumeandtwo typesofinterviewassessments.Eachtopicisrepresentedby severalwordswiththehighestprobability. Wecanobservethatthetopicofjobdescription,containing ﬁExperienceﬂandﬁFoundationﬂofﬁPHPﬂandﬁWebﬂ,should berelatedtowebdevelopment.Similarly,thecorresponding topicsofresumeandtechnicalinterviewassessmentalsocon- tainfront-end-relatedkeywords,ﬁHTTPﬂ,ﬁWebSiteﬂ,ﬁEle- mentﬂ,ﬁJSﬂandﬁCSSﬂ,whichindicatetheprofessionalskills ofcandidates.Thuswebelievethatourmodelcaneffectively revealthelatentrelationshipamongjobdescription,resume andinterviewassessment.Moreinterestingly,wecanthat topic#1andtopic#11ofresume,whicharebothgenerated fromtopic#1ofjobdescription,containdifferentkeywords, Table1:TopicexampleofJLMIA JobDescription Resume Tech.Interview Com.Interview Topic1 Topic1 Topic1 Topic1 Experience Function Foundation Technology Foundation Management Knowledge Communication Technology Backstage Code Study PHP HTTP Element Knowledge Web Moudle Development Development  Topic11 Topic11 Topic11 Engineer WebSite JS Job Interest WebPage Methods Pressure Development System CSS Solution WebPage Web Elements Like Maintenance Framework Events WorkOvertime whichvalidatetheassumptionthatthediversityofjobdes- criptionislessthanthatofresume.Meanwhile,compared withtechnicalinterviewassessment,therearemorekeywords likeﬁCommunicationﬂ,ﬁPressureﬂorﬁWorkOvertimeﬂin comprehensiveinterviewassessment,whicharerelatedtothe evaluationofpersonalqualities. 5.3PerformanceofPerson-JobFit Here,weevaluatetheperformanceofJLMIAintermsof Person-JobFit.,givenajobdescriptionandare- sume,wetreattheirlatenttopicdistributionslearnedbyour modelastheirrepresentationvectors.Then,wetrainclassic topredictthematchingdegreebetweenthejoband thecandidate.Besides,tofurtherdemonstratetheeffective- nessofourmodel,wealsousethesimilaritiesbetweentheir representationvectorsformeasuringPerson-JobFit. BenchmarkMethods WeselectedLatentDirichletAllocation(LDA)andbag-of- words(BOW)vectorrepresentationasbaselines.ForLDA, wemergedtheresumeofacandidateandthejobsheapplied forasadocumentforlearningthelatenttopics.Andforbag- of-words,wherethe i -thdimensionofeachvectoristhefre- quencyofthe i -thwordofthevocabulary,ititselfisakindof representation.Duetothelimitedspaceandsimilartrendsof results,hereweonlyselectedCosineandKullback-Leibler similaritybasedapproaches,andselectedRandomForests andGBDTasPleasenotethatbecausethesimi- laritybetweentwoBOWvectorsismeaningless,wedidnot treatitasabaselinehere. DataPreparation Differentfromthesimilaritybasedapproaches,onlyonetype ofsamples,i.e.,positivesamples,isrequired,the basedapproachesneedtoprepareunsuitablepairsofjobdes- criptionandresumeasnegativesamplestotrain Althoughwecanintuitivelyregardthehistoricalfailedjob applicationsasnegativesamples,wedonotknowtheexact reasonsbehindthesefailures.Forexample,somefailedap- plicationsarejustduetothelowpayorothersimilar reasonsinoffernegotiation.Therefore,wemanuallygener- atedthesamenumberofnegativesamplestotrain byrandomlyselectingresumesandjobdescriptionsfromthe successfuljobinterviewrecords.Alongthisline,theexperi- mentswillonlyfocusontherepresentationoflatenttopics, whileinterferencefromotherfactorswillbeimpaired.After that,werandomlyselected80%dataformodeltrainingand theother20%datafortest. (a)ROCAUCs (b)PRAUCs Figure2:ThePerson-JobFitperformanceofJLMIAbasedon Cosinesimilarityanddifferentparameters. Table2:ThePerson-JobFitperformanceofdifferentapproaches. ROCAUC PRAUC CosineSimilarity JLMIA 0.8279 0.7935 LDA 0.7026 0.7223 Kullback-Leibler Divergence JLMIA 0.8234 0.8094 LDA 0.6589 0.6579 RandomForest (n estimators=400) JLMIA 0.9012 0.8975 LDA 0.7359 0.7341 BOW 0.6716 0.6761 GBDT (n estimators=100, max depth=9) JLMIA 0.8564 0.8311 LDA 0.7092 0.6810 BOW 0.6531 0.6723 PerformanceAnalysis Toevaluatetheparametersensitivity,wetrainedJLMIAby varyingtheparameter K from 10 to 50 ,andtheparameter C from 1 to 5 .Theperson-jobperformanceofJLMIA basedonCosinesimilarityanddifferentparametersisshown inFigure2(a)and2(b).WecanthattheReceiverOper- atingCharacteristic(ROC)AUCsandPrecision-Recall(PR) AUCsarebothbetterwithsmall K ,andreachthehighest with K =10 and C =2 .Therefore,wechosethebestpara- meters K and C forthefollowingexperiments.Similarly,we alsoevaluatedLDAmodelwithdifferenttopicnumberpara- meters K ,andchose K =30 forotherexperiments. Table2showsthePerson-JobFitperformancesofJLMIA andbaselines.Fromtheresults,wethatourmodelcon- sistentlyoutperformsotherbaselinesinbothsimilaritybased approachesandbasedapproaches.Itindicatesthat JLMIAcaneffectivelycapturethelatentrelationshipbetween jobdescriptionandresume.Moreinterestingly,theperfor- mancesofJLMIAinsimilaritybasedapproachisalsohigher thanmostofbaselines.Itclearlydemonstratestheeffective- nessoftherepresentationlearnedbyJLMIA. 5.4PerformanceofQuestionRecommendation Toevaluatetheperformanceofinterviewquestionrecom- mendationofJLMIA.wecollected1,085interviewques- tionsasthecandidatesetfromhistoricalinterviewassess- ments,andthen,comparedJLMIA( K =10 and C =2 ) withBM25,aclassicinformationretrievalmodelbasedon keywordsmatchingwhichignoresthelatentrelationshipbe- tweenqueriesandquestions.Inouralgorithm,theparameters areempiricallysetas Rel =5 , Div =20 and  =0 : 9 . Werandomlyselected100experienceitemsasthequeries. Foreachquery,werecommend10questionsbyJLMIAand BM25.Then,weasked3seniorinterviewerstoevaluate Table3:ThequestionrecommendationperformanceofJLMIAand BM25with10questionsrecommended Relevance Diversity PersonalQuality JLMIA-TI 8.06 2.90 2.17 JLMIA-CI 7.72 2.84 3.22 BM25 7.14 1.67 1.00 Table4:Thecasestudyofquestionrecommendation. Givenexperience item Iamfamiliarwith HTMLandCSSprogramming ,andhavesome webdevelopment experience. Questions recommendedby JLMIAfor technicalinterview T1.Whatare Ajax and InteractiveModel ?Whatarethedifferences between SynchronousandAsynchronousrequests ?Howtosolve Cross-domainissues ? T2.Whatarethemeaningsof GracefulDegradation and Progres- siveEnhancement ? T3.Howtomaketextcenteredverticallyby CSSprogramming . T4.Whatistheroleofthe HTTPstatuscode ? Questions recommendedby JLMIAfor comprehensive interview C1.Talkabout OSI , TCP/IP and Five-layersNetworkModel . C2.Whatarethedifferencesbetween HTML and XHTML ? C3.Doyouthinkajobisnoteasyforyou? C4.Whatarethedifferencesbetween Scrollbar and JScrollPane ? Questions recommendedby BM25 B1.Whatare web applications? B2.Talkaboutyourunderstandingofthesemanticsof HTML . B3.Please program aread-writelockwithanormalmutex. B4.Talkaboutyourunderstandingofthe web standardsandW3C. theperformanceofrecommendationquestions.Theywere requiredtojudgewhichquestionsarerelevanttothis query,wherethenumberofrelevantquestionsisthe rele- vance measure.Then,theyneededtojudgehowmanydif- ferenttechnicalaspectsmentionedinthoserelevantques- tions,whichis diversity measure,andhowmanyquestions areabout personalquality ,whichshouldbedifferentbe- tweentechnicalinterview(TI)andcomprehensiveinterview (CI).AstheaverageresultsshowninTable3,wecan thatcomparedwithtraditionalkeywordsmatchingbasedap- proachBM25,JLMIAcanrecommendquestionswithmore relevanceanddiversity.Meanwhile,JLMIAalsocanrec- ommendmorequestionsrelatedtopersonalqualities,espe- cially,thenumberofpersonalqualityquestionsforcompre- hensiveinterviewismorethantechnicalinterview,whichdis- tinguishesthedifferentfocusesofthemtwo. Furthermore,toillustratetheeffectivenessofourquestion recommendationapproach,wealsoshowanexampleoftop 4questionsrecommendedbydifferentapproachesinTable4. Obviously,thegivenexperienceitemisaboutwebdevelop- ment.WequestionsrecommendedbyJLMIAcontainall technicalaspectsmentionedinexperienceitem(e.g.,T1,T2 andT3isaboutﬁ CSSandHTMLprogramming ﬂ,andC4is aboutﬁ webdevelopment ﬂ).Also,JLMIArecommendsques- tionsdesignedforﬁHTTPﬂ(i.e.,T4andC1),whichisuseful knowledgeforwebdevelopers.Second,forthecomprehen- siveinterview,JLMIAalsorecommendedquestionstoeval- uatethepersonalqualitiesofcandidates,suchasC2,which isrelatedtothecommunicationabilityandproblemanalysis ability.Last,forthequestionsrecommendedbyBM25,since theymusthavethesamewordsingivenrequirement,the semanticrelationshipbetweenkeywordsareneglected(e.g., ﬁHTTPﬂandﬁwebﬂ).Thus,therecommendedquestionsby BM25donotcontainmoretechnicaldetails. 6RelatedWork RecruitmentAnalysis. Withtheimportanceoftalentsatan alltimehighandtheavailabilityofrecruitmentbigdata,re- cruitmentanalysishasbeenattractingmoreandmoreatten- tions [ Xu etal. ,2016;Zhu etal. ,2016 ] .Asearlyas2006, Malinowski etal. triedtoagoodmatchbetweental- entsandjobsbytwodistinctrecommendationsystems [ Ma- linowski etal. ,2006 ] .In2011,Paparrizos etal. exploitedall historicaljobtransitionsaswellasthedataassociatedwith employeesandinstitutionstopredictthenextjobtransition ofemployees [ Paparrizos etal. ,2011 ] .Recently,besidesthe matchoftalentsandjobs [ R ´ acz etal. ,2016 ] ,researchersare alsodevotedtoanalyzerecruitmentmarketfrommorenovel perspective,suchasmarkettrendanalysis [ Zhu etal. ,2016; Lin etal. ,2017 ] ,careerdevelopmentanalysis [ Li etal. , 2017 ] ,talentcircles [ Xu etal. ,2016 ] andpopularitymea- sureofjobskills [ Xu etal. ,2018 ] .Althoughtheabovestud- ieshaveexploreddifferentresearchaspectsofrecruitment market,fewofthemaredevelopedforenhancingthequality andexperienceofjobinterviews.Tothisend,inthispaper, weproposedanovelapproachforintelligentjobinterview assessmentbyjointlearningofmultipleperspectivesfrom large-scalereal-worldinterviewdata. TextMiningwithTopicModel. Probabilistictopicmod- elsarecapableofgroupingsemanticcoherentwordsintohu- maninterpretabletopics.Asanimportantmemberofarchety- paltopicmodels,LatentDirichletAllocation(LDA) [ Blei etal. ,2003 ] hasalotofextensions [ Zhu etal. ,2014; Mimno etal. ,2009;Pyo etal. ,2015 ] .etc..Amongthem, someworksfocusonmodelingsharedlatenttopicdistribu- tionamongmultiplecategoriesofdocuments,andhavea widerangeofpracticalapplications.Forexample,Mimno et al. [ Mimno etal. ,2009 ] designedapolylingualtopicmodel thatdiscoverstopicsalignedacrossmultiplelanguages.Pyo etal. [ Pyo etal. ,2015 ] proposedanovelmodeltolearn thesharedtopicdistributionbetweenusersandTVprograms forTVprogramrecommendation.Differentfromexistingre- searchefforts,inthispaperwedevelopedanovelmodel JLMIAtojointlymodeljobdescription,candidateresume andinterviewassessment. 7ConcludingRemarks Inthispaper,weproposedanovelapproachforintelligentjob interviewassessmentbylearningthelarge-scalereal-world interviewdata.Tobewedevelopedalatentvari- ablemodelJLMIAtojointlymodeljobdescription,candi- dateresumeandinterviewassessment.JLMIAcaneffectively learntherepresentativeperspectivesofdifferentjobinterview processesfromthesuccessfuljobinterviewrecordsinhistory. Furthermore,weexploitedJLMIAfortworeal-worldappli- cations,namelyperson-jobandinterviewquestionrecom- mendation.Extensiveexperimentsconductedonreal-world dataclearlyvalidatetheeffectivenessofJLMIA,whichcan leadtosubstantiallylessbiasinjobinterviewsandprovidea valuableunderstandingofjobinterviewassessment. 8Acknowledgments Thisworkwaspartiallysupportedbygrantsfromthe NationalNaturalScienceFoundationofChina(Grant No.91746301,61727809,61703386). References [ Bersin,2013 ] JoshBersin.https://www.forbes.com/sites/ joshbersin/2013/05/23/corporate-recruitment- transformed-new-breed-of-service-providers/.2013. [ Blei etal. ,2003 ] DavidM.Blei,AndrewY.Ng,and MichaelI.Jordan.Latentdirichletallocation. J.Mach. Learn.Res. ,3:993Œ1022,March2003. [ Blei etal. ,2017 ] DavidMBlei,AlpKucukelbir,andJonD McAuliffe.Variationalinference:Areviewforstatisti- cians. JournaloftheAmericanStatisticalAssociation , (just-accepted),2017. [ Li etal. ,2017 ] HuayuLi,YongGe,HengshuZhu,Hui Xiong,andHongkeZhao.Prospectingthecareerdevel- opmentoftalents:Asurvivalanalysisperspective.In Pro- ceedingsofthe23rdACMSIGKDDInternationalConfer- enceonKnowledgeDiscoveryandDataMining,Halifax, NS,Canada,August13-17,2017 ,pages917Œ925,2017. [ Lin etal. ,2017 ] HaoLin,HengshuZhu,YuanZuo,Chen Zhu,JunjieWu,andHuiXiong.Collaborativecompany Insightsfromanemployee'sperspective.In Pro- ceedingsoftheThirty-FirstAAAIConferenceon Intelligence,February4-9,2017,SanFrancisco,Califor- nia,USA. ,pages1417Œ1423,2017. [ Malinowski etal. ,2006 ] JochenMalinowski,TobiasKeim, OliverWendt,andTimWeitzel.Matchingpeopleandjobs: Abilateralrecommendationapproach.In SystemSciences, 2006.HICSS'06.Proceedingsofthe39thAnnualHawaii InternationalConferenceon ,volume6,pages137cŒ137c. IEEE,2006. [ Mimno etal. ,2009 ] DavidMimno,HannaMWallach,Ja- sonNaradowsky,DavidASmith,andAndrewMcCallum. Polylingualtopicmodels.In Proceedingsofthe2009Con- ferenceonEmpiricalMethodsinNaturalLanguagePro- cessing:Volume2-Volume2 ,pages880Œ889.Association forComputationalLinguistics,2009. [ Paparrizos etal. ,2011 ] IoannisPaparrizos,BBarlaCam- bazoglu,andAristidesGionis.Machinelearnedjobrec- ommendation.In ProceedingsoftheACMconference onRecommendersystems ,pages325Œ328.ACM,2011. [ Pyo etal. ,2015 ] ShinjeePyo,EunhuiKim,etal.Lda-based topicmodelingforsimilartvusergroupingandtv programrecommendation. IEEEtransactionsoncyber- netics ,45(8):1476Œ1490,2015. [ R ´ acz etal. ,2016 ] G ´ aborR ´ acz,AttilaSali,andKlausDi- eterSchewe. SemanticMatchingStrategiesforJobRe- cruitment:AComparisonofNewandKnownApproaches . SpringerInternationalPublishing,2016. [ Tang etal. ,2014 ] FangshuangTang,QiLiu,HengshuZhu, EnhongChen,andFeidaZhu.Divsocial maximization.In AdvancesinSocialNetworksAnalysis andMining(ASONAM),2014IEEE/ACMInternational Conferenceon ,pages455Œ459.IEEE,2014. [ WangandBlei,2011 ] ChongWangandDavidM.Blei. Collaborativetopicmodelingforrecommending articles.In Proceedingsofthe17thACMSIGKDDInter- nationalConferenceonKnowledgeDiscoveryandData Mining ,KDD'11,pages448Œ456,NewYork,NY,USA, 2011.ACM. [ WangandMcCallum,2006 ] XueruiWangandAndrewMc- Callum.Topicsovertime:anon-markovcontinuous-time modeloftopicaltrends.In Proceedingsofthe12thACM SIGKDDinternationalconferenceonKnowledgediscov- eryanddatamining ,pages424Œ433.ACM,2006. [ Xu etal. ,2016 ] HuangXu,ZhiwenYu,JingyuanYang,Hui Xiong,andHengshuZhu.Talentcircledetectioninjob transitionnetworks.In Proceedingsofthe22ndACM SIGKDDInternationalConferenceonKnowledgeDiscov- eryandDataMining ,pages655Œ664.ACM,2016. [ Xu etal. ,2018 ] TongXu,HengshuZhu,ChenZhu,PanLi, andHuiXiong.Measuringthepopularityofjobskillsin recruitmentmarket:Amulti-criteriaapproach.In Proceed- ingsoftheThirty-SecondAAAIConferenceon Intelligence,February2-7,2018,NewOrleans,Louisiana, USA. ,2018. [ Zhang etal. ,2014 ] YingyaZhang,ChengYang,andZhixi- angNiu.Aresearchofjobrecommendationsystembased oncollaborativeIn ComputationalIntelligence andDesign(ISCID),2014SeventhInternationalSympo- siumon ,volume1,pages533Œ538.IEEE,2014. [ Zhu etal. ,2014 ] ChenZhu,HengshuZhu,YongGe,En- hongChen,andQiLiu.Trackingtheevolutionofso- cialemotions:Atime-awaretopicmodelingperspective. In 2014IEEEInternationalConferenceonDataMining, ICDM2014,Shenzhen,China,December14-17,2014 , pages697Œ706,2014. [ Zhu etal. ,2016 ] ChenZhu,HengshuZhu,HuiXiong, PengliangDing,andFangXie.Recruitmentmarkettrend analysiswithsequentiallatentvariablemodels.In Pro- ceedingsofthe22ndACMSIGKDDInternationalCon- ferenceonKnowledgeDiscoveryandDataMining ,pages 383Œ392.ACM,2016. 9Appendix InthisappendixwegivesomedetailsoftheEM-stylealgo- rithmofvariationalinferenceoutlinedinsection3.2 Firstofall,weeachvariationaldistributionterm ofthevariationalfamiliesinEquation2.Tobethe variationaldistributionofeachtopicproportionvector  J m is Dirichletparameterizedbyvector  J m .Thevariationaldistri- butionof  A md;k ,the k -thdimensionoftopicproportionvector  A md ,isunivariateGaussians f  A md;k ; 2 g .Thevariationaldis- tributionof z J mj , z R mdr and z E mde arebyfreeMulti- nomialwithparameters ˚ J mj; 1: K ;˚ R mdr; 1: CK and ˚ E mde; 1: CK respectively.Thevariationaldistributionof ' J k , ' R k , ' ET k and ' EC k areDirithletparameterizedby  J k; 1: j V J j ,  R k; 1: j V R j ,  ET k; 1: j V E j and  EC k; 1: j V E j ,where j V J j , j V R j and j V E j arethe lengthsofvocabulariesofjobdescription,resumeandinter- viewassessment,respectively. Actually,weeachtermof ELBO inJLMIAis similartosomepartsof ELBO inLDAmodel [ Blei et al. ,2003 ] orCTMmodel [ WangandBlei,2011 ] ,except E q [ logp (  A md j  J ; 2 )] ,whichcanbecomputedby: E q [ logp (  A md j  J ; 2 )]= E q [ logN (  A md j h (  J m ;C ) ; 2 I )]=  CK 2 ( log 2 + log 2 ˇ )  1 2  2 CK X k =1 E q [(  A md;k  log J m;k 0 ) 2 ] ; E q [(  A md;k  log J m;k 0 ) 2 ]=  2 + 0 (  J m;k 0 )   0 ( j  J m; 1: K j ) +(  A md;k    J m;k 0 )+ j  J m; 1: K j )) 2 ; whereweassumethat j  J m; 1: K j = K P i =1  J m;k ,and k 0 = kmodK .Similarsymbolsarenotdescribedlaterforsimplic- ity.Andthe   ) isDigammafunctionwithderivative  0 (  ) . Then,wedescribeourEM-stylealgorithm.InE-step,we employcoordinateascentapproachtooptimizeallvariational parameters.First,weoptimizethe  md inEquation4: ^  md = CK X k =1 exp f  A md;k +  2 = 2 g : Second,weoptimize ˚ J mj; 1: K , ˚ R mdr; 1: CK and ˚ E mde; 1: CK foreachcoordinate.Assumethat w J mj = c , w R mdr = t and w E mde = i , I mde = TI : ^ ˚ J mj;k / exp f   J k;c )   j  J k; 1: j V J j j )+  J m;k )   j  J m; 1: K j ) g ; ^ ˚ R mdr;k / exp f   R k;t )   j  R k; 1: j V R j j )+  A md;k g ; ^ ˚ E mde;k / exp f   ET k;i )   j  ET k; 1: j V E j j )+  A md;k g : Third,weoptimize  J m .Duetonoanalyticsolution,weuse Newton'smethodforeachcoordinate: dELBO  J m;i =  1 2  2 D m X d =1 CK X k =1    J m;k 0 )   j  J m; 1: K j )   A md;k )  (  ( k 0 ;i  0 (  J m;k 0 )   0 ( j  J m; 1: K j ))+  ( k 0 ;i  00 (  J m;k 0 )   00 ( j  J m; 1: K j )  + K X k =1 ( j ˚ J m 1: N J m ;k j +  k   J m;k )(  ( k;i  0 (  J m;i )   0 ( j  J m; 1: K j )) ; wherefunction  ( x;y )=1 ,onlyif x = y ,otherwise,  ( x;y )=0 . Fourth,weoptimize  A md; 1: CK .Duetonoanalyticsolution, again,weuseconjugategradientalgorithmwithderivative: dELBO  A md;k =  1  2   A md;k    J m;k 0 )+ j  J m; 1: K j )  + j ˚ R md 1: N R md ;k j + j ˚ E md 1: N E md ;k j ( N R md + N E md )   1 md exp f  A md;k +  2 = 2 g : Last,weoptimize  J ,  R ,  ET and  EC .Theircalculation processaresimilar,token  J k;c and  ET k;i asexamples:  J k;c =  J c + M X m =1 N J m X j =1 ˚ J mj;k  ( w J mj ;c ) ;  ET k;i =  E i + M X m =1 D m X d =1 N E md X e =1 ˚ J mde;k  ( w E mde ;i )  ( I mde ;TI ) : IntheM-step,wemaximizethe ELBO withrespectto parameter  ,similartoLDA,andregardtheotherhyper- parametersin  asedparameters.  
ImprovedNeuralMachineTranslationwithSMTFeatures WeiHe,ZhongjunHe, HuaWu, andHaifengWang BaiduInc.No.10,Shangdi10thStreet,Beijing,100085,China {hewei06,hezhongjun,wu hua,wanghaifeng }@baidu.comAbstractNeuralmachinetranslation(NMT)conductsend-to-end translationwithasourcelanguageencoderandatargetlan-  guagedecoder,makingpromisingtranslationperformance. However,asanewlyemergedapproach,themethodhassome limitations.AnNMTsystemusuallyhastoapplyavocabu-  laryofcertainsizetoavoidthetime-consumingtrainingand decoding,thusitcausesaseriousout-of-vocabularyproblem. Furthermore,thedecoderlacksamechanismtoguaranteeall  thesourcewordstobetranslatedandusuallyfavorsshort translations,resultinginﬂuentbutinadequatetranslations.In ordertosolvetheaboveproblems,weincorporatestatisti- calmachinetranslation(SMT)features,suchasatranslation modelandan n-gramlanguagemodel,withtheNMTmodel underthelog-linearframework.Ourexperimentsshowthat theproposedmethodsigniﬁcantlyimprovesthetranslation qualityofthestate-of-the-artNMTsystemonChinese-to-  Englishtranslationtasks.Ourmethodproducesagainofup to2.33BLEUscoreonNISTopentestsets. Introduction Neuralnetworkshaverecentlybeenappliedtomachine translationandbeguntoshowpromisingresults.Sutskever,  Vinyals,andLe(2014)andBahdanau,Cho,andBen-  gio(2014)directlybuiltneuralnetworkstoperformend-to-  endtranslation,namedneuralmachinetranslation(NMT).  Typically,anNMTsystemcontainstwocomponents,anen-  coderthatconvertsasourcesentenceintoavector,anda decoderthatgeneratestargettranslationbasedonthevector. ThestrengthofNMTliesinthatthesemanticandstruc- turalinformationcanbelearnedbytakingglobalcontext intoconsideration.However,asanewlyemergedapproach,  theNMTmethodhassomelimitationsthatmayjeopardize  itsabilitytogeneratebettertranslation. 1.Toreducemodelcomplexity,anNMTsystemusually usesthetop- Nfrequentwordsinthetrainingcorpus andregardsotherwordsasunseenones,whichcauses  aseriousout-of-vocabulary(OOV)problem.WhenOOV  wordsoccurinthesentencestobetranslated,thetransla-  tionqualitywouldbebadlyhurt. Correspondingauthor:ZhongjunHehezhongjun@baidu.com. Copyright c2016,AssociationfortheAdvancementofArtiﬁcial Intelligence(www.aaai.org).Allrightsreserved. 2.TheNMTdecoderlacksamechanismtoguaranteethat allthesourcewordsaretranslatedandusuallyfavors  shorttranslations.Thissometimesresultsinaninadequate  translationthatdoesnotconveythecompletemeaningof  sourcesentence. 3.NMTmodelscannotmakeuseoflargeamountoftarget monolingualcorpus.Therefore,itisdifﬁcultforanNMT  systemtobeneﬁtfromtargetlanguagemodeltrainedon targetmonolingualcorpus,whichisproventobeuse- fulforimprovingtranslationqualityinstatisticalmachine  translation(SMT). Luongetal.(2015)usedadictionarytotranslatethe OOVwordsinapost-processingstep.Gulcehreetal.(2015)  proposedtwowaystointegratearecurrentneuralnetwork  (RNN)basedlanguagemodelintotheNMTmodel.How-  ever,thesemethodsonlyfocusononeoftheaboveNMT  problems.Intuitively,theseproblemscouldbealleviatedwithsome oftheSMTcomponents,suchasthetranslationtable,the n-gramlanguagemodel.Nevertheless,thecurrentNMT frameworksuffersfromafactthatitisdifﬁculttoaddef-  fectivefeaturesintothemodeltofurtherimprovetranslation  quality. Inthispaper,weproposetoimproveNMTbyintegrat- ingSMTfeatureswiththeNMTmodelunderthelog-linear  framework.Weincorporate3SMTfeatures,includingthe  translationmodel,thewordrewardfeatureandthe n-gramlanguagemodel.Thetranslationmodelistrainedonword-  alignedbilingualcorpuswiththeconventionalphrase-based  SMTapproach(Koehn,Och,andMarcu2003),andem-  ployedtoscorewordpairsandalleviatetheOOVproblem.  Thewordrewardfeaturecontrolsthelengthofthetransla-  tion.Andthe n-gramlanguagemodelaimstoenhancethe localﬂuencywhichistrainedontargetmonolingualsen-  tences.Comparedtopreviousmethods,ourmethodhasthefol- lowingadvantages: 1.Thelog-linearframeworkmakesanNMTsystembeeas- ilyextended.Itcanbeintegratedwitheffectivefeatures  usedinconventionalSMTmodels. 2.Weintegrateawordtranslationtableintothelog-linear frameworkwiththetranslationprobabilitiesestimated  fromtheword-alignedbilingualcorpuswhichistrained Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)dajia doubei chuanran releihengliu− x1− x2−  x3−  x4−  x5−  x6−  x7− h1−  h1−  h2−  h2−  h3−  h3−  h4−  h4−  h5−  h5−  h6−  h6−  h7−  h7Allthepeopleareinfectedandtears UNKEOS− y1− y2− y3− y4− y5− y6− y7− y8− y9− s0− s1− s2− s3− s4− s5− s6− s7− s8− s9− c1− c2− c3− c4− c5− c6− c7− c8− c9DECODERENCODER˜f:x :henc:c :s dec:y :˜e:Figure1:IllustrationoftheRNNEncoder-Decoder.NotethatthereisaUNKsymbolonthetargetsidebecauseoftheOOV problem.onIBMmodels(Brownetal.1993).Thetranslationta-  blecannotonlybeusedtorecoverthetranslationofsuch  wordsthataretakenasunknownwordsintheNMTsys-  tem,butalsoprovidesanotherwaytomeasurethecorre-  lationbetweensourceandtargetwords.Thetranslation  tableisintegratedintothedecodingprocedureratherthan  usedinapost-processstep(Luongetal.2015). 3.ToalleviatetheinadequatetranslationprobleminNMT, weuseawordrewardfeaturetomakethedecoderfavor  longtranslation,ratherthanusingacoveragevectorthat  guaranteesallsourcewordsaretranslated. 4.Weaddan n-gramlanguagemodeltothelog-linear framework,soastomakefulluseofthelarge-scalemono-  lingualcorpustofurtherimprovetranslationquality.The languagemodelscorealongwithotherfeaturesisusedto  selectcandidatesduringdecoding.Whileintheshallow  fusion,Gulcehreetal.(2015)usedthelanguagemodelto rescoretop- NcandidatesgeneratedbytheNMTdecoder. Ourapproachisdifferentfromtheconventionalmeth- odswhichintegratedneuralnetworksintoSMTsystems (Devlinetal.2014;Aulietal.2013;Choetal.2014;  Li,Liu,andSun2013;Zhaietal.2013).Themaindifference isthattheconventionalmethodsareconductedwithinan  SMTframework.OursystemisanNMTsystem,enhanced  byeffectiveSMTfeatures. Wecarriedoutexperimentswithanopen-sourceNMT systemGroundHog 1(Bahdanau,Cho,andBengio2014). 1https://github.com/lisa-groundhog/GroundHog ThesystembuildstwoRNNstoperformend-to-endtrans-  lation:oneasanencoderandtheotherasadecoder.We  trainedthesystemwithalargeamountcorpus(contain-  ingabout200millionsentencepairs)collectedfromthe  web.ExperimentsonChinese-to-Englishtranslationtasks  demonstratethattheproposedmethodachievessigniﬁcant  improvementsoverthestate-of-the-artNMTsystem. Background ThissectionbrieﬂyreviewstheRNNencoder-decoder,are-  centlyproposedNMTapproachbasedonrecurrentneural  network,andthelog-linearmodels,thedominantframework  forSMTinthelastdecade. RNNEncoder-Decoder Figure1showsthetranslationprocedureoftheRNN  encoder-decoder(Bahdanau,Cho,andBengio2014)for Chinese-to-Englishtranslation.Givenasourcesentence ˜f=f1,f2,...,f I,theencoderﬁrstencodes ˜fintoasequence ofvectors,thenthedecodergeneratesthetargettranslation ˜e=e1,e2,...,e Jbasedonthevectorsandthetargetwords previouslygenerated. TheencoderisabidirectionalRNN(SchusterandPaliwal 1997)withahiddenlayer.Attheencodingstep,theencoder  ﬁrstlyprojectstheinputsentence ˜fintowordvectors x =(x1,x2,...,x I),xiRKx,where Kxisthevocabularysize ofthesourcelanguage.Thenthenetworkupdatesthehidden statehiencateachstepby hienc=genc(xi,hi−1enc)(1)where,gencisanactivationfunction,e.g.the tanhfunction.hienc=[−− hienc,−− hienc]istheconcatenationoftheforward andbackwardhiddenstatescalculatedbasedonthesource sentence.Atthedecodingstep,theprobabilityoftheoutputse- quenceiscomputedas: p(y )=Jj=1p(yj|{yj−1,yj−2,...,y 1}x)(2)=Jj=1gdec(sjdec,yj−1,cj)(3)where,sj decisthehiddenstateatstep j,whichiscom- putedby, sj dec=gdec(sj−1dec,yj−1,cj)(4)gdecandgdecarenon-linearactivationfunctions.Thecon- textvector cjiscomputedasaweightedsumofthehidden statesoftheencoder: cj=Txi=1jihienc(5)where,theweight jicanbeconsideredasanassociation measurethathowwellatargetword yjistranslatedfroma sourceword xi.Bahdanau,Cho,andBengio(2014)used afeed-forwardneuralnetworktoparametrizeanalignment modeltoestimate .Thisisanimportantdifferencefromthe basicRNNencoder-decoderproposedbyChoetal.(2014),  whichencodesthesourcesentenceintoasinglevectorwith  ﬁxedlength,thusunabletoreﬂectthestrengthoftherela-  tionshipbetweensourceandtargetwords. FollowingChoetal.(2014),Bahdanau,Cho,andBen- gio(2014)alsousedtwotypesofhiddenunits, reset gatesandupdategates.The reset gatesallowthenetworkstoig- noretheinformationofsomeprevioushiddenstates,which  maybenoisyforthecurrentstate.The updategatescontrol thedegreeoftheinformationbeingtransferedtothecurrent  statefromthepreviousstates.Theroleofthetwokindsof  gatesisanalogoustothelong-short-term-memory(LSTM)  (Sutskever,Vinyals,andLe2014),butmuchsimpler. TheRNNencoder-decoderistrainedonbilingualcorpora andperformsanend-to-endtranslation.However,underthe  currentarchitecture,itisdifﬁculttoimprovethetranslation  qualitybyintegratingadditionaltranslationknowledge. Log-linearModels Thewidelyusedlog-linearframeworkinSMTwasintro-  ducedbyOchandNey(2002). p(˜e|˜f)=exp(mi=1iHi(˜f,˜e))˜eexp(m i=1iHi(˜f,˜e))(6)where,Hi(˜f,˜e)isafeaturefunctionand iistheweight. Thestrengthofthelog-linearmodelisthatfeaturescanbe easilyaddedintoit.Astandardphrase-basedSMT(Koehn, Och,andMarcu2003)typicallycontains8features:thebi-  directionaltranslationprobabilities p(f|e)andp(e|f),the x1x2...xIcjsjyj...yj−n+1...yj−1n-gramlangugemodel wordtranslatio nprobabilitiesRNNwordprediction jijip(yj|xi)Figure2:IllustrationoftheLog-linearNMT.Topredict thetargetword yj,weintroduceSMTfeatures,suchasthe wordtranslationprobabilities,the n-gramlanguagemodel, togetherwiththeprobabilitiesestimatedbyRNNs. bi-directionallexicalweights plex (f|e)andplex (e|f),the languagemodel,thereorderingmodel,thewordpenalty,and thephrasepenalty.Thesefeatureshavebeenproveneffec-  tivetoimprovetranslationquality. Recently,someresearchersintegratedneuralnetworksas acomponentintoSMTsystems,toimprovelanguagemod-  eling(Devlinetal.2014),translationmodeling(Aulietal.  2013;Choetal.2014),andphrasereordering(Li,Liu,and Sun2013;Zhaietal.2013). Theimportantdifferencebetweenourmethodandthepre- viousmethodsisthatweinsteadintegrateSMTfeatureswith theNMTmodelviathelog-linearframework,makingthe  NMTmodelextendable. Log-linearNMT WebelievethatintegratingSMTfeaturesmighthelpim-  provetranslationqualityfortheNMTsystems.WeuseFig-  ure2toillustrateouridea.Ateachsteptopredictatar-  getword yj,inadditiontotheprobabilitiesestimatedby RNN,weaddawordtranslationtableandan n-gramlan- guagemodel.Thetranslationtable,estimatingfromword-  alignedbilingualcorpus,canimprovelexicaltranslationand  translatethelow-frequencywordswhicharetakenasun-  knownwords.Thelanguagemodelcanmakefulluseoftar-  getmonolingualcorpustoimprovelocalﬂuency.Weusea  log-linearframeworktointegratetheseeffectivefeatures. FeatureDeﬁnition Ourmethodincludesthefollowingfeaturefunctions: 1.TheRNNencoder-decoderfeature.Thisfeatureisthe conditionalprobabilityestimatedbytheNMTmodelthat  predictsatargetwordbasedonthesourcesentenceand  previouslyproducedtargetwords. Hrnn =Jj=1log (g(yj−1,sj,cj))(7)2.Thebi-directionalwordtranslationprobabilities.Ateach stepofdecoding,weestimatethelexicaltranslationprob- abilitiesbetweentargetcandidatesandthecorresponding sourcewords. Htp1=Jj=1Ii=1jilog (p(yj|xi))(8)Htp2=Jj=1Ii=1jilog (p(xi|yj))(9)where,jiistheweightedsoftalignmentsbetweenthe targetword yjandassociatedsourcewords,estimated bytheRNNencode-decoder(Section RNNEncoder- Decoder).p(y|x)andp(x|y)isthewordtranslationprob- abilitiesestimatedfromword-alignedbilingualcorpus,  wherethewordalignmentistrainedwithGIZA++(Och  andNey2004)andthe“grow-diag-ﬁnal”(Koehn,Och,  andMarcu2003)method. Thewordtranslationprobabilitiesarecomputedasfol- lows: p(x|y)=N(x,y )xN(x,y)(10)p(y|x)=N(y,x )yN(y,x)(11)whereN(x,y )istheco-occurrenceofthecorresponding words xandy.3.Thestandard n-gramlanguagemodel. Hlm=Jj=1log (p(yj|yj−1,...,y j−n+1))(12)Thelanguagemodelistrainedontargetmonolingualcor-  pus.Thusthisfeatureallowsustomakeuseofalarge-  scalemonolingualcorpusoftargetlanguage. 4.Thewordrewardfeature. Hwp=Jj=11(13)Thefeatureisthenumberofwordsinthetargetsentence,  whichcouldcontrolanappropriatelengthoftranslation. ComparedwiththeoriginalRNNmodel,weadd threeadditionalfeaturesforeachstategeneratedbythe RNNdecoderduringdecoding.Thetranslationcanbe  generatedfromtheﬁnalstatewiththehighesttotalscore. HandlingtheOOVProblem Asmentioned,theNMTencoder-decoderusuallyfacesase-  riousOOVproblem.Thepost-processingmethod(Luong  etal.2015)didnotbeneﬁtfromthecontextualinformation  duringdecoding.Weinsteaduseawordtranslationtable, automaticallyextractedfromword-alignedbilingualcorpus,  totranslatetheOOVwordsduringdecoding. SeeFigure3forillustration.Inordertoproducethecor- recttranslationfortheOOVword,weﬁrstlyﬁnditscor- respondingsourceword.Accordingtothealignmentprob-  abilitiesestimatedbytheRNNmodel,the“UNK”symbol releihengliuandtearsUNK13=0.123=0.233=0.7hengliucrossﬂow0.6 hengliuover0.2 ···translationtable Figure3:Illustrationforrecoveringunknownwordusing translationtable. ijisthealignmentprobabilitiesestimated bytheRNNencoder-decoder. referstothesourceword“ hengliu”.Thenweobtaintrans- lationcandidatesfromthewordtranslationtable.Theﬁnal translationisdeterminedbytheproposedlog-linearmodel,  consideringrichcontextualinformationonboththesource  andtargetsidesduringdecoding. DecodingTheRNNdecoderutilizesabeamsearchalgorithm(Bah-  danau,Cho,andBengio2014)toproducetranslationthat  maximizestheconditionaltranslationprobability, ˆy=argmax ˜yp(˜y|˜x)(14)Givenasourcesentence,thedecoderpredictsandoutputs  targetsentenceswordbyword.Thedecodingstartsfroman  initialstate.Ateachtimestep,thedecoderselectstop- N states( Nisthebeamsize)andexpandsthemuntiltheend- of-sentence(EOS)symbolisproduced.Theﬁnaltranslation  isgeneratedbytracingbackfromtheﬁnalstatewiththe  highestscore. WemodiﬁedthedecoderofGroundHogtoperformtrans- lationunderthelog-linearframework.Ateachdecoding  state,theGroundHogonlyusethescoreestimatedbyRNN  (Equation.7)toselecttop- Ncandidatesfromthetargetvo- cabulary.Inourdecoder,weadditionallycalculatetheword  translationprobabilities,thelanguagemodelscoreandthe  currentsentencelengthateachstate.Foreachwordintar-  getvocabulary,wecalculateatotalscorewithEquation6.  Thenweusethescoretogeneratebettercandidatelistsince  morefeaturesareusedthantheoriginalNMTmodel.The  weightsofthelog-linearmodelsaretunedusingthestan-  dardminimum-error-rate-training(MERT)(Och2003)al-  gorithm.Tospeedupthedecoder,weuseapriorityqueue  tochoosethebestcandidatetobeextended(Huetal.2015)  ratherthanextendingallcandidatesateachstate. ExperimentsWecarriedoutexperimentsonChinese-to-Englishtransla-  tion.Thetrainingcorporaareautomaticallycrawledfrom  theweb,containingabout2.2billionChinesewordsand2.3  billionEnglishwords.Sofarasweknow,thisisthelargest  amountofcorpusthatisusedtotrainanNMTsystem. SystemDEVTEST GroundHog36.7230.61 +TM+WR37.5931.57 +LM38.1532.94 PBSMT33.8229.57 Table1:BLEUscoresondevelopmentandtestsets. TM=translationmodel,WR=wordreward,LM=language  model,PBSMT=phrasebasedSMT. WeusedNISTMT06asthedevelopmentsetandtested oursystemonNISTMT08.Theevaluationmetriciscase-  insensitiveBLEU-4(Papinenietal.,2002).Thefeature  weightsofthetranslationsystemaretunedwiththestan-  dardminimum-error-rate-training(MERT)(Och2003)to  maximizethesystems’BLEUscoreonthedevelopmentset. Weusetheopen-sourceNMTsystem,GroundHog(Bah- danau,Cho,andBengio2014),withdefaultsettingsasour baselinesystem.Wesetbeamsizeto10fordecoding.As  acomparison,wealsoreporttheperformanceofaphrase-  basedSMT(PBSMT)system,whichisare-implementation ofthestate-of-the-artphrase-basedsystem,Moses(Koehn  etal.2007).OurSMTsystemismuchmoreefﬁcientbothon  traininganddecodingonourlargebilingualcorpus,andthe  translationqualityiscomparablewithMoses.FortheSMT  system,wesetthestack-limitto200andthetranslation-  option-limitto20. Training Totrainthe GroundHog system,welimitthevocabulary to30Kmostfrequentwordsforboththesourceandtar-  getlanguages.Otherwordsarereplacedbyaspecialsym-  bol“UNK”.TheencoderconsistsofaforwardRNNand  abackwardRNN,andeachhas1000hiddenunits.The  decoderhas1000hiddenunits.Thewordembeddingsare 620-dimensional.Amini-batchstochasticgradientdescent (SGD)togetherwithAdadelta(Zeiler2012)areusedto  trainthenetworks.Eachmini-batchofSGDcontains50  sentencepairs.Adadeltaisusedtoadaptthelearningrate  ofparameters( =10 −6and=0.95).Weranboththe traininganddecodingonasinglemachinewithoneGPU  card(NVIDIATeslaK10).Thesystemistrainedwithabout  1,570,000updatesfortheRNNencoder. Forthe PBSMTsystem,weobtainedwordalignmentvia theGIZA++(OchandNey2004)andthe“grow-diag-ﬁnal”  (Koehn,Och,andMarcu2003)method.Wetraineda5-gram  languagemodel(Stolcke2002)withKN-discountonthe  targetsideofthebilingualcorpus.Thewordtranslationta-  bleandthelanguagemodelarethenusedasfeaturesbeing  integratedwiththe GroundHog system.ResultsTable1liststheresultsonNISTtestsets.Weobservedthat  theproposedmethodsigniﬁcantlyimprovesthetranslation  qualityoftheconventionalNMTsystem.Moreover,oursys-  temoutperformsthephrase-basedSMTsystemonthesame  largetrainingcorpus. Speciﬁcally,wecandrawthefollowingconclusionsfrom Table1: 1.Byaddingthewordtranslationtableandthewordre- wardfeatures,ourmethodobtainedsigniﬁcantimprove-  mentsoverthebaseline(theresultsareshownintherow  “+TM+WR”).Therearethreemainreasonsfortheim-  provements.Firstly,thetranslationprobabilitieshelpthe  NMTsystemtoperformbetterlexicaltranslation.Sec-  ondly,thetranslationtableisusedtorecovertransla-  tionsofunknownwords.Thirdly,thewordrewardfeature  makesthedecoderfavorslongtranslation.Theaverage  lengthsoftheoutputsonthetestsetofoursystemand  GroundHogare23.5and21.4,respectively.Thisindicates  thatourmethodalleviatestheinadequatetranslationprob-  lem.Furtheranalysesanddiscussionswillbedescribedin  thenextSection. 2.OurmethodallowstheNMTsystemtoincorporatead- ditionallanguagemodels.Weaddeda 5-gramlanguage modeltrainedonthetargetsideofthebilingualcorpus  tothe GroundHog system(theresultsareshowninthe row“ +LM”).Itisobservedthatourmethodobtainedfur-  therimprovementsonthetestset,asthe n-gramlanguage modelcaptureslocaltargetcontextualinformationand  improvetheﬂuency. Comparedwiththe GroundHog system,oursystem (GroundHog+TM+WR+LM )achievesanabsoluteimprove- mentof2.33pointsinBLEUscore,whichisstatistically  signiﬁcantatp=0.01level(RiezlerandMaxwell2005). AnalysisandDiscussion Inordertofurtherstudytheperformanceoftheproposed  method,wecomparedtheoutputsofthesystems. ImprovingLexicalTranslation TakingtheﬁrstsentenceinTable2asanexample,the GroundHogsystemomitsthetranslationof“ (chuanshu)transmission ”.Infact,thetargetwordsareinthevocabulary butnotselectedbytheRNNmodel. Byintegratingatranslationtable,ourmethodproduces thecorrecttranslationforthesourcewordsomitedbythe GroundHog.Thiscanbeattributedtothefactthatthetrans- lationtableconsistsofwordpairswithtranslationprob-  abilitiesestimatedfromtheword-alignedtrainingcorpus,  providinganotherwaytomeasuretherelationshipbetween  sourceandtargetwords.Inthisexample,thetranslationta-  blecontainsthewordpairs“ chuanshu,transmission ”with highprobabilities. Tofurtherimprovethequalityoflexicaltranslation,we employaconventional n-gram( n=5)languagemodeltoim- provethelocalﬂuency.Forexample,thereisanotheren- try“ chuanshu,transfer ”fortheChineseword“ chuanshu ”inthetranslationtable.The n-gramlanguagemodelcould helpthedecoderpredictthecorrecttranslation,because plm(transmission |seriesofhighspeed )isgreater thanplm(transfer |seriesofhighspeed ).SourceR400  PBSMTYes,liketheR400laptop,anexpansionslotofaseriesofhighspeed wirelesstransmission. GroundHogYes,liketheR400laptop,aseriesofhighspeedwirelessexpansionslot. OurMethod Yes,liketheR400laptop,aseriesofhighspeed transmissionofthe wirelessexpansionslot. Source  PBSMTAllofthepeoplehavebeeninfected,crossﬂowoftears. GroundHogAllthepeopleareinfected,andtearsUNK. OurMethod Allthepeopleareinfected,andtears crossﬂow .Table2:Translationexamples.Chinesewordsinboldarecorrectlytranslatedbyoursystem. SystemOOVPercentage DEVTEST PBSMT1.6%1.8% GroundHog4.4%4.6% OurMethod 0.8%0.9% Table3:StatisticsofthepercentagesoftheOOVwordsfor thePBSMT,GroundHogandourmethod. TranslatingtheOOVWords Table3showsthestatisticsoftheOOVwordsfor PBSMT,GroundHog andoursystemsonNIST06andNIST08test sets.ItisobservedthatallthesystemsconfronttheOOV  problembecausethesourcewordsdonotoccurinthetrain-  ingcorpusorthewordpairsarenotlearnedduetotheword  alignmenterror.However,theproblemismuchmoreserious  forthe GroundHog systemsinceitlimitsthevocabularysize toreducethemodelcomplexity. TheOOVwordsharmthetranslationquality.Asshownin thesecondexampleinTable2,thesourceword“ł(hengliu)”  isnottranslatedby GroundHog .Afterintegratingthetrans- lationtablewithinthelog-linearframework,thiswordwas  correctlytranslatedinto“ crossﬂow ”.AsdemonstratedinTable3,ourmethodreducesabout 82%oftheOOVwordsfortheNMTsystem.Moreover,  thenumberofOOVwordsinoursystemishalfofthat  inthePBSMTsystem.Asweknow,PBSMTsystemex-  tractsword/phrasetranslationsfromword-alignedbilingual  corpus.However,constrainedbytheinaccuratewordalign-  ment,notallwordsinthebilingualcorpusarecoveredinthe  phrasetable,causingOOVproblemsinthePBSMTsystem.  Ideally,theRNNencoder-decoderiscapableoftranslating  allthewordsaslongastheyareencodedinthevocabulary.  AsthevocabularyusedintheRNNencoder-decoderislim-  itedforpracticalreasons,addingwordtranslationtableinto  RNNencoder-decodercombinesthestrengthofbothRNN  andPBSMT,leadingtoafurtherreductionofOOVratio. Table4showstheeffectoftranslatingOOVwordswith thetranslationtable.Therow“ OurMethod ”sharesthesame  settingswithSection Experiment.Inthesesettings,thetrans- lationtableisnotonlyusedtoscorewordpairs,butalso SystemDEVTEST OurMethod 38.1532.94 -OOV 37.9032.57 Table4:EffectoftranslatingOOVwords.OurMethod=  GroundHog+TM+WR+LM,-OOVmeansthetranslationta-  bleisnotusedtorecoverOOVwords. Figure4:BLEUscoreswithrespecttothebeamsizes. usedtotranslateOOVwords.Therow“ -OOV ”meansthat  thetranslationtableisonlyusedtoscorewordpairsinthe  vocabularyoftheRNNencoder-decoder,butnotgenerate  newtranslationcandidatesfortheOOVwords.Itisobserved  thatbytranslatingOOVwords,weobtainedanabsoluteim-  provementof0.37pointsinBLEUscoresonNIST08. ImprovingCandidateList Figure4showstheperformancewithrespecttothebeam  sizeinthedecoder.Wevarybeamsizeattesttimewhile  keepingthefeatureweightsafterMERT.Wecanseethatthe  theBLEUscoreofGroundHogisnotimprovedasthebeam  sizeincreasesafter3.Ontheotherhand,theperformanceof  theproposedmethodisimprovedwiththeincreaseofbeam  size.Thereasonisthat,ourmethodusesmorefeaturesthan  GroundHogtogeneratecandidatelists.Withthebeamsize  increasing,morebettercandidatesareselectedfromthetar- getvocabularybythedecoder. ConclusionandFutureWork Inthispaper,weimproveNMTbyintegratingadditional SMTcomponents(e.g.thetranslationtable,thelanguage  model)underthelog-linearframework,whichmakesthe  NMTapproachbeeasilyextended.Thetranslationtable  istrainedonword-alignedbilingualcorpusviathestan-  dardphrase-basedSMTmethod,andthelanguagemodel  istrainedonmonolingualtargetsentences.Theproposed  methodalleviatesmajorlimitationsofthecurrentNMTar-  chitecture.Thetranslationtablerecoverstheomittedtransla-  tionsofsourcewordsandtheOOVwords,andthelanguage  modelincreaseslocalﬂuencybymakingfulluseofmono-  lingualcorpus.ExperimentsonChinese-to-Englishtrans-  lationtasksshowthatoursystemachievessigniﬁcantim-  provementsoverthebaselineonlargeamountofthetraining  corpuscrawledfromtheweb. Asanewapproach,NMTstillhasmoreroomforim- provement.CurrentRNNencoder-decoderisactuallya  word-basedtranslationsystem.Inthefuture,weplantoim-  proveNMTwithphrasepairs,whicharegoodatcapturing  localwordreordering,idiomtranslation,etc. Acknowledgements ThisresearchissupportedbytheNationalBasicResearch  ProgramofChina(973programNo.2014CB340505).We  wouldliketothankXuanLiuandtheanonymousreviewers  fortheirinsightfulcomments. References Auli,M.;Galley,M.;Quirk,C.;andZweig,G.2013.Joint  languageandtranslationmodelingwithrecurrentneuralnet- works.In Proceedingsofthe2013ConferenceonEmpirical MethodsinNaturalLanguageProcessing ,10441054. Bahdanau,D.;Cho,K.;andBengio,Y.2014.Neuralma- chinetranslationbyjointlylearningtoalignandtranslate.In arXiv:1409.0473[cs.CL] .Brown,P.F.;Pietra,S.A.D.;Pietra,V.J.D.;andMer- cer,R.L.1993.Themathematicsofstatisticalmachine translation:Parameterestimation. ComputationalLinguis- tics19(2):263–311.Cho,K.;vanMerrienboer,B.;Gulcehre,C.;Bahdanau,D.; Bougares,F.;Schwenk,H.;andBengio,Y.2014.Learning phraserepresentationsusingrnnencoder-decoderforstatis- ticalmachinetranslation.In Proceedingsofthe2014Con- ferenceonEmpiricalMethodsinNaturalLanguagePro-  cessing(EMNLP) ,17241734. Devlin,J.;Zbib,R.;Huang,Z.;Lamar,T.;Schwartz,R.;and Makhoul,J.2014.Fastandrobustneuralnetworkjointmod-  elsforstatisticalmachinetranslation.In Proceedingsofthe 52ndAnnualMeetingoftheAssociationforComputational  Linguistics,13701380. Gulcehre,C.;Firat,O.;Xu,K.;Cho,K.;Barrault,L.;Lin, H.-C.;Bougares,F.;Schwenk,H.;andBengio,Y.2015.On  usingmonolingualcorporainneuralmachinetranslation.In  arXiv:1503.03535[cs.CL] .Hu,X.;Li,W.;Lan,X.;Wu,H.;andWang,H.2015.Op- timizedbeamsearchwithconstrainedsoftmaxfornmt.In MTSummitXV .Koehn,P.;Hoang,H.;Birch,A.;Callison-Burch,C.;Fed- erico,M.;Bertoldi,N.;Cowan,B.;Shen,W.;Moran,C.;  Zens,R.;Dyer,C.;Bojar,O.;Constantin,A.;andHerbst,  E.2007.Moses:Opensourcetoolkitforstatisticalmachine  translation.In ACL2007demonstrationsession .Koehn,P.;Och,F.J.;andMarcu,D.2003.Statisticalphrase-  basedtranslation.In ProceedingsofHLT-NAACL2003 ,127–133.Li,P.;Liu,Y.;andSun,M.2013.Recursiveautoencoders foritg-basedtranslation.In ProceedingsoftheConfer- enceonEmpiricalMethodsinNaturalLanguageProcess-  ing,567577. Luong,M.-T.;Sutskever,I.;Le,Q.V.;Vinyals,O.;and Zaremba,W.2015.Addressingtherarewordproblemin neuralmachinetranslation.In Proceedingsofthe53rdAn- nualMeetingoftheAssociationforComputationalLinguis-  ticsandthe7thInternationalJointConferenceonNatural  LanguageProcessing(Volume1:LongPapers) ,11–19. Och,F.J.,andNey,H.2002.Discriminativetrainingand maximumentropymodelsforstatisticalmachinetranslation.  InProceedingsofthe40thAnnualMeetingoftheAssocia- tionforComputationalLinguistics ,295–302. Och,F.J.,andNey,H.2004.Thealignmenttemplateap- proachtostatisticalmachinetranslation.30:417–449. Och,F.J.2003.Minimumerrorratetraininginstatisti- calmachinetranslation.In Proceedingsofthe41stAnnual MeetingoftheAssociationforComputationalLinguistics ,160–167.Riezler,S.,andMaxwell,J.T.2005.Onsomepitfallsin automaticevaluationandsigniﬁcancetestingformt.In Pro- ceedingsoftheACLWorkshoponIntrinsicandExtrinsic EvaluationMeasuresforMachineTranslationand/orSum-  marization,5764. Schuster,M.,andPaliwal,K.K.1997.Bidirectionalrecur- rentneuralnetworks. SignalProcessing IEEETransactions on45(11),2673–2681. Stolcke,A.2002.Srilm–anextensiblelanguagemodeling toolkit.In ProceedingsoftheInternationalConferenceon SpokenlanguageProcessing ,volume2,901–904. Sutskever,I.;Vinyals,O.;andLe,Q.V.2014.Sequenceto sequencelearningwithneuralnetworks.In NIPS2014 .Zeiler,M.D.2012.Adadelta:Anadaptivelearningrate  method.In arXiv:1212.5701[cs.LG] .Zhai,F.;Zhang,J.;Zhou,Y.;andZong,C.2013.Rnn-based  derivationstructurepredictionforsmt.In Proceedingsofthe 52ndAnnualMeetingoftheAssociationforComputational Linguistics,779784.  
EXPLORINGNEURALTRANSDUCERSFOREND-TO-ENDSPEECHRECOGNITION EricBattenberg,JitongChen,RewonChild,AdamCoates,YasheshGaur,YiLi, HairongLiu,SanjeevSatheesh,DavidSeetapun,AnuroopSriram,ZhenyaoZhu BaiduSiliconValleyAILab ABSTRACT Inthiswork,weperformanempiricalcomparisonamongthe CTC,RNN-Transducer,andattention-basedSeq2Seqmodels forend-to-endspeechrecognition.Weshowthat, without anylanguagemodel,Seq2SeqandRNN-Transducermod- elsbothoutperformthebestreportedCTCmodels with a languagemodel,onthepopularHub5'00benchmark.On ourinternaldiversedataset,thesetrendscontinue-RNN- Transducermodelsrescoredwithalanguagemodelafter beamsearchoutperformourbestCTCmodels.Theseresults simplifythespeechrecognitionpipelinesothatdecodingcan nowbeexpressedpurelyasneuralnetworkoperations.We alsostudyhowthechoiceofencoderarchitectureaffectsthe performanceofthethreemodels-whenallencoderlayers areforwardonly,andwhenencodersdownsampletheinput representationaggressively. 1.INTRODUCTION Inrecentyears,deepneuralnetworkshaveadvancedthestate- of-the-artonlargescaleautomaticspeechrecognition(ASR) tasks[24,28,2].Deepneuralnetworkscannotonlyextract acousticfeatures,whichareusedasinputstotraditionalASR modelslikeHiddenMarkovModels(HMM)[24,28],butalso actassequencetransducers,whichresultsinend-to-endneu- ralASRsystems[2,6]. Onemajorchallengeofsequencetransductionisthat theinputandoutputsequencesdifferinlengths,andboth lengthsarevariable.Asaresult,aspeechtransducerhasto learnboththealignmentandthemappingbetweenacoustic inputsandlinguisticoutputssimultaneously.Severalneural network-basedspeechmodelshavebeenproposedduringthe pastyearstosolvethischallenge.Inthiswork,wefocus onunderstandingthedifferencesbetweenthesetransduction mechanisms.,wecomparethreetransduction models-ConnectionistTemporal(CTC)[12], RNN-Transducer[11],andsequence-to-sequence(Seq2Seq) withattention[5,3].FortheASRtask,thesemodelsdiffer mainlyalongassumptionsmadeinthesethreeaxes:  Conditionalindependencebetweenpredictionsatdif- ferenttimesteps,givenaudio .Thisisnotareason- ableassumptionfortheASRtask.CTCmakesthisas- sumption,butRNN-TransducersandAttentionmodels donot.  Thealignmentbetweeninputandoutputunitsismono- tonic .ThisisareasonableassumptionfortheASR task,whichenablesmodelstodostreamingtranscrip- tion.CTCandRNN-Transducersmakethisassump- tion,butAttentionmodels 1 donot.  HardvsSoftalignments .CTCandRNN-Transducer modelsexplicitlytreatalignmentbetweeninputand outputasalatentvariableandmarginalizeoverallpos- siblehardalignmentswhiletheattentionmechanism modelsasoftalignmentbetweeneachoutputstepand everyinputstep.ItisunclearifthismatterstotheASR task. Therearenoconclusivestudiescomparingthesearchitec- turesatscale.Inthiswork,wetrainallthreemodelsonthe samedatasetsusingthesamemethodology,inordertoper- formafaircomparison.Modelswhichdonotassumecon- ditionalindependencebetweenpredictionsgiventhefullin- put( viz, RNN-Transducers,Attention)areabletolearnan implicitlanguagemodelfromthetrainingcorpusandopti- mizeWERmoredirectlythanothermodels.Wethat theythereforeperformquitecompetitively,evenoutperform- ingCTC+LMmodelswithouttheuseofanexternallan- guagemodel.Amongthem,RNN-Transducershavethesim- plestdecodingprocedureandfewerhyper-parameterstotune. Inthefollowingsections,wewillrevisitthethree models,anddescribeinterestingdetailsofourimple- mentations.Then,insection3,wepresentourresultsonthe Hub5'00benchmark(whichuses 2000 hoursoftrainingdata), andourowninternaldataset(of 10 ; 000 hours).Insection4we studyhowwelltheytrainwhenusingonlyforward-onlylay- ers,andwhenwedoexcessivepoolingintheencoderlayers ontheWSJdatasetbycontrollingthenumberofparameters ineachmodel.Section6presentsrelatedworkandSection7 summarizesthekeytakeawaysandpresentsthescopeoffu- turework. 2.NEURALSPEECHTRANSDUCERS Aspeechtransduceristypicallycomposedofan encoder (alsoknownasacousticmodel),whichtransformstheacous- ticinputsintohighlevelrepresentations,anda decoder , 1 HerewefocusonthevanillaSeq2Seqmodelswithfullattention[6,3], thoughthereexistsomeeffortsinenforcinglocalandmonotonicattention recently,andtheytypicallyresultsinalossinperformance arXiv:1707.07413v1  [cs.CL]  24 Jul 2017(a)CTC (b)RNN-Transducer (c)Attention Fig.1 .Illustrationofprobabilitytransitionsofthreetransducersonanutteranceoflength5andlabelledasﬁCATﬂ.Thenodeat t (horizontalaxis), u (verticalaxis)representstheprobabilityofhavingoutputthe u elementsoftheoutputsequencebypoint t inthetranscriptionsequence.Theverticalarrowrepresentspredictingmultiplecharactersatonetimestep(notallowedfor CTC).Thehorizontalarrowrepresentspredictingrepeatingcharacters(forCTC)orpredictingnothing(forRNN-Transducer). Thesolidarrowsrepresenthardalignments(forCTCandRNN-Transducer)andsoftones(forAttention).Asnoticed,inCTC andRNN-Transducer,statescanonlymovetowardsthetoprightdirectiononestepbyone,whileinAttention,allinputframes couldpotentiallybeattendedinanydecodingstep. whichproduceslinguisticoutputs( i.e, charactersorwords) fromtheencodedrepresentations.Thechallengeistheinput andoutputsequenceshavevariable(alsodifferent)lengths, andusuallyalignmentsbetweenthemareunavailable.So neuraltransducershavetolearnboththefrom acousticfeaturestolinguisticpredictionsaswellasthealign- mentbetweenthem.Transducermodelsdifferintheformu- lationsoftheandthealigner. Moreformally,giventhetheinputsequence x =( x 1 ;::;x T ) oflength T ,andtheoutputsequence y =( y 1 ;:::;y U ) of length U ,witheach y u beinga V dimensionalone-hotvec- tor,transducersmodeltheconditionaldistribution p ( y j x ) . Theencodermapstheinput x intoahighlevelrepresenta- tion h =( h 1 ;:::;h T 0 ) ,whichcanbeshorterthantheinput ( T 0  T )withtime-scaledownsampling.Theencodercan bebuiltwithfeed-forwardneuralnetworks(DNNs)[16],re- currentneuralnetworks(RNNs)[14],orconvolutionneural networks(CNNs)[10].Thedecoderthealignment(s) a andthemappingfrom h to y . 2.1.CTC CTC[12,2]computestheconditionalprobabilitybymarginal- izingallpossiblealignmentsanditassumesconditionalinde- pendencebetweenoutputpredictionsatdifferenttimesteps givenalignedinputs.Anextra`blank'label,whichcanbein- terpretedasnolabel,isintroducedtomap h and y tothesame length, i.e, analignment(path) a isobtainedbyinserting( T 0 - U )blanksinto y .Amapping B : a ! y isbetween a and y ,whichcanbedonebyremovingallblanksandrepeat- inglettersin a .Theconditionalprobability P CTC ( y j x ) can beefcalculatedusingaforward-backwarddynamic- programmingalgorithm,asdetailedin[12].Notethatthe alignments f a g arebothlocalandmonotonic. P CTC ( y j x )= X a 2B  1 ( y ) P ( a j h ) (1) = X a 2B  1 ( y ) T 0 Y t =1 P ( a t j h t ) (2) P ( a t j h t )= softmax ( a t ;h t ) (3) whereweusetheconventionalofsoftmax 2 .The CTCoutputcouldbedecodedbygreedilypickingthemost likelylabelateachtime-step 3 .Tomakebeamsearcheffec- tive,theconditionalindependenceassumptionis brokenbytheinclusionofalanguagemodel,anddecodingis thenthetaskoftheargmaxof log ( P CTC ( y j x ))+  log ( P LM ( y ))+  wordcount ( y ) (4) Thisdecodingisapproximate,andperformedusingbeam search,typicallywithalargebeamorlattice[15,19].The aboveequationpresentsadiscrepancybetweenhowthese modelsaretrainedandtested.Toaddressthis,modelscould befurtherwithalossfunctionthatalsoincorpo- rateslanguagemodelinformationlikesMBR[24],butthe principleissueisstilltheabsenceofdependencebetween predictions. 2.2.RNN-Transducer RNN-Transducer[11,14]alsomarginalizesoverallpossible alignments,likeCTCdoes,whileextendingCTCbyaddition- allymodelingthedependenciesbetweenoutputsatdifferent 2 softmax ( a;v )=exp( v = a ) = P k 2 V exp( v = k ) 3 strictlyspeaking,thisthemostlikelyalignment,not y ,butwe thatforafullytrainedmodel P ( y j x ) isdominatedbyasinglealignment timesteps f y u ;u 2 1 ;::;U g .More,theprediction of y u attimestep u dependsonnotonlyalignedinput h but alsothepreviouspredictions f y <u g . P RT ( y j x )= X a 2B  1 ( y ) P ( a j h ) (5) = X a 2B  1 ( y ) T 0 Y t =1 P ( a t j h t ;y <u t ) (6) where u t donatestheoutputtimestepalignedtotheinput timestep t .Anextrarecurrentnetworkisusedtohelpdeter- mine a t bypredictingdecoderlogits g u = g ( y <u t ) ,andthe conditionaldistributionattime t iscomputedbynormalizing thesummationofthe h t andthe g u t : P ( a t j h t ;y <u t )= P ( a t j e t;u )= softmax ( a t ;e t;u ) (7) e t;u = f ( h t ;g u ) (8) f couldbeanyparametricfunction,weuse e t;u = h t + g u asin[11].LikeinCTC,themarginalizedalignments f a g are localandmonotonic,andthelikelihoodofthelabelcanbe calculatedefusingdynamicprogramming.Decod- ingusesbeamsearchasin[11],butwedonotuselength normalizationasoriginallysuggested,sincewedonotit necessary. 2.3.AttentionModel Attentionmodel[8,3,5]alignstheinputsandoutputsus- ingtheattentionmechanism.LikeRNN-transducer,attention modelremovestheconditionalindependenceassumptionin thelabelsequencethatCTCmakes.UnlikeCTCandRNN- transducerhowever,itdoesnotassumemonotonicalignment, nordoesitexplicitlymarginalizeoveralignments.Itcom- putes p ( y j x ) bypickingasoftalignmentbetweeneachoutput stepandeveryinputstep. P Attn ( y j x )= P ( y j h )= U Y u =1 P ( y u j c u ;y <u ) (9) where c u isthecontextfordecodingtimestep u ,whichis computedasthesumoftheentire h weightedby  (known asattention). c u = T X t =1  u;t h t (10)  u;t =exp( e u;t ) = T X t 0 =1 exp( e u;t 0 )) (11) e u = f ( h ; u  1 ;g u  1 ) (12) where g u isthehiddenstatesofthedecoderatdecodingstep u .Thereexistdifferentways[6,3]tocompute e u .Weused alocation-awarehybridattentionmechanisminourexperi- ments,whichcanbedescribedas: g attn u = AttentionRNN ( y u  1 ;g attn u  1 ) (13) e u = ComputeAttention ( h ; u  1 ;g attn u ) (14) g u = DecoderRNN ( c u ;g attn u ;g u  1 ) (15) Theattentionmechanismallowsthemodeltoattendany- whereintheinputsequenceateachtime,andthusthealign- mentscanbenon-localandnon-monotonic.However,this excessivegeneralitycomeswithamorecomplicateddecod- ingfortheASRtask,sincethesemodelscanbothterminate prematurelyaswellasneverterminatebyrepeatedlyattend- ingoverthesameencodingsteps.Therefore,thedecoding tasktheargmaxof log ( P Attn ( y j x )) = j y j  +  cov (  )+  log ( P LM ( y )) (16) where  isthelengthnormalizationhyperparameter[27].The coveragetermﬁcovﬂencouragesthemodeltoattendoverall encodertimesteps,andstopsrewardingrepeatedattendance overthesametimesteps.Thecoveragetermaddressesboth shortaswellaslongdecoding. 3.PERFORMANCEATSCALE Inthissection,wecomparetheperformanceofthemodelson apublicbenchmarkaswellasourowninternaldataset. Thepromiseofend-to-endmodelsforASRwasthesim- ofthetrainingandinferencepipelinesofspeech systems.End-to-endCTCmodelsonlysimplthetrain- ingprocess,butinferencestillinvolves decoding withmas- sivelanguagemodels,whichoftenrequiresteamstobuild andmaintaincomplicateddecoders.Sinceattentionand RNN-Transducersimplicitlylearnalanguagemodelfromthe speechtrainingcorpus,rescoringordecodingusinglanguage modelstrainedsolelyfromthetextofthespeechcorpus,does notcontributetoimprovementsinWER(Table1).Whenan externalLMtrainedonmoredataisavailable,simplyrescor- ingthebeam(typicallysmall,between32and256) recoversalltheperformancedifference(Table3).Thedecod- ingandbeamsearchisthereforecanbeexpressed asneuralnetworkoperationsandneednotsupportmassive languagemodels.Thistrendisalreadyseenintheneural machinetranslationtasks,wherestate-of-artNMTsystems donottypicallyuseanexternallanguagemodel[27]. 3.1.Hub5'00results TheperformanceofthemodelsontheHub5'00benchmark ispresentedinTable1alongwithotherpublishedresultson in-domain data.AllofthemodelsinTable1usethestandard languagemodelthatispairedwiththedataset,exceptforthe rowsmarkedﬁNOLMﬂ.Withoutusinganylanguagemodel, boththeattentionandRNN-Transducermodelsoutperform Architecture SWBD CH WER WER Published Iterated-CTC[29] 11.3 18.7 BLSTM+LFMMI[21] 8.5 15.3 LACE+LFMMI 4 [28] 8.3 14.8 Dilatedconvolutions[25] 7.7 14.5 CTC+Gram-CTC[17] 7.3 14.7 BLSTM+Featurefusion[23] 7.2 12.7 Ours CTC[17] 9.0 17.7 RNN-Transducer BeamSearch NOLM 8.5 16.4 BeamSearch+LM 8.1 17.5 Attention BeamSearch NOLM 8.6 17.8 BeamSearch+LM 8.6 17.8 Table1 .WERcomparisonagainstpreviouspublishedresults onFisher-SwitchboardHub5'00benchmarkusing in-domain data.Weonlylistresultsusingsinglemodelshere.Allthe previousworksreportedWERusinglanguagemodels.We don'tleverageanyspeakerinformationinourmodels,though ithasbeenshowntoreduceWERinpreviousworks[28,25]. theCTCmodeltrainedonthesamecorpus,andarehighly competitivewiththebestresultsonthisdataset.SincetheLM isalsotrainedonthesametrainingcorpus,rescoringwiththe LMhaslittleeffectonattentionandRNN-Transducermodels. Wefoundthatbeamsearchinattentionworkedbestwhen usingonlylengthnormalization(  =1 ,  =0 inEqua- tion16).However,asthedistributionoferrorsinTable2 show,theRNN-Transducerhasnoobviousproblemswith pre-matureterminationasthenumberofdeletionsisvery smalleventhoughthereisnolengthnormalization.Atten- tionandRNN-Transducerbothuseabeamwidthof32. Model WER Subs Ins Dels CTC 9.0 5.5 2.5 1.0 RNN-Transducer 8.1 4.7 2.6 0.8 Attention 8.6 5.4 1.2 2.0 Table2 .ErrordistributionforSWBDsliceinHub5'00 3.2.DeepSpeechcorpus TheDeepSpeechcorpuscontainsabout 10 ; 000 hoursof speechinadiversesetofscenarios,suchasfarwith backgroundnoise,accentsetc.,Additionally,thetrainand targetssetsaredrawnfromadifferentdistributionsincewe don'thaveaccesstolargevolumesofdatafromthetarget distribution.Werelyonexternallanguagemodelstrainedon largercorpusoftexttoclosethegapbetween trainandtestdistributions.Thissettingthereforeprovidesus Model DevTest CTC[4] Greedydecoding 23.03- Beamsearch+LM(beam=2000) 15.916.44 RNN-Transducer Greedydecoding 18.99- Beamsearch(beam=32) 17.41- +LMrescoring 15.616.50 Attention Greedydecoding 22.67- Beamsearch(beam=256) 18.71- +Length-normweight 19.5- +Coveragecost 18.9- +LMrescoring 16.016.48 Table3 .ComparisonofWERobtainedbydifferenttransduc- tionmodelsontheDeepSpeechdatasetwhichhasamismatch betweentrainingandtestdistributions. Model Prediction GroundTruth SILENCE CTC SILENCE RNN-Transducer SILENCE Attention iwanttogettogettogettogetto gettogettogettogettodothat GroundTruth playtheblackeyedpeassongs CTC +Greedy ladingtoblack irpen songs +BeamSearch+LM leadingtoblack european songs RNN-Transducer +Greedy playtheblack eyepiece songs +BeamSearch playtheblack eyepiece songs +LMrescore playtheblackeyedpeassongs Attention +Greedy playtheblack eyedpea songs +BeamSearch playtheblack eyedpea songs +LMrescore playtheblackeyedpeassongs Table4 .Samplesfromdecodingthesameutteranceacross differentmodelsontheDeepSpeechdevset.Wethat abigreasonfortherelativelyworseWERoftheattention modelcouldbeattributedtoafewutterancesliketheone whichcontributestotheeditdistancealot.Theexam- pleshowsonlygreedydecodingcasesforallthemodels,the secondsetshowshowthepredictionevolvesthroughvarious stagesofdecoding. thebestopportunitytostudytheimpactoflanguagemodels onattentionandRNN-Transducers. Onthedevelopmentset,notethatRNN-Transducermodel matchestheperformanceofthebestCTCmodelwithin1.5 WERwithoutanylanguagemodel,andcompletelycloses thegapbyrescoringtheresultingbeamofonly32candi- dates.Surprisingly,attentionmodelsstartfromaWERsim- ilartothatofCTCmodelsaftergreedydecoding,butthe twoarchitecturesmakeverydifferenterrors.CTCmodels haveapoorerWERmainlybecauseofmis-spellings,butthe relativelyhigherWERofattentionmodelscouldbelargely attributedtonoisyutterances.Inthesecases,theattention modelsactsimilartoalanguagemodelandarbitrarilyout- putcharacterswhilerepeatedlyattendingoverthesameen- codertimesteps.WhilethecoverageterminEquation16 helpsaddressthisissueduringbeamsearch,thegreedyde- codingcannotbeimproved.Anexampleofthissituationis showninTable4.Themonotonicleft-to-rightdecodingof CTCandRNN-Transducersnaturallyavoidtheseissues.Fur- ther,thecoveragetermonlyhelpskeepthecorrectanswersin thebeamandlanguagemodelrescoringofthebeamis stillrequiredtobringthecorrectanswersbacktothetop. 3.3.Experimentaldetails Data .Throughoutthepaper,allaudiodatais sampledat16kHzandnormalizedtoaconstantpower.Log- LinearorLog-Melspectrograms(thetypeoffeatur- izationisahyper-parameterwetuneover)areextractedwith ahopsizeof10msandwindowsizeof20ms,andthenglob- allynormalizedsothateachinputspectrogrambinhaszero meanandunitvariance.Wedonotusespeakerinformation inanyofourmodels.Everyepoch,40%oftheutterancesare randomlyselectedtoaddbackgroundnoiseto. AllmodelsinTable1,weretrainedonthestandard Fisher-SwbddatasetcomprisingoftheLDCcorpora(97S62, 2004S13,2004T19,2005S13,2005T19).Weuseaportionof theRT02corpus(2004S11)forhyper-parametertuning.The languagemodelusedfordecodingtheCTCmodelaswell aswhenrescoringtheothermodelsisthesame4-gramLM availableforthisbenchmarkfromtheKaldireceipe[20].The languagemodelusedbyallmodelsinTable3isbuiltfroma sampleofthecommoncrawldataset[26]. Model .AllmodelsinTables1and3are tunedindependentofeachother-weperformarandom searchoverencoderanddecodersizes,amountofpooling, minibatchsize,choiceofoptimizer,learningandannealing rates.Further,noconstraintsareplacedonanymodel,in termsofnumberofparameters,wallclocktime,orothers. Thetrainingproceduremainlyfollows[2],anduses SortaGrad,andallmodelsusebi-directionalReLUGRU encoderswithbatch-normalizationthroughdepth 5 ,andmay useaconvolutionalfront-end.Inshorthand,[2x2D-Conv (2),3x2560GRU]representsastackof2layersof2D- convolutionfollowedbyastackof3bidirectionalReLU GRU.ﬁ(2)ﬂrepresentsthatthelayerdownsamplestheinput 5 WealsothattheseencoderlayerscouldbereplacedwithLSTM layerswithtanhactivation,weightnoise,andnobatchnormalization.Inmost cases,only512LSTMcellswithweightnoisecanmatchtheperformanceof largeun-regularizedGRUcellswithbatch-normalization by2alongthetimedimension.Inshorthand,thebestCTC modelis[2x2D-Conv(2),3x2560GRU],thebestRNN- Transducer'sencoderis[2x2D-Conv(2),4x2048GRU]and decoderis[3x1024Fwd-GRU].Thebestattentionmodel worksbestwithoutaconvolutionalfront-end,theencoder is[4x2560GRU(4)]andthedecoderis[1x512Fwd-GRU]. Allmodelsthereforehaveabout120Mparameters.Allmod- elsweretrainedwithaminibatchof512on16M40gpus usingsynchronousSGD,andtypicallyconvergewithin70k iterationstothesolution. 4.IMPACTOFENCODERARCHITECTURE Inthissection,weusethestandardWSJdatasettounderstand howthemodelsperformwithdifferentencodingchoices. Sinceencoderlayersarefarawayfromthelossfunctionswe areevaluating,oneexpectthatanencoderthatworkswell onCTCwouldalsoperformwellonattentionandRNN- Transducer.However,differenttrainingtargetsallowfor differentkindsofencoders:particularly,1)theamountof downsamplingintheencoderisanimportantfactorthatim- pactsbothtrainingwallclocktimeaswellastheaccuracyof themodel.2)Encoderswithforward-onlylayersalsoallow forstreamingdecoding,sowealsoexplorethataspect.We believethattheseresultsonthesmallerandmoreuniform datasetshouldstillholdatscale,andthereforefocusonthe trendsratherthanoptimizingforWER. Wecontrolallthemodelsinthissectiontohave4layers of256bidirectionalLSTMcellsintheencoder,withweight noise.Weperformrandomsearchoverpoolingintheen- coder,whethertouseaconvolutionalfront-end,dataaugmen- tation,weightnoiseandoptimizationhyper-parameters.We reportthebestnumberswithinthe60kiterationsoftrain- ing 6 .Thissearchoverhyper-parameterspacehasallowedus tomatchpreviouslypublishedresults.Theattentionmodel inTable5hasaWERof17.4afterbeamsearchontheWSJ dev'93set,whichmatchesthepreviouslypublishedresults (17.9)in[9].Similarly,theCTCmodelhasbetterresults thanreportedin[13].Wethereforebelievethatthisprovides agoodbaselinetoexplorethetrade-offsinmodelingchoices. 4.1.Forward-onlyencoders StreamingtranscriptionisanimportantrequirementforASR models.Thesteptowardsdeployingthesemodelsinthis settingistoreplacethebidirectionallayerswithforward-only recurrentlayers.NotethatwhilethisimmediatelymakesCTC andRNN-Transducermodelsdeployable,attentionmodels stillneedtobeabletoprocesstheentireutterancebeforeout- puttingthecharacter.Alternativeshavebeenproposed tocircumventthisissue[22,1]andbuildattentionmodels withmonotonicattentionandstreamingdecoders,butnone 6 Betterresultsareobservedforallmodelsiftheyaretrainedfor400k iterations- e.g, aWERof15.72forAttentionmodelafterbeamsearchonthe WSJdev'93set-buttheconclusionsofcomparisonremainunchanged. ofthemareabletocompletelymatchtheperformanceofthe fullattentionmodels.Nevertheless,webelieveacomparison withmodelswithfullattentionisimportantforustoout iffullattentionovertheentireaudioprovidesadditionalper- formanceorimprovestraining.Inourexperiment,wereplace everylayerof256bidirectionalLSTMcellsintheencoder withalayerof512forward-onlyLSTMcells. Model Bidirectional Forward-only Decoding Greedy Beam Beam NoLM +LM +LM CTC 15.73 10.08 13.78 RNN-Transducer 15.29 14.05 22.38 Attention 14.99 14.07 19.19 Table5 .WERofbaselinemodelsonWSJeval'92set.On smallerdatasets,RNN-TransducersandAttentionmodelsdo nothaveenoughdatatolearnagoodimplicitlanguagemodel andthereforeperformpoorercomparedtoCTCevenafter rescoringwithanexternalLM(RNN-TransducersandAtten- tionmodelslearnabetterimplicitlanguagemodelatscale,as showninTables1and3). FromTable5,wethatCTCmodelsarecantly morestable,easiertotrainandperformbetterintheforward onlysetting.Also,sincetheattentionmodelsarequiteabit betterthanRNN-Transducermodels,thefullattentionover allencodertimestepsseemstobevaluable. 4.2.Downsamplingintheencoder Fig.2 .Effectofincreasingtheframe-rateonWER Oneeffectivewaytocontrolboththememoryusageas wellasthetrainingtimeofthesemodelsistocompressalong thetimedimensionintheencoder,sothattherecurrentlay- ersareunrolledoverfewertime-steps.Previousresultshave shownthatCTCmodelsworkbestat50stepspersecondof audio[2](a 2  reductionsincespectrogramsareoftenmade at100stepspersecondofaudio),andattentionmodelswork bestatabout12stepspersecondofaudio[6].Sogiventhe sameencoderarchitecture,theencoderlayeronanatten- tionmodelwith3layersofpyramidalpoolinghas 4  lesser computewhencomparedtoaCTCmodel.Thisisimportant sincetheattentionnowonlyneedstobecomputedoversucha smallnumberofencodertimesteps.SinceRNN-Transducers andattentionmodelscanoutputmultiplecharactersforthe sameencodertimestep,weexpectRNN-Transducerstobe asrobustasattentionmodelsasweincreasetheamountof poolingintheencoder.WhileFigure2showsthattheyare fairlyrobustcomparedtheCTCmodels,wethatatten- tionmodelsaremorerobust.Inaddition,we havesuccessfullytrainedattentionmodelswithupto5lay- ersofpooling- 32  reductionintheencoderwhichforcesto compressonesecondofaudiointoonly3encodersteps. 5.ALIGNMENTVISUALIZATION Thethreetransductionmodelsformulatethealignmentsbe- tweeninputandoutputindifferentways.CTCandRNN- Transducermodelsexplicitlytreatalignmentasalatentvari- ableandmarginalizeoverallpossiblehardalignmentswhile attentionmodelsasoftalignmentbetweeneachoutputstep andeveryinputstep.Inaddition,RNN-TransducerandAtten- tionmodelsallowforproducingmultiplecharactersbyread- ingthesameinputlocationswhileCTCcanonlyproduceone. Herein,wevisualizethealignmentslearnedbythreemod- elstounderstandtheformulationsmadebyeachmodel.Fig- ure3plotsthealignmentforoneutterancefromtheWSJde- vset.Sincethealignmentiscomputedbasedonground-truth text(insteadofpredictions),allthreemodelsproducereason- ablealignments,especiallybeingmonotonicforAttention. Severalnotableobservationsarelistedasbelow:  Wecanseethesmalljumpsalongx-axisintheleftsub- asCTCinsertsblanksintooutputlabelsinorder toalignwithinputs.  Multipleattending(producingcharacters)alongthe sameinput(thesamecolumn)canbefoundinRNN- Transducer(middle)andAttention(right)models.  ThealignmentscomputedbyCTCandRNN-Transducer aremoreconcentrated(orpeaky)comparedtothatof Attention.Inaddition,Attentionmodelproducesdif- fuseddistributionsatthebeginningoftheaudio. 6.RELATEDWORK SegmentalRNNs[18]provideanotheralternativewayto modeltheASRtask.SegmentalRNNsmodel P ( y j x ) using azeroth-orderCRF.Whileglobalnormalizationhelpaddress thelabelbiasissuesinCTC,webelievethatthebiggerissue isstilltheconditionalindependenceassumptionsmadeby bothCTCandSegmentalRNNs. [5,8,3]directlycomparetheWERsofattentionmodels withthoseofCTCandRNN-transducerlistedintheoriginal Fig.3 .VisualizationoflearnedalignmentsforthesameutteranceusingCTC(left),RNN-Transducer(middle),andAttention (right).Thealignmentsarebetweenground-truthtext(y-axis)andaudiofeaturesfedintothedecoder(x-axis).Notethat Attentiondoestwomoretime-scaledownsampling,whichresultsin 4  shortersequences(xaxis)comparedtotheothertwo. papers,withoutanycontrolineitheracousticmodelsoropti- mizationmethodology.[7]didaninitialcontrolledcompari- sonoverseveralspeechtransductionmodels,butonlypresent resultsonasmalldatset-TIMIT. Thereisalsosomerecenteffort[22,1]inintroducing localandmonotonicconstraintsintoattentionmodelsespe- ciallyforonlineapplications.Theseeffortswillintheory bridgethemodellingassumptionsbetweenattentionand RNN-transducermodels.Withtheseconstraints,theing capabilityofattentionmodelswouldbelimited,butthey mightbemorerobusttonoisytestdatainreturn.Inother words,attentionmodelscanworkwithoutextratricksduring beamsearchdecoding, e.g, ,coveragepenalty. 7.CONCLUSIONANDFUTUREWORK Wepresentathoroughcomparisonofthreepopularmodels fortheend-to-endASRtaskatscale,andthatinthe bidirectionalsetting,allthreemodelsperformroughlythe same.However,thesemodelsdifferinthesimplicityoftheir traininganddecodingpipelines.Notably,end-to-endmodels trainedwiththeCTCloss,simplifythetrainingprocessbut stillrequiretobedecodedwithlargelanguagemodels.RNN- TransducersandAttentionalsosimplifythedecodingpro- cessandrequirethelanguagemodelstobeintroducedonly inapostprocessingstagetobeequallyifnotmoreeffec- tive.Betweenthesetwo,RNN-Transducershavethesimplest decodingprocesswithnoextrahyper-parameterstuningfor decoding,whichleadsustobelievethatRNN-Transducers presentthenextgenerationofend-to-endspeechmodels.In attempttotrainRNN-Transducermodelswiththestreaming constraint,andinreducingcomputationinencoderlayers,we thatCTCandattentionmodelsstillhavestrengthsthatwe aimtoleverageinourfutureworkwithRNN-Transducers. 8.ACKNOWLEDGEMENTS WewouldliketothankXiangangLi,oftheBaiduSpeech TechnologyGroupforfeedbackabouttheworkandalsohelp- ingimprovethedraft. 9.REFERENCES [1]RoeeAharoniandYoavGoldberg.Sequencetose- quencetransductionwithhardmonotonicattention. arXivpreprintarXiv:1611.01487 ,2016. [2]DarioAmodei,RishitaAnubhai,EricBattenberg,Carl Case,JaredCasper,BryanCatanzaro,JingdongChen, MikeChrzanowski,AdamCoates,GregDiamos,etal. Deepspeech2:End-to-endspeechrecognitioninen- glishandmandarin. arXivpreprintarXiv:1512.02595 , 2015. [3]DzmitryBahdanau,JanChorowski,DmitriySerdyuk, PhilemonBrakel,andYoshuaBengio.End-to-end attention-basedlargevocabularyspeechrecognition. abs/1508.04395,2015.http://arxiv.org/abs/1508.04395. [4]EricBattenberg,RewonChild,AdamCoates,Christo- pherFougner,YasheshGaur,JiajiHuang,HeewooJun, AjayKannan,MarkusKliegl,AtulKumar,etal.Reduc- ingbiasinproductionspeechmodels. arXivpreprint arXiv:1705.04400 ,2017. [5]WilliamChan,NavdeepJaitly,QuocLe,andOriol Vinyals.Listen,attend,andspell.abs/1508.01211, 2015.http://arxiv.org/abs/1508.01211. [6]WilliamChan,NavdeepJaitly,QuocVLe,andOriol Vinyals.Listen,attendandspell. arXivpreprint arXiv:1508.01211 ,2015. [7]Chung-ChengChiu,DieterichLawson,YupingLuo, GeorgeTucker,KevinSwersky,IlyaSutskever,and NavdeepJaitly.Anonlinesequence-to-sequence modelfornoisyspeechrecognition. arXivpreprint arXiv:1706.06428 ,2017. [8]JanChorowski,DzmitryBahdanau,DmitrySerdyuk, KyunghyunCho,andYoshuaBengio.Attention-based modelsforspeechrecognition.abs/1506.07503,2015. http://arxiv.org/abs/1506.07503. [9]JanChorowskiandNavdeepJaitly.Towardsbetterde- codingandlanguagemodelintegrationinsequenceto sequencemodels. arXivpreprintarXiv:1612.02695 , 2016. [10]RonanCollobert,ChristianPuhrsch,andGabrielSyn- naeve.Wav2letter:anend-to-endconvnet-basedspeech recognitionsystem. arXivpreprintarXiv:1609.03193 , 2016. [11]AlexGraves.Sequencetransductionwithrecurrentneu- ralnetworks. arXivpreprintarXiv:1211.3711 ,2012. [12]AlexGraves,SantiagoFern ´ andez,FaustinoGomez,and J ¨ urgenSchmidhuber.Connectionisttemporal tion:labellingunsegmentedsequencedatawithrecur- rentneuralnetworks.In Proceedingsofthe23rdinter- nationalconferenceonMachinelearning ,pages369Œ 376.ACM,2006. [13]AlexGravesandNavdeepJaitly.Towardsend-to-end speechrecognitionwithrecurrentneuralnetworks.In Proceedingsofthe31stInternationalConferenceon MachineLearning(ICML-14) ,pages1764Œ1772,2014. [14]AlexGraves,Abdel-rahmanMohamed,andGeoffrey Hinton.Speechrecognitionwithdeeprecurrentneural networks.In ICASSP ,2013. [15]AwniY.Hannun,AndrewL.Maas,DanielJurafsky,and AndrewY.Ng.First-passlargevocabularycontinuous speechrecognitionusingbi-directionalrecurrentDNNs. abs/1408.2873,2014.http://arxiv.org/abs/1408.2873. [16]G.E.Hinton,L.Deng,D.Yu,G.E.Dahl,A.Mo- hamed,N.Jaitly,A.Senior,V.Vanhoucke,P.Nguyen, T.Sainath,andB.Kingsbury.Deepneuralnetworksfor acousticmodelinginspeechrecognition. IEEESignal ProcessingMagazine ,29(November):82Œ97,2012. [17]HairongLiu,ZhenyaoZhu,XiangangLi,andSan- jeevSatheesh.Gram-ctc:Automaticunitselectionand targetdecompositionforsequencelabelling. CoRR , abs/1703.00096,2017. [18]LiangLu,LingpengKong,ChrisDyer,NoahA.Smith, andSteveRenals.Segmentalrecurrentneuralnetworks forend-to-endspeechrecognition.In INTERSPEECH , 2016. [19]YajieMiao,MohammadGowayyed,andFlorianMetze. Eesen:End-to-endspeechrecognitionusingdeeprnn modelsandwfst-baseddecoding.In AutomaticSpeech RecognitionandUnderstanding(ASRU),2015IEEE Workshopon ,pages167Œ174.IEEE,2015. [20]D.Povey,A.Ghoshal,G.Boulianne,L.Burget, O.Glembek,K.Vesel ´ y,N.Goel,M.Hannemann, P.Motlicek,Y.Qian,P.Schwarz,J.Silovsky,and G.Stemmer.TheKaldispeechrecognitiontoolkit.In ASRU ,2011. [21]DanielPovey,VijayadityaPeddinti,DanielGalvez,Pe- gahGhahremani,VimalManohar,XingyuNa,Yiming Wang,andSanjeevKhudanpur.Purelysequence-trained neuralnetworksforasrbasedonlattice-freemmi.In IN- TERSPEECH ,pages2751Œ2755,2016. [22]ColinRaffel,ThangLuong,PeterJLiu,RonJWeiss, andDouglasEck.Onlineandlinear-timeattention byenforcingmonotonicalignments. arXivpreprint arXiv:1704.00784 ,2017. [23]GeorgeSaon,GakutoKurata,TomSercu,KartikAu- dhkhasi,SamuelThomas,DimitriosDimitriadis,Xi- aodongCui,BhuvanaRamabhadran,MichaelPicheny, Lynn-LiLim,etal.Englishconversationaltelephone speechrecognitionbyhumansandmachines. arXiv preprintarXiv:1703.02136 ,2017. [24]AndrewW.Senior,HasimSak,FelixdeChau- montQuitry,TaraN.Sainath,andKanishkaRao. Acousticmodellingwithcd-ctc-smbrlstmrnns.In ASRU ,2015. [25]TomSercuandVaibhavaGoel.Densepredictionon sequenceswithtime-dilatedconvolutionsforspeech recognition. arXivpreprintarXiv:1611.09288 ,2016. [26]JasonRSmith,HerveSaint-Amand,MagdalenaPla- mada,PhilippKoehn,ChrisCallison-Burch,andAdam Lopez.Dirtcheapweb-scaleparalleltextfromthecom- moncrawl.In ACL(1) ,pages1374Œ1383,2013. [27]YonghuiWu,MikeSchuster,ZhifengChen,QuocV Le,MohammadNorouzi,WolfgangMacherey,Maxim Krikun,YuanCao,QinGao,KlausMacherey,etal. Google'sneuralmachinetranslationsystem:Bridging thegapbetweenhumanandmachinetranslation. arXiv preprintarXiv:1609.08144 ,2016. [28]WayneXiong,JashaDroppo,XuedongHuang,Frank Seide,MikeSeltzer,AndreasStolcke,DongYu,andGe- offreyZweig.Achievinghumanparityinconversational speechrecognition. arXivpreprintarXiv:1610.05256 , 2016. [29]GeofferyZweig,GhengzhuYu,JashaDroppo,andAn- dreasStolcke.Advancesinall-neuralspeechrecogni- tion. arXivpreprintarXiv:1609.05935 ,2016.  
TransductiveLearningwithMulti-classVolumeApproximation GangNiu NIUGANG @ BAIDU . COM TokyoInstituteofTechnology,Tokyo,152-8552,Japan BaiduInc.,Beijing,100085,China BoDai BODAI @ GATECH . EDU GeorgiaInstituteofTechnology,Atlanta,GA30332,USA MarthinusChristoffelduPlessis CHRISTO @ SG . CS . TITECH . AC . JP MasashiSugiyama SUGI @ CS . TITECH . AC . JP TokyoInstituteofTechnology,Tokyo,152-8552,Japan Abstract Givenahypothesisspace,the largevolumeprin- ciple byVladimirVapnikprioritizesequivalence classesaccordingtotheirvolumeinthehypoth- esisspace.The volumeapproximation hashith- ertobeensuccessfullyappliedtobinarylearning problems.Inthispaper,weproposeanovelgen- eralizationtomultipleclasses,allowingapplica- tionsofthelargevolumeprincipleonmorelearn- ingproblemssuchas multi-class , multi-label and serendipitous learninginatransductivemanner. Althoughtheresultantlearningmethodinvolves anon-convexoptimizationproblem,theglobally optimalsolutionisalmostsurelyuniqueandcan beobtainedusing O ( n 3 ) time.Noveltheoretical analysesarepresentedfortheproposedmethod, andexperimentalresultsshowitcomparesfavor- ablywiththeone-vs-restextension. 1.Introduction Thehistoryofthe largevolumeprinciple (LVP)goesback totheearlyageofthestatisticallearningtheorywhen Vap- nik ( 1982 )introduceditforthecaseofhyperplanes.Butit didnotgainmuchattentionuntilacreativeapproximation wasproposedin El-Yanivetal. ( 2008 )toimplementLVP forthecaseofsoftresponsevectors.Fromthenon,ithas beenappliedtovariousbinarylearningproblemssuccess- fully,suchasbinarytransductivelearning( El-Yanivetal. , 2008 ),binaryclustering( Niuetal. , 2013a ),andoutlierde- tection( Li&Ng , 2013 ). Proceedingsofthe 31 st InternationalConferenceonMachine Learning ,Beijing,China,2014.JMLR:W&CPvolume32.Copy- right2014bytheauthor(s). Figure1. Thelarge volumeprincipleand itsapproximation. LVPisalearning-theoreticprinciplewhichviewslearning as hypothesisselecting fromacertain hypothesisspace H . Regardlessofthehypothesisform, H canalwaysbeparti- tionedintoanumberofequivalenceclassesonsome observeddataset,whereeach equivalenceclass isasetof hypothesesthatgeneratethesamelabelingoftheobserved data.LVP,asoneofthelearning-theoreticprinciplesfrom thestatisticallearningtheory,prioritizesthoseequivalence classesaccordingtothevolumetheyoccupyin H .Seethe illustrationinFigure 1 :Theblueellipserepresents H ,and itispartitionedinto C 1 ;:::; C 4 eachoccupyingaquadrant oftheCartesiancoordinatesystem R 2 intersectedwith H ; LVPclaimsthat C 1 and C 3 aremorepreferablethan C 2 and C 4 ,since C 1 and C 3 havelargervolumethan C 2 and C 4 . Inpractice,thehypothesisspace H cannotbeassimpleas H inFigure 1 .Itisoftenlocatedinveryhigh-dimensional spaceswhereexactoreven volumeestimation ischallenging.Therefore, El-Yanivetal. ( 2008 )proposed a volumeapproximation tobypassthevolumeestimation. Insteadoffocusingontheequivalenceclassesof H ,itdi- rectlyfocusesonthehypothesesin H sincelearningisre- gardedashypothesisselectinginLVP.It H viaan ellipsoid,measurestheanglesfromhypothesestotheprin- cipalaxesof H ,andthenprefershypothesesnearthelong TransductiveLearningwithMulti-classVolumeApproximation principalaxestothoseneartheshortones.Thismanneris reasonable,sincethelongprincipalaxesof H lieinlarge- volumeregions.InFigure 1 , h and h 0 aretwohypotheses and v 1 / v 2 isthelong/shortprincipalaxis;LVPadvocates that h ismorepreferablethan h 0 as h iscloseto v 1 and h 0 iscloseto v 2 .Wecanadoptthisvolumeapproximationto regularizeourlossfunction,whichhasbeendemonstrated helpfulforvariousbinarylearningproblems. Nevertheless,thevolumeapproximationin El-Yanivetal. ( 2008 )onlybinarylearningproblemsettingsdespiteits potentialadvantage.Inthispaperweextenditnaturallyto amoregeneralwhichcanbeappliedtoseveral transductiveproblemsettings,includingbutnotlimitedto multi-classlearning ( Zhouetal. , 2003 ), multi-labellearn- ing ( Kongetal. , 2013 ),and serendipitouslearning ( Zhang etal. , 2011 ).Weadoptthesamestrategyas El-Yanivetal. ( 2008 ):For n dataand c labels,ahypothesisspaceisde- in R n  c andlinkedtoanellipsoidin R nc ,suchthat theequivalenceclassesandthevolumeapproximationcan beaccordingly.Wenamethelearningmethodthat realizestheaboveapproximation multi-classapproximate volumeregularization (MAVR).Itinvolvesanon-convex optimizationproblem,butthegloballyoptimalsolutionis almostsurelyuniqueandaccessiblein O ( n 3 ) timefollow- ing Forsythe&Golub ( 1965 ).Moreover,wetheoretically providenovelstabilityanderroranalysesforMAVR,and experimentallyshowthatMAVRcomparesfavorablywith theone-vs-restextensionof El-Yanivetal. ( 2008 ). Therestofthispaperisorganizedasfollows.InSection2 theproblemsettingsarediscussed.InSection3thebinary volumeapproximationisreviewedandthemulti-classvol- umeapproximationisderived.Thentheproposedmethod MAVRisdevelopedandanalyzedinSection4.Atlastthe experimentalresultsarereportedinSection5. 2.TransductiveProblemSettings Recallthesettingoftransductivebinaryproblems( Vapnik , 1998 ,p.341).Supposethat X isthedomainofinputdata, andmostoftenbutnotnecessarily, Xˆ R d where d isa naturalnumber.Aedset X n = f x 1 ;:::;x n g of n points from X isobserved,andthelabels y 1 ;:::;y n 2 1 ; +1 g ofthesepointsarealsoedbutunknown.Asubset X l ˆ X n ofsize l ispickeduniformlyatrandom,andthen y i is revealedif x i 2 X l .Wecall S l = f ( x i ;y i ) j x i 2 X l g the labeleddataand X u = X n n X l theunlabeleddata.Using S l and X u ,thegoalistopredict y i of x i 2 X u (whileany unobserved x 2Xn X n iscurrentlyleftoutofaccount). Transductivelearning (TL)(e.g., Blum&Chawla , 2001 ; Szummer&Jaakkola , 2001 ; Joachims , 2003 ; Zhouetal. , 2003 ; El-Yanivetal. , 2008 )slightlydiffersfrom semi- supervisedlearning (SSL)(e.g., Bennett&Demiriz , 1998 ; Zhuetal. , 2003 ; Grandvalet&Bengio , 2004 ; Belkinetal. , 2006 ; Lietal. , 2009 ; Li&Zhou , 2011 ; Niuetal. , 2013b ): TLfocusesonpredicting X u whileSSLaimsatpredicting Xn X l ,andTLisdistributionfreebutSSLisnot. 1 More ,TLgenerallymakesnoassumptionaboutthe underlyingdistributions,andthetruelabelsaredetermin- istic;SSLusuallyassumesthat S l issampledfrom p ( x;y ) and X u issampledfrom p ( x ) ,andthenthetruelabelsare stochastic.Moreover,ifthereisanydistributionalchange, SSLshouldspecifytheformofthechange,butTLmight dealwithitdirectly.Tosumup,SSLisinductivelearning innature,andtheadvantageofTLoverinductivelearning isconceptuallycriticalforus. Asanextensionof El-Yanivetal. ( 2008 ),thevolumeap- proximationtobeproposedcanbeappliedtomanytrans- ductiveproblemsettings,wherethedifferencesaretheen- codingoflabelsandthedecodingofhypotheses.The settingis multi-classlearning :Insteadof y i 2 1 ; +1 g , wehave y i 2Y where Y = f 1 ;:::;c g and c isanatural number.Eachofthe c labelsherehavesomelabeleddata inspiteofanydistributionalchange.Thesecondsettingis multi-labellearning : y i Y with Y = f 1 ;:::;c g where y i isalabelset,or y i 2Y with Y =  1 ; 0 ; 1 g c where y i isalabelvector(cf. Kongetal. , 2013 ).Thethirdsettingis serendipitouslearning whichisamulti-classsettingwith missingclassesin S l ,thatis,someofthe c labelshaveno labeleddata(cf. Zhangetal. , 2011 ).Itisnon-trivialtosee thedistributionalchangeis covariateshift ( Yamadaetal. , 2010 )or class-priorchange ( duPlessis&Sugiyama , 2012 ) fromsemi-supervisedpointofview,whereasitisunneces- sarytospecifytheformofthechangeinoursettings. Inprinciple,alltransductivemethodscansolvemulti-class problemswiththeone-vs-restextension.Butthismaynot beagoodideaformethodsintermsofnon-convex optimizationslike El-Yanivetal. ( 2008 ).Furthermore,the encodingoflabelsformulti-labelandserendipitousprob- lemsisanissuewhenusingtheone-vs-restextension.The volumeapproximationtobeproposedcanhandleallthese settingsinamanner,butinthispaperwefocuson multi-classandserendipitouslearningsincetheydonotre- quiresophisticatedpost-processingas Kongetal. ( 2013 ). 3.VolumeApproximations Inthissectionwereviewthebinaryvolumeapproximation andproposeourmulti-classvolumeapproximation. 3.1.Binaryvolumeapproximation Thebinaryvolumeapproximationinvolvesafewkeycon- cepts( El-Yanivetal. , 2008 ):Thesoftresponsevector,the hypothesisspaceandtheequivalenceclass,andthepower 1 Somemethodsliebetweenthem,e.g., Wangetal. ( 2013 ). TransductiveLearningwithMulti-classVolumeApproximation andvolumeofequivalenceclasses. Givenasetof n data X n = f x 1 ;:::;x n g where x i 2X ,a softresponsevector isan n -dimensionalvector h :=( h 1 ;:::;h n ) > 2 R n ; (1) sothat h i standsforasoftorlabelof x i . Forbinaryproblems, h suggeststhat x i isfromthepositive classif h i > 0 , x i isfromthenegativeclassif h i < 0 ,and theabovetwocasesareequallypossibleif h i =0 . A hypothesisspace isacollectionofhypotheses.Thevol- umeapproximationrequiresasymmetricpositiv matrix Q 2 R n  n whichcontainsthepairwiseinformation about X n .Considerthehypothesisspace H Q := f h j h > Q h  1 g ; (2) wherethehypothesesaresoftresponsevectors.Thesetof signvectors f sign( h ) j h 2H Q g containsallof N =2 n possibledichotomiesof X n ,and H Q canbepartitionedin- toanumberof equivalenceclasses C 1 ;:::; C N ,such thatfored k ,allhypothesesin C k willgeneratethesame labelingof X n . Then,instatisticallearningtheory,the power ofanequiv- alenceclass C k isastheprobabilitymassofallhy- pothesesinit( Vapnik , 1998 ,p.708),i.e., P ( C k ):= Z C k p ( h )d h ;k =1 ;:::;N; where p ( h ) istheunderlyingprobabilitydensityof h over H Q .Thehypothesesin C k whichhasalargepowershould bepreferredaccordingto Vapnik ( 1998 ). Whennodomainknowledgeisavailable(i.e., p ( h ) isunknown),itwouldbenaturaltoassumethecontinuous uniformdistribution p ( h )=1 = P N k =1 V ( C k ) ,where V ( C k ):= Z C k d h ;k =1 ;:::;N; isthe volume of C k .Thatis,thevolumeofanequivalence classisasthegeometricvolumeofallhypotheses init.Asaresult, P ( C k ) isproportionalto V ( C k ) ,andthe largerthevalue V ( C k ) is,themoreweareofthe hypotheseschosenfrom C k . However,itisveryhardtoaccuratelycomputethegeomet- ricvolumeofevenasingleconvexbodyin R n ,letaloneall 2 n convexbodies,so El-Yanivetal. ( 2008 )introducedan efapproximation.Let  1   n betheeigen- valuesof Q ,and v 1 ;:::; v n betheassociatedorthonormal eigenvectors.Actually,thehypothesisspace H Q inEq.( 2 ) isgeometricallyanorigin-centeredellipsoidin R n with v i and 1 = p  i asthedirectionandlengthofits i -thprincipal axis.Notethatasmallanglefromahypothesis h in C k to some v i withasmall/largeindex i (i.e.,along/shortprin- cipalaxis)impliesthat V ( C k ) islarge/small(cf.Figure 1 ). Basedonthiscrucialobservation,we V ( h ):= n X i =1  i   h > v i k h k 2 ! 2 = h > Q h k h k 2 2 ; (3) where h > v i = k h k 2 meansthecosineoftheanglebetween h and v i .Wesubsequentlyexpect V ( h ) tobesmallwhen h liesinalarge-volumeequivalenceclass,andconverselyto belargewhen h liesinasmall-volumeequivalenceclass. 3.2.Multi-classvolumeapproximation Themulti-classvolumeapproximationcoulddealwiththe aforementionedtransductiveproblemsettingsina manner.InordertoextendtheEq.( 3 ),weneed onlytoextendthehypothesisandthehypothesisspace. Tobeginwith,weallocateasoftresponsevectorinEq.( 1 ) foreachofthe c labels: h 1 =( h 1 ; 1 ;:::;h n; 1 ) > ;:::; h c =( h 1 ;c ;:::;h n;c ) > : Thevalue h i;j isasoftorlabelof x i con- cerningthe j -thlabelanditsuggeststhat  x i shouldpossessthe j -thlabel,if h i;j > 0 ;  x i shouldnotpossessthe j -thlabel,if h i;j < 0 ;  theabovetwocasesareequallypossible,if h i;j =0 . Formulti-classandserendipitousproblems, y i ispredicted by ^ y i =argmax j h i;j .Formulti-labelproblems,weneed athreshold T h thatiseitherpresetorlearnedsinceusually positiveandnegativelabelsareimbalanced,and y i canbe predictedby ^ y i = f j j h i;j  T h g ;orwecanemploythe predictionmethodsproposedin Kongetal. ( 2013 ).Then, a softresponsematrix asourtransductivehypothesisisan n -by- c matrixby H =( h 1 ;:::; h c ) 2 R n  c ; (4) anda stackedsoftresponsevector asanequivalenthypoth- esisisan nc -dimensionalvectorby h =vec( H )=( h > 1 ;:::; h > c ) > 2 R nc ; where vec( H ) isthevectorizationof H formedbystacking itscolumnsintoasinglevector. Asthebinaryofthehypothesisspace,asymmet- ricpositivmatrix Q 2 R n  n whichcontainsthe pairwiseinformationabout X n isprovided,andweassume furtherthatasymmetricpositivmatrix P 2 R c  c whichcontainsthepairwiseinformationabout Y isavail- able.Considerthehypothesisspace H P;Q := f H j tr( H > QHP )  1 g ; (5) TransductiveLearningwithMulti-classVolumeApproximation wherethehypothesesaresoftresponsematrices.Let P  Q 2 R nc  nc bethe Kroneckerproduct of P and Q .Dueto thesymmetryandthepositivetenessof P and Q ,the Kroneckerproduct P  Q isalsosymmetricandpositive and H P;Q in( 5 )couldbeequivalentlyas H P;Q := f H j vec( H ) > ( P  Q )vec( H )  1 g : (6) TheequivalencebetweenEqs.( 5 )and( 6 )comesfromthe factthat tr( H > QHP )=vec( H ) > ( P  Q )vec( H ) follow- ingthewell-knownidentity(see,e.g.,Theorem13.26of Laub , 2005 ) ( P >  Q )vec( H )=vec( QHP ) : Asaconsequence,thereisabijectionbetween H P;Q and E P;Q := f h j h > ( P  Q ) h  1 g whichisgeometricallyanorigin-centeredellipsoidin R nc . Thesetofsignvectors f sign( h ) j h 2E P;Q g spreadsover allthe N =2 nc quadrantsof R nc ,andthusthesetofsign matrices f sign( H ) j H 2H P;Q g containsallof N possi- bledichotomiesof X n f 1 ;:::;c g .Inotherwords, H P;Q canbepartitionedinto N equivalenceclasses C 1 ;:::; C N , suchthatfored k ,allsoftresponsematricesin C k will generatethesamelabelingof X n f 1 ;:::;c g . Theofthepowerissameasbefore,andsoisthe ofthevolume: V ( C k ):= Z C k d H;k =1 ;:::;N: Becauseofthebijectionbetween H P;Q and E P;Q , V ( C k ) is likewisethegeometricvolumeofallstackedsoftresponse vectorsintheintersectionofthe k -thquadrantof R nc and E P;Q .Byasimilarargumenttotheof V ( h ) ,we  V ( H ):= h > ( P  Q ) h k h k 2 2 = tr( H > QHP ) k H k 2 Fro ; (7) where h =vec( H ) and k H k Fro meanstheFrobeniusnorm of H .Wesubsequentlyexpect V ( H ) tobesmallwhen H liesinalarge-volumeequivalenceclass,andconverselyto belargewhen H liesinasmall-volumeequivalenceclass. Notethat V ( H ) and V ( h ) areconsistentforbinarylearn- ingproblems.When c =2 ,wemayconstrain h 1 + h 2 = 0 n where 0 n meanstheall-zerovectorin R n .Let P = I 2 where I 2 meanstheidentitymatrixofsize2,then V ( H )= h > 1 Q h 1 + h > 2 Q h 2 k h 1 k 2 2 + k h 2 k 2 2 = h > 1 Q h 1 k h 1 k 2 2 = V ( h 1 ) ; whichcoincideswith V ( h ) inEq.( 3 ).Similarlyto V ( h ) ,fortwosoftresponsematrices H and H 0 fromthe sameequivalenceclass, V ( H ) and V ( H 0 ) maynotneces- sarilybethesamevalue.Inaddition,thedomainof V ( H ) couldbeextendedto R n  c thoughtheof V ( H ) isoriginallynullfor H outside H P;Q . 4.Multi-classApproximateVolume Regularization Theproposedvolumeapproximationmotivatesafamilyof newtransductivemethodstakingitasaregularization.We developandanalyzeaninstantiationinthissectionwhose optimizationproblemisnon-convexbutcanbesolvedex- actlyandef. 4.1.Model Firstofall,wethelabelindicatormatrix Y 2 R n  c forconveniencewhoseentriescanbefromeither f 0 ; 1 g or  1 ; 0 ; 1 g dependingontheproblemsettingsandwhether negativelabelseverappear.,wecanset Y i;j = 1 if x i islabeledtohavethe j -thlabeland Y i;j =0 other- wise,oralternativelywecanset Y i;j =1 if x i islabeledto havethe j -thlabel, Y i;j =  1 if x i islabeledtonothave the j -thlabel,and Y i;j =0 otherwise. Let  Y;H ) beourlossfunctionmeasuringthedifference between Y and H .Themulti-classvolumeapproximation motivatesthefollowingfamilyoftransductivemethods: min H 2H P;Q  Y;H )+   tr( H > QHP ) k H k 2 Fro ; where > 0 isaregularizationparameter.Thedenomina- tor k H k 2 Fro isannoyingsowewouldliketogetridofitas in El-Yanivetal. ( 2008 )or Niuetal. ( 2013a ).We ˝> 0 asascaleparameter,constrain H tobeofnorm ˝ ,replace thefeasibleregion H P;Q with R n  c ,anditbecomes min H 2 R n  c  Y;H )+  tr( H > QHP ) s : t : k H k Fro = ˝: (8) Althoughtheoptimizationisdonein R n  c ,theregulariza- tionisrelativeto H P;Q ,since tr( H > QHP ) isaweighted sumofsquaredcosinesbetween vec( H ) andtheprincipal axesof E P;Q undertheconstraint k H k Fro = ˝ . Subsequently,wedenoteby y 1 ;:::; y n and r 1 ;:::; r n the c -dimensionalvectorsthatsatisfy Y =( y 1 ;:::; y n ) > and H =( r 1 ;:::; r n ) > .Considerthefollowinglossfunctions tobe  Y;H ) inoptimization( 8 ): 1. Squaredlossesoveralldata P X n k y i  r i k 2 2 ; 2. Squaredlossesoverlabeleddata P X l k y i  r i k 2 2 ; 3. Linearlossesoveralldata P X n  y > i r i ; 4. Linearlossesoverlabeleddata P X l  y > i r i ; Theyortheirbinarycounterpartshavebeenusedin Zhou etal. ( 2003 ), El-Yanivetal. ( 2008 )and Niuetal. ( 2013a ). Actually,thethirdandfourthonesareidenticalsince y i is zerofor x i 2 X u ,andtheoneisequivalenttothem TransductiveLearningwithMulti-classVolumeApproximation in( 8 )since P X n k y i k 2 2 and P X l k y i k 2 2 areconstantsand P X n k r i k 2 2 = ˝ 2 isalsoaconstant.Thesecondoneisun- desirableduetoanissueofthetimecomplexity.Thus,we instantiate  Y;H ):= P X n k y i  r i k 2 2 = k Y  H k 2 Fro , andoptimization( 8 )becomes min H 2 R n  c k Y  H k 2 Fro +  tr( H > QHP ) s : t : k H k Fro = ˝: (9) Werefertoconstrainedoptimizationproblem( 9 )as multi- classapproximatevolumeregularization (MAVR).Anun- constrainedversionofMAVRisthen min H 2 R n  c k Y  H k 2 Fro +  tr( H > QHP ) : (10) 4.2.Algorithm Optimization( 9 )isnon-convex,butwecanrewriteitusing thestackedsoftresponsevector h =vec( H ) as min h 2 R nc k y  h k 2 2 +  h > ( P  Q ) h s : t : k h k 2 = ˝; (11) where y =vec( Y ) isthevectorizationof Y .Inthisrepre- sentation,theobjectiveisasecond-degreepolynomialand theconstraintisanorigin-centeredsphere,andfortunately wecouldsolveitexactlyandeffollowing Forsythe &Golub ( 1965 ).Tothisend,afundamentalpropertyofthe Kroneckerproductisnecessary(see,e.g.,Theorems13.10 and13.12of Laub , 2005 ): Theorem1. Let  Q; 1   Q;n betheeigenvalues and v Q; 1 ;:::; v Q;n betheassociatedorthonormaleigen- vectorsof Q ,  P; 1   P;c and v P; 1 ;:::; v P;c be thoseof P ,andtheeigen-decompositionsof Q and P be Q = V Q  Q V > Q and P = V P  P V > P .Then,theeigenvalues of P  Q are  P;j  Q;i associatedwithorthonormaleigen- vectors v P;j  v Q;i for j =1 ;:::;c , i =1 ;:::;n ,andthe eigen-decompositionof P  Q is P  Q = V PQ  PQ V > PQ , where  PQ = P   Q and V PQ = V P  V Q . Afterweignoretheconstants k y k 2 2 and k h k 2 2 intheobjec- tiveofoptimization( 11 ),theLagrangefunctionis  h ;ˆ )=  2 h > y +  h > ( P  Q ) h  ˆ ( h > h  ˝ 2 ) ; where ˆ 2 R istheLagrangianmultiplierfor k h k 2 2 = ˝ 2 . Thestationaryconditionsare @  =@ h =  y +  ( P  Q ) h  ˆ h = 0 nc ; (12) @  =@ˆ = h > h  ˝ 2 =0 : (13) Hence,foranylocallyoptimalsolution ( h ;ˆ ) where  is notaneigenvalueof P  Q ,wehave h =( P  Q  ˆI nc )  1 y (14) = V PQ (   PQ  ˆI nc )  1 V > PQ y =( V P  V Q )(   PQ  ˆI nc )  1 vec( V > Q YV P ) (15) Algorithm1 MAVR Input: P , Q , Y ,  and ˝ Output: H and ˆ 1: Eigen-decompose P and Q ; 2: Constructthefunction g ( ˆ ) ; 3: Findthesmallestrootof g ( ˆ ) ; 4: Recover h using ˆ andreshape h to H . basedonEq.( 12 )andTheorem 1 .Next,wesearchforthe feasible ˆ for( 12 )and( 13 )whichwillleadtotheglobally optimal h .Let z =vec( V > Q YV P ) ,thenplugging( 15 )into ( 13 )givesus z > (   PQ  ˆI nc )  2 z  ˝ 2 =0 : (16) Letussorttheeigenvalues  P; 1  Q; 1 ;:::; P;c  Q;n intoa non-descendingsequence f  PQ; 1 ;:::; PQ;nc g ,rearrange f z 1 ;:::;z nc g accordingly,andthesmallest k 0 which  z k 0 6 =0 .Asaresult,Eq.( 16 )impliesthat g ( ˆ )= nc X k = k 0 z 2 k (  PQ;k  ˆ ) 2  ˝ 2 =0 (17) foranystationary ˆ .ByTheorem4.1of Forsythe&Golub ( 1965 ),thesmallestrootof g ( ˆ ) determinesaunique h so that ( h ;ˆ ) isthegloballyoptimalsolutionto  h ;ˆ ) ,i.e., h minimizestheobjectiveof( 11 )globally.Here,theonly exceptionwherewecannotdetermine h byEq.( 14 )fora valueof ˆ iswhen  isaneigenvalueof P  Q . This,however,happenswithprobabilityzero.Thetheorem belowpointsoutthelocationoftheoptimal ˆ (theproofis intheappendix): Theorem2. Thefunction g ( ˆ ) inEq. ( 17 ) hasex- actlyonerootintheinterval [ ˆ 0 ; PQ;k 0 ) andnorootin theinterval (  ;ˆ 0 ) ,where ˆ 0 =  PQ;k 0 k y k 2 =˝ . ThealgorithmofMAVRissummarizedinAlgorithm 1 .It iseasytoseethat ˆ =  1 inAlgorithm 1 insteadof thesmallestrootof g ( ˆ ) suftosolveoptimiza- tion( 10 ).Moreover,foraspecialcase P = I c where I c is theidentitymatrixofsize c ,anystationary H issimply H =( Q  ˆI n )  1 Y = V Q (   Q  ˆI n )  1 V > Q Y: Let z = V > Q Y 1 c where 1 c meanstheall-onevectorin R c , and k 0 isthesmallestnumberthat z k 0 6 =0 .Then thesmallestrootof g ( ˆ )= P n k = k 0 z 2 k = (  Q;k  ˆ ) 2  ˝ 2 givesusthefeasible ˆ leadingtothegloballyoptimal H . TheasymptotictimecomplexityofAlgorithm 1 is O ( n 3 ) . More,eigen-decomposing Q inthestepof Algorithm 1 costs O ( n 3 ) ,andthisisthedominatingcom- putationtime.Eigen-decomposing P justneeds O ( c 3 ) and isnegligibleundertheassumptionthat n ˛ c withoutloss TransductiveLearningwithMulti-classVolumeApproximation ofgenerality.Inthesecondstep,itrequires O ( nc log( nc )) forsortingtheeigenvaluesof P  Q and O ( n 2 c ) forcom- puting z .Findingthesmallestrootof g ( ˆ ) basedonabi- narysearchalgorithmuses O (log( k y k 2 )) inthethirdstep. Inthefourthstep,recovering h isessentiallysameascom- puting z andcosts O ( n 2 c ) . Wewouldliketocommentalittlemoreontheasymptotic timecomplexityofMAVR.Givened P and Q butdif- ferent Y ,  and ˝ ,thecomputationalcomplexityis O ( n 2 c ) ifwereusetheeigen-decompositionsof P and Q andthe sortedeigenvaluesof P  Q .Thispropertyisparticular- lyadvantageousfortryingdifferenthyperparameters.Itis alsoquiteusefulforchoosingdifferent X l ˆ X n tobela- beledfollowingtransductiveproblemsettings.Finally,the asymptotictimecomplexity O ( n 3 ) forsolvingMAVRex- actlycanhardlybeimprovedbasedonexistingtechniques. Evenif ˆ isedinoptimization( 10 ),thestationarycon- ditionEq.( 12 )isa discreteSylvesterequation whichcon- sumes O ( n 3 ) forsolvingit( Sima , 1996 ). 4.3.Theoreticalanalyses Weprovidetwotheoreticalresults.Undercertainassump- tions,thestabilityanalysisupperboundsthedifferenceof twooptimal H and H 0 trainedwithtwodifferentlabelin- dicatormatrices Y and Y 0 ,andtheerroranalysisbounds thedifferenceof H fromthegroundtruth. Theorem 2 guaranteesthat ˆ< PQ;k 0 .Infact,withhigh probabilityoverthechoiceof Y ,itholdsthat k 0 =1 (we didnotmeet k 0 > 1 inourexperiments).Forthisreason, wemakethefollowingassumption: Fix P and Q ,andallow Y tochangeaccordingtothepar- titionof X n intodifferent X l and X u .Thereis C ;˝ > 0 , whichjustdependson  and ˝ ,suchthatforalloptimal ˆ trainedwithdifferent Y , ˆ   PQ; 1  C ;˝ . NotethatforunconstrainedMAVR,theremustbe C ;˝ > 1 since  PQ; 1 > 0 and ˆ =  1 .Wecanprovethethe- orembelowbasedontheassumptionaboveandthelower boundof ˆ inTheorem 2 . Theorem3 (StabilityofMAVR) . Assumetheexistenceof C ;˝ .Let ( H;ˆ ) and ( H 0 ;ˆ 0 ) betwogloballyoptimalso- lutionstrainedwithtwodifferentlabelindicatormatrices Y and Y 0 respectively.Then, k H  H 0 k Fro k Y  Y 0 k Fro =C ;˝ + j ˆ  ˆ 0 j min fk Y k Fro ; k Y 0 k Fro g =C 2 ;˝ : (18) Consequently,forMAVRinoptimization ( 9 ) wehave k H  H 0 k Fro k Y  Y 0 k Fro =C ;˝ + k Y k Fro k Y 0 k Fro =˝C 2 ;˝ ; andforunconstrainedMAVRinoptimization ( 10 ) wehave k H  H 0 k Fro k Y  Y 0 k Fro =C ;˝ : Inordertopresentanerroranalysis,weassumethereisa ground-truthsoftresponsematrix H  withtwoproperties. Firstly,thevalueof V ( H  ) shouldbebounded,namely, V ( H  )=tr( H > QH  P ) = k H  k 2 Fro  C h ; where C h > 0 isasmallnumber.Thisensuresthat H  lies inalarge-volumeregion.OtherwiseMAVRimplementing thelargevolumeprinciplecanbynomeanslearnsome H closeto H  .Secondly, Y shouldcontaincertaininforma- tionabout H  .MAVRmakesuseof P , Q and Y onlyand themeaningsof P and Q areedalready,soMAVRmay accesstheinformationabout H  onlythrough Y .Tomake Y and H  correlated,weassumethat Y = H  + E where E 2 R n  c isanoisematrixofthesamesizeas Y and H  . Allentriesof E areindependentwithzeromean,andthe varianceofthemis ˙ l or ˙ u dependingonitscorrespon- dencetoalabeledoranunlabeledpositionin Y .Wecould expectthat ˙ l ˝ ˙ u ,suchthattheentriesof Y inlabeled positionsareclosetothecorrespondingentriesof H  ,but theentriesof Y inunlabeledpositionsarecompletelycor- ruptedanduninformativeforrecovering H  .Noticethat weneedthisgeneratingmechanismof Y evenif C h  is thesmallesteigenvalueof P  Q ,since P  Q mayhave multiplesmallesteigenvaluesand  H havetotallydiffer- entmeanings.Basedontheseassumptions,wecanprove thetheorembelow. Theorem4 (AccuracyofMAVR) . Assumetheexistenceof C ;˝ , C h ,andthegeneratingprocessof Y from H  and E . Let e l and e u bethenumbersofthelabeledandunlabeled positionsin Y andassumethat E E k Y k 2 Fro  e l wherethe expectationiswithrespecttothenoisematrix E .Foreach possible Y ,let H bethegloballyoptimalsolutiontrained withit.Then, E E k H  H  k Fro  ( p C h  PQ; 1 =C ;˝ ) k H  k Fro +(max f p e l=˝   PQ; 1  1 ; PQ; 1  C ;˝ +1 g =C ;˝ ) k H  k Fro + q e l˙ 2 l + e u˙ 2 u =C ;˝ (19) forMAVRinoptimization ( 9 ) ,and E E k H  H  k 2 Fro  ( C h = 4) k H  k 2 Fro + e l˙ 2 l + e u˙ 2 u (20) forunconstrainedMAVRinoptimization ( 10 ) . TheproofsofTheorems 3 and 4 areintheappendix.Con- sideringtheinstabilityboundsinTheorem 3 andtheerror boundsinTheorem 4 ,unconstrainedMAVRissuperiorto constrainedMAVRinbothcases.Thatbeingsaid,bounds arejustbounds.Wewilldemonstratethepotentialofcon- strainedMAVRinthenextsectionbyexperiments. 5.Experiments Inthissection,wenumericallyevaluateMAVR.Thebase- linemethodsincludetheone-vs-restextensionofthe bina- TransductiveLearningwithMulti-classVolumeApproximation (a)Data1 (b)Data2 (c)Data3 (d)Data4 (e)Data5 (f)Data1, P 1 (g)Data1, P 2 (h)Data1, P 3 (i)Data2, P 4 (j)Data3, P 5 (k)Data4, P 5 (l)Data5, P 5 Figure2. SerendipitouslearningbyMAVR. ryapproximatevolumeregularization (BAVR)aswellasa multi-classtransductivemethodnamed learningwithlocal andglobalconsistency (LGC)( Zhouetal. , 2003 ). 5.1.Serendipitouslearning WeshowhowtohandleserendipitousproblemsbyMAVR directlywithoutperformingclustering( Hartigan&Wong , 1979 ; Ngetal. , 2001 ; Sugiyamaetal. , 2014 )orestimating theclass-priorchange( duPlessis&Sugiyama , 2012 ).The experimentalresultsaredisplayedinFigure 2 .Thereare5 datasetsintotalwherethelatter3datasetscome from Zelnik-Manor&Perona ( 2004 ).Thematrix Q was asthe normalizedgraphLaplacian (see,e.g., von Luxburg , 2007 ) 2 Q = L nor = I n  D  1 = 2 WD  1 = 2 ; where W 2 R n  n isasimilaritymatrixand D 2 R n  n is thedegreematrixof W .Thematrix P wasby P 1 = 0 B B @ 1000 0100 0031 0011 1 C C A ;P 2 = 0 B B @ 1000 0301 0010 0101 1 C C A ;P 3 = 0 B B @ 1000 0101 0011 0113 1 C C A ; P 4 = 0 B B @ 11 = 21 = 21 = 2 1 = 2201 = 2 1 = 2021 = 2 1 = 21 = 21 = 23 1 C C A ;P 5 = 0 @ 11 = 21 = 2 1 = 210 1 = 201 1 A : Fordatasets1and2weusedtheGaussiansimilarity W i;j =exp(  x i  x j k 2 2 = (2 ˙ 2 )) ;W i;i =0 withthekernelwidth ˙ =0 : 25 ,andfordatasets3to5we appliedthelocal-scalingsimilarity( Zelnik-Manor&Per- ona , 2004 ) W i;j =exp(  x i  x j k 2 2 = (2 ˙ i ˙ j )) ;W i;i =0 2 ThoughthegraphLaplacianmatriceshavezeroeigenvalues, theywouldnotcausealgorithmicproblemswhenusedas Q . (a)Data1 (b)Data2 (c)Data3 (d)Data4 (e)Data5 Figure3. SerendipitouslearningbyLGC. withthenearest-neighbornumber k =7 ,whereeach ˙ i = k x i  x ( k ) i k 2 isthescaleparameterof x i and x ( k ) i isthe k - thnearestneighborof x i in X n .Forthehyperparameters, weset  =99 and ˝ = p l .Furthermore,aclass-balance regularizationwasimposedfordatasets2to5,whichtries tominimize  0 tr( H > ( 1 n 1 > n ) H ( I c  1 c 1 > c =c )) : Thedetailedderivationisomittedduetothelimitedspace, buttheideaistoencouragebalancedtotalresponsesamong c classes.Forthisregularization,theregularizationparam- eterwassetto  0 =1 . WecanseethatinFigure 2 ,MAVRsuccessfully thedatabelongingtotheknownclassesandsimultaneously clusteredthedatabelongingtotheunknownclasses.More- over,wecancontroltheoftheknownclasseson theunknownclassesbyspecifyingdifferent P ,asshownin (f),(g)and(h)ofFigure 2 .Ontheotherhand, BAVRcannotfromtheclass-balanceregularization andLGCwiththeclass-balanceregularizationfordatasets 2to5inFigure 3 wasnotasperfectasMAVR. 5.2.Multi-classlearning Ascommentedintheendofourtheoreticalanalyses,we woulddemonstratethepotentialofconstrainedMAVRby experiments.Actually,LGCcouldbesubsumedinMAVR asaspecialcaseofunconstrainedMAVR:AlthoughLGC ismotivatedbythelabelpropagationpointofview,itcan berewrittenasthefollowingoptimizationproblem min H 2 R n  c k Y  H k 2 Fro +  tr( H > L nor H ) : Therefore,unconstrainedMAVRwillbereducedexactlyto LGCif P = I c and Q = L nor .Nowwespecify P = I c and Q = L nor andillustratethenuanceofconstrainedMAVR, LGC,andBAVRusingandataset. Thedataset 3circles isgeneratedasfollows.We havethreeclasseswiththeclassratio 1 = 6:1 = 3:1 = 2 .Let y i betheground-truthlabelof x i ,then x i isgeneratedby x i =(6 y i cos( a i )+  i; 1 ; 5 y i sin( a i )+  i; 2 ) > 2 R 2 ; where a i isanangledrawni.i.d.fromtheuniformdistribu- tion U (0 ; 2 ˇ ) ,and  i; 1 and  i; 2 arenoisesdrawni.i.d.from thenormaldistribution N (0 ;˙ 2  ) .Inourexperiments,we variedonefactorwhileedallotherfactors.Thedefault TransductiveLearningwithMulti-classVolumeApproximation (a)Default (b)Small n (c)Large n (d)Small ˙  (e)Large ˙  Figure4. Visualizationofthedataset3circles. (a)Varying ˙  (b)Varying l (c)Varying n (d)Varying ˙ (e)Varying  (f)Varying ˝ Figure5. MeanswithstandarderrorsofLGC,BAVRandMAVRon3circles. valuesoffactorswere ˙  =0 : 5 , ˙ =0 : 5 , l =3 , n =300 ,  =99 ,and ˝ = p l ,andtherangesofthesefactorswere  ˙  2 0 : 5  exp  1 : 5 ;  1 : 4 ;  1 : 3 :::; 0 : 5 g ;  l 2f 3 ; 4 ; 5 ;:::; 20 g ;  n 2f 120 ; 138 ; 156 ;:::; 480 g ;  ˙ 2 0 : 5  exp  1 ;  0 : 9 ;  0 : 8 ;:::; 1 g ;   2 99  exp  4 ;  3 ;  2 ;:::; 16 g ;  ˝ 2 p l  exp  2 ;  1 ; 0 ;:::; 18 g . Notethattherewasadistributionalchange,sincewesam- pledlabeleddataasbalancedaspossibleacrossthreeclass- es.Figure 4 exhibitsseveralrealizationsof3circlesgiven differentvaluesoffactors. Figure 5 showstheexperimentalresults,wherethemean- swiththestandarderrorsoftheerrorrates areplotted.Foreachtaskthatcorrespondstoafull cationofthosefactors,threemethodswererepeatedlyrun on100randomsamplings.WecanseefromFigure 5 that theperformanceofLGCorBAVRwasusuallynotasgood asMAVR.ThedrawbackofLGCisthatwewouldalways have ˆ =  1 sinceitisunconstrained.Though ˆ isadap- tiveinBAVR,wewouldhave c different ˆ valuessinceit isbasedontheone-vs-restextension. 6.Conclusions Weproposedamulti-classvolumeapproximationthatcan beappliedtoseveraltransductiveproblemsettingssuchas multi-class,multi-labelandserendipitouslearning.There- sultantlearningmethodisnon-convex,butcanhoweverbe solvedexactlyandef.Themethodwastheoretical- lybystabilityanderroranalysesandempirically demonstratedapromisingapproachviaexperiments. Acknowledgments GNwassupportedbytheFIRSTProgramandthe973Pro- gramNo.2014CB340505.MCdPwassupportedbyKAK- ENHI23120004,andMSwassupportedbyKAKENHI 25700022andAOARD. TransductiveLearningwithMulti-classVolumeApproximation References Belkin,M.,Niyogi,P.,andSindhwani,V.Manifoldreg- ularization:ageometricframeworkforlearningfrom labeledandunlabeledexamples. JournalofMachine LearningResearch ,7:2399Œ2434,2006. Bennett,K.andDemiriz,A.Semi-supervisedsupportvec- tormachines.In NIPS ,1998. Blum,A.andChawla,S.Learningfromlabeledandunla- beleddatausinggraphmincuts.In ICML ,2001. duPlessis,M.C.andSugiyama,M.Semi-supervisedlearn- ingofclassbalanceunderclass-priorchangebydistribu- tionmatching.In ICML ,2012. El-Yaniv,R.,Pechyony,D.,andVapnik,V.Largemar- ginvs.largevolumeintransductivelearning. Machine Learning ,72(3):173Œ188,2008. Forsythe,G.andGolub,G.Onthestationaryvaluesofa second-degreepolynomialontheunitsphere. Journal oftheSocietyforIndustrialandAppliedMathematics , 13(4):1050Œ1068,1965. Grandvalet,Y.andBengio,Y.Semi-supervisedlearningby entropyminimization.In NIPS ,2004. Hartigan,J.A.andWong,M.A.A k -meansclustering algorithm. AppliedStatistics ,28:100Œ108,1979. Joachims,T.Transductivelearningviaspectralgraphpar- titioning.In ICML ,2003. Kong,X.,Ng,M.,andZhou,Z.-H.Transductivemulti- labellearningvialabelsetpropagation. IEEETransac- tiononKnowledgeandDataEngineering ,25(3):704Œ 719,2013. Laub,A.J. MatrixAnalysisforScientistsandEngineers . SocietyforIndustrialandAppliedMathematics,2005. Li,S.andNg,W.Maximumvolumeoutlierdetection anditsapplicationsincreditriskanalysis. International JournalonIntelligenceTools ,22(5),2013. Li,Y.-F.andZhou,Z.-H.Towardsmakingunlabeleddata neverhurt.In ICML ,2011. Li,Y.-F.,Kwok,J.,andZhou,Z.-H.Semi-supervisedlearn- ingusinglabelmean.In ICML ,2009. Ng,A.,Jordan,M.I.,andWeiss,Y.Onspectralclustering: Analysisandanalgorithm.In NIPS ,2001. Niu,G.,Dai,B.,Shang,L.,andSugiyama,M.Maxi- mumvolumeclustering:Anewdiscriminativecluster- ingapproach. JournalofMachineLearningResearch , 14:2641Œ2687,2013a. Niu,G.,Jitkrittum,W.,Dai,B.,Hachiya,H.,andSugiya- ma,M.Squared-lossmutualinformationregulariza- tion:Anovelinformation-theoreticapproachtosemi- supervisedlearning.In ICML ,2013b. Sima,V. AlgorithmsforLinear-QuadraticOptimization . MarcelDekker,1996. Sugiyama,M.,Niu,G.,Yamada,M.,Kimura,M.,and Hachiya,H.Information-maximizationclusteringbased onsquared-lossmutualinformation. NeuralComputa- tion ,26(1):84Œ131,2014. Szummer,M.andJaakkola,T.Partiallylabeled tionwithMarkovrandomwalks.In NIPS ,2001. Vapnik,V.N. EstimationofDependencesBasedonEmpir- icalData .SpringerVerlag,1982. Vapnik,V.N. StatisticalLearningTheory .JohnWiley& Sons,1998. vonLuxburg,U.Atutorialonspectralclustering. Statistics andComputing ,17(4):395Œ416,2007. Wang,J.,Jebara,T.,andChang,S.-F.Semi-supervised learningusinggreedymax-cut. JournalofMachine LearningResearch ,14:771Œ800,2013. Yamada,M.,Sugiyama,M.,andMatsui,T.Semi- supervisedspeakeronundercovariateshift. SignalProcessing ,90(8):2353Œ2361,2010. Zelnik-Manor,L.andPerona,P.Self-tuningspectralclus- tering.In NIPS ,2004. Zhang,D.,Liu,Y.,andSi,L.Serendipitouslearning: Learningbeyondthelabelspace.In KDD , 2011. Zhou,D.,Bousquet,O.,NavinLal,T.,Weston,J.,and Sch ¨ olkopf,B.Learningwithlocalandglobalconsis- tency.In NIPS ,2003. Zhu,X.,Ghahramani,Z.,andLafferty,J.Semi-supervised learningusingGaussianandharmonicfunctions. In ICML ,2003.  
BayesianProbabilisticCo-SubspaceAddition LeiShi Baidu.com,Inc shilei06@baidu.com Abstract Formodelingdatamatrices,thispaperintroducesProbabilisticCo-SubspaceAd- dition(PCSA)modelbysimultaneouslycapturingthedependentstructuresamong bothrowsandcolumns.,PCSAassumesthateachentryofamatrixisgen- eratedbytheadditivecombinationofthelinearmappingsoftwolow-dimensional features,whichdistributeintherow-wiseandcolumn-wiselatentsubspacesre- spectively.Inconsequence,PCSAcapturesthedependenciesamongentriesin- tricately,andisabletohandlenon-Gaussianandheteroscedasticdensities.By formulatingtheposteriorupdatingintothetaskofsolvingSylvesterequations, weproposeanefvariationalinferencealgorithm.Furthermore,PCSAis extendedtotacklingandmissingvalues,toadaptingmodelsparseness,and tomodellingtensordata.Incomparisonwithseveralstate-of-artmethods,experi- mentsdemonstratetheeffectivenessandefyofBayesian(sparse)PCSAon modelingmatrix(tensor)dataandmissingvalues. 1Introduction Thispaperfocusesonmodelingdatamatricesbysimultaneouslycapturingthedependentstructures amongbothrowsandcolumns,whichisespeciallyusefulformissingvalues.UsingGaussian Process(GP),Xu etal [25]thekerneltoincorporaterelationalinformationanddrew outputsfromGPs.Widelyusedingeostatistics,LinearModelsofCorregionalization(LMC)[5] learnsthecovariancestructuresoverthevectorizeddatamatrix.In[12,16],Bayesianprobabilistic matrixfactorization(PMF)isinvestigatedviamodelingtherow-wiseandcolumn-wise variancesandinferredbasedonsuitablepriors.ProbabilisticMatrixAddition(PMA)[1]describes thecovariancestructuresamongrowsandcolumns,showingpromisingresultscomparedwithGP regression,PMFandLMC.However,bothLMCandPMAareinefonlargescalematrices. Onhighdimensionaldata,subspacestructuresareusuallydesignedinstatisticalmodelswithre- ducednumbersoffreeparameters,leadingtoimprovementonbothlearningefyandaccuracy [3,11,24].EquippingPMAwiththesubspacestructures,thispaperproposesasimpleyetnovel generativeProbabilisticCo-SubspaceAddition(PCSA)model,which,asitsname,assumesthatall entriesinamatrixcomefromthesumsoflinearmappingsoflatentfeaturesinrow-wiseandcolumn- wisehiddensubspaces.Includingmanyexistingmodelsasitsspecialcases(seeSection2.1),PCSA isabletocapturethedependenciesamongentriesintricately,thenon-Gaussianandheteroscedas- ticdensity,andextractthehiddenfeaturesintheco-subspaces. WeproposeavariationalBayesianalgorithmforinferringboththeparametersandthelatentdi- mensionalitiesofPCSA.Forquickandstableconvergence,weformulatetheposteriorupdating procedureintosolvingSylvesterequations[10].Furthermore,BayesianPCSAisimplementedin threeextensions.First,missingvaluesindatamatricesareeasilytackledandbyiterating withthevariationalinference.Second,withaJeffreysprior,BayesiansparsePCSAisimplemented withanadaptivemodelsparseness[4].Finally,weextendthePCSAonmatrixdata(i.e.,2nd-order tensor)toPCSA- k formodellingtensordatawithanarbitraryorder k . 1 Onthetaskofmissingvaluesinmatrixdata,wecompare(sparse)PCSAwithseveralstate- of-artmodels/approaches,includingPMA,RobustBayesianPMFandBayesianGPLVM[21].The datasetsunderconsiderationrangefrommulti-labeldata,user-itemratingdatafor collaborativeandfaceimages.Furtherontensorstructuredfaceimagedata,PCSAiscom- paredwiththe M 2 SA method[6]thatusesconsecutiveSVDsonallmodesofthetensor.Although simpleandnotdesignedforanyparticularapplication,throughexperimentsPCSAshowsresults promisinglycomparabletoorbetterthanthecompetingapproaches. 2PCSAModelandVariationalBayesianInference 2.1ProbabilisticCo-SubspaceAddition(PCSA) ThePCSAmodeldistributionsoverrealvaluedmatrices.Letting X 2R D 1  D 2 bean observedmatrixwith D 1  D 2 withoutlossofgenerality 1 ,westartbyoutliningagenerativemodel for X .Considertwohiddenvariables y ˘N ( y j 0 d 1 ; I d 1 ) and z ˘N ( z j 0 d 2 ; I d 2 ) with d 1 <D 1 and d 2 <D 2 ,where 0 d denotesa d -dimvectorwithallentriesbeingzerosand I d denotesa d  d identitymatrix.UsingtheconcatenationnomenclatureofMatlab,twomatricesofhidden factors Y =[ y  1 ;:::; y  D 2 ] 2R d 1  D 2 and Z =[ z  1 ;:::; z  D 1 ] 2R d 2  D 1 arecolumn-wise independentlygenerated,respectively.Throughtwolinearmappingmatrices A 2R D 1  d 1 and B 2R D 2  d 2 ,eachentry x ij 2 X isindependentgiven Y and Z by x ij = a i  y  j + b j  z  i + e ij , where a i  isthe i -throwof A .Each e ij ˘N ( e ij j 0 ; 1 =˝ ) isindependentlyGaussiandistributedand independentfrom Y ; Z .Thegenerativeprocessof X thusis:  Get Y byindependentlydrawingeachvector y  j ˘N ( y  j j 0 d 1 ; I d 1 ) for j =1 ;:::;D 2 ;  Get Z byindependentlydrawingeachvector z  i ˘N ( z  i j 0 d 2 ; I d 2 ) for i =1 ;:::;D 1 ;  Get E 2R D 1  D 2 byindependentlydrawingeachelement e ij ˘N ( e ij j 0 ; 1 =˝ ) for 8 i;j ;  Get X = AY +( BZ ) > + E given Y and Z ,i.e.,additivelycombinestheco-subspaces. Givenparameters  = f A ; B ;˝ g ,thejointdistributionof X , Y and Z is p ( X ; Y ; Z j  )= [ D 2 Y j =1 N ( y  j j 0 d 1 ; I d 1 )]  [ D 1 Y i =1 N ( z  i j 0 d 2 ; I d 2 )]  [ D 1 Y i =1 D 2 Y j =1 N ( x ij j a i  y  j + b j  z  i ; 1 =˝ )] : (1) Propertiesandrelationstoexistingwork. Albeititssimplegenerativeprocess,PCSAowns meaningfulpropertiesandcanbeviewedasanextensionofseveralexistingmodels.  Intricatedependenciesbetweenentriesin X . Althougheachentry x ij 2 X isindepen- dentgiven Y and Z ,thePCSAmodelcapturesthedependenciesalongrowsaswellas columnsinthejoint X .Particularly,assuming D 1 isthedatadimensionalitywhile D 2 is thesamplesize,thesamples(columnvectors)in X isdependentfromeachotherbyPC- SA.When B isconstrainedas 0 ,PCSAwilldegeneratetoProbabilisticPCA(PPCA)[3], whichinsiststhesamplei.i.d.assumption.  Non-Gaussianityandheteroscedasticity. Ifwestillconsider D 1 asthedatadimensional- ityand D 2 asthesamplesize,thePCSAmodelhandlesthenon-Gaussianityinsamplesof X .Asanextremeexample,ifallcolumnsof Z > B > arediscretizedtotakevaluesfroma setof n vectors,PCSAdegeneratestoMixtureofPPCA[7,20]with n components,whose subspaceloadingsarethesame.Thatis,learningsuchaPCSAmodelactuallyimplements thegroupPPCA[24]throughoutdifferentcomponents.Also,ifmarginalizing Z ,weare describingthecolumnsamplesof X withadependentheteroscedasticity.  Co-subspacefeatureextraction. Althoughabletodescribetherow-wiseandcolumn- wisecovariances,PMA[1]requiresestimatingandinvertingtwo(large)kernelmatrices withsizes D 1  D 1 and D 2  D 2 respectively,whichisintractableformanyrealworld applications.Incontrast,PCSAhas( D 1 d 1 + D 2 d 2 +1 )freeparametersandinvertssmaller matrices,andrecoversPMAwhen d 1 = D 1 and d 2 = D 2 .Moreover,PCSAisableto extractthehiddenfeatures Y and Z simultaneously. 1 Otherwise,wecantranspose X .ThisassumptionisforefSylvesterequationsolvinginthesequel. 2 2.2VariationalBayesianInference Given X andthehiddendimensionalities ( d 1 ;d 2 ) ,wecanestimatePCSA'sparameters  = f A ; B ;˝ g bymaximizingthelikelihood p ( X j  ) .However,thecapacitycontrolisessentialtogen- eralizationability,forwhichweproceedtodeliveravariationalBayesianinferenceonPCSA. Byintroducinghyper-parameters & =[ & 1 ;:::;& d 1 ] > and ' =[ ' 1 ;:::;' d 2 ] > fora hierarchical Normal-Gammaprior on ( A ; & ) and ( B ; ' ) respectively[3,7,20],wehavetheprior p (  ) as p (  ; & ; ' )= p ( ˝ ) p ( A ; & ) p ( B ; ' ) ;p ( ˝ )= ˝ j u ˝ ;v ˝ ) ; p ( A ; & )= p ( A j & ) p ( & ) ;p ( A j & )= d 1 Y i =1 N ( a  i j 0 D 1 ; I D 1 =& i ) ;p ( & )= d 1 Y i =1  & i j u & i ;v & i ) ; p ( B ; ' )= p ( B j ' ) p ( ' ) ;p ( B j ' )= d 2 Y i =1 N ( b  i j 0 D 2 ; I D 2 =' i ) ;p ( ' )= d 2 Y i =1  ' i j u ' i ;v ' i ) ; (2) where   u;v ) denotesaGammadistributionwithashapeparameter u andaninversescaleparam- eter v .Eachcolumn a  i ofthemappingmatrix A priori independentlyfollowsasphericalGaussian withaprecisionscalar & i ,i.e.,anautomaticrelevancedetermination(ARD)typeprior[14].Each precision & i furtherfollowsaGammapriorforcompletingtheoftheBayesianmodel. Itiscomputationallyintractabletoevaluatethemarginallikelihood p ( X )= R p ( X j  ) p (  )d  , where  = f Z ; Y ;  ; & ; ' g representsthesetofallparametersandlatentvariables.SinceMCMC samplersareinefforhighdimensionaldata,thispaperchoosesvariationalinferenceinstead [11],whichintroducesadistribution Q (  ) andapproximatesmaximizingthelogmarginallike- lihood log p ( X ) bymaximizingalowerbound L ( Q )= R Q (  )log p ( X ;  ) Q (  ) d  .Fortractability, Q (  ) isfactorizedintothefollowingform: Q (  )= Q ( Y ) Q ( Z ) Q ( A ) Q ( B ) Q ( ˝ ) Q ( & ) Q ( ' ) ;Q ( Y )= D 2 Y i =1 Q ( y  i ) ;Q ( Z )= D 1 Y i =1 Q ( z  i ) ; Q ( A )= d 1 Y i =1 Q ( a  i ) ;Q ( B )= d 2 Y i =1 Q ( b  i ) ;Q ( & )= d 1 Y i =1 Q ( & i ) ;Q ( ' )= d 2 Y i =1 Q ( ' i ) : (3) Maximizing L ( Q ) w.r.t.theabove Q ( # ) for 8 # 2  leadstothefollowingexplicitconjugateforms Q ( y  t )= N ( y  t j  y  t ;   Y ) ;Q ( z  i )= N ( z  i j  z  i ;   Z ) ; Q ( & i )= & i j  u & i ;  v & i ) ;Q ( ' i )= ' i j  u ' i ;  v ' i ) ; Q ( a  i )= N ( a  i j  a  i ;  A I D 1 ) ;Q ( b  i )= N ( b  i j  b  i ;  B I D 2 ) ;Q ( ˝ )= ˝ j  u ˝ ;  v ˝ ) : (4) Forexpressionsimplicity,wedenote  A =[  a  1 ;:::;  a  d 1 ] andsimilarlyfor  B ,  Y and  Z .During maximizing L ( Q ) ,thesolutionsof  Y and  Z arebundledandconditionaloneachother:  Y = h ˝ i S A  A > ( X   Z >  B > ) ;   Y = S A ; S A =  (1+ h ˝ i D 1   A ) I d 1 + h ˝ i  A >  A   1 ;  Z = h ˝ i S B  B > ( X   A  Y ) > ;   Z = S B ; S B =  (1+ h ˝ i D 2   B ) I d 2 + h ˝ i  B >  B   1 ; (5) where  isexpectationand h ˝ i = u ˝ =  v ˝ .Directlyupdatingbytheaboveconvergesneitherquickly norstably.Insteadafterputtingoneequationintotheother,weattainaSylvesterequation[10]and canefsolveitbymanytools.Then  Z isobtainedbysolving L Z 1  ZL Z 2   Z + L Z 3 = 0 , with L Z 1 = h ˝ i 2 S B  B >  B ; L Z 2 =  AS A  A > ; L Z 3 = h ˝ i S B  B > X >  I D 1 h ˝ i  AS A  A >  ; (6) whosesolutionisfurtherputintoEq.(5)toupdate  Y 2 .Given  Y and  Z ,updating (  A ;  B ) issimilar. Theremaindersof Q (  ) inEq.(4)areupdatedas   A = d 1 = tr  S Y  1  ; S Y =  h ˝ i K Y +diag( h & i )   1 ; K Y =  Y  Y > + D 2   Y ;   B = d 2 = tr  S Z  1  ; S Z =  h ˝ i K Z +diag( h ' i )   1 ; K Z =  Z  Z > + D 1   Z ; 2 Thechoiceofcomputing  Z isbasedontheassumption D 1  D 2 forlearningefy. 3  u ˝ = u ˝ + D 1 D 2 2 ;  u & = u & + D 1 2 1 d 1 ;  u ' = u ' + D 2 2 1 d 2 ;  v ˝ = v ˝ + D 1 2 tr(   Y  A >  A +   A K Y )+ D 2 2 tr(   Z  B >  B +   B K Z )+ 1 2 jj ( X   A  Y   Z >  B > ) jj 2 F ;  v & = v & + 1 2 diag(  A >  A )+ D 1   A 2 1 d 1 ;  v ' = v ' + 1 2 diag(  B >  B )+ D 2   B 2 1 d 2 ; (7) where h & i =[ u & 1 =  v & 1 ;:::;  u & d 1 =  v & d 1 ] > , h ' i =[ u ' 1 =  v & 1 ;:::;  u ' d 2 =  v ' d 2 ] > , tr(  ) standsfortrace, diag(  ) inter-convertsbetweenavectorandadiagonalmatrix,and jjjj F istheFrobeniusnorm. Inimplementation,allGammapriorsinEq.(2)aresettobevagueas   10  3 ; 10  3 ) .During learning,redundantcolumnsof  A and  B willbepushedtoapproachzeros,whichactuallymakes Bayesianmodelselectiononhiddendimensionalities d 1 and d 2 . 3Extensions 3.1FillingMissingValues Inmanyrealapplications, X isusuallypartiallyobservedwithsomemissingentries.Thegoalhere istoinfernotonlythePCSAmodelbutalsothemissingvaluesin X basedonthemodelstructure. SimilartothesettingsofPMAin[1],letusbeginwithafullmatrix e X ,wherethemissingvalues arerandomlyWedenote M = f ( i;j ):~ x ij ismissing g astheindexsetofthemissing valuestherein.Ineachiteration,weﬁpretendﬂthat e X istheobservedmatrixandupdate Q (  ) by Eqs.(6 ˘ 7).Thengiven Q (  ) ,themissingentries f ~ x ij :( i;j ) 2Mg areupdatedbymaximizing L ( Q ) ,i.e., ~ x ij = x ij with  X =argmax X L ( Q )=  A  Y +  Z >  B > .Thisupdatingmannerplaysa roleofadaptiveregularization[2],andperformswellinexperimentsastobeshowninSection4. Moreover,missingvaluesinPMA[1]needstoinferthecolumnandrowfactorsbyeither GibbssamplingorMAP.Incontrast,PCSAdirectlyemploys  Y [  Z thatwereestimatedalreadyin thevariationalinference,andthussavesthecomputingcost. 3.2BayesianSparsePCSA Asdiscussedabove,PCSAdescribesobservationsbymappinghiddenfeatures( Y and Z )inthe co-subspacesthrough A and B respectively,i.e., A and B servesimilarlytothetransformation matrixinFactorAnalysisandPPCA.Forhighdimensionaldata,theparameters A and B probably sufferfrominaccurateestimationsandarediftointerpret.isonepopularly- usedmethodtoimprovemodelinterpretabilityintheliterature.Inthispart,weextendtoprovidea BayesiantreatmentonthesparsePCSAmodel. LASSO[19]encouragesmodelsparsenessbyaddingan ` 1 regularizer,whichisequivalenttoa Laplacianprior.In[4],thesparsenessisadaptivelycontrolledbyassigningahierarchicalNormal- Jeffreys(NJ)prior.Paper[9]showedthattheNJpriorperformsbetterthantheLaplacianonsparse PPCA.Inthispaper,wechoosetoadopttheNJpriorforlearningasparsePCSAmodel. DifferentfromEq.(2),eachcolumnof A and B followsahierarchicalNormal-Jeffreysprior: p ( A j  A )= d 1 Y i =1 N ( a  i j 0 ; A i I D 1 ) ;p (  A )= d 1 Y i =1 1  A i ; with  A =[  A 1 ;:::; A d 1 ] > ; p ( B j  B )= d 2 Y i =1 N ( b  i j 0 ; B i I D 2 ) ;p (  B )= d 2 Y i =1 1  B i ; with  B =[  B 1 ;:::; B d 2 ] > ; (8) whichalsoencouragesthevariancesin  A and  B ofredundantdimensionstoapproachzeros. Theprioron ˝ remainsthesameasinEq.(2).Stillunderthevariationalinferenceframework, wenowlet  = f Z ; Y ;  g and Q (  )= Q ( Y ) Q ( Z ) Q (  ) takestheconjugateformsameasin Eq.(4).Inconsequence,weoptimize L ( Q ;  A ;  B )= R Q (  )log p ( X ;  j  A ;  B ) Q (  ) d  w.r.t. Q (  ) ,  A and  B ,where L ( Q ;  A ;  B )  log p ( X j  A ;  B ) .Posteriorinferenceremainsthesame asabove,exceptthatallappearancesof h & i and h ' i arereplacedwith [1  A 1 ;:::; 1  A d 1 ] T and 4 [1  B 1 ;:::; 1  B d 2 ] T respectively.Thengiven Q (  ) ,thevariances  A and  B areupdatedvia  A = 1 D 1 +2 [diag(  A >  A )+   A ] and  B = 1 D 2 +2 [diag(  B >  B )+   B ] . 3.3ModelingHigh-OrderTensorData Uptillnow,wehavebeentalkingaboutmodeling X whenitisamatrix,andthispartextendsthe PCSAmodelanditsBayesianinferencetocoverthecaseswhen X isstructuredasatensor.Tensors arehigher-ordergeneralizationsofvectors(1st-ordertensors)andmatrices(2nd-ordertensors)[6]. Eachdimensionofatensoriscalledasa mode ,andthe order ofatensorisdeterminedasthenumber ofitsmodes.Letusdenotetensorswithopen-faceuppercaseletters(e.g., X , Y , Z ),incomparison withthebolduppercaseletters(e.g., X , Y , Z )formatrices.A k th-ordertensor X canbedenoted by X 2R D 1  D 2  :::  D k ,whereitsdimensionalitiesineachmodeare D 1 ;D 2 ;:::;D k respectively. Anelementanda(1st-mode)vectorof X aredenotedby x j 1 j 2 :::j k and x  j 2 :::j k respectively,where 1  j i  D i foreach i =1 ;:::;k .Moreover,the1st-mode  transformof X ,denotedby F ( X ) 2R D 1  ( D 2 D 3 :::D k ) ,isobtainedbyconcatenatingallthe(1st-mode)vectorsof X .Viceversa, a ([ D 1 ;:::;D k ]) - tensorization ofamatrix X 2R D 1  ( D 2 :::D k ) isas T ( X ; [ D 1 ;:::;D k ]) 2 R D 1  D 2  :::  D k ,sothat T ( F ( X ) ; [ D 1 ;:::;D k ])= X .An i th mode-shift transformisas M ( X ;i ) 2R D i  D i +1  :::  D k  D 1  :::  D i  1 ,whichshiftsthemodessequentiallyinacycleanduntil the i th-modein X becomesthe1st-modein M ( X ;i ) . BasedontheabovethePCSAmodeldescribesa k th-ordertensordata X D 1  :::  D k throughthefollowinggenerativeprocess:(i)foreachmode i ,allelementsofthehiddentensor Y ( i ) 2R d i  D i +1  :::  D k  D 1  :::  D i  1 areassumedi.i.d.drawnfrom N ( y ( i ) j i j i +1 :::j k j 1 j i  1 j 0 ; 1) ; (ii)draweachelement x j 1 :::j k ˘N ( x j 1 :::j k j P k i =1 a ( i ) j i  y ( i )  j i +1 :::j k j 1 :::j i  1 ; 1 =˝ ) ,i.e., X isactually generatedbyamode-shiftco-subspaceaddition: X = E + k X i =1 M  T   X ( i ) ; [ D i ;:::;D k ;D 1 ;:::;D i  1 ]  ;k +2  i  ; (9) whereeach  X ( i ) = A ( i ) F ( Y ( i ) ) andthematrix A ( i ) 2R D i  d i maps Y ( i ) to X .Shortlynamedas PCSA- k ,thismodelhaslatenttensors f Y ( i ) g k i =1 andparameters  = f ˝ g[f A ( i ) g k i =1 withlatent scales f d i g k i =1 .When k =2 ,PCSA- 2 isexactlythePCSAinSection2.1onmatrixdata.Also,it canbeimaginedasakindofgroupFactorAnalysis[24]. SameasEq.(2),eachcolumnof A ( i ) takesahierarchicalNormal-Gammaprior,andtheBayesian inferenceinSection2.2canbetriviallyextendedforcoveringPCSA- k model.Pleaseseethedetails inthesupplementarymaterials.Excepttheinvolvementofthetensorstructureanditsoperators, thereisanotherdifferencecomparedwiththevariationalposteriorupdatingbasedonamatrix X . Remembering ( Q ( Y ) ;Q ( Z )) and ( Q ( A ) ;Q ( B )) pairwiseweredecoupledandupdatedbysolving Sylvesterequations,wecandecoupleneither f Q ( Y ( i ) ) g i nor f Q ( A ( i ) ) g i intoSylvesterequations forthegeneral k> 2 .Instead,sequentiallyforeach i =1 ;:::;k ,weupdateonly Q ( Y ( i ) ) (or Q ( A ( i ) ) )andkeeptheremaining f Q ( Y ( u ) ) g u 6 = i (or f Q ( A ( u ) ) g u 6 = i )ed. 4ExperimentalResults 4.1PredictingMissingEntriesinWeightMatrices On Emotions and CAL500 Data. TheproposedPCSAmodelcanbeviewedasaratherdirect extensionofthePMAmodel,whichshowedadvantagesoverGPR,LMCandPMFin[1].Following [1],theexperimentcomparesPCSAwithPMAinthemissingentriesofatruncatedlog- oddsmatrixinmulti-labelFor n samplesand m classes,theclassmembershipscan berepresentedasan n  m binarymatrix G .Atruncatedlog-oddsmatrix X isconstructedwith x ij = c if g ij =1 and x ij =  c if g ij =0 ,where c isnonzeroconstant.Inexperiments,certain entries x ij areassumedtobemissingandas ~ x ij byanalgorithm,andtheperformanceis evaluatedbytheclassmembershippredictionaccuracybasedon sign(~ x ij ) . Twomulti-labeldatasetsareunderconsideration,namely Emotions [22]and CAL500 [23].Alreadyusedin[1],the Emotions contains593sampleswith72numericat- 5 tributesin6classes,andthenumberofclassesthateachsamplebelongstorangesfrom1to3.The constructed X for Emotions isthus 593  6 .The CAL500 contains502sampleswith68numeric attributesin174classes,andtheminandmaxnumbersofclassesthateachsamplebelongstoare 13and48respectively.Theconstructed X for CAL500 isthus 502  174 ,i.e.,itssizeislargerand morebalancedthantheonefor Emotions . Totestthecapabilityindealingwithmissingvalues,theproportionofthemissinglabelsisincreased from 10% to 50% ,with 5% asastepsize.InsteadofGibbssampling,theMAPinferenceisusedin PMAimplementationforafaircomparison.After10independentrunsoneachdataset,Fig.1report- stheerrorratesforrecoveringthemissinglabelsinthetruncatedlog-oddsmatrices,byBayesian PCSA,BayesiansparsePCSAandPMA.Ontherelativelyunbalanced Emotions data,PCSAout- performssparsePCSAwhenthemissingproportionisnolargerthan 40% ,whilesparsePCSAtakes overtheadvantagewhentoomanyentriesaremissingduetotheincreasingimportanceofmodel sparsity.Onthemorebalanced CAL500 data,thesparsePCSAkeepsaslightoutperformanceover PCSA,againduetothesparsity.Moreover,PCSAandsparsePCSAalwaysperformconsiderably betterthanPMAonbothdatasets.Table1reportstheaveragetimecost,wheresparsePCSAshows alittlequickerconvergencethanPCSA.BotharemuchquickerthanPMA,sincetheydonotneed toeitherinvertlargecovariancesorinferthefactorduringmissingvalues(seeSection3.1). Figure1:Errorratesof10independentrunsfor recoveringthemissinglabelsin Emotions (left) and CAL500 (right)data. dataset: EmotionsCAL500 PCSA 4.0 17.3 sparsePCSA3.5 11.6 PMA 22.9 198.3 Table1:Averagetimecost(inseconds) oneachdatasetthroughout10indepen- dentrunsandallmissingproportions. On MovieLens and JesterJoke Data. Inmanyrealapplications,e.g.collaborative thesizeofthematrix X ismuchlargerthantheabove.Weproceedtoconsiderontwolarger weightdatasets:the MovieLens100K data 3 andthe JesterJoke3 data[8].Particularly,the MovieLens100K datasetcontains100Kratingsof943userson1682movies,whichareordinal valuesonthescale [1 ; 5] .The JesterJoke3 datacontainsratingsof 24983 userswhohaverated between15and35piecesofthetotal100jokes,wheretheratingsarecontinuousin [  10 : 0 ; 10 : 0] . Recentlyin[12],RobustBayesianMatrixFactorization(RBMF)wasproposedbyadoptinga Student- t priorinprobabilisticmatrixfactorization,andshowedpromisingresultsonpredicting entriesonboth MovieLens100K and JesterJoke3 data.Following[12],ineachrunweran- domlychoose 70% oftheratingsfortraining,andusetheremainingratingsasthemissingvalues fortesting.Giventhetruetestratings f r t g T t =1 andthepredictions f ~ r t g T t =1 ,theperformanceiseval- uatedbasedonthe rootedmeansquarederror (RMSE),i.e., RMSE= q 1 T P T t =1 ( r t  ~ r t ) 2 ,and the meanabsoluteerror (MAE),i.e., MAE= 1 T P T t =1 j r t  ~ r t j . After10independentruns,theaverageRMSEandMAEvaluesobtainedby(sparse)PCSAare reportedinTable2,incomparisonwiththebestresultsbyRBMF(i.e.,RBMF-RR)collectedfrom [12].SincePMArunsinefonhighdimensionaldataasinTable1,itisnotconsideredto theratingsinthisexperiment.ItisobservedthattheperformancebyPCSAonpredictingratingsis comparablewithRBMF.OnbothRMSEandMAEscores,thesparsePCSAfurtherimprovesthe correctnessandperformssimilarlytoorbetterthanRBMF. Table2:AverageRMSEandMAEon MovieLens100K (left)and JesterJoke3 (right). model RMSEMAE PCSA 0.9030.708 sparsePCSA 0.898 0.706 RBMF-RR[12]0.900 0.705 model RMSEMAE PCSA 4.4463.447 sparsePCSA 4.4133.434 RBMF-RR[12]4.4543.439 3 Downloadedfromwww.grouplens.org/node/73. 6 4.2CompletingPartiallyObservedImages Weconsidertwogreyscalefaceimagedatasets,namely Frey [15]and ORL [17]., Frey has 1965 imagesofsize 28  20 takenfromoneperson,andthedata X isthusa 560  1965 matrix; ORL has 400 imagesofsize 64  64 takenfrom 40 persons( 10 imagesperperson),andthedata X isthusa 4096  400 matrix.Appliedonthesematrices,thePCSAmodelisexpectedtoextractthe latentcorrelationsamongpixelsandimages.In[13],NeilLawrenceproposedaGaussianprocess latentvariablemodel(GPLVM)formodelingandvisualizinghighdimensionaldata.Recentlya BayesianGPLVM[21]wasdevelopedandshowedmuchimprovedperformanceonpixelsin partiallyobserved Frey faces.ThisexperimentcomparesPCSAwithBayesianGPLVM 4 . WhilePCSAcanutilizethepartialobservedsamples,theBayesianGPLVMcannot.Thusineach run,werandomlypick n f imagesasfullyobserved,andahalfpixelsoftheremainingimagesare furtherrandomlychosenasmissingvalues.Sameas[21],BayesianGPLVMusesthe n f imagesfor trainingandtheninfersthemissingpixels.Incontrast,(sparse)PCSAusesallimagesasawhole matrix.Inordertotesttherobustness,the n f for Frey isdecreasedgraduallyfrom1000to200, andfor ORL isdecreasedgraduallyfrom300to50.Performanceisevaluatedbythe correlation coef (CORR)andtheMAEbetweenthepixelsandthegroundtruth. On Frey and ORL datarespectively,Fig.2andFig.3reporttheCORRandMAEvaluesof10 independentrunsbyPCSA,sparsePCSAandBayesianGPLVM.BothPCSAandsparsePCSA performmoreaccuratelythanBayesianGPLVMincompletingthemissingpixels,andPCSAgives thebestmatching.Also,(sparse)PCSAshowspromisingstabilityagainstthedecreasingfully observedsamplesize n f ,andthistendencyiskeptevenwhenweassign all imagesarepartially observed(i.e., n f =0 ),asexbyFig.4.TheresultsbyBayesianGPLVMdeterioratemore obviously,becausethepartiallyobservedimageshavenocontributionduringlearning.Furthermore, theadvantageofPCSAbecomesmoreasweshiftfromthe Frey dataforasingle person,tothe ORL dataformultiplepersons.Itindirectlytheimportanceofextractingthe correlationsamongdifferentimages,ratherthankeepingthemindependent.SparsePCSAperforms worsethanPCSAinthistask,mainlybecauseitleadstoalittletoomanysparsedimensions. Figure2:Resultsof10runson Frey faces. Figure3:Resultsof10runson ORL faces. Figure4:ReconstructionexamplesbyPCSAwhenallimagesarepartiallyobserved: Frey (left) and ORL (right).Threerowsfromtoparetrue,observed,andreconstructedimages,respectively. 4.3CompletingPartiallyObservedImageTensor Weproceedtoconsidermodelingthefaceimagedataarrangedinatensor.Thedatasetunderconsid- erationisasubsetoftheCMU PIE database[18],andtotallyhas5100faceimagesfrom30individ- uals.Eachperson'sfaceexhibits170imagescorrespondingto170differentpose-and-illumination combinations.Eachnormalizedimagehas 32  32 greyscalepixels,andthedatasetisthusatensor X 2R 1024  30  170 ,whosethreemodescorrespondtopixel,identity,andpose/illumination,respec- tively.Figure5showssomeimageexamplesoftwopersons.ThePCSA- k model(with k =3 onthe 4 Weusethecodeinhttp://staffwww.dcs.shef.ac.uk/people/N.Lawrence/vargplvm/. 7 3rd-ordertensor X )inSection3.3isexpectedtoextracttheco-subspacestructures(i.e.,correlations amongpixels,identities,andposes/illuminationsrespectively)andthemissingvaluesin X .In [6],an M 2 SA methodwasproposedtoconductmultilinearsubspaceanalysiswithmissingvalues onthetensordata,viaconsecutiveSVDdimensionreductionsoneachmode. Figure5:TypicalnormalizedfaceimagesfromtheCMU PIE database. true:  true:  Figure6:TypicalmissingimagesbyPCSA- 3 .Originalimages(intheoddrows)are randomlypickedandremoved,andPCSA- 3 theimagesintheevenrows. Table3:AverageCORR(left)andMAE(right)of10runs byPCSA- 3 and M 2 SA ontheCMU PIE data. missingproportion: 10%20%30% PCSA- 3 0.9370.9260.908 M 2 SA 0.9280.9140.893 missingproportion: 10%20%30% PCSA- 3 14.618.321.5 M 2 SA 17.821.924.8 Here,therandomlydrawnmissingvaluesarenotpixelsasinSection4.2but images .Comparedwith thetruemissingimages,thegoodnessofthemissingimagesisevaluatedagainbyCORRand MAE.Stilltotestthecapabilityindealingwithmissingvalues,theproportionofthemissingimages isconsideredas 10% , 20% and 30% ,respectively.After10independentrunsforeachproportion,the averagesCORRandaverageMAEofthemissingimagesbyPCSA- 3 and M 2 SA arecompared inTable3.Duringimplementing M 2 SA ,theratioofthesubspacerankovertheoriginalrankisset as 0 : 6 accordingtoFig.9in[6].AsshowninTable3,PCSA- 3 achievesthebetterperformancein allcases.Fordemonstration,Fig.6showssomemissingimageswhenthemissingproportion is 20% ,whichmatchtheoriginalimagessteadilywell. 5ConcludingRemarks WehaveintroducedtheProbabilisticCo-SubspaceAddition(PCSA)model,whichsimultaneously capturesthedependentstructuresamongbothrowsandcolumnsindatamatrices(tensors).Vari- ationalinferenceisproposedonPCSAforanapproximateBayesianlearning,andtheposteriors canbeefandstablyupdatedbysolvingSylvesterequations.Capabletomissingvalues, PCSAisextendedtonotonlysparsePCSAwiththehelpofaJeffreysprior,butalsoPCSA- k that modelsarbitrary k th-ordertensordata.Althoughsomewhatsimpleandnotdesignedforanypartic- ularapplication,theexperimentsdemonstratetheeffectivenessandefyofPCSAonmodeling matrix(tensor)dataandmissingvalues.TheperformancebyPCSAmaybefurtherimproved byconsideringnonlinearmappingswiththekerneltrick,whichhoweverisnotthatdirectduetothe couplinginnerproductsbetweentheco-subspaces. Acknowledgments Theauthorwouldliketothanktheanonymousreviewersfortheirusefulcommentsonthispaper. 8 References [1] A.Agovic,A.Banerjee,andS.Chatterjee.Probabilisticmatrixaddition.In Proc.ICML ,pages1025Œ 1032,2011. [2] C.M.Bishop.TrainingwithnoiseisequivalenttoTikhonovregularization. NeuralComputation , 7(1):108Œ116,1995. [3] C.M.Bishop.Variationalprincipalcomponents.In Proc.ICANN'1999 ,volume1,pages509Œ514,1999. [4] M.A.T.Figueiredo.AdaptivesparsenessusingJeffreysprior.In AdvancesinNIPS ,volume14,pages 679Œ704.MITPress,Cambridge,MA,2002. [5] A.E.GelfandandS.Banerjee.Multivariatespatialprocessmodels.InA.E.Gelfand,P.Diggle,P.Guttorp, andM.Fuentes,editors, HandbookofSpatialStatistics .CRCPress,2010. [6] X.Geng,K.Smith-Miles,Z.-H.Zhou,andL.Wang.Faceimagemodelingbymultilinearsubspace analysiswithmissingvalues. IEEETrans.Syst.,Man,Cybern.B,Cybern. ,41(3):881Œ892,2011. [7] Z.GhahramaniandG.Hinton.TheEMalgorithmformixturesoffactoranalyzers.TechnicalReport CRG-TR-96-1,DepartmentofComputerScience,UniversityofToronto,Toronto,Canada,1997. [8] K.Goldberg,T.Roeder,D.Gupta,andC.Perkins.Eigentaset:Aconstanttimecollaborative algorithm. InformationRetrieval ,4(2):133Œ151,2001. [9] Y.GuanandJ.Dy.Sparseprobabilisticprincipalcomponentanalysis.In Proc.AISTATS'2009,JMLR W&CP ,volume5,pages185Œ192.2009. [10] D.Y.HuandL.Reichel.Krylov-subspacemethodsfortheSylvesterequation. LinearAlgebraandIts Applications ,172:283Œ313,1992. [11] M.I.Jordan,editor. Learningingraphicalmodels .MITPress,CambridgeMA,1999. [12] B.Lakshimanarayan,G.Bouchard,andC.Archambeau.RobustBayesianmatrixfactorisation.In Proc. AISTATS'2011,JMLRW&CP ,volume15,pages425Œ433.2011. [13] N.D.Lawrence.Gaussianprocesslatentvariablemodelsforvisualisationofhighdimensionaldata.In AdvancesinNIPS ,volume16,pages329Œ336.MITPress,Cambridge,MA,2003. [14] R.M.Neal. BayesianLearningforNeuralNetworks .Springer-Verlag,NewYork,1996. [15] S.Roweis,L.K.Saul,andG.Hinton.Globalcoordinationoflocallinearmodels.In AdvancesinNIPS , volume14,pages889Œ896.MITPress,Cambridge,MA,2002. [16] R.SalakhutdinovandA.Mnih.Probabilisticmatrixfactorization.In AdvancesinNIPS ,volume20,pages 1257Œ1264.MITPress,Cambridge,MA,2008. [17] F.SamariaandA.Harter.ParameterisationofastochasticmodelforhumanfaceIn Proc. 2ndIEEEWorkshoponApplicationsofComputerVision ,pages138Œ142,1994. [18] T.Sim,S.Baker,andM.Bsat.TheCMUpose,illumination,andexpressiondatabase. IEEETrans.Patten Anal.Mach.Intell. ,25(12):1615Œ1618,2003. [19] R.Tibshirani.Regressionshrinkageandselectionviathelasso. J.R.Stat.Soc.B ,58(1):267Œ288,1996. [20] M.E.TippingandC.M.Bishop.Mixturesofprobabilisticprincipalcomponentanalyzers. Neural Computation ,11(2):443Œ482,1999. [21] M.TitsiasandN.Lawrence.BayesianGaussianprocesslatentvariablemodel.In Proc.AISTATS'2009, JMLRW&CP ,volume9,pages844Œ851.2010. [22] K.Trohidis,G.Tsoumakas,G.Kalliris,andI.Vlahavas.Multilabelofmusicintoemotions. In Proc.Intl.Conf.onMusicInformationRetrieval(ISMIR) ,pages325Œ330,2008. [23] D.Turnbull,L.Barrington,D.Torres,andG.Lanckriet.Semanticannotationandretrievalofmusicand soundeffects. IEEETrans.Audio,SpeechandLang.Process. ,16(2):467Œ476,2008. [24] S.Virtanen,A.Klami,S.A.Khan,andS.Kaski.Bayesiangroupfactoranalysis.In Proc.AISTATS'2012, JMLRW&CP ,volume22,pages1269Œ1277.2012. [25] Z.Xu,K.Kersting,andV.Tresp.Multi-relationallearningwithGaussianprocesses.In Proc.IJCAI'2009 , pages1309Œ1314,2009. 9  
MeasuringthePopularityofJobSkills inRecruitmentMarket:AMulti-CriteriaApproach TongXu 1 ; 2 ,HengshuZhu 2 ;  ,ChenZhu 2 ,PanLi 1 ; 2 ,andHuiXiong 1 ; 3 ;  1 AnhuiProvinceKeyLabofBigDataAnalysisandApplication,UniversityofScienceandTechnologyofChina tongxu@ustc.edu.cn,lp970625@mail.ustc.edu.cn 2 BaiduTalentIntelligenceCenter,BaiduInc. f zhuhengshu,zhuchen02 g @baidu.com 3 RutgersBusinessSchool,RutgersUniversity hxiong@rutgers.edu Abstract Tocopewiththeacceleratingpaceoftechnologicalchanges, talentsareurgedtoaddandrefreshtheirskillsforstayingin activeandgainfulemployment.Thisraisesanaturalquestion: whataretherightskillstolearn?Indeed,itisanontrivialtask tomeasurethepopularityofjobskillsduetothediv criteriaofjobsandthecomplicatedconnectionswithinjob skills.Tothatend,inthispaper,weproposeadatadriven approachformodelingthepopularityofjobskillsbasedon theanalysisoflarge-scalerecruitmentdata.ally,we buildajobskillnetworkbyexploringalargecorpusof jobpostings.Then,wedevelopanovel S kill P opularitybased T opic M odel( SPTM )formodelingthegenerationoftheskill network.Inparticular,SPTMcanintegratedifferentcriteria ofjobs(e.g.,salarylevels,companysize)aswellasthela- tentconnectionswithinskills,thuswecaneffectivelyrank thejobskillsbasedontheirmulti-facetedpopularity.Exten- siveexperimentsonreal-worldrecruitmentdatavalidatethe effectivenessofSPTMformeasuringthepopularityofjob skills,andalsorevealsomeinterestingrules,suchasthepop- ularjobskillswhichleadtohigh-paidemployment. Introduction Withthecomingofknowledgeeconomyera,thecompeti- tionforskilledtalentsbecomesextremelysevere.Indeed, talentswhoarewithemployer-targetedskillswill gethiredeasilyandobtainattractivereward,suchashigh compensationandquickpromotion.However,therestillex- istsaﬁSkillGapﬂ(SkillsYouNeed2017)betweenemployers andjobseekers.,ontheonehand,companies areurgenttoseekhighly-skilledtalents,especiallywhen theirdemandsofjobskillschangefrequentlyduetothe severebusinesscampaigns.Ontheotherhand,tocompete againstothercandidatesandoutstandfromthelargeperson- nelpool,jobseekershavetoacquirecertainjobskillsthat targetcompaniespaythemostattentionon.Suchsituation isespeciallyinmassive-populationcountrieslike China,where6millionfresh-graduatestudentsenterthejob marketeveryyear(Daily2016),farexceedingthenumber ofavailablejobsontherecruitmentmarket.Therefore,itis veryappealingtotimelymeasurethepopularityofjobskills inrecruitmentmarkettoguidetheemployment.  CorrespondingAuthor. Copyright c  2018,AssociationfortheAdvancementofial Intelligence(www.aaai.org).Allrightsreserved. Indeed,theanalysisofjobskillswithmeasuringtheir popularityisnotnovel.Currently,manyorganizations likeLinkedin(LinkedIn2017)andComputerWorld(World 2017)havereleasedannualreportsforthehottestjobskills, whichprovideinsightfulguidanceforjobseekers.How- ever,therestillexistsseveralcriticalchallengesalongthis line.First,inpractice,differentpeopletendtohavedifferent criteriafortheirdreamjobs,e.g.,someapplicantspursue highsalary,sotheychoosethebigcompaniesinhugecities, whilesomeotherstendtoselectsmallcompaniesinhome- town,evenwithlowerincome.Second,jobskillsarenotiso- lated,butmutuallyconnectedtoformthelatentconnections, e.g.,skillslikeﬁJavascriptﬂ,ﬁPHPﬂandﬁNode.jsﬂusually appearedtogether,astheyareallnecessaryfortheﬁWeb Front-EndDevelopmentﬂ.Therefore,hierarchicalstructure withmutualconnectionsshouldbeintegratedwhenmeasur- ingpopularityofjobsskills. Toaddresstheaforementionedissues,inthispaper,we proposeadata-drivenapproachformeasuringthepopular- ityofthejobskillsbasedontheanalysisoflarge-scalere- cruitmentdata.,webuildajobskillnetwork ﬁ Skill-Net ﬂbyexploringalargecorpusofjobpostingsinre- cruitmentmarket.Then,wedevelopanovel S kill P opularity based T opic M odel( SPTM )formodelingthegenerationof jobskills.Particularly,SPTMcanintegratedifferentcrite- riaofjobs(e.g.,salarylevel,companysize),skillcategories, andthelatentconnectionswithinskills.Therefore,withthe helpofSPTM,wecaneffectivelyrankthejobskillsbasedon theirmulti-perspectivepopularity.Finally,weevaluatethe proposedmodelwithreal-worldrecruitmentdataandexten- siveexperiments.,thecontributionofthispaper canbesummarizedasfollows:  Weexploretomeasurethepopularityofjobskillsinre- cruitmentmarketwithamulti-criteriaperspective,which canhelptoeliminatetheﬁSkillGapﬂbetweenemployers andjobseekersintalentrecruitment.  WeproposeanovelSPTMmodelbasedontheanalysisof large-scalerecruitmentdata,whichcaneffectivelymodel therelationsbetweencategoricalskillsandjobcriteria.  Weevaluatetheproposedmodelwithreal-worldrecruit- mentdataandextensiveexperiments.Theexperimental resultsclearlyvalidatetheeffectivenessofSPTM,and furtherrevealsomeinterestingdiscoveries. arXiv:1712.03087v1  [cs.CY]  6 Dec 2017Table1:Atoyexampleofourjobpostingdataset. PostIDCompanyScaleSalaryLocationFinancialRoundWorkTypeJobDescription 1000037MediumLowHugeCitiesARoundFulltimeFamiliarwithPython... 1000046MediumVeryLowHugeCitiesBRoundFulltimeDevelopmentofAndroid... 1000082SmallLowBigCitiesDRoundFulltimeCSBachelorDegreerequired... 1000144VeryBigHighHugeCitiesListedFulltimeWeb-designoriented... 1000268BigMediumNormalCitiesDRoundFulltimeSufwithJava... 1000462BigHighHugeCitiesListedFulltimeProgrammingwithinLinux... DataDescription Inthispaper,weattempttodevelopadata-drivenapproach forcomprehensivelymeasuringthejobskillpopularity.Spe- cially,todealwiththistask,wecollectedtworeal-worldre- cruitmentdatasets,includingalarge-scalejobpostingdata setwithdetailedrequirementforprovidingjobcriteriaand trainingmodel,aswellasacategoricalskilllist togeneratetheﬁSkill-Netﬂ. JobpostingDataset Tobeweextractedthejobpostingdatasetfroma famousonlinerecruitingmarket(Zhuetal.2016b),rang- ingfrom2013to2015,whichcontains892,454jobpostings intotal.Amongthem,381,578recordsareselectedwithin- cludingatleastonedetailedskillrequirementintheirjob descriptions.Someexamplesofjobpostinginourdataset ispresentedinTable1,inwhichwemainlylistthecrite- riaofjobs,whilethedetailsofjobrequirementisomitted. Besides,thedistributionsofjobpostingswithrespecttodif- ferentinformationcategoriesareillustratedinFigure1. ,tosummarizethecriteriaofjobs,wecon- clude5categoriesofcriterialabelsasfollows:  Salary :Dividedinto5levels(VeryHigh,High,Medium, Low,VeryLow),correspondingtomonthlysalary > 30k, 20k-30k,10k-20k,5k-10k, < 5krespectively. (a)Location (b)Company (c)Salary (d)PostDate Figure1:Thedistributionsofjobpostingswithrespectto differentinformationcategories. (a)Frequencyofskills (b)Frequencyofskills-Log Figure2:Thedistributionofskillsfrequency,inwhichY- axisindicateshowmanypostingscontaintheskill.  CompanyScale :Dividedinto5categories(VeryBig, Big,Medium,Small,VerySmall),correspondingtothe numberofemployees > 2000,500-2000,100-500,50- 100, < 50,respectively.  Location :Dividedinto3categories(HugeCities,Big Cities,NormalCities),correspondingtothescaleoflo- catedcities.  FinancingRound :Dividedinto7categories(Angel,A, B,C,D,Listed,Unknown),correspondingtothedifferent statusofround.  WorkType :Dividedinto3categories(Fulltime,Part- time,Intern),correspondingtothetypeofjobs. SkillListandﬁSkill-Netﬂ Atthesametime,fordescribingthejobskills,weextracted alistofskillsfromanonlineITcommunityin China(CSDN2016),includingtwolevelsofskills,i.e.,the skillcategories ,andthe detailedskills whichbelongsto onecategory.Totally,1,729skillswithin54categorieswere extracted.Toensurethequalityofouranalysis,wehavere- movedseveraldetailedskillsthatarenotwellortoo sparsetobeshowninajobdescription,andthenbuiltthe standardﬁ dictionary ﬂofjobskillsformodeling.Thefre- quenciesofskillsinourjobpostingdatasetanditslogform ( e asthebase)areshowninFigure2,whichindicatesthe skillssuffera longtaileffect ,i.e.,onlyafewjob skillsarepopularinrecruitmentmarket. Atthesametime,thisdatasetwillnotonlyprovidethe categoryofskills,butalsohelptogeneratetheﬁ Skill-Net ﬂ. Asmentionedabove,jobskillsarenotisolated,butmutu- allyconnectedfollowinglatenthierarchicalstructure,which cannotbeneglectedwhenmeasuringskillpopularity.Un- fortunately,noexposedrelationshipbetweenskillscouldbe Figure3:ThesnapshotofﬁSkill-Netﬂ. directlyextractedfromoriginaldata.Thus,inthispaper, basedonthestandardﬁdictionaryﬂabove,wepickupthe skillsineachjobposting,andthenbuildthe ﬁSkill-Netﬂfollowingtheheuristicmethodwhichiscom- monlyusedinsocialnetworkanalysis(Xuetal.2016b).Par- ticularly,weassumethatiftwoskillsco-occurinthesame jobposting,theywillbemutuallyconnected.Figure3shows asnapshotoftheﬁSkill-Netﬂ,whichisgeneratedbasedon ourreal-worldrecruitmentdataset. PopularityModelingforJobSkills AsthedatasetsandﬁSkill-Netﬂareprepared,inthissection, weturntointroducethedetailsofournovelSPTMapproach foreffectivelymeasuringthepopularityofjobskills.Tofa- cilitatetheunderstanding,weillustratetheoverviewofour approachformeasuringtheskillpopularityinFigure4. OverviewofSPTMModel Generally,wetargetatmeasuringthepopularityofskillsun- derdifferentcriteria(e.g.,salarylevel,companysize,etc.). Therefore,weproposetheSPTMapproachtolearnthela- tentskilltopicscombiningmulti-criteriaandhierarchical categoriesofjobskills,whichcouldbeusedtoestimatethe popularityofajobskillundercertaincriteria,viaestimating theprobabilityofskillincorrespondingtopic. Specially,inSPTM,wetreateachskill,named central skill inourpaper,asaﬁ document ﬂinﬁSkill-Netﬂ.Also, thoseneighboringskills,named skilltokens ,areregardedas correspondingﬁ wordtokens ﬂwithin,whichissimilarwith thebasicideaofTopicModel.Figure5showsanexample ofthedocumentgenerationprocessinSPTM,inwhicheach lineoftherightpartindicatesaﬁdocumentﬂ.Besides,aswe knowneachjobcanbebysomestandardcriteria. Andwefurtherthinkifacentralskillhasappearedinajob posting,itshouldhassomerelationshipswiththecriteria labelsofthejob(e.g.,veryhighsalary,verybigcompany, etc.).Bythisassumption,wegenerateasetof criteriala- bels foreachcentralskill. JustasinLDA,wealsouse  m and ˚ k torepresentthe distributionovertopicsfor m -thcentralskillandthedistri- Table2:MathematicalNotations. SymbolDescription  Dirichletpriorforgenerating   Dirichletpriorforgenerating ˚  Dirichletpriorforgenerating ˇ  Distributionparametersforgenerating  m ? ˇ Distributionparametersforgenerating  l ?  m ? Topicpresence/absencevectorof m -thcentralskill  m Distributionovertopicsfor m -thcentralskill z m;i Topiclabelof i -thskilltokenof m -thcentralskill l m;i skillcategoryof i -thskilltokenof m -thcentralskill l s skillcategoryof s -thskill w m;i i -thskilltokenof m -thcentralskill ˚ k;l Distributionoverskillsfor k -thtopicand l -thcategory K Numberoftopics/criteria S Numberofskills L Numberofskillcategories M Numberofcentralskills N m Numberofskilltokensof m -thcentralskill Algorithm1 TheGenerationProcessofSPTM. 1: for eachtopic k 2f 1 ; 2 ;  ;K g do 2: Generate ˚ k ˘ Dir (   ) 3: for eachcentralskill m 2f 1 ; 2 ;  ;M g do 4: for eachtopic k 2f 1 ; 2 ;  ;K g do 5: Generate  m k 2f 0 ; 1 g˘ Bernoulli (   k ) 6: Generate ˇ m ˘ Dir (   ) 7: Generate  m ˘ Dir (     m ? ) 8: for eachskilltoken i 2f 1 ; 2 ;  ;N m g do 9: Generate z m;i ˘ Multi (   m ) 10: Generate l m;i ˘ Multi (  ˇ m ) 11: Generate w m;i ˘ Multi (  ˚ z m;i ;l m;i ) butionoverskillsfor k -thtopics.Besides,tomodelthecon- nectionbetweencriterialabelsandcentralskills,wefollow theideasin(Ramageetal.2009)andassociateeachtopic withacriterialabel.Specially,wehave  m ? topresentsthe criterialabelsof m -thcentralskill,where  m ? isabinary topicpresence/absencevectorandeach  m k 2f 0 ; 1 g .Then tokeeptheabovehypothesis,wegeneratethe m -thcentral skill'sdistributionovertopic  m by Dir (     m ? ) . Besides,accordingtoourbasicassumptions,thepopular- ityofskillswillbeaffectednotonlybymutualconnections andcriteria,butalsocorrespondingskillcategories,which canfurtherhelpusunderstandhierarchicalskillrelationship frommacroperspective.Wefollowtheideasin(Huaietal. 2014)andhypothesizeeachcentralskillalsohasadistri- butionoverskillcategories. l s isusedtorepresenttheskill categoryofskill s (e.g.,thecategoryofskillﬁCNNﬂisﬁma- chinelearningﬂ).Thustheskilltoken w m;i isgener- atedby Multi (  ˚ z m;i ;l m;i ) . ThedetailedgenerationprocessofSPTMisillustratedin Algorithm1,andrelatedmathematicalnotationsaresum- marizedinTable2. ModelInference Then,wewillintroducethetechnicalsolutionofmodel inference.Specially,weusecollapsedGibbssamplingfor training,wherethesamplingprobabilityof z m;i ,whichin- Figure4:Theoverviewofourapproachformeasuringthepopularityofjobskills. Figure5:ThegenerationofdocumentsinSPTM. dicatesthetopicof i -thskilltokenin m -thcentralskill,is givenby: P ( z m;i = j j z m;  i ;w m;i ;l m;i ) / P ( w m;i ;l m;i j z m;i = j;z m;  i ;w m;  i ;l m;  i ) P ( z m;i = j j z m;  i ) ; (1) where w m;  i isallofskilltokensof m -thcentralskillex- cept i -thoneand l m;  i issimilar. z m;  i isthetopiclabelsof w m;  i .Here P ( w m;i ;l m;i j z m;i = j;z m;  i ;w m;  i ;l m;  i ) standsfortheappearanceprobabilityoftheskilltoken w m;i anditscategory l m;i given z m;i = j ,and P ( z m;i = j j z m;  i ) representstheprobabilityof z m;i = j in m -thcentralskill regardlessof i -thskilltoken.Thenwecancalculatethese twoprobabilitiesasfollows: P ( w m;i ;l m;i j z m;i = j;z m;  i ;w m;  i ;l m;  i ) = n w m;i ;l m;i ;j;  ( m;i ) +  w P S s =1 ( n s;l s ;j;  ( m;i ) +  s )  n m;l m;i ;  ( m;i ) +  l m;i P L l 0 =1 ( n m;l 0 ;  ( m;i ) +  l 0 ) ; (2) P ( z m;i = j j z m;  i )= ( n m;j;  ( m;i ) +  j )   m j N m  1+ P K k =1  k   m k ; (3) where n s;l;j;  ( m;i ) iscountof s -thskillin l -thskillcategory and j -thtopicregardlessofskilltoken w m;i and n m;j;  ( m;i ) iscountof j -thtopicappearingin m -thcentralskillexpect w m;i . n m;l;  ( m;i ) isthecountof l -thskillcategoryappear- ingin m -thcentralskillexpect w m;i .Afterseveraliterations ofGibbssampling,wecouldthenestimatetheconditional probabilityofaskill w andthecorrespondingskillcategory l withineachtopicasfollows: P ( w;l j z = j )= P ( s j l;z = j ) P ( l ) ; = n w;l;j +  w P S s 0 =1 ( n s 0 ;l s 0 ;j +  s 0 ) n m;l +  l P L l 0 =1 ( n m;l 0 +  l 0 ) ; (4) AfterlearningandinferencingofSPTM,wecouldnow measurethepopularityofajobskillunderdifferentcriteria. Specially,givenagroupofcriteria = f  m i g ,in whicheachindicatesacriterion,(e.g.salary,location,com- panyscale,roundorworktype),consideringthat differentcriteriaareindependentwitheachother,wecanes- timatethepopularityofskill w andthecorrespondingskill category l viaBayes'Formulaby: P ( w;l j = X  m i 2   P  m i )   K X j =1 P ( z = j j  m i ) P ( w;l j z = j )   : (5) Experiments Inthissection,wewillevaluatetheeffectivenessofourpur- posedmodel.Specially,ourSPTMwillbevalidatedonreal- worldrecruitmentdatasetscomparedwithtwotypicalbase- lines.Afterthat,wediscussabouttheexperimentalresults withcapturingsomeinterestingrules,andacasestudywill beconductedtofurtherourbasicideas. ExperimentalSetup Firstly,wesummarizethedetailsofexperimentalsetupin thissubsection,includingdatapre-processing,parameter settingandbaselinemethods. DataPre-processing. AsintroducedinSection2,wecon- ductedourexperimentsonthereal-worlddatasetcollected fromanonlinerecruitingmarketandITcommunity.Toen- suretheeffectivenessandavoidmissingvalues,we outthosejob-descriptionrecordswithoutanydetailedskill, whichcomposeof57.24%oftheoriginaldata.Correspond- ingly,thoseskillswhichneverappearinjob-description recordswillalsoberemoved. ParameterSetting. InourSPTMmodel,threesetsofpa- rametersshouldbepreset.First,forthegenerationprocess ofSPTM,wemanuallyselectedthevaluesforDirichletprior with(  =1,  ,  ,  =0.01)basedonmultipleoptimizations. Second,wesettothenumberoftopic K as23,correspond- ingtothe23uniquecriteriawithin5differentcategories mentionedin DataDescription .Finally,forthemodeltrain- ingprocess,themaximaliterationtimeswassetas800,and thresholdofstopconditionwassetas 10  3 . BaselineMethods. TovalidatetheperformanceofSPTM formeasuringskillpopularity,twotypicalbaselinemethods areselectedasbaselinemethodsasfollows:  Frequency. Theintuitivetechniquetomeasurethepop- ularityofjobskills.Specially,wecalculatedtheappear- anceofeachskillunderdifferentcriterialabels,andthen usedthenormalizationofskillfrequencyasconditional probabilityforrankingthepopularity.  LLDA. Labeled-LDA(LLDA)(Ramageetal.2009)isa topicmodelsimilarwithourSPTM,whichhoweverdoes notconsidertheskillcategoriesandmutualconnections withinskills.Weusedthesamefeaturelabelsandidenti- calparametersasSPTMfortrainingL-LDA. ExperimentalResults ForthecomprehensivevalidationofSPTM,twokindsof experimentswereexecuted,i.e.,skill-orientedtopicevalua- tion,andjobskillrecommendationforagivenjobposting. Skill-orientedTopicEvaluation. Toevaluatetheeffective- nessofourSPTMapproach,especiallyforthetopicmod- elingperformance,wepickedupthetop8skillsofeach topiclistedbyallthethreetechniques,andthenasked5se- niorHRexpertstojudgethecorrectness.Indetail,twofac- torswillbeconsidered,namelythe ValidityMeasure ( VM ) whichmeasureshowmanytopicsextractedare valid (i.e.,at least4skillsamong8skillsaremutuallycoherentinjobre- cruitment),andthe CoherenceMeasure ( CM ),whichcounts howmanyskillsarerelevanttotheother7skillsinthesame topic.Thesetwometricscouldbecalculatedasfollows: VM = # of valid topics # oftopics ;CM = # of relevant skills # ofskills : (6) Theaverageresultsof5expertsareshowninTable3,in whichourSPTMoutperformsallthebaselinesintermsof bothVMandCMmetrics.Moreover,asweonlyselected thetop8skillsofeachtopic,therankingperformanceofjob skillsviaourSPTMapproachhasbeenalsovalidated. JobSkillRecommendation. Then,weturntoevaluatethe jobskillrecommendationtask.AsSPTMcouldmeasurethe popularityofjobskillswithdifferentcriteria,itisintuitive torecommendappropriatejobskillswithgivenjobdescrip- tion.,theseresultswillbeforbothem- ployersandjobseekers,sincetheycouldofferguidancefor companiestofollowthetrends,andalsohelpthecandidates toselecttheproperpositions,orpreparefortheinterview. Specially,fortheevaluation,weselectthetestsetformed by50,000jobpostingsexcludedfromthetrainingset,which contains623differentcombinationofcriteria,andthenuse thelog-likelihoodasmeasurementtoevaluatetheperfor- mance.TheresultsareshowninTable4,whichindicates thatourSPTMapproachperformsbetterwithrecommend- ingmoreappropriatejobskillscomparedwiththebaselines. Table3:AverageVM/CMComparison ModelVMCM SPTM 0.8350.625 Labeled-LDA0.7000.300 Frequency0.5750.325 Table4:Recommendationperformanceinlog-likelihood Modellog-likelihood SPTM -6320262.155 Labeled-LDA-7065396.092 Frequency-6511843.161 DiscussionandInsights AstheeffectivenessofSPTMhasbeenvalidated,inthissub- section,wewillfurtherdiscussaboutsomeinterestingdis- coveriesasfollows.Specially,welistthetop8skillsranked byourSPTMframworkfordifferentjobcriteriainFigure6 and7,inwhichthesizeindicatestherankingofskill.  Q1:Willdifferentcompaniesleadtodifferentskills? Indeed,theimpactofdifferentcompaniesdependson thebusinessrange,whichisusuallyrelatedtothescaleof companies.Forinstance,asshowninFigure6,bigcom- panieslikeGoogleandBaiduusuallyholdawiderange ofbusinessandlargeamountofusers,whichresultinthe massivedata.Thus,technicalskillslikeﬁOracleﬂtoman- agedatabase,ﬁPythonﬂfordataanalysis,orevenﬁHadoopﬂ fordistributedcomputingarerequired.Onthecontrary,the smallcompaniesusuallyfocusonacertainapplication thusﬁHTMLﬂforfront-enddesigning,orﬁAndroidﬂand (a)VeryBigCompany (b)VerySmallCompany Figure6:Top8popularjobskills(CompanyScale). (a)VeryHighSalary (b)VeryLowSalary Figure7:Top8popularjobskills(Salary). Table5:Changesoftop10popularjobskills. 2015 2014 2013 SQL Software Test Database Class Hardware IO Framework DataAnalysis Framework Mathematics Communication DataStructure Android IO Python Management Relation SearchEngine DataAnalysis Network Software Algorithm UI Network DataStructure Framework ﬁiOSﬂformobileAppdeveloperarerequired,whichare shownin.Thus,theskillsfordifferentcompanyscalecould bedifferent.  Q2:Whataretheﬁsalary-orientedﬂskills,andwhy? AsshowninFigure7,wecanthatjobskillsrelatedto designanddevelopmentinmobileservices,likeﬁAndroidﬂ, ﬁiOSﬂandﬁWeb-Appﬂtendtoreceivehighersalary,which requirenotonlytechnicaltraining,butalsoinspirationand innovation.Meanwhile,theincreasingdevelopmentofmo- bileinternetalsostimulatethepopularityofcorresponding skills.Onthecontrary,thebasicskillswithlesstechniques, likeﬁServiceﬂandﬁEditﬂ,leadtorelativelylowersalary. Clearly,theirreplaceablequalityofacertainskillcouldpos- siblyaffecttherewarditgains.  Q3:Howdoesthepopularityofjobskillschangeover differentyears? InTable5,wepresentthecomparisonofpopularskills inthepastthreeyearsbyjointlyconsideringpositivejob criteria(i.e.,VeryHighSalary,VeryBigCompany,Huge City,ListedandFulltime).Withoutconsideringthecommon skills,likeDevelopmentandC,whichkeeprelativelysta- ble,wecanseeaclearincreasingdemandofdata-driven skillscomparedwithseveralyearsago.Onthecontrary, thenetwork-relatedskillsdecline,andwecouldwitnessthe transferofmarkettrendsfromhardware-orientedjobskills tosoftware-orientedjobskills. CaseStudy:WillTalentswithPopularSkillsGet HiredEasily? Finally,wewillpresentacasestudyshowingtherelation- shipbetweenourmodelingresultsandrealisticrecruiting resultswithareal-worldrecruitmentdataset. RecruitmentDataset. ThedatasetwasprovidedbyanIT companyinChina,whichcontains140,757resumesthat haveappliedfortechnicaljobsinthiscompany.Specially, alltheresumeshadbeenscoredbytheHumanResourceDe- partmentasfollows:  Score0: Candidatewasfailedwithoutinterview .  Score1: Candidatewasfailedintheinterview .  Score2: Candidatewasadmittedbutfailedtojointhecompany .  Score3: Candidatewasadmittedandjoinedthecompany . Thedistributionofeachlabelinourdatasetislistedas histograminFigure8,whichindicatesthataround85%of candidatesareoutbeforereceivinginterview. CorrelationMeasurement. Accordingtoourbasicas- sumptions,thecandidateswithpopularjobskillswillbe moreprobabletobeenrolled.Astheeffectivenessofour modelhasbeenwenowapplySPTMforresume analysistosimulatetherecruitmentprocess.Tobe weextractedthecorrespondingskillsfromresumes,and thencalculatedtheﬁskillscoreﬂforeachresume,whichis bythefollowingformula: Score Skill = X w j w j P ( w j  m ? ) ; (7) where w isajobskillappearedintheresume,and j w j isthe frequencyactingastheweight.Besides,consideringthesta- tusofthiscompany,thecriterialabels  m ? aresetas(Very HighSalary,HugeCity,BigCompay,Listed). Tovalidatetheperformance,weintroducetheSpearman correlationcoefandKendall's ˝ tomeasurethecor- relationbetweenﬁskillscoreﬂandcorrespondingresume scores.ThecorrelationmeasurementsareshowninTable6, inwhichtheﬁskillscoreﬂbyourSPTMholdsmuchstronger correlationwithresumescoresthanallthebaselines. HypothesisTest. Besides,hypothesistestonKendall's ˝ in- dicatesthatresumerankingobtainedbyourSPTMisstatis- ticallywhichfurtherprovesthepracticabilityof ourmodel.Specially,thesamplingdistributionofKendall's ˝ couldbeestimatedbythenormaldistribution,withmean as0andvarianceas 2(2 N +5) 9 N ( N  1) ,where N equalstothenum- berofsamples.Thus,weconductedthe z -teston ˝ obtained bySPTM,andthe z -valuecouldbecalculatedbythefor- mulabelow,whichvalidatestheresultwithhigh z value = 3  ˝ p N ( N  1) p 2(2 N +5) =156 : 748  z 0 : 0001 : (8) Insummary,basedontheaboveexperiments,weprove thattalentswithpopularskillsindeedgethiredeasily.Also, werealizethatourSPTMapproachhaspotentialforfacili- tatingtheprocessofresumescreeningintalentrecruitment. Figure8:Thedistributionofthenumberofdifferentresume scoresinourrecruitmentdataset. Table6:Theresultsofcorrelationanalysis. MeasurementŠModelSPTMLLDAFrequency Spearman 0.3002 0.18550.2015 Kendall's ˝ 0.2452 0.15280.1603 RelatedWork Inthissection,wesummarizetherelatedworksfollowing threeaspects,namelySkillRanking,RecruitmentMarket AnalysisandTopicModels. SkillRanking SkillrankingofITtechniqueshasattractedgrowinginter- estinthelastdecade.Previousstudyin2005(Prabhakar, Litecky,andArnett2005)discussedaboutthechangingten- dencyof14differentITskillsusingtheirpercentageinjob postingsduringacertainperiod,whereWebprogramming, Unix,C++,Java,SQLprogramming,andOracledatabase werelistedasthetopsixskills.Recently,someonlinere- cruitingcompaniesalsopublishtheirannualreportforthe hottestskillslike(Miller2005)and(Skomorochetal.2012). Forexample,in(LinkedIn2017),Linkedinpredictedtop skillsthatcangetpeoplehiredin2017,basedontherank- ingresultsofhiringandrecruitingactivities.Also,in(World 2017),Computerworldpublishedanarticleillustrating10 hottesttechskillsfor2017basedontheevaluationsand judgementsofexperts. However,thepriorartsabovenotonlyfailtoprovidede- scriptionofdetailedskillstobelearned(instead,theypro- videonlyroughcategoryofskills),butalsolackofinter- pretabilityandscalabilityinmodeling,especiallyforde- scribingthestructureofﬁSkillNetﬂ. RecruitmentMarketAnalysis Recruitmentmarketanalysishasalwaysbeenanappeal- ingtopicforappliedbusinessresearchers,datingbackfrom AdamSmithandhis TheWealth ,forlabordistribution isacrucialelementinthedevelopmentofmodernsoci- ety.Economistslookintotheproblemfromtwodiffer- entprospective:Macroprospective(Encyclopedias2015), whereSolowproposedhisgrowthmodel,andothersstudy topicsaboutthedemographicstructureandparticipation rateoflabor,therelationbetweenandunemploy- mentrateandhowlaborcontributesingrossproductivityor expenditures,etc.Microprospective,wherestudiesareall basedonthebasicmarketcleaningframework,statingthat allemployeeschoosetheirbestbalancebetweenleisureand work,whileallemployershirewithbudgetconstrain,and consequentlythewageisderivedasthemarginallaborcost. Recently,researchersaredevotedtocombiningdatamin- ingtechniqueswithrecruitmentmarketanalysis,includ- ingoffercategorization(Malherbe,Cataldi,andBallatore 2015),talentcareerpathanalysis(Lietal.2017),market trendanalysis(Zhuetal.2016b;Linetal.2017),andtal- entcircles(Xuetal.2016a).However,fewofthemstudied theproblemofmeasuringthepopularityofjobskillinre- cruitmentmarket,nottomentionthemulti-facetedpopular- ityranking,whichisthefocusofthispaper.. TopicModels Topicmodelshavebeensuccessfullyappliedtoproblems inavarietyofsuchasmarketingcampaign(Liuet al.2014),emotionrecognition(Zhuetal.2016a),biomet- rics(Wangetal.2011),genetics(BleiandLafferty2007), socialmedia(Xuetal.2014)andmobiledatamining(Far- rahiandGatica-Perez2012;Zhuetal.2015). Inthepastyears,anumberofvariationsoftheclassi- calLatentDirichletAllocation(LDA)(Blei,Ng,andJor- dan2003)havebeenproposedforsolvingdifferentkinds ofproblems.Toutilizetheexistingdocumentlabels,super- visedLDAmodels(McauliffeandBlei2008)haveshown theeffectivenessonandregressiontasks, whiletheyarelimitedtoassignonetopicforeachdocu- ment.Tothisend,Labeled-LDA(Ramageetal.2009)has emancipatedthislimitationbyallowingmultipletopicsfor asingledocument,thusoutperformstheprevioussupervised models.However,theabovemodelscannottakethehierar- chicalinformationofwordsintoaccount.Therefore,some hierarchicaltopicmodelshavebeenproposed,suchasHier- archicalLDA(GrifandTenenbaum2004),andLDAC (LatentDirichletAllocationonContext)(Baoetal.2010). Inthispaper,differentfromtheabovestudies,thepro- posedmodelSPTMcanintegratebothjobcriterialabelsand skillcategoriesformodelingthegenerationofjobskills,and thenmeasurethepopularity.Further,theﬁdocumentsﬂhere aregeneratedbyﬁcentralskillsﬂandtheirneighboringnodes inﬁSkill-Netﬂ,whichtheconnectionswithinskills. Conclusion Inthispaper,weproposedadata-drivenapproachformea- suringthepopularityofjobskillsbasedontheanalysisof large-scalerecruitmentdata.,webuiltajob skillnetworkbyexploringalargecorpusofjobpostings inrecruitmentmarket.Then,wedevelopedaSkillPopu- laritybasedTopicModel(SPTM)formodelingthegener- ationofskillnetwork.AuniqueperspectiveofSPTMis thatitcanintegratedifferentcriteriaofjobsandthehier- archicaldependenceofskills.Therefore,withthehelpof SPTM,wecaneffectivelyrankthejobskillsbasedontheir multi-perspectivepopularity.Extensiveexperimentsonreal- worldrecruitmentdatavalidatedtheeffectiveness,andalso revealedsomerecruitment-orientedrules,whichprovedthe potentialofSPTMformeasuringthepopularityofjobskills. Inthefuture,wewillconsidermoreﬁpersonalizedﬂfac- torsforcompanies,i.e.,torevealtheirpreferenceorbusiness rangeofjobskills,aswellaslatentbusinesscompetitionbe- tweenthem.Also,weplantostudymorereal-worldapplica- tionsbasedontheskillpopularityobtainedbyourapproach, suchasresumescreeningandtalentrecommendation.Be- sides,morecomprehensivestudieswillbeconducted,such asthejob-candidatemappingperformance,andthepopular- ityofjobskillsinotherprofessionaldomains. Acknowledgments Thisresearchwaspartiallysupportedbygrantsfromthe NationalNaturalScienceFoundationofChina(GrantNo. U1605251,71531001,61703386and61325010),theAn- huiProvincialNaturalScienceFoundation(GrantNo. 1708085QF140),andtheFundamentalResearchFundsfor theCentralUniversities(GrantNo.WK2150110006). References [Baoetal.2010] Bao,T.;Cao,H.;Chen,E.;andTian,J. 2010.Anunsupervisedapproachtomodelingpersonalized contextsofmobileusers.In IEEEInternationalConference onDataMining ,38Œ47. [BleiandLafferty2007] Blei,D.M.,andLafferty,J.D. 2007.Correction:Acorrelatedtopicmodelofscience. Statistics 1(1):17Œ35. [Blei,Ng,andJordan2003] Blei,D.M.;Ng,A.Y.;andJor- dan,M.I.2003.Latentdirichletallocation. Journalof machineLearningresearch 3(Jan):993Œ1022. [CSDN2016] CSDN.2016.Skillknowledgebase. http: //lib.csdn.net/bases . [Daily2016] Daily,P.2016.Op-ed:Chinasfresh graduatesstrugglingtooutearnmigrantwork- ers. http://en.people.cn/n3/2016/0920/ c90000-9116833.html .Accessed:2016-09-20. [Encyclopedias2015] Encyclopedias,L.o.O.2015.Ency- clopediaofmathematics. [FarrahiandGatica-Perez2012] Farrahi,K.,andGatica- Perez,D.2012.Extractingmobilebehavioralpatternswith thedistantn-gramtopicmodel.In InternationalSymposium onWearableComputers ,1Œ8. [GrifandTenenbaum2004] GrifD.,andTenen- baum,M.2004.Hierarchicaltopicmodelsandthenested chineserestaurantprocess. Advancesinneuralinformation processingsystems 16:17. [Huaietal.2014] Huai,B.;Chen,E.;Zhu,H.;Xiong,H.; Bao,T.;Liu,Q.;andTian,J.2014.Towardpersonal- izedcontextrecognitionformobileusers:Asemisupervised bayesianhmmapproach. ACMTransactionsonKnowledge DiscoveryfromData(TKDD) 9(2):10. [Lietal.2017] Li,H.;Ge,Y.;Zhu,H.;Xiong,H.;andZhao, H.2017.Prospectingthecareerdevelopmentoftalents: Asurvivalanalysisperspective.In Proceedingsofthe 23rdACMSIGKDDInternationalConferenceonKnowl- edgeDiscoveryandDataMining ,917Œ925.ACM. [Linetal.2017] Lin,H.;Zhu,H.;Zuo,Y.;Zhu,C.;Wu,J.; andXiong,H.2017.CollaborativecompanyIn- sightsfromanemployee'sperspective.In Proceedingsof theThirty-FirstAAAIConferenceonIntelligence, February4-9,2017,SanFrancisco,California,USA. ,1417Œ 1423. [LinkedIn2017] LinkedIn.2017.Linkedinun- veilsthetopskillsthatcangetyouhiredin2017. https://blog.linkedin.com/2016/10/20/ top-skills-2016-week-of-learning-linkedin . Accessed:2016-10-20. [Liuetal.2014] Liu,G.;Fu,Y.;Xu,T.;Xiong,H.;andChen, G.2014.Discoveringtemporalretweetingpatternsforsocial mediamarketingcampaigns.In DataMining(ICDM),2014 IEEEInternationalConferenceon ,905Œ910.IEEE. [Malherbe,Cataldi,andBallatore2015] Malherbe,E.; Cataldi,M.;andBallatore,A.2015.Bringingorder tothejobmarket:efjoboffercategorizationin e-recruitment.In SymposiumonIrinPractice ,1101Œ1104. [McauliffeandBlei2008] Mcauliffe,J.D.,andBlei,D.M. 2008.Supervisedtopicmodels.In Advancesinneuralin- formationprocessingsystems ,121Œ128. [Miller2005] Miller,R.J.2005.Skill-rankingmethodand systemforemploymentapplicants. [Prabhakar,Litecky,andArnett2005] Prabhakar,B.; Litecky,C.R.;andArnett,K.2005.Itskillsinatoughjob market. CommunicationsoftheAcm 48(10):91Œ94. [Ramageetal.2009] Ramage,D.;Hall,D.;Nallapati,R.; andManning,C.D.2009.Labeledlda:Asupervised topicmodelforcreditattributioninmulti-labeledcorpora. In Proceedingsofthe2009ConferenceonEmpiricalMeth- odsinNaturalLanguageProcessing:Volume1-Volume1 , 248Œ256.AssociationforComputationalLinguistics. [SkillsYouNeed2017] SkillsYouNeed.2017.Theskillsgap. https://www.skillsyouneed.com/general/ skills-gap.html . [Skomorochetal.2012] Skomoroch,P.N.;Hayes,M.T.; Gupta,A.;andPatil,D.A.S.2012.Skillrankingsystem. [Wangetal.2011] Wang,H.;Ding,Y.;Tang,J.;Dong,X.; He,B.;Qiu,J.;andWild,D.J.2011.Findingcomplex biologicalrelationshipsinrecentpubmedarticlesusingbio- lda. PlosOne 6(3):e17243. [World2017] World,C.2017.10itskillsthatemployers needin2017. http://www.computerworld. com/article/3164835/it-careers/ 10-it-skills-that-employers-need-in-2017. html .Accessed:2017-02-01. [Xuetal.2014] Xu,T.;Zhu,H.;Chen,E.;Huai,B.;Xiong, H.;andTian,J.2014.Learningtoannotateviasocial interactionanalytics. Knowledgeandinformationsystems 41(2):251Œ276. [Xuetal.2016a] Xu,H.;Yu,Z.;Yang,J.;Xiong,H.;and Zhu,H.2016a.Talentcircledetectioninjobtransition networks.In Proceedingsofthe22ndACMSIGKDDIn- ternationalConferenceonKnowledgeDiscoveryandData Mining,SanFrancisco,CA,USA,August13-17,2016 ,655Œ 664. [Xuetal.2016b] Xu,T.;Zhu,H.;Zhao,X.;Liu,Q.;Zhong, H.;Chen,E.;andXiong,H.2016b.Taxidrivingbehav- ioranalysisinlatentvehicle-to-vehiclenetworks:Asocial perspective.In Proceedingsofthe22ndACM SIGKDDInternationalConferenceonKnowledgeDiscov- eryandDataMining,SanFrancisco,CA,USA,August13- 17,2016 ,1285Œ1294. [Zhuetal.2015] Zhu,H.;Liu,C.;Ge,Y.;Xiong,H.;and Chen,E.2015.Popularitymodelingformobileapps: Asequentialapproach. IEEEtransactionsoncybernetics 45(7):1303Œ1314. [Zhuetal.2016a] Zhu,C.;Zhu,H.;Ge,Y.;Chen,E.;Liu, Q.;Xu,T.;andXiong,H.2016a.Trackingtheevolutionof socialemotionswithtopicmodels. KnowledgeandInfor- mationSystems 47(3):517Œ544. [Zhuetal.2016b] Zhu,C.;Zhu,H.;Xiong,H.;Ding,P.;and Xie,F.2016b.Recruitmentmarkettrendanalysiswithse- quentiallatentvariablemodels.In TheACMSIGKDDInter- nationalConference ,383Œ392.  
MinimumRiskTrainingforNeuralMachineTranslation ShiqiShen y ,YongCheng # ,ZhongjunHe + ,WeiHe + ,HuaWu + ,MaosongSun y ,YangLiu y y StateKeyLaboratoryofIntelligentTechnologyandSystems TsinghuaNationalLaboratoryforInformationScienceandTechnology DepartmentofComputerScienceandTechnology,TsinghuaUniversity,Beijing,China # InstituteforInterdisciplinaryInformationSciences,TsinghuaUniversity,Beijing,China + BaiduInc.,Beijing,China f vicapple22,chengyong3001 g @gmail.com, f hezhongjun,hewei06,wu hua g @baidu.com, f sms,liuyang2011 g @tsinghua.edu.cn Abstract Weproposeminimumrisktrainingfor end-to-endneuralmachinetranslation. Unlikeconventionalmaximumlikelihood estimation,minimumrisktrainingisca- pableofoptimizingmodelparametersdi- rectlywithrespecttoarbitraryevaluation metrics,whicharenotnecessarilydiffer- entiable.Experimentsshowthatourap- proachachievesimprovements overmaximumlikelihoodestimationona state-of-the-artneuralmachinetranslation systemacrossvariouslanguagespairs. Transparenttoarchitectures,ourapproach canbeappliedtomoreneuralnetworks andpotentiallymoreNLPtasks. 1Introduction Recently,end-to-endneuralmachinetransla- tion(NMT)(KalchbrennerandBlunsom,2013; Sutskeveretal.,2014;Bahdanauetal.,2015) hasattractedincreasingattentionfromthecom- munity.Providinganewparadigmformachine translation,NMTaimsattrainingasingle,large neuralnetworkthatdirectlytransformsasource- languagesentencetoatarget-languagesentence withoutexplicitlymodelinglatentstructures(e.g., wordalignment,phrasesegmentation,phrasere- ordering,andSCFGderivation)thatarevitalin conventionalstatisticalmachinetranslation(SMT) (Brownetal.,1993;Koehnetal.,2003;Chiang, 2005). CurrentNMTmodelsarebasedonthe encoder- decoder framework(Choetal.,2014;Sutskever etal.,2014),withanencodertoreadandencode asource-languagesentenceintoavector,from whichadecodergeneratesatarget-languagesen- tence.Whileearlyeffortsencodetheinputintoa  Correspondingauthor:YangLiu. ed-lengthvector,Bahdanauetal.(2015)advo- catetheattentionmechanismtodynamicallygen- erateacontextvectorforatargetwordbeinggen- erated. AlthoughNMTmodelshaveachievedresultson parwithorbetterthanconventionalSMT,theystill sufferfromamajordrawback:themodelsareop- timizedtomaximizethelikelihoodoftrainingdata insteadofevaluationmetricsthatactuallyquantify translationquality.Ranzatoetal.(2015)indicate twodrawbacksof maximumlikelihoodestimation (MLE)forNMT.First,themodelsareonlyex- posedtothetrainingdistributioninsteadofmodel predictions.Second,thelossfunctionisat thewordlevelinsteadofthesentencelevel. Inthiswork,weintroduce minimumrisktrain- ing (MRT)forneuralmachinetranslation.The newtrainingobjectiveistominimizetheexpected loss(i.e.,risk)onthetrainingdata.MRThasthe followingadvantagesoverMLE: 1. Directoptimizationwithrespecttoevalu- ationmetrics :MRTintroducesevaluation metricsaslossfunctionsandaimstomini- mizeexpectedlossonthetrainingdata. 2. Applicabletoarbitrarylossfunctions :our approachallowsarbitrarysentence-levelloss functions,whicharenotnecessarilydifferen- tiable. 3. Transparenttoarchitectures :MRTdoesnot assumethearchitecturesofNMTand canbeappliedtoanyend-to-endNMTsys- tems. WhileMRThasbeenwidelyusedinconven- tionalSMT(Och,2003;SmithandEisner,2006; HeandDeng,2012)anddeeplearningbasedMT (Gaoetal.,2014),tothebestofourknowledge, thisworkistheefforttointroduceMRT arXiv:1512.02433v3  [cs.CL]  15 Jun 2016intoend-to-endNMT.Experimentsonavarietyof languagepairs(Chinese-English,English-French, andEnglish-German)showthatMRTleadstosig- improvementsoverMLEonastate-of- the-artNMTsystem(Bahdanauetal.,2015). 2Background Givenasourcesentence x = x 1 ;:::; x m ;:::; x M andatargetsentence y = y 1 ;:::; y n ;:::; y N , end-to-endNMTdirectlymodelsthetranslation probability: P ( y j x ;  )= N Y n =1 P ( y n j x ; y <n ;  ) ; (1) where  isasetofmodelparametersand y <n = y 1 ;:::; y n  1 isapartialtranslation. Predictingthe n -thtargetwordcanbemodeled byusingarecurrentneuralnetwork: P ( y n j x ; y <n ;  ) / exp n q ( y n  1 ; z n ; c n ;  ) o ; (2) where z n isthe n -thhiddenstateonthetarget side, c n isthecontextforgeneratingthe n -thtar- getword,and q (  ) isanon-linearfunction.Cur- rentNMTapproachesdifferincalculating z n and c n and q (  ) .Pleasereferto(Sutskeveret al.,2014;Bahdanauetal.,2015)formoredetails. Givenasetoftrainingexamples D = fh x ( s ) ; y ( s ) ig S s =1 ,thestandardtrainingobjective istomaximizethelog-likelihoodofthetraining data: ^  MLE =argmax  n L (  ) o ; (3) where L (  )= S X s =1 log P ( y ( s ) j x ( s ) ;  ) (4) = S X s =1 N ( s ) X n =1 log P ( y ( s ) n j x ( s ) ; y ( s ) <n ;  ) : (5) Weuse N ( s ) todenotethelengthofthe s -thtarget sentence y ( s ) . Thepartialderivativewithrespecttoamodel parameter  i iscalculatedas @ L (  ) @  i = S X s =1 N ( s ) X n =1 @P ( y ( s ) n j x ( s ) ; y ( s ) <n ;  ) =@  i P ( y ( s ) n j x ( s ) ; y ( s ) <n ;  ) : (6) Ranzatoetal.(2015)pointoutthatMLE forend-to-endNMTsuffersfromtwodrawbacks. First,whilethemodelsaretrainedonlyonthe trainingdatadistribution,theyareusedtogenerate targetwordsonpreviousmodelpredictions,which canbeerroneous,attesttime.Thisisreferredto as exposurebias (Ranzatoetal.,2015).Second, MLEusuallyusesthecross-entropylossfocus- ingonword-levelerrorstomaximizetheproba- bilityofthenextcorrectword,whichmighthardly correlatewellwithcorpus-levelandsentence-level evaluationmetricssuchasBLEU(Papinenietal., 2002)andTER(Snoveretal.,2006). Asaresult,itisimportanttointroducenew trainingalgorithmsforend-to-endNMTtoinclude modelpredictionsduringtrainingandoptimize modelparametersdirectlywithrespecttoevalu- ationmetrics. 3MinimumRiskTrainingforNeural MachineTranslation Minimumrisktraining(MRT),whichaimsto minimizetheexpectedlossonthetrainingdata, hasbeenwidelyusedinconventionalSMT(Och, 2003;SmithandEisner,2006;HeandDeng, 2012)anddeeplearningbasedMT(Gaoetal., 2014).Thebasicideaistointroduceevaluation metricsaslossfunctionsandassumethattheopti- malsetofmodelparametersshouldminimizethe expectedlossonthetrainingdata. Let h x ( s ) ; y ( s ) i bethe s -thsentencepairinthe trainingdataand y beamodelprediction.Weuse a lossfunction  y ; y ( s ) ) tomeasurethediscrep- ancybetweenthemodelprediction y andthegold- standardtranslation y ( s ) .Suchalossfunction canbenegativesmoothedsentence-levelevalua- tionmetricssuchasBLEU(Papinenietal.,2002), NIST(Doddington,2002),TER(Snoveretal., 2006),orMETEOR(LavieandDenkowski,2009) thathavebeenwidelyusedinmachinetranslation evaluation.Notethatalossfunctionisnotparam- eterizedandthusnotdifferentiable. InMRT,the risk isastheexpectedloss withrespecttotheposteriordistribution: R (  )= S X s =1 E y j x ( s ) ;  h  y ; y ( s ) ) i (7) = S X s =1 X y 2Y ( x ( s ) ) P ( y j x ( s ) ;   y ; y ( s ) ) ; (8) where Y ( x ( s ) ) isasetofallpossiblecandidate translationsfor x ( s ) .  y ; y ( s ) ) P ( y j x ( s ) ;  ) y 1  1 : 0 0 : 2 0 : 3 0 : 5 0 : 7 y 2  0 : 3 0 : 5 0 : 2 0 : 2 0 : 1 y 3  0 : 5 0 : 3 0 : 5 0 : 3 0 : 2 E y j x ( s ) ;   y ; y ( s ) )]  0 : 50  0 : 61  0 : 71  0 : 83 Table1:Exampleofminimumrisktraining. x ( s ) isanobservedsourcesentence, y ( s ) isitscorresponding gold-standardtranslation,and y 1 , y 2 ,and y 3 aremodelpredictions.Forsimplicity,wesupposethatthe fullsearchspacecontainsonlythreecandidates.Thelossfunction  y ; y ( s ) ) measuresthedifference betweenmodelpredictionandgold-standard.ThegoalofMRTistoadistribution(thelastcolumn) thatcorrelateswellwiththegold-standardbyminimizingtheexpectedloss. ThetrainingobjectiveofMRTistominimize theriskonthetrainingdata: ^  MRT =argmin  n R (  ) o : (9) Intuitively,whileMLEaimstomaximizethe likelihoodoftrainingdata,ourtrainingobjectiveis todiscriminatebetweencandidates.Forexample, inTable1,supposethecandidateset Y ( x ( s ) ) con- tainsonlythreecandidates: y 1 , y 2 ,and y 3 .Ac- cordingtothelossescalculatedbycomparingwith thegold-standardtranslation y ( s ) ,itisclearthat y 1 isthebestcandidate, y 3 isthesecondbest,and y 2 istheworst: y 1 > y 3 > y 2 .Therighthalfof Table1showsfourmodels.Asmodel1(column 3)ranksthecandidatesinareverseorderascom- paredwiththegold-standard(i.e., y 2 > y 3 > y 1 ), itobtainsthehighestriskof  0 : 50 .Achieving abettercorrelationwiththegold-standardthan model1bypredicting y 3 > y 1 > y 2 ,model2 (column4)reducestheriskto  0 : 61 .Asmodel 3(column5)ranksthecandidatesinthesameor- derwiththegold-standard,theriskgoesdownto  0 : 71 .Theriskcanbefurtherreducedbycon- centratingtheprobabilitymasson y 1 (column6). Asaresult,byminimizingtheriskonthetraining data,weexpecttoobtainamodelthatcorrelates wellwiththegold-standard. InMRT,thepartialderivativewithrespecttoa modelparameter  i isgivenby @ R (  ) @  i = S X s =1 E y j x ( s ) ;  "  y ; y ( s ) )  N ( s ) X n =1 @P ( y n j x ( s ) ; y <n ;  ) =@  i P ( y n j x ( s ) ; y <n ;  ) # : (10) SinceEq.(10)suggeststhereisnoneedtodif- ferentiate  y ; y ( s ) ) ,MRTallowsarbitrarynon- differentiablelossfunctions.Inaddition,ourap- proachistransparenttoarchitecturesandcanbe appliedtoarbitraryend-to-endNMTmodels. Despitetheseadvantages,MRTfacesamajor challenge:theexpectationsinEq.(10)areusu- allyintractabletocalculateduetotheexponential searchspaceof Y ( x ( s ) ) ,thenon-decomposability ofthelossfunction  y ; y ( s ) ) ,andthecontext sensitivenessofNMT. Toalleviatethisproblem,weproposetoonly useasubsetofthefullsearchspacetoapproxi- matetheposteriordistributionandintroduceanew trainingobjective: ~ R (  )= S X s =1 E y j x ( s ) ;   h  y ; y ( s ) ) i (11) = S X s =1 X y 2S ( x ( s ) ) Q ( y j x ( s ) ;  ;  y ; y ( s ) ) ; (12) where S ( x ( s ) ) ˆY ( x ( s ) ) isasampledsubsetof thefullsearchspace,and Q ( y j x ( s ) ;  ; ) isadis- tributiononthesubspace S ( x ( s ) ) : Q ( y j x ( s ) ;  ; )= P ( y j x ( s ) ;  )  P y 0 2S ( x ( s ) ) P ( y 0 j x ( s ) ;  )  : (13) Notethat  isahyper-parameterthatcontrolsthe sharpnessofthe Q distribution(Och,2003). Algorithm1showshowtobuild S ( x ( s ) ) by samplingthefullsearchspace.Thesampledsub- setinitializeswiththegold-standardtranslation (line1).Then,thealgorithmkeepssamplingatar- getwordgiventhesourcesentenceandthepartial translationuntilreachingtheendofsentence(lines 3-16).Notethatsamplingmightproducedupli- catecandidates,whichareremovedwhenbuilding Input :the s -thsourcesentenceinthetrainingdata x ( s ) ,the s -thtargetsentenceinthetrainingdata y ( s ) ,thesetof modelparameters  ,thelimitonthelengthofacandidatetranslation l ,andthelimitonthesizeofsampled space k . Output :sampledspace S ( x ( s ) ) : 1 S ( x ( s ) )  f y ( s ) g ; //thegold-standardtranslationisincluded 2 i   1 ; 3 while i  k do 4 y  ; ; //anemptycandidatetranslation 5 n   1 ; 6 while n  l do 7 y ˘ P ( y n j x ( s ) ; y <n ;  ) ; //samplethe n -thtargetword 8 y   y [f y g ; 9 if y =EOS then 10 break ; //terminateifreachtheendofsentence 11 end 12 n   n +1 ; 13 end 14 S ( x ( s ) )  S ( x ( s ) ) [f y g ; 15 i   i +1 ; 16 end Algorithm1: Samplingthefullsearchspace. thesubspace.Wethatitisineftoforce thealgorithmtogenerateexactly k distinctcandi- datesbecausehigh-probabilitycandidatescanbe sampledrepeatedly,especiallywhentheprobabil- itymasshighlyconcentratesonafewcandidates. Inpractice,wetakeadvantageofGPU'sparallel architecturestospeedupthesampling. 1 Giventhesampledspace,thepartialderivative withrespecttoamodelparameter  i of ~ R (  ) is givenby @ ~ R (  ) @  i =  S X s =1 E y j x ( s ) ;   " @P ( y j x ( s ) ;  ) =@  i P ( y j x ( s ) ;  )    y ; y ( s ) )  E y 0 j x ( s ) ;    y 0 ; y ( s ) )]  # : (14) Since jS ( x ( s ) ) j˝jY ( x ( s ) ) j ,theexpectations inEq.(14)canbeefcalculatedbyex- plicitlyenumeratingallcandidatesin S ( x ( s ) ) .In ourexperiments,wethatapproximatingthe fullspacewith 100 samples(i.e., k =100 )works verywellandfurtherincreasingsamplesizedoes notleadtosimprovements(seeSection 4.3). 1 Tobuildthesubset,analternativetosamplingiscom- putingtop- k translations.Weprefersamplingtocomput- ingtop- k translationsforefy:samplingismoreef cientandeasy-to-implementthancalculating k -bestlists,es- peciallygiventheextremelyparallelarchitecturesofGPUs. 4Experiments 4.1Setup Weevaluatedourapproachonthreetransla- tiontasks:Chinese-English,English-French,and English-German.TheevaluationmetricisBLEU (Papinenietal.,2002)ascalculatedbythe multi-bleu.perl script. ForChinese-English,thetrainingdataconsists of2.56Mpairsofsentenceswith67.5MChinese wordsand74.8MEnglishwords,respectively.We usedtheNIST2006datasetasthevalidationset (hyper-parameteroptimizationandmodelselec- tion)andtheNIST2002,2003,2004,2005,and 2008datasetsastestsets. ForEnglish-French,tocomparewiththeresults reportedbypreviousworkonend-to-endNMT (Sutskeveretal.,2014;Bahdanauetal.,2015; Jeanetal.,2015;Luongetal.,2015b),weused thesamesubsetoftheWMT2014trainingcor- pusthatcontains12Msentencepairswith304M Englishwordsand348MFrenchwords.Thecon- catenationofnews-test2012andnews-test2013 servesasthevalidationsetandnews-test2014as thetestset. ForEnglish-German,tocomparewiththe resultsreportedbypreviouswork(Jeanetal., 2015;Luongetal.,2015a),weusedthesamesub- setoftheWMT2014trainingcorpusthatcontains 4Msentencepairswith91MEnglishwordsand 87MGermanwords.Theconcatenationofnews- test2012andnews-test2013isusedasthevalida- tionsetandnews-test2014asthetestset. Figure1:Effectof  ontheChinese-Englishvali- dationset. Wecompareourapproachwithtwostate-of- the-artSMTandNMTsystems: 1.M OSES (KoehnandHoang,2007):aphrase- basedSMTsystemusingminimumerrorrate training(Och,2003). 2.RNN SEARCH (Bahdanauetal.,2015):an attention-basedNMTsystemusingmaxi- mumlikelihoodestimation. M OSES usestheparallelcorpustotraina phrase-basedtranslationmodelandthetarget parttotraina4-gramlanguagemodelusingthe SRILMtoolkit(Stolcke,2002). 2 Thelog-linear modelMosesusesistrainedbytheminimumerror ratetraining(MERT)algorithm(Och,2003)that directlyoptimizesmodelparameterswithrespect toevaluationmetrics. RNN SEARCH usestheparallelcorpustotrain anattention-basedneuraltranslationmodelusing themaximumlikelihoodcriterion. OntopofRNN SEARCH ,ourapproachreplaces MLEwithMRT.Weinitializeourmodelwiththe RNNsearch50model(Bahdanauetal.,2015).We setthevocabularysizeto30KforChinese-English andEnglish-Frenchand50KforEnglish-German. Thebeamsizefordecodingis10.Thedefault lossfunctionisnegativesmoothedsentence-level BLEU. 4.2Effectof  Thehyper-parameter  controlsthesmoothnessof the Q distribution(seeEq.(13)).Asshownin 2 Itispossibletoexploitlargermonolingualcorporafor bothM OSES andRNN SEARCH (Gulcehreetal.,2015;Sen- nrichetal.,2015).Weleavethisforfuturework. Figure2:EffectofsamplesizeontheChinese- Englishvalidationset. criterion loss BLEU TER NIST MLE N/A 30.48 60.85 8.26  sBLEU 36.71 53.48 8.90 MRT sTER 30.14 53.83 6.02  sNIST 32.32 56.85 8.90 Table2:EffectoflossfunctionontheChinese- Englishvalidationset. Figure1,wethat  hasacriticaleffecton BLEUscoresontheChinese-Englishvalidation set.While  =1  10  1 deceasesBLEUscores dramatically,  =5  10  3 improvestranslation qualityandconsistently.Reducing  furtherto 1  10  4 ,however,resultsinlower BLEUscores.Therefore,weset  =5  10  3 in thefollowingexperiments. 4.3EffectofSampleSize Forefy,wesample k candidatetranslations fromthefullsearchspace Y ( x ( s ) ) tobuildan approximateposteriordistribution Q (Section3). Figure2showstheeffectofsamplesize k on theChinese-Englishvalidationset.Itisclearthat BLEUscoresconsistentlyrisewiththeincreaseof k .However,wethatasamplesizelargerthan 100(e.g., k =200 )usuallydoesnotleadtosigni- improvementsandincreasestheGPUmem- oryrequirement.Therefore,weset k =100 inthe followingexperiments. 4.4EffectofLossFunction Asourapproachiscapableofincorporatingevalu- ationmetricsaslossfunctions,weinvestigatethe effectofdifferentlossfunctionsonBLEU,TER Figure3:Comparisonoftrainingtimeonthe Chinese-Englishvalidationset. andNISTscoresontheChinese-Englishvalida- tionset.AsshowninTable2,negativesmoothed sentence-levelBLEU(i.e,  sBLEU)leadstosta- tisticallyimprovementsoverMLE( p< 0 : 01 ).Notethatthelossfunctionsareallat thesentencelevelwhileevaluationmetricsarecal- culatedatthecorpuslevel.Thisdiscrepancymight explainwhyoptimizingwithrespecttosTERdoes notresultinthelowestTERonthevalidationset. As  sBLEUconsistentlyimprovesallevaluation metrics,weuseitasthedefaultlossfunctionin ourexperiments. 4.5ComparisonofTrainingTime Weusedaclusterwith20TelsaK40GPUstotrain theNMTmodel.ForMLE,ittakesthecluster aboutonehourtotrain20,000mini-batches,each ofwhichcontains80sentences.Thetrainingtime forMRTislongerthanMLE:13,000mini-batches canbeprocessedinonehouronthesamecluster. Figure3showsthelearningcurvesofMLEand MRTonthevalidationset.ForMLE,theBLEU scorereachesitspeakafterabout20hoursand thenkeepsgoingdowndramatically.Initializing withthebestMLEmodel,MRTincreasesBLEU scoresdramaticallywithinabout30hours. 3 Af- terwards,theBLEUscorekeepsimprovinggrad- uallybutthereareslightoscillations. 4.6ResultsonChinese-EnglishTranslation 4.6.1ComparisonofBLEUScores Table3showsBLEUscoresonChinese-English datasets.ForRNN SEARCH ,wefollowLuong 3 Althoughitispossibletoinitializewitharandomized model,ittakesmuchlongertimetoconverge. Figure4:BLEUscoresontheChinese-English testsetovervariousinputsentencelengths. Figure5:Comparisonofoutputsentenceslengths ontheChinese-Englishtestset. etal.(2015b)tohandlerarewords.We thatintroducingminimumrisktrainingintoneu- ralMTleadstosurprisinglysubstantialimprove- mentsoverM OSES andRNN SEARCH withMLE asthetrainingcriterion(upto+8.61and+7.20 BLEUpoints,respectively)acrossalltestsets.All theimprovementsarestatistically 4.6.2ComparisonofTERScores Table4givesTERscoresonChinese-English datasets.ThelossfunctionusedinMRTis  sBLEU.MRTstillobtainsdramaticimprove- mentsoverM OSES andRNN SEARCH withMLE asthetrainingcriterion(upto-10.27and-8.32 TERpoints,respectively)acrossalltestsets.All theimprovementsarestatistically 4.6.3BLEUScoresoverSentenceLengths Figure4showstheBLEUscoresoftranslations generatedbyM OSES ,RNN SEARCH withMLE, System Training MT06 MT02MT03MT04MT05MT08 M OSES MERT 32.74 32.4932.4033.3830.2025.28 RNN SEARCH MLE 30.70 35.1333.7334.5831.7623.57 MRT 37.34 40.3640.9341.3738.8129.23 Table3:Case-insensitiveBLEUscoresonChinese-Englishtranslation. System Training MT06 MT02MT03MT04MT05MT08 M OSES MERT 59.22 62.9762.4461.2063.4462.36 RNN SEARCH MLE 60.74 58.9460.1058.9161.7464.52 MRT 52.86 52.8752.1751.4953.4257.21 Table4:Case-insensitiveTERscoresonChinese-Englishtranslation. MLEvs.MRT < = > evaluator1 54% 24% 22% evaluator2 53% 22% 25% Table5:SubjectiveevaluationofMLEandMRT onChinese-Englishtranslation. andRNN SEARCH withMRTontheChinese- Englishtestsetwithrespecttoinputsentence lengths.WhileMRTconsistentlyimprovesover MLEforalllengths,itachievesworsetranslation performanceforsentenceslongerthan60words. OnereasonisthatRNN SEARCH tendstopro- duceshorttranslationsforlongsentences.As showninFigure5,bothMLEandMREgen- eratemuchshortertranslationsthanM OSES . Thisresultsfromthelengthlimitimposedby RNN SEARCH forefyreasons:asentence inthetrainingsetisnolongerthan50words.This limitdeterioratestranslationperformancebecause thesentencesinthetestsetareusuallylongerthan 50words. 4.6.4SubjectiveEvaluation Wealsoconductedasubjectiveevaluationtovali- datetheofreplacingMLEwithMRT.Two humanevaluatorswereaskedtocompareMLE andMRTtranslationsof100sourcesentencesran- domlysampledfromthetestsetswithoutknow- ingfromwhichsystemacandidatetranslationwas generated. Table5showstheresultsofsubjectiveevalua- tion.Thetwohumanevaluatorsmadeclosejudge- ments:around54%ofMLEtranslationsareworse thanMRE,23%areequal,and23%arebetter. 4.6.5ExampleTranslations Table6showssomeexampletranslations.We thatM OSES translatesaChinesestringﬁ yi weifuzeyupingrangdangjudajiaodaodeqian guowuyuanguanyuan ﬂthatrequireslong-distance reorderinginawrongway,whichisanotorious challengeforstatisticalmachinetranslation.In contrast,RNN SEARCH -MLEseemstoovercome thisprobleminthisexamplethankstothecapa- bilityofgatedRNNstocapturelong-distancede- pendencies.However,asMLEusesalossfunc- tiononlyatthewordlevel,itstranslation lackssentence-levelconsistency:ﬁchineseﬂoc- curstwicewhileﬁtwosenateﬂismissing.Byopti- mizingmodelparametersdirectlywithrespectto sentence-levelBLEU,RNN SEARCH -MRTseems tobeabletogeneratetranslationsmoreconsis- tentlyatthesentencelevel. 4.7ResultsonEnglish-FrenchTranslation Table7showstheresultsonEnglish-Frenchtrans- lation.Welistexistingend-to-endNMTsystems thatarecomparabletooursystem.Allthesesys- temsusethesamesubsetoftheWMT2014train- ingcorpusandadoptMLEasthetrainingcrite- rion.Theydifferinnetworkarchitecturesandvo- cabularysizes.OurRNN SEARCH -MLEsystem achievesaBLEUscorecomparabletothatofJean etal.(2015).RNN SEARCH -MRTachievesthe highestBLEUscoreinthissettingevenwithavo- cabularysizesmallerthanLuongetal.(2015b) andSutskeveretal.(2014).Notethatourap- proachdoesnotassumearchitecturesand caninprinciplebeappliedtoanyNMTsystems. 4.8ResultsonEnglish-GermanTranslation Table8showstheresultsonEnglish-German translation.Ourapproachstillout- Source meiguodaibiaotuanbaokuolaizishidanfudaxuedeyiweizhongguo zhuanjia,liangmingcanyuanwaijiaozhengcezhuliyijiyiweifuzeyu pingrangdangjudajiaodaodeqianguowuyuanguanyuan. Reference theusdelegationconsistsofachineseexpertfromthestanforduniversity ,twosenateforeignaffairspolicyassistantsandaformerstatedepartment ofwhowasinchargeofdealingwithpyongyangauthority. M OSES theunitedstatestomembersofthedelegationincluderepresentativesfrom thestanforduniversity,achineseexpert,twoassistantsenateforeignpolicy andaresponsiblefordealingwithpyongyangbeforetheofofthestate council. RNN SEARCH -MLE theusdelegationcomprisesachineseexpertfromstanforduniversity,a chineseforeignofassistantpolicyassistantandaformerofwhois responsiblefordealingwiththepyongyangauthorities. RNN SEARCH -MRT theusdelegationincludedachineseexpertfromthestanforduniversity, twosenateforeignpolicyassistants,andaformerstatedepartmentof whohaddealingswiththepyongyangauthorities. Table6:ExampleChinese-Englishtranslations.ﬁSourceﬂisaromanizedChinesesentence,ﬁRefer- enceﬂisagold-standardtranslation.ﬁM OSES ﬂandﬁRNN SEARCH -MLEﬂarebaselineSMTandNMT systems.ﬁRNN SEARCH -MRTﬂisoursystem. System Architecture Training Vocab BLEU Existingend-to-endNMTsystems Bahdanauetal.(2015) gatedRNNwithsearch MLE 30K 28.45 Jeanetal.(2015) gatedRNNwithsearch 30K 29.97 Jeanetal.(2015) gatedRNNwithsearch+PosUnk 30K 33.08 Luongetal.(2015b) LSTMwith4layers 40K 29.50 Luongetal.(2015b) LSTMwith4layers+PosUnk 40K 31.80 Luongetal.(2015b) LSTMwith6layers 40K 30.40 Luongetal.(2015b) LSTMwith6layers+PosUnk 40K 32.70 Sutskeveretal.(2014) LSTMwith4layers 80K 30.59 Ourend-to-endNMTsystems thiswork gatedRNNwithsearch MLE 30K 29.88 gatedRNNwithsearch MRT 30K 31.30 gatedRNNwithsearch+PosUnk MRT 30K 34.23 Table7:ComparisonwithpreviousworkonEnglish-Frenchtranslation.TheBLEUscoresarecase- sensitive.ﬁPosUnkﬂdenotesLuongetal.(2015b)'stechniqueofhandlingrarewords. System Architecture Training BLEU Existingend-to-endNMTsystems Jeanetal.(2015) gatedRNNwithsearch MLE 16.46 Jeanetal.(2015) gatedRNNwithsearch+PosUnk 18.97 Jeanetal.(2015) gatedRNNwithsearch+LV+PosUnk 19.40 Luongetal.(2015a) LSTMwith4layers+dropout+localatt.+PosUnk 20.90 Ourend-to-endNMTsystems thiswork gatedRNNwithsearch MLE 16.45 gatedRNNwithsearch MRT 18.02 gatedRNNwithsearch+PosUnk MRT 20.45 Table8:ComparisonwithpreviousworkonEnglish-Germantranslation.TheBLEUscoresarecase- sensitive. performsMLEandachievescomparableresults withstate-of-the-artsystemseventhoughLuong etal.(2015a)usedamuchdeeperneuralnetwork. Webelievethatourworkcanbeappliedtotheir architectureeasily. Despitetheseimprovements,the marginsonEnglish-GermanandEnglish-French datasetsaremuchsmallerthanChinese-English. Weconjecturethattherearetwopossiblerea- sons.First,theChinese-Englishdatasetscontain fourreferencetranslationsforeachsentencewhile bothEnglish-FrenchandEnglish-Germandatasets onlyhavesinglereferences.Second,Chineseand EnglisharemoredistantlyrelatedthanEnglish, FrenchandGermanandthusmorefrom MRTthatincorporatesevaluationmetricsintoop- timizationtocapturestructuraldivergence. 5RelatedWork Ourworkoriginatedfromtheminimumrisktrain- ingalgorithmsinconventionalstatisticalmachine translation(Och,2003;SmithandEisner,2006; HeandDeng,2012).Och(2003)describesa smoothederrorcounttoallowcalculatinggradi- ents,whichdirectlyinspiresustouseaparam- eter  toadjustthesmoothnessoftheobjective function.Asneuralnetworksarenon-linear,our approachhastominimizetheexpectedlosson thesentencelevelratherthanthelossof1-best translationsonthecorpuslevel.SmithandEis- ner(2006)introduceminimumriskannealingfor traininglog-linearmodelsthatiscapableofgrad- uallyannealingtofocusonthe1-besthypothe- sis.Heetal.(2012)applyminimumrisktraining tolearningphrasetranslationprobabilities.Gao etal.(2014)leverageMRTforlearningcontinu- ousphraserepresentationsforstatisticalmachine translation.ThedifferenceisthattheyuseMRT tooptimizeasub-modelofSMTwhilewearein- terestedindirectlyoptimizingend-to-endneural translationmodels. TheMixedIncrementalCross-EntropyRein- force(MIXER)algorithm(Ranzatoetal.,2015) isinspiritclosesttoourwork.Buildingon theREINFORCEalgorithmproposedbyWilliams (1992),MIXERallowsincrementallearningand theuseofhybridlossfunctionthatcombinesboth REINFORCEandcross-entropy.Themajordif- ferenceisthatRanzatoetal.(2015)leveragerein- forcementlearningwhileourworkresortstomini- mumrisktraining.Inaddition,MIXERonlysam- plesonecandidatetocalculatereinforcementre- wardwhileMRTgeneratesmultiplesamplesto calculatetheexpectedrisk.Figure2indicatesthat multiplesamplespotentiallyincreasesMRT'sca- pabilityofdiscriminatingbetweendiversecandi- datesandthustranslationquality.Ourex- perimentsRanzatoetal.(2015)'s thattakingevaluationmetricsintoaccountwhen optimizingmodelparametersdoeshelptoimprove sentence-leveltextgeneration. Morerecently,ourapproachhasbeensuc- cessfullyappliedtosummarization(Ayanaetal., 2016).Theyoptimizeneuralnetworksforhead- linegenerationwithrespecttoROUGE(Lin, 2004)andalsoachieveimprovements, theeffectivenessandapplicabilityof ourapproach. 6Conclusion Inthispaper,wehavepresentedaframeworkfor minimumrisktraininginend-to-endneuralma- chinetranslation.Thebasicideaistominimize theexpectedlossintermsofevaluationmetrics onthetrainingdata.Wesamplethefullsearch spacetoapproximatetheposteriordistributionto improveefy.ExperimentsshowthatMRT leadstoimprovementsovermaximum likelihoodestimationforneuralmachinetrans- lation,especiallyfordistantly-relatedlanguages suchasChineseandEnglish. Inthefuture,weplantotestourapproachon morelanguagepairsandmoreend-to-endneural MTsystems.Itisalsointerestingtoextendmini- mumrisktrainingtominimumriskannealingfol- lowingSmithandEisner(2006).Asourapproach istransparenttolossfunctionsandarchitectures, webelievethatitwillalsomoreend-to-end neuralarchitecturesforotherNLPtasks. Acknowledgments ThisworkwasdonewhileShiqiShenandYong ChengwerevisitingBaidu.MaosongSunand HuaWuaresupportedbythe973Program (2014CB340501&2014CB34505).YangLiuis supportedbytheNationalNaturalScienceFoun- dationofChina(No.61522204andNo.61432013) andthe863Program(2015AA011808).Thisre- searchisalsosupportedbytheSingaporeNational ResearchFoundationunderitsInternationalRe- searchCentre@SingaporeFundingInitiativeand administeredbytheIDMProgramme. References [Ayanaetal.2016]Ayana,ShiqiShen,ZhiyuanLiu, andMaosongSun.2016.Neuralheadlinegenera- tionwithminimumrisktraining.arXiv:1604.01904. [Bahdanauetal.2015]DzmitryBahdanau,KyungHyun Cho,andYoshuaBengio.2015.Neuralmachine translationbyjointlylearningtoalignandtranslate. In ProceedingsofICLR . [Brownetal.1993]PeterF.Brown,StephenA. DellaPietra,VincentJ.DellaPietra,andRobertL. Mercer.1993.Themathematicsofstatisti- calmachinetranslation:Parameterestimation. ComputationalLinguisitics . [Chiang2005]DavidChiang.2005.Ahierarchical phrase-basedmodelforstatisticalmachinetransla- tion.In ProceedingsofACL . [Choetal.2014]KyunghyunCho,BartvanMerrien- boer,CaglarGulcehre,DzmitryBahdanau,Fethi Bougares,HolgerSchwenk,andYoshuaBengio. 2014.Learningphraserepresentationsusingrnn encoder-decoderforstatisticalmachinetranslation. In ProceedingsofEMNLP . [Doddington2002]GeorgeDoddington.2002.Auto- maticevaluationofmachinetranslationqualityus- ingn-gramco-occurrencestatistics.In Proceedings ofHLT . [Gaoetal.2014]JianfengGao,XiaodongHe,Wentao Yih,andLiDeng.2014.Learningcontinuous phraserepresentationsfortranslationmodeling.In ProceedingsofACL . [Gulcehreetal.2015]CaglarGulcehre,OrhanFirat, KelvinXu,KyunghyunCho,LoicBarrault,Huei- ChiLin,FethiBougares,HolgerSchwenk,and YoshuaBengio.2015.Onusingmono- lingualcorporainneuralmachinetranslation. arXiv:1503.03535. [HeandDeng2012]XiaodongHeandLiDeng.2012. Maximumexpectedbleutrainingofphraseandlex- icontranslationmodels.In ProceedingsofACL . [Jeanetal.2015]SebastienJean,KyunghyunCho, RolandMemisevic,andYoshuaBengio.2015.On usingverylargetargetvocabularyforneuralma- chinetranslation.In ProceedingsofACL . [KalchbrennerandBlunsom2013]NalKalchbrenner andPhilBlunsom.2013.Recurrentcontinuous translationmodels.In ProceedingsofEMNLP . [KoehnandHoang2007]PhilippKoehnandHieu Hoang.2007.Factoredtranslationmodels.In ProceedingsofEMNLP . [Koehnetal.2003]PhilippKoehn,FranzJ.Och,and DanielMarcu.2003.Statisticalphrase-basedtrans- lation.In ProceedingsofHLT-NAACL . [LavieandDenkowski2009]AlonLavieandMichael Denkowski.2009.Themereormetricforautomatic evaluationofmachinetranslation. MachineTrans- lation . [Lin2004]Chin-YewLin.2004.Rouge:Apackagefor automaticevaluationofsummaries.In Proceedings ofACL . [Luongetal.2015a]Minh-ThangLuong,HieuPham, andChristopherDManning.2015a.Effectiveap- proachestoattention-basedneuralmachinetransla- tion.In ProceedingsofEMNLP . [Luongetal.2015b]Minh-ThangLuong,Ilya Sutskever,QuocV.Le,OriolVinyals,andWo- jciechZaremba.2015b.Addressingtherare wordprobleminneuralmachinetranslation.In ProceedingsofACL . [Och2003]FranzJ.Och.2003.Minimumerrorrate traininginstatisticalmachinetranslation.In Pro- ceedingsofACL . [Papinenietal.2002]KishorePapineni,SalimRoukos, ToddWard,andWei-JingZhu.2002.Bleu:a methodforautomaticevaluationofmachinetrans- lation.In ProceedingsofACL . [Ranzatoetal.2015]Marc'AurelioRanzato,Sumit Chopra,MichaelAuli,andWojciechZaremba. 2015.Sequenceleveltrainingwithrecurrentneural networks.arXiv:1511.06732v1. [Sennrichetal.2015]RicoSennrich,BarryHaddow, andAlexandraBirch.2015.Improvingneural machinetranslationmodelswithmonolingualdata. arXiv:1511.06709. [SmithandEisner2006]DavidA.SmithandJasonEis- ner.2006.Minimumriskannealingfortraininglog- linearmodels.In ProceedingsofACL . [Snoveretal.2006]MatthewSnover,BonnieDorr, RichardSchwartz,LinneaMicciulla,andJohn Makhoul.2006.Astudyoftranslationeditrate withtargetedhumanannotation.In Proceedingsof AMTA . [Stolcke2002]AndreasStolcke.2002.Srilm-amex- tensiblelanguagemodelingtoolkit.In Proceedings ofICSLP . [Sutskeveretal.2014]IlyaSutskever,OriolVinyals, andQuocV.Le.2014.Sequencetosequencelearn- ingwithneuralnetworks.In ProceedingsofNIPS . [Willams1992]RonaldJ.Willams.1992.Simplesta- tisticalgradient-followingalgorithmsforconnec- tionistreinforcementlearning. MachineLearning .  
Gram-CTC:AutomaticUnitSelectionandTargetDecompositionforSequence Labelling HairongLiu* 1 ZhenyaoZhu* 1 XiangangLi 1 SanjeevSatheesh 1 Abstract Mostexistingsequencelabellingmodelsrelyon aeddecompositionofatargetsequenceinto asequenceofbasicunits.Thesemethodssuffer fromtwomajordrawbacks: 1 )thesetofbasic unitsised,suchasthesetofwords,charac- tersorphonemesinspeechrecognition,and 2 ) thedecompositionoftargetsequencesised. Thesedrawbacksusuallyresultinsub-optimal performanceofmodelingsequences.Inthispa- per,weextendthepopularCTClosscriterion toalleviatetheselimitations,andproposeanew lossfunctioncalled Gram-CTC .Whilepreserv- ingtheadvantagesofCTC,Gram-CTCautomat- icallylearnsthebestsetofbasicunits(grams),as wellasthemostsuitabledecompositionoftar- getsequences.UnlikeCTC,Gram-CTCallows themodeltooutputvariablenumberofcharac- tersateachtimestep,whichenablesthemodel tocapturelongertermdependencyandimproves thecomputationalefy.Wedemonstrate thattheproposedGram-CTCimprovesCTCin termsofbothperformanceandefyonthe largevocabularyspeechrecognitiontaskatmul- tiplescalesofdata,andthatwithGram-CTCwe canoutperformthestate-of-the-artonastandard speechbenchmark. 1.Introduction Inrecentyears,therehasbeenanexplosionofinterestin sequencelabellingtasks.ConnectionistTemporalClassi- (CTC)loss( Gravesetal. , 2006 )andSequence- to-sequence(seq2seq)models( Choetal. , 2014 ; Sutskever etal. , 2014 )presentpowerfulapproachestomultipleap- plications,suchasAutomaticSpeechRecognition(ASR) ( Chanetal. , 2016a ; Hannunetal. , 2014 ; Bahdanauetal. , * Equalcontribution 1 BaiduSiliconValleyAILab,1195 BordeauxDr,Sunnyvale,CA94089,USA.Correspondenceto: HairongLiu<liuhairong@baidu.com>. Proceedingsofthe 34 th InternationalConferenceonMachine Learning ,Sydney,Australia,PMLR70,2017.Copyright2017 bytheauthor(s). 2016 ),machinetranslation( Sébastienetal. , 2015 ),and parsing( Vinyalsetal. , 2015 ).Thesemethodsarebasedon 1) aedandcarefullychosensetofbasicunits,suchas words( Sutskeveretal. , 2014 ),phonemes( Chorowskietal. , 2015 )orcharacters( Chanetal. , 2016a ),and 2) aed andpre-determineddecompositionoftargetsequencesinto thesebasicunits.Whilethesetwopreconditionsgreatly simplifytheproblems,especiallythetrainingprocesses, theyarealsostrictandunnecessaryconstraints,whichusu- allyleadtosuboptimalsolutions.CTCmodelsareespe- ciallyharmedbyedbasicunitsintargetspace,because theybuildontheindependenceassumptionbetweensuc- cessiveoutputsinthatspace-anassumptionwhichisoften violatedinpractice. Theproblemwithedsetofbasicunitsisobvious:itis reallyhard,ifnotimpossible,todeterminetheoptimalset ofbasicunitsbeforehand.Forexample,inEnglishASR, ifweusewordsasbasicunits,wewillneedtodealwith thelargevocabulary-sizedsoftmax,aswellasrarewords anddatasparsityproblem.Ontheotherhand,ifweuse charactersasbasicunits,themodelisforcedtolearnthe complexrulesofEnglishspellingandpronunciation.For example,the"oh"soundcanbespelledinanyoffollow- ingways,dependingontheworditoccursin-{o,oa, oe,ow,ough,eau,oo,ew}.WhileCTCcaneasilymodel commonlyco-occuringgramstogether,itisimpossibleto giveroughlyequalprobabilitytomanypossiblespellings whentranscribingunseenwords.Mostspeechrecognition systemsmodelphonemes,sub-phonemeunitsandsenones e.g, ( Xiongetal. , 2016a )togetaroundtheseproblems. Similarly,state-of-the-artneuralmachinetranslationsys- temsusepre-segmentedwordpieces e.g, ( Wuetal. , 2016a ) aimingtothebestofbothworlds. Inreality,groupsofcharactersaretypicallycohesiveunits formanytasks.FortheASRtask,wordscanbedecom- posedintogroupsofcharactersthatcanbeassociatedwith sound(suchas`tion'and`eaux').Forthemachinetrans- lationtask,theremaybevaluesindecomposingwordsas rootwordsandextensions(sothatmeaningmaybeshared explicitlybetween`paternal'and`paternity').Sincethis informationisalreadyavailableinthetrainingdata,itis perhaps,bettertoletthemodelitoutbyitself.At thesametime,itraisesanotherimportquestion:howtode- arXiv:1703.00096v2  [cs.CL]  12 Aug 2017Gram-CTC composeatargetsequenceintobasicunits?Thisiscoupled withtheproblemofautomaticselectionofbasicunits,thus alsobettertoletthemodeldetermine.Recently,thereare someinterestingattemptsinthesedirectionsintheseq2seq framework.Forexample,Chanetal( Chanetal. , 2016b ) proposedtheLatentSequenceDecompositiontodecom- posetargetsequenceswithvariablelengthunitsasafunc- tionofbothinputsequenceandtheoutputsequence. Inthiswork,wepropose Gram-CTC -astrictlymoregen- eralversionofCTC-toautomaticallyseekthebestset ofbasicunitsfromthetrainingdata,called grams ,and automaticallydecomposetargetsequencesintosequences ofgrams.Justassequencepredictionwithcrossentropy trainingcanbeseenasspecialcaseoftheCTClosswith aedalignment,CTCcanbeseenasaspecialcaseof Gram-CTCwithaeddecompositionoftargetsequences. Sinceitisalossfunction,itcanbeappliedtomanyseq2seq taskstoenableautomaticselectionofgramsanddecompo- sitionoftargetsequenceswithoutmodifyingtheunderly- ingnetworks.Extensiveexperimentsonmultiplescalesof datavalidatethatGram-CTCcanimproveCTCintermsof bothperformanceandefy,andthatusingGram-CTC themodelsoutperformstate-of-the-artsonstandardspeech benchmarks. 2.RelatedWork Thebasictextunitsthatpreviousworksutilizedfortext predictiontasks( e.g, ,automaticspeechrecognition,hand- writingrecognition,machinetranslation,andimagecap- tioning)canbegenerallydividedintotwocategories:hand- craftedonesandlearning-basedones. Hand-craftedBasicUnits. Fixedsetsofcharacters (graphemes)( Gravesetal. , 2006 ; Amodeietal. , 2015 ), word-pieces( Wuetal. , 2016b ; Collobertetal. , 2016 ; Zweigetal. , 2016a ),words( Soltauetal. , 2016 ; Sébastien etal. , 2015 ),andphonemes( LeeandHon , 1988 ; Sercuand Goel , 2016 ; Xiongetal. , 2016b )havebeenwidelyusedas basicunitsfortextprediction,butallofthemhavedraw- backs.Usingtheseeddeterministicdecompositionsof textsequencesaprior,whichisnotnecessarilyop- timalforend-to-endlearning. Ł Word-segmentedmodelsremovethecomponentof learningtospellandthusenabledirectoptimizationto- wardsreducingWordErrorRate(WER).However,these modelssufferfromhavingtohandlealargevocabulary (1.7millionin( Soltauetal. , 2016 )),out-of-vocabulary words( Soltauetal. , 2016 ; Sébastienetal. , 2015 )and datasparsityproblems( Soltauetal. , 2016 ). Ł Usingcharactersresultsinmuchsmallervocabularies ( e.g, 26forEnglishandthousandsforChinese),butitre- quiresmuchlongercontextscomparedtousingwordsor word-piecesandposesthechallengeofcomposingchar- acterstowords( Gravesetal. , 2006 ; Chanetal. , 2015 ), whichisverynoisyforlanguageslikeEnglish. Ł Word-pieceslieatthemiddle-groundofwordsandchar- acters,providingagoodtrade-offbetweenvocabulary sizeandcontextsize,whiletheperformanceofusing wordpiecesissensitivetothechoiceoftheword-piece setanditsdecomposition. Ł FortheASRtask,theuseofphonemeswaspopular inthepastfewdecadesasiteasesacousticmodeling ( LeeandHon , 1988 )andgoodresultswerereported withphonemicmodels( Xiongetal. , 2016b ; Sercuand Goel , 2016 ).However,itintroducestheuncertainties ofmappingphonemestowordsduringdecoding( Doss etal. , 2003 ),whichbecomeslessrobustespeciallyfor accentedspeechdata. Learning-basedBasicUnits. Morerecently,attempts havebeenmadetolearnbasicunitsetsautomatically. ( LuongandManning , 2016 )proposedahybridWord- Charactermodelwhichtranslatesmostlyatthewordlevel andconsultsthecharactercomponentsforrarewords. Chanetal( Chanetal. , 2016b )proposedtheLatentSe- quenceDecompositionsframeworktodecomposestarget sequenceswithvariablelength-edbasicunitsasafunction ofbothinputsequenceandtheoutputsequence. Thereexistsomeearlierworksontheﬁunitdiscoveryﬂtask ( CartwrightandBrent , 1994 ; Goldwateretal. , 2006 ).A standardproblemwithMLEsolutionstothistaskisthat therearedegeneratesolutions, i.e. ,predictingthefullcor- puswithprobability 1 atthestart.OftenBayesianpriorsor ﬁminimumdescriptionlengthﬂconstraintsareusedtorem- edythis. 3.Gram-CTC 3.1.CTC CTC( Gravesetal. , 2006 )isaverypopularmethodin seq2seqlearningsinceitdoesnotrequirethealignment informationbetweeninputsandoutputs,whichisusually expensive,ifnotimpossible,toobtain. Sincethereisnoalignmentinformation,CTCmarginalizes overallpossiblealignments.Thatis,ittriestomaximize p ( l j x )= P ˇ p ( ˇ j x ) ,where x isinput,and ˇ representa validalignment.Forexample,ifthesizeofinputis 3 ,and theoutputis`hi',whoselengthis 2 ,therearethreepossible alignments,`-hi',`h-i'and`hi-',where`-'represents blank . Forthedetails,pleaserefertotheoriginalpaper( Graves etal. , 2006 ). 3.2.FromCTCtoGram-CTC InCTC,thebasicunitsareed,whichisnotdesirablein someapplications.HerewegeneralizeCTCbyconsidering asequenceofbasicunits,called gram ,asawhole,which isusuallymorereasonableinmanyapplications. Gram-CTC Figure1. Illustrationofthestatesandtheforward-backwardtransitionsforthelabel`CAT'.Herewelet G bethesetofalluni-gramsand bi-gramsoftheEnglishalphabet.Thesetofallvalidstates S forthelabel l =`CAT'arelistedtotheleft.Thesetofstatesandtransitions thatarecommontobothvanillaandGram-CTCareinblack,andthosethatareuniquetoGram-CTCareinorange.Ingeneral,any extensionthatcollapsesbackto l isavalidtransition-Forexample,wecantransitioninto(`CAT',1)from(`CAT',1),(`CA',2),(`CA', 1)and(`CA',0)butnotfrom(`CAT',0)or(`CAT',2) Let G beasetofn-gramsofthesetofbasicunits C ofthe targetsequence,and ˝ bethelengthofthelongestgramin G .AGram-CTCnetworkhasasoftmaxoutputlayerwith j G j +1 units,thatis,theprobabilityoverallgramsin G and oneadditionalsymbol, blank .Tosimplifytheproblem,we alsoassume C  G . 1 Foraninputsequence x oflength T ,let y = N w ( x ) be thesequenceofnetworkoutputs,anddenoteby y t k asthe probabilityofthe k -thgramattime t ,where k istheindex ofgramsin G 0 = G [f blank g ,thenwehave p ( ˇ j x )= T Y t =1 y t ˇ t , 8 ˇ 2 G 0 T (1) JustasinthecaseofCTC,herewerefertotheelements of G 0 T aspaths,anddenotethemby ˇ ,whichrepresents apossiblealignmentbetweeninputandoutput.Thedif- ferenceisthatforeachwordinthetargetsequence,itmay bedecomposedintodifferentsequencesofgrams.Forex- ample,theword`hello'canonlybedecomposedintothe sequence[`h',`e',`l',`l',`o']forCTC(assumeuni-gram CTChere),butitalsocanbedecomposedintothesequence [`he',`ll',`o']if`he'and`ll'arein G . Foreach ˇ ,wemapitintoatargetsequenceinthesame wayasCTCusingthecollapsingfunctionthat 1 )removes allrepeatedlabelsfromthepathandthen 2 )removesall blanks.Notethatessentiallyitistheseruleswhichde- 1 Thisisbecausetheremaybenovaliddecompositionsfor sometargetsequencesif C  G .SinceGram-CTCwill outtheidealdecompositionoftargetsequencesintogramsduring training,thisconditionguaranteesthatthereisatleastonevalid decompositionforeverytargetsequence. terminethetransitionsbetweenthestatesofadjacenttime stepsinFigure 1 .Thisisamany-to-onemappingandwe denoteitby B .Notethatotherrulescanbeadoptedhere andthegeneralideapresentedinthispaperdoesnotdepend ontheserules.Foratargetsequence l , B  1 ( l ) rep- resentsallpathsmappedto l .Then,wehave p ( l j x )= X ˇ 2 B  1 ( l ) p ( ˇ j x ) (2) Thisequationallowsfortrainingsequencelabelingmod- elswithoutanyalignmentinformationusingCTCloss,be- causeitmarginalizesoverallpossiblealignmentsduring training.Gram-CTCusesthesameeffecttoenablethe modeltomarginalizeovernotonlyalignments,butalso decompositionsofthetargetsequence. Notethatforeachtargetsequence l ,theset B  1 ( l ) has O ( ˝ 2 ) morepathsthanitdoesinCTC.Thisisbecause thereare O ( ˝ ) timesmorevalidstatespertimestep,and eachstatemayhaveavalidtransitionfrom O ( ˝ ) statesin theprevioustimestep.TheoriginalCTCmethodisthus, aspecialcaseofGram-CTCwhen G = C and ˝ =1 . Whilethequadraticincreaseinthecomplexityofthealgo- rithmisnontrivial,weassertthatitisatrivialincreasein theoveralltrainingtimeoftypicalneuralnetworks,where thecomputationtimeisdominatedbytheneuralnetworks themselves.Additionally,thealgorithmextendsgenerally toanyarbitrary G andneednothaveallpossiblen-grams uptolength ˝ . 3.3.TheForward-BackwardAlgorithm Toefcompute p ( l j x ) ,wealsoadoptthedynamic programmingalgorithm.Theessencehereisidentifying Gram-CTC thestatesoftheproblem,sothatwemaysolvefuture statesbyreusingsolutionstoearlierstates.Inourcase,the statemustcontainalltheinformationrequiredtoidentify allvalidextensionsofanincompletepath ˇ suchthatthe collapsingfunctionwilleventuallycollapsethecomplete ˇ backto l .ForGram-CTC,thiscanbedonebycollaps- ingallbutthelastelementofthepath ˇ .Therefore,the stateisatuple ( l 1: i , j ) ,wheretheitemisacollapsed path,representingaofthetargetlabelsequence,and j 2f 0, ::: , ˝ g isthelengthofthelastgram ( l i  j +1: i ) usedformakingthe j =0 isvalidandmeansthat blank wasused.Wedenotethegram ( l i  j +1: i ) by g j i ( l ) , andthestate ( l 1: i , j ) as s j i ( l ) .Forreadability,wewillfur- thershorten s j i ( l ) to s j i and g j i ( l ) to g j i .Forastate s ,its correspondinggramisdenotedby s g ,andthepositionsof thecharacterandlastcharacterof s g aredenotedby b ( s ) and e ( s ) ,respectively.Duringdynamicprogramming, wearedealingwithsequenceofstates,forastatesequence  ,itscorrespondinggramsequencesisunique,denotedby  g . Figure 1 illustratespartiallythedynamicprogrammingpro- cessforthetargetsequence`CAT'.Herewesuppose G containsallpossibleuni-gramsandbi-grams.Thus,for eachcharacterin`CAT',therearethreepossiblestatesas- sociatedwithit: 1 )thecurrentcharacter, 2 )thebi-gram endingincurrentcharacter,and 3) the blank aftercurrent character.Thereisalsoone blank atbeginning.Intotalwe have 10 states. Supposingthemaximumlengthofgramsin G is ˝ ,we scan l togettheset S ofallpossiblestates,suchthatforall s j i 2 S ,itscorresponding g j i 2 G 0 . i 2f 0, ::: , j l jg and j 2f 0, ::: , ˝ g .Foratargetsequence l ,theforward variable  t ( s ) forany s 2 S tothetotalprobabilityofall validpathsesthatendatstate s attime t .  t ( s ) def = X  j B (  g )= l 1: e ( s ) ,  t = s t Y t 0 =1 y t 0  t 0 g (3) Followingthiswehavethefollowingrulesfor initialization  1 ( s )= 8 < : y 1 b s = s 0 0 y 1 g i i s = s i i 8 i 2f 1, ::: , ˝ g 0 otherwise (4) andrecursion  t ( s )= 8 > > > > > > > > < > > > > > > > > : ^  i t  1  y t b when s = s 0 i , [^  i  j t  1 +  t  1 ( s )]  y t g j i when s = s j i and g j i 6 = g j i  j , [^  i  j t  1 +  t  1 ( s )   t  1 ( s j i  j )]  y t g j i when s = s j i and g j i = g j i  j (5) where ^  i t = P ˝ j =0  t ( s j i ) and y t b istheprobabilityof blank attime t . Thetotalprobabilityofthetargetsequence l isthenex- pressedinthefollowingway: p ( l j x )= ˝ X j =0  T ( s j j l j ) (6) similarly,wecanthebackwardvariable  t ( s ) as:  t ( s ) def = X  j B (  g )= l b ( s ): l ,  t = s T Y t 0 = t y t 0  t 0 g (7) Fortheinitializationandrecursionof  t ( s ) ,wehave  T ( s )= 8 < : y T b s = s 0 T y T g i T s = s i T 8 i 2f 1, ::: , ˝ g 0 otherwise (8) and  t ( s )= 8 > > > > > > > > < > > > > > > > > : ^  i t +1  y t b when s = s 0 i , [ ^  i + j t +1 +  t +1 ( s )]  y t g j i when s = s j i and g j i 6 = g j i + j , [ ^  i + j t +1 +  t +1 ( s )   t +1 ( s j i + j )]  y t g j i when s = s j i and g j i = g j i + j (9) where ^  i t = P ˝ j =0  t ( s j i + j ) 3.4.BackPropagation SimilartoCTC,wehavethefollowingexpression: p ( l j x )= X s 2 S  t ( s )  t ( s ) y t s g 8 t 2f 1, ::: , T g (10) Thederivativewithregardsto y t k is: @p ( l j x ) @y t k = 1 y t k 2 X s 2 lab ( l , k )  t ( s )  t ( s ) (11) where lab ( l , k ) isthesetofstatesin S whosecorrespond- inggramis k .Thisisbecausetheremaybemultiplestates correspondingtothesamegram. Forthebackpropagation,themostimportantformulaisthe partialderivativeoflosswithregardtotheunnormalized output u t k . @ ln p ( l j x ) @u t k = y t k  1 y t k Z t X s 2 lab ( l , k )  t ( s )  t ( s ) (12) where Z t def = P s 2 S  t ( s )  t ( s ) y t s g . Gram-CTC (a)Trainingcurvesbefore( blue )andafter( or- ange )ofgrams. (b)Trainingcurveswithout( blue )andwith ( orange )joint-training (c)Joint-trainingArchitecture Figure2. (Figure 2a )comparesthetrainingcurvesbefore(blue)andafter(orange)ofgrams.Theylookverysimilar, althoughthenumberofgramsisgreatlyreducedafterwhichmakestrainingfasterandpotentiallymorerobustduetoless gramsparsity.Figure( 2b )Trainingcurveofmodelwithandwithoutjoint-training.Themodelcorrespondingtotheorangetraining curveisjointlytrainedtogetherwithvanillaCTC,suchmodelsareoftenmorestableduringtraining.Figure( 2c )Typicaljoint-training modelarchitecture-vanillaCTClossisbestappliedafewlevelslowerthantheGram-CTCloss. 4.Methodology Herewedescribeadditionaltechniqueswefoundusefulin practicetoenabletheGram-CTCtoworkefaswell aseffectively. 4.1.IterativeGramSelection AlthoughGram-CTCcanautomaticallyselectuseful grams,itischallengingtotrainwithalarge G .Theto- talnumberofpossiblegramsisusuallyhuge.Forexample, inEnglish,wehave 26 characters,thenthetotalnumberof bi-gramsis 26 2 =676 ,thetotalnumberoftri-gramsare 26 3 =17576 ,...,whichgrowsexponentiallyandquickly becomesintractable.However,itisunnecessarytocon- sidermanygrams,suchas`aaaa',whichareobviouslyuse- less. Inourexperiments,weeliminatemostofuseless gramsfromthestatisticsofahugecorpus,thatis,wecount thefrequencyofeachgraminthecorpusanddropthese gramswithrarefrequencies.Then,wetrainamodelwith Gram-CTConalltheremaininggrams.Byapplying(de- coding)thetrainedmodelonalargespeechdataset,weget therealstatisticsofgram'susage.Ultimately,wechoose highfrequencygramstogetherwithalluni-gramsasour nalgramset G .Table 1 showstheimpactofiterativegram selectiononWSJ(withoutLM).Figure 2a showsitscorre- spondingtrainingcurve.Fordetails,pleaserefertoSection 5.2 . 4.2.JointTrainingwithVanillaCTC Gram-CTCneedstosolvebothdecompositionandalign- menttasks,whichisahardertaskforamodeltolearnthan CTC.Thisisoftenmanifestedinunstabletrainingcurves, forcingustolowerthelearningratewhichinturnresults inmodelsconvergingtoaworseoptima.Toovercomethis dif,wefoundittotrainamodelwithboth theGram-CTC,aswellasthevanillaCTCloss(similar tojoint-trainingCTCtogetherwithCElossasmentioned in( Saketal. , 2015 )).Jointtrainingofmultipleobjectives forsequencelabellinghasalsobeenexploredinprevious works( Kimetal. , 2016 ; KimandRush , 2016 ). Atypicaljoint-trainingmodellookslikeFigure 2c ,andthe correspondingtrainingcurveisshowninFigure 2b .The effectofjoint-trainingareshowninTable 4 andTable 5 in theexperiments. 5.Experiments WetesttheGram-CTClossontheASRtask,whileboth CTCandtheintroducedGram-CTCaregenerictechniques forothersequencelabellingtasks.Foralloftheexperi- ments,themodelandtrainingprocedureare thesameasin( Amodeietal. , 2015 )-Themodelisarecur- rentneuralnetwork(RNN)with2two-dimensionalconvo- lutionalinputlayers,followedbyKforward(Fwd)orbidi- rectional(Bidi)GatedRecurrentlayers,Ncellseach,and onefullyconnectedlayerbeforeasoftmaxlayer.Inshort hand,suchamodeliswrittenas`2x2DConv-KxNGRU'. Thenetworkistrainedend-to-endwiththeCTC,Gram- CTCoraweightedcombinationofboth.Thiscombination isdescribedintheearliersection. Inallexperiments,audiodataisissampledat16kHz.Lin- earFFTfeaturesareextractedwithahopsizeof10ms andwindowsizeof20ms,andarenormalizedsothat eachinputfeaturehaszeromeanandunitvariance.The networkinputsarethusspectralmagnitudemapsranging from0-8kHzwith161featuresper10msframe.Ateach epoch, 40% oftheutterancesarerandomlyselectedtoadd Gram-CTC Loss WER CTC,uni-gram 16.91 CTC,bi-gram 21.63 Gram-CTC,handpick 17.01 Gram-CTC,alluni-grams+bi-grams 16.89 Gram-CTC, 16.66 Table1. ResultsofdifferentgramselectionmethodsonWSJ dataset. backgroundnoiseto.Theoptimizationmethodweuse isstochasticgradientdescentwithNesterovmomentum. Learninghyperparameters(batch-size,learning-rate,mo- mentum,andetc.)varyacrossdifferentdatasetsandare tunedforeachmodelbyoptimizingahold-outset.Typical valuesarealearningrateof 10  3 andmomentumof 0.99 . 5.1.DataandSetup WallStreetJournal(WSJ) .Thiscorporaconsistspri- marilyofreadspeechwithtextsdrawnfromamachine- readablecorpusofWallStreetJournalnewstext,andcon- tainsabout 80hours speechdata.Weusedthestandard oftrainsi284datasetfortraining,dev93for validationandeval92fortesting.Thisisarelatively`clean' taskandoftenusedformodelprototyping( Miaoetal. , 2015 ; Bahdanauetal. , 2016 ; Zhangetal. , 2016 ; Chanetal. , 2016b ). Fisher-Switchboard .ThisisacommonlyusedEnglish conversationaltelephonespeech(CTS)corpora,which contains 2300hours CTSdata.Followingtheprevious works( Zweigetal. , 2016b ; Poveyetal. , 2016 ; Xiongetal. , 2016b ; SercuandGoel , 2016 ),evaluationiscarriedouton theNIST2000CTStestset,whichcomprisesbothSwitch- board(SWB)andCallHome(CH)subsets. 10KSpeechDataset .WeconductlargescaleASRexper- imentsonanoisyinternaldatasetof 10,000hours .This datasetcontainsspeechcollectedfromvariousscenarios, suchasdifferentbackgroundnoises,fardifferentac- cents,andsoon.Duetoitsinherentcomplexities,itisa verychallengingtask,andcanthusvalidatetheeffective- nessoftheproposedmethodforreal-worldapplication. 5.2.GramSelection WeemploytheWSJdatasetfordemonstratingdifferent strategiesofselectinggramsforGram-CTC,sinceitisa widelyuseddatasetandalsosmallenoughforrapididea vHowever,becauseitissmall,wecannotuse largegramshereduetodatasparsityproblem.Thus,the gramsetonWSJisnotoptimalforotherlarger datasets,wherelargergramscouldbeeffectivelyused,but theprocedureofisthesameforthem. Wetrainamodelusingalluni-gramsandbi-grams( 29 Loss WER EpochTime(mins) (stride) 24 24 CTC,uni-gram 16.9123.76 2916 CTC,bi-gram 20.5721.63 2312 Gram-CTC 16.6618.87 3518 Table2. PerformanceswithdifferentmodelstridesonWSJ dataset. uni-gramsand 26 2 =676 bi-grams,intotal 705 grams), andthendodecodingwiththeobtainedmodelonanother speechdatasettogetthestatisticsoftheusageofgrams. Top 100 bi-gramstogetherwithall 29 uni-grams(auto- grams)areusedforthesecondroundoftraining. Forcomparison,wealsopresenttheresultofthebesthand- pickedgrams,aswellastheresultsonuni-grams.Allthe resultsareshowninTable 1 . SomeinterestingobservationscanbefoundinTable 1 . First,theperformanceofgramsisonlyslightly betterthanthecombinationofalluni-gramsandallbi- grams.ThisisprobablybecauseWSJissosmallthatgram learningsuffersfromthedatasparsityproblemhere(simi- lartoword-segmentedmodels).Thegramset containsonlyasmallsubsetofbi-grams,thusmorero- bust.Thisisalsowhyweonlytrybi-grams,notinclud- inghigher-ordergrams.Second,theperformanceofbest handpickedgramsisworsethangrams.This isdesirable.Itistime-consumingtohandpickgrams,es- peciallywhenyouconsiderhigh-ordergrams.Themethod ofiterativegramselectionisnotonlyfast,butusuallybet- ter.Third,theperformanceofGram-CTCon gramsisonlyslightlybetterthanCTConuni-grams.This isbecauseGram-CTCisinherentlydiftotrain,since itneedstolearnbothdecompositionandalignment.WSJ istoosmalltoprovideenoughdatatotrainGram-CTC. 5.3.SequenceLabellinginLargeStride UsingalargetimestrideforsequencelabellingwithRNNs cangreatlyboosttheoverallcomputationefy,since iteffectivelyreducesthetimestepsforrecurrentcomputa- tion,thusspeedsuptheprocessofbothforwardinference andbackwardpropagation.However,thelargeststridethat canbeusedislimitedbythegramsetweuse.The(uni- gram)CTChastoworkinahightimeresolution(small stride)inordertohaveenoughnumberofframestoout- puteverycharacter.Thisisveryinefasweknowthe sameacousticfeaturecouldcorrespondtoseveralgramsof differentlengths( e.g. ,{`i',`igh',`eye'}).Thelargerthe gramsare,thelargerstridewearepotentiallyabletouse. DS2( Amodeietal. , 2015 )employednon-overlappingbi- gramoutputstoallowforalargerstride.Thisimposesan constraintforcingthemodeltolearn,notonlythe spellingofeachword,butalsohowtosplitwordsintobi- grams.Forexample, part issplitas[ pa , rt ]buttheword Gram-CTC Figure3. Max-decodingresults(withoutcollapsing)ofCTCandGram-CTConutterancesfromSwitchboarddataset.Thepredicted characters(byCTC)orgrams(byGram-CTC)ateachtimestepareseparatedby"|".AstheGram-CTCmodelistrainedwithdoubled strideasthatofCTCmodel,weplacethegramsatadoubledwidthaswedowithcharactersforbetterviewing.The"_"represents blank . apart isforcedtobedecomposedas[ ap , ar , t ].Gram- CTCremovesthisconstraintbyallowingthemodeltode- composewordsintolargerunitsintothemostconvenient orsensibledecomposition.Comparisonresultsshowthis changeenablesGram-CTCtoworkmuchbetterthanbi- gramCTC,asinTable 2 . InTable 2 ,wecomparetheperformanceoftrainedmodel andtrainingefyontwostrides, 2 and 4 .ForGram- CTC,weusethegramsetfromprevioussec- tion.Asexpected,usingstride 4 almostcutsthetrain- ingtimeperepochintohalf,comparedtostride 2 .From stride 2 tostride 4 ,theperformanceofuni-gramCTC dropsquickly.Thisisbecausesmallgramsinherentlyneed highertimeresolutions.AsforGram-CTC,fromstride 2 tostride 4 ,itsperformancedecreasesalittlebit,whilein experimentsontheotherdatasets,Gram-CTCconstantly worksbetterinstride 4 .Onepossibleexplanationisthat WSJistoosmallforGram-CTCtolearnlargegramswell. Incontrast,theperformanceofbi-gramCTCisnotasgood asthatofGram-CTCineitherstride. 5.4.DecodingExamples Figure 3 illustratesthemax-decodingresultsofbothCTC andGram-CTConnineutterances.Herethelabelsetfor CTCisthesetofallcharacters,andthelabelsetforGram- CTCisangramsetcontainingalluni-grams andsomehigh-frequencyhigh-ordergrams.HereGram- CTCusesstride 4 whileCTCusesstride 2 . FromFigure 3 ,wecanthat: 1) Gram-CTCdoesauto- maticallymanyintuitiveandmeaningfulgrams,such as`the',`ng',and`are'. 2) Italsodecomposesthesen- tencesintosegmentswhicharemeaningfulintermofpro- nunciation.Thisdecompositionresemblesthephoneticde- composition,butinlargergranularityandarguablymore natural. 3) SinceGram-CTCpredictsachunkofcharac- ters(agram)eachtime,eachpredictionutilizeslargercon- textandthesecharactersinthesamepredictedchunkare dependent,thuspotentiallymorerobust.Oneexampleis theword`will'inthelastsentenceinFigure 3 . 4) Since theoutputofnetworkistheprobabilityoverallgrams,the decodingprocessisalmostthesameasCTC,stillend-to- end.Thismakessuchdecompositionsuperiortophonetic decomposition.Insummary,Gram-CTCcombinesthead- vantagesofbothCTConcharactersandCTConphonemes. 5.5.ComparisonwithOtherMethods 5.5.1.WSJ DATASET Themodelusedhereis[2x2Dconv,3x1280BidiGRU] withaCTCorGram-CTCloss.Theresultsareshown inTable 3 .Forallmodelswetrained,languagemodel cangreatlyimprovetheirperformances,intermofWER. Thoughthisdatasetcontainsverylimitedamountoftext dataforlearninggramselectionanddecomposition,Gram- Gram-CTC Architecture WER PhonemeCTC+trigramLM( Miaoetal. , 2015 ) 7.3 GraphemeCTC+trigramLM( Miaoetal. , 2015 ) 9.0 Attention+trigramLM( Bahdanauetal. , 2016 ) 9.3 DeepConvLAS+noLM( Zhangetal. , 2016 ) 10.5 DeepConvLAS+LSD+noLM( Chanetal. , 2016b ) 9.6 TemporalLS+Cov+LM ( ChorowskiandNavdeep , 2016 ) 6.7 VanillaCTC+noLM(ours) 16.91 VanillaCTC+LM(ours) 7.11 Gram-CTC+noLM(ours) 16.66 Gram-CTC+LM(ours) 6.75 Table3. Comparisonwithpreviouspublishedresultswithend-to- endtrainingonWSJspeechdataset.Thenumbersinboldarethe bestresultswithandwithoutalanguagemodel CTCcanstillimprovethevanillaCTCnotably. 5.5.2.F ISHER -S WITCHBOARD Theacousticmodeltrainedhereiscomposedoftwo2D convolutionsandsixbi-directionalGRUlayerin2048di- mension.Thecorrespondinglabelsareusedfortraining N-gramlanguagemodels. Ł SwitchboardEnglishspeech97S62 Ł FisherEnglishspeechPart1-2004S13,2004T19 Ł FisherEnglishspeechPart2-2005S13,2005T19 WeuseasampleoftheSwitchboard-1portionoftheNIST 2002dataset(2004S11RT-02)fortuninglanguagemodel hyper-parameters.TheevaluationisdoneontheNIST 2000set.Thisformsastandardbenchmark forevaluatingASRmodels.ResultsareinTable 4 . Wecompareourmodelagainstbestpublishedresultson in-domain data.Theseresultscanoftenbeimprovedusing out-of-domain datafortrainingthelanguagemodel,and sometimestheacousticmodelaswell.Togetherthesetech- niquesallow( Xiongetal. , 2016b )toreachaWERof5.9 ontheSWBDset. 5.5.3.10KS PEECH D ATASET Finally,weexperimentonalargenoisydatasetcollectedby ourselfforbuildinglarge-vocabularyContinuousSpeech Recognition(LVCSR)systems.Thisdatasetcontainsabout 10000 hoursspeechinadiversityofscenarios,suchasfar- backgroundnoises,accents.Inallcases,themodel is[2x2DConv,3x2560FwdGRU,LAConv]withonlya changeinthelossfunction.`LAConv'referstoalook aheadconvolutionlayerasseenin( Amodeietal. , 2015 ) whichworkstogetherwithforward-onlyRNNsfordeploy- mentpurpose. AswiththeFisher-Switchboarddataset,theoptimalstride is 4 forGram-CTCand 2 forvanillaCTConthisdataset. Thus,inbothexperiments,bothGram-CTCandvanilla Architecture SWBD CH WER WER Iterated-CTC( Zweigetal. , 2016b ) 11.3 18.7 BLSTM+LFMMI( Poveyetal. , 2016 ) 8.5 15.3 LACE+LFMMI 2 ( Xiongetal. , 2016b ) 8.3 14.8 Dilatedconvolutions( SercuandGoel , 2016 ) 7.7 14.5 VanillaCTC(ours) 9.0 17.7 Gram-CTC(ours) 7.9 15.8 VanillaCTC+Gram-CTC(ours) 7.3 14.7 Table4. ComparisonwithpreviouspublishedresultsonFisher- Switchboardbenchmark(ﬁSWBDﬂandﬁCHﬂrepresentSwitch- boardandCallhomeportions,respectively)using in-domain data. Weonlylistresultsusingsinglemodelshere. Architecture WER(NoLM) WER(WithLM) VanillaCTC 29.1 19.77 Gram-CTC 27.56 19.53 VanillaCTC+Gram-CTC 25.59 18.52 Table5. LVCSRresultson10Kspeechdataset. CTC+Gram-CTCaretrainedmushfasterthanvanilla CTCitself.TheresultisshowninTable 5 .Gram-CTC performsbetterthanCTC.Afterjoint-trainingwithvanilla CTCandalignmentinformationthroughaCEloss,itsper- formanceisfurtherboosted,whichvjoint-training helpstraining.Infact,withonlyasmalladditionalcost oftime,iteffectivelyreducestheWERfrom 27.56% to 25.59% (withoutlanguagemodel). 6.ConclusionsandFutureWork Inthispaper,wehaveproposedthe Gram-CTC lossto enableautomaticdecompositionoftargetsequencesinto learnedgrams.Wealsopresenttechniquestotrainthe Gram-CTCinacleanandstableway.Ourextensiveexper- imentsdemonstratetheproposedGram-CTCenablesthe modelstorunmoreefthanthevanillaCTC,by usinglargerstride,whileobtainingbetterperformanceof sequencelabelling.Comparisonexperimentsonmultiple- scaledatasetsshowtheproposedGram-CTCobtainsstate- of-the-artresultsonvariousASRtasks. AninterestingobservationisthatthelearningofGram- CTCimplicitlyavoidstheﬁdegeneratedsolutionﬂthatoc- curringinthetraditionalﬁunitdiscoveryﬂtask,withoutin- volvinganyBayesianpriorsortheﬁminimumdescription lengthﬂconstraint.Usingasmallgramsetthatcontains onlyshort(upto 5 inourexperiments)aswellashigh- frequencygramsmayexplainthesuccesshere. Wewillcontinueinvestigatingtechniquesofimprovingthe optimizationofGram-CTCloss,aswellastheapplications ofGram-CTCforothersequencelabellingtasks. Gram-CTC References AlexGraves,SantiagoFernández,FaustinoGomez,and JürgenSchmidhuber.Connectionisttemporal tion:labellingunsegmentedsequencedatawithrecur- rentneuralnetworks.In Proceedingsofthe23rdinterna- tionalconferenceonMachinelearning ,pages369Œ376. ACM,2006. KyunghyunCho,BartVanMerriënboer,CaglarGulcehre, DzmitryBahdanau,FethiBougares,HolgerSchwenk, andYoshuaBengio.Learningphraserepresentationsus- ingrnnencoder-decoderforstatisticalmachinetransla- tion. arXivpreprintarXiv:1406.1078 ,2014. IlyaSutskever,OriolVinyals,andQuocVLe.Sequence tosequencelearningwithneuralnetworks.In Advances inneuralinformationprocessingsystems ,pages3104Œ 3112,2014. WilliamChan,NavdeepJaitly,QuocLe,andOriolVinyals. Listen,attendandspell:Aneuralnetworkforlarge vocabularyconversationalspeechrecognition.In 2016 IEEEInternationalConferenceonAcoustics,Speech andSignalProcessing(ICASSP) ,pages4960Œ4964. IEEE,2016a. AwniY.Hannun,CarlCase,JaredCasper,BryanCatan- zaro,GregDiamos,ErichElsen,RyanPrenger,San- jeevSatheesh,ShubhoSengupta,AdamCoates,andAn- drewY.Ng.Deepspeech:Scalingupend-to-endspeech recognition. CoRR ,abs/1412.5567,2014. DzmitryBahdanau,JanChorowski,DmitriySerdyuk, YoshuaBengio,etal.End-to-endattention-basedlarge vocabularyspeechrecognition.In 2016IEEEInterna- tionalConferenceonAcoustics,SpeechandSignalPro- cessing(ICASSP) ,pages4945Œ4949.IEEE,2016. JeanSébastien,KyunghyunCho,RolandMemisevic,and YoshuaBengio.Onusingverylargetargetvocabulary forneuralmachinetranslation.2015. OriolVinyals,Kaiser,TerryKoo,SlavPetrov,Ilya Sutskever,andGeoffreyHinton.Grammarasaforeign language.In AdvancesinNeuralInformationProcessing Systems ,pages2773Œ2781,2015. JanKChorowski,DzmitryBahdanau,DmitriySerdyuk, KyunghyunCho,andYoshuaBengio.Attention-based modelsforspeechrecognition.In AdvancesinNeural InformationProcessingSystems ,pages577Œ585,2015. WXiong,JDroppo,XHuang,FSeide,MSeltzer,AStol- cke,DYu,andGZweig.Themicrosoft2016con- versationalspeechrecognitionsystem. arXivpreprint arXiv:1609.03528 ,2016a. YonghuiWu,MikeSchuster,ZhifengChen,QuocV. Le,MohammadNorouzi,WolfgangMacherey,Maxim Krikun,YuanCao,QinGao,KlausMacherey,Jeff Klingner,ApurvaShah,MelvinJohnson,Xiaobing Liu,LukaszKaiser,StephanGouws,YoshikiyoKato, TakuKudo,HidetoKazawa,KeithStevens,George Kurian,NishantPatil,WeiWang,CliffYoung,Jason Smith,JasonRiesa,AlexRudnick,OriolVinyals,Gre- goryS.Corrado,MacduffHughes,andJeffreyDean. Google'sneuralmachinetranslationsystem:Bridging thegapbetweenhumanandmachinetranslation. CoRR , abs/1609.08144,2016a. WilliamChan,YuZhang,QuocLe,andNavdeepJaitly. Latentsequencedecompositions.In Arxiv ,2016b. DarioAmodei,RishitaAnubhai,EricBattenberg,Carl Case,JaredCasper,BryanCatanzaro,JingdongChen, MikeChrzanowski,AdamCoates,GregDiamos,etal. Deepspeech2:End-to-endspeechrecognitioninen- glishandmandarin. arXivpreprintarXiv:1512.02595 , 2015. YonghuiWu,MikeSchuster,ZhifengChen,QuocV Le,MohammadNorouzi,WolfgangMacherey,Maxim Krikun,YuanCao,QinGao,KlausMacherey,etal. Google'sneuralmachinetranslationsystem:Bridging thegapbetweenhumanandmachinetranslation. arXiv preprintarXiv:1609.08144 ,2016b. RonanCollobert,ChristianPuhrsch,andGabrielSynnaeve. Wav2letter:anend-to-endconvnet-basedspeechrecog- nitionsystem. arXivpreprintarXiv:1609.03193 ,2016. GeoffreyZweig,ChengzhuYu,JashaDroppo,andAn- dreasStolcke.Advancesinall-neuralspeechrecogni- tion. arXivpreprintarXiv:1609.05935 ,2016a. HagenSoltau,HankLiao,andHasimSak.Neural speechrecognizer:Acoustic-to-wordlstmmodelfor largevocabularyspeechrecognition. arXivpreprint arXiv:1610.09975 ,2016. K-FLeeandH-WHon.Large-vocabularyspeaker- independentcontinuousspeechrecognitionusinghmm. In Acoustics,Speech,andSignalProcessing,1988. ICASSP-88.,1988InternationalConferenceon ,pages 123Œ126.IEEE,1988. TomSercuandVaibhavaGoel.Densepredictionon sequenceswithtime-dilatedconvolutionsforspeech recognition. arXivpreprintarXiv:1611.09288 ,2016. WayneXiong,JashaDroppo,XuedongHuang,Frank Seide,MikeSeltzer,AndreasStolcke,DongYu,andGe- offreyZweig.Achievinghumanparityinconversational speechrecognition. arXivpreprintarXiv:1610.05256 , 2016b. Gram-CTC WilliamChan,NavdeepJaitly,QuocVLe,andOriol Vinyals.Listen,attendandspell. arXivpreprint arXiv:1508.01211 ,2015. MathewMagimaiDoss,ToddAStephenson,Hervé Bourlard,andSamyBengio.Phoneme-graphemebased speechrecognitionsystem.In AutomaticSpeechRecog- nitionandUnderstanding,2003.ASRU'03.2003IEEE Workshopon ,pages94Œ98.IEEE,2003. Minh-ThangLuongandChristopherDManning.Achiev- ingopenvocabularyneuralmachinetranslation withhybridword-charactermodels. arXivpreprint arXiv:1604.00788 ,2016. TimothyAndrewCartwrightandMichaelRBrent.Seg- mentingspeechwithoutalexicon:Therolesof phonotacticsandspeechsource. arXivpreprintcmp- lg/9412005 ,1994. SharonGoldwater,ThomasLGrifandMarkJohnson. Contextualdependenciesinunsupervisedwordsegmen- tation.In Proceedingsofthe21stInternationalConfer- enceonComputationalLinguisticsandthe44thannual meetingoftheAssociationforComputationalLinguis- tics ,pages673Œ680.AssociationforComputationalLin- guistics,2006. HasimSak,AndrewW.Senior,KanishkaRao,and FranÃ  goiseBeaufays.Fastandaccuraterecurrentneural networkacousticmodelsforspeechrecognition. CoRR , abs/1507.06947,2015. SuyounKim,TakaakiHori,andShinjiWatanabe.Joint ctc-attentionbasedend-to-endspeechrecognitionusing multi-tasklearning. arXivpreprintarXiv:1609.06773 , 2016. YoonKimandAlexanderMRush.Sequence-levelknowl- edgedistillation. arXivpreprintarXiv:1606.07947 , 2016. YajieMiao,MohammadGowayyed,andFlorianMetze. Eesen:End-to-endspeechrecognitionusingdeeprnn modelsandwfst-baseddecoding.In AutomaticSpeech RecognitionandUnderstanding(ASRU),2015IEEE Workshopon ,pages167Œ174.IEEE,2015. YuZhang,WilliamChan,andNavdeepJaitly.Verydeep convolutionalnetworksforend-to-endspeechrecogni- tion. arXivpreprintarXiv:1610.03022 ,2016. GeofferyZweig,GhengzhuYu,JashaDroppo,andAn- dreasStolcke.Advancesinall-neuralspeechrecogni- tion. arXivpreprintarXiv:1609.05935 ,2016b. DanielPovey,VijayadityaPeddinti,DanielGalvez,Pegah Ghahrmani,VimalManohar,XingyuNa,YimingWang, andSanjeevKhudanpur.Purelysequence-trainedneural networksforasrbasedonlattice-freemmi. Submittedto Interspeech ,2016. JanChorowskiandJaitlyNavdeep.Towardsbetterdecod- ingandlanguagemodelintegrationinsequencetose- quencemodels. arXivpreprintarXiv:1612.02695 ,2016.  
WordSup:ExploitingWordAnnotationsforCharacterbasedTextDetection HanHu 1  ChengquanZhang 2  YuxuanLuo 2 YuzhuoWang 2 JunyuHan 2 ErruiDing 2 MicrosoftResearchAsia 1 IDL,BaiduResearch 2 hanhu@microsoft.comf zhangchengquan,luoyuxuan,wangyuzhuo,hanjunyu,dingerruig @baidu.comAbstractImagerytextsareusuallyorganizedasahierarchyof severalvisualelements,i.e.characters,words,textlines  andtextblocks.Amongtheseelements,characteristhe  mostbasiconeforvariouslanguagessuchasWestern,Chi-  nese,Japanese,mathematicalexpressionandetc.Itisnatu-  ralandconvenienttoconstructacommontextdetectionen-  ginebasedoncharacterdetectors.However,trainingchar-  acterdetectorsrequiresavastoflocationannotatedchar-  acters,whichareexpensivetoobtain.Actually,theexisting  realtextdatasetsaremostlyannotatedinwordorlinelevel.  Toremedythisdilemma,weproposeaweaklysupervised  frameworkthatcanutilizewordannotations,eitherintight  quadranglesorthemorelooseboundingboxes,forcharac-  terdetectortraining.Whenappliedinscenetextdetection,  wearethusabletotrainarobustcharacterdetectorbyex-  ploitingwordannotationsintherichlarge-scalerealscene  textdatasets,e.g.ICDAR15[19]andCOCO-text[39].The  characterdetectoractsasakeyroleinthepipelineofour  textdetectionengine.Itachievesthestate-of-the-artperfor-  manceonseveralchallengingscenetextdetectionbench-  marks.Wealsodemonstratethexibilityofourpipeline  byvariousscenarios,includingdeformedtextdetectionand  mathexpressionrecognition.  1.Introduction Understandingopticaltextshasalonghistorydating backtotheearlytwentiethcentury[34].Foralongtime,  theattemptsweremadefortextsofafewlanguagescap-  turedbyveryspecialdevices,e.g.scannedEnglishdocu-  ments.Withthegrowingpopularityofsmartphones,there  havebeenincreasingdemandsforreadingtextsofvarious  languagescapturedunderdifferentscenarios. Weareinterestedindevelopingacommontextextrac- tionengineforvariouslanguagesandscenarios.The  stepistolocalizetexts.Itisnoteasy.Firstly,languages  differinorganizationstructures.Foranexample,English  Equalcontribution.ThisworkisdonewhenHanHuisatIDL,Baidu Research.Figure1: Thevisualhierarchiesforvariouslanguagetextsunder differentscenarios.Differentlanguagesandscenariosmaydiffer  inhierarchy,buttheyareallformedbyabasicelement, character .textsincludevisualblankseparationbetweenwords,while  Chinesenot.Foranotherexample,regularhumanlanguage  textsareorganizedsequentially,whilemathexpressionsare  structural.Secondly,textsmaydifferinvisualshapesand  distortionsaccordingtotheindividualscenarios.Neverthe-  less,allopticaltextsshareonecommonpropertythatthey  areallformedbycharacters,asillustratedinFig.1.Itis  naturalandconvenientthatwebaseacommontextdetec-  tionframeworkoncharacterdetection. Whencharactersarelocalized,wecanthendetermine thestructureoftextsinabottom-upmanner.Theatomicity  anduniversalityofcharactersenablestructureanalysisfor  variouslanguagesandscenarios,e.g.,oriented/deformed  textlinesandstructuralmathexpressionrecognition(see  representativesamplesinFig.1). Thetrainingofcharacterdetectorsrequireavastoflo- cationannotatedcharacters.However,annotatingcharac-  terlocationsisveryinconvenientandexpensive,because  thecharactersaresmall,easilygluingwitheachotherand  blurry.Actually,mostexistinglargescalerealtextimage  datasetsarelabeledcoarselyinwordlevel,asillustratedin  Table1. Inthispaper,weproposeaweaklysupervisedlearning frameworktoaddresstheproblemoflackingrealcharac- 14940 Dataset#im #word Real/Synth.Anno.ICDAR13[20] 4621,944RealcharICDAR15[19] 1,500˘ 12KRealword SVT[41] 350725Realword COCO-Text[39] ˘ 63K˘ 174KRealword IIIT5k-word[30] N.A.3000Realword Char90K[15] N.A.9MSynth.charVGGSynthText [7]800K-Synth.charTable1: Populardatasetsandtheirproperties.Nearlyallmedian andlargescalerealdatasetsareannotatedinwordlevel.  terlevelannotations.Itutilizeswordannotationsassuper-  visionsourcetotrainthecharacterdetectors.,  twoalternativestepsareiteratedtograduallyboththe  charactercentermaskandthecharactermodel,asillustrated  inFig.2.Byapplyingthisframework,weareabletotrain  arobustcharactermodelbyexploitingrichsamplesinsev-  erallargescalechallengingdatasets,e.g.ICDAR15[19]  andCOCO-Text[39]. Thecharactermodelactsasakeymoduletoourtextde- tectionpipeline.Whenappliedtochallengingscenetexts,it  achievesthestate-of-the-artperformanceonseveralbench-  marks,i.e.ICDAR13[20],ICDAR15[19]andCOCO-  Text[39].Itisalsoprovedapplicableonvariousscenarios,  includingdeformedtextlineextractionandstructuralmath  expressionrecognition.  1.1.RelatedWorks Therehavebeennumerousapproachesfortextdetec- tion.Accordingtothebasicelementstheyrelyon,theap-  proachescanberoughlygroupedintovecategories:  Characterbased Asmentionedearlier,characterisanat- uralchoicetobuildcommondetectionengines.Nearlyall  existingcharacterbasedmethodsrelyonsyntheticdatasets  fortraining[37,9,40,42,17,51],becauseoflackingchar-  acterlevelannotatedrealdata.However,syntheticdatacan-  nothaveafullcoverageofcharactersfromvariousscenes,  limitingthemodel'sperformanceinrepresentingchalleng-  ingrealscenetexts.Actually,noneofthecurrenttopmeth-  odsforthepopularICDAR15benchmark[19]arebasedon  characterdetection.Recently,somesophisticatedsynthetic  technologies[7]havebeeninventedthatthesynthetictext  imageslookmoreﬁrealﬂ.Nevertheless,realtextimagesare  stillindispensableintrainingmorerobustcharactermodels,  aswewillshowinourexperiments. Ourpipelineisalsocharacterinduced,butbyincorpo- ratingaweaklysupervisedframework,weareabletoex-  ploitwordannotationsinseverallargescalerealdatasetsto  strengthenthecharactermodel.Usingthismodelasakeyto  ourpipeline,weachievethestate-of-the-artperformanceon  severalchallengingscenetextdetectionbenchmarks.The Update Mask Update Network Detection Network Figure2: Illustrationofourwordsupervisiontrainingapproach foracharactermodel.Twoalternativestepsareconducted:giving  thecurrentmodel,computearesponsemapwhichisthenused  togetherwithwordannotationstogetacharactercentermask(red  andgreenpoints);givingthecharactercentermask,supervisethe  trainingofcharactermodel.  pipelineisxibleforvariousscenariossuchasdeformed  textsandstructuralmathexpressions.  TextLinebased Textlinebasedmethodsdirectlyesti- matelinemodels.Thesemethodsarewidelyadoptedinthe  ofdocumentanalysis[29],wherearticlelayoutpro-  videsstrongpriors.Theyarehardtobeappliedfornon-  documentscenarios.  Wordbased Ameritofthesemethodsisthatthemodern objectdetectionframeworks,suchasfasterRCNN[12]and  SSD[24],canbeconvenientlyadjusted[16,7,25,50].Yet,  thesemethodsarelimitedtolanguageswhichhaveword  representationandvisualseparationbetweenthewords.  Componentbased Earlycomponentorwordfragment basedmethods[13,47,14,22,43,46,18,48]extractcan-  didatetextfragmentsbysomemanuallydesignedfeatures,  e.g.MSER[3]andSWT[5],andthendeterminewhether  thefragmentsarerealtextornot.Thesemethodsonceled  somepopularcompetitionsforwellfocusedtexts,e.g.IC-  DAR13[20].However,theperformanceofthesemethods  heavilydegradeswhenappliedtomorechallengingscenar-  iossuchasICDAR15[19]wheretextsarecapturedacciden-  tally.Inaddition,aslongassometextsaremissedbythe  manuallydesignedfeatures,theywouldneverberecalledin  thesubsequentsteps. Recently,somecomponentbasedmethods[49,44,8, 38,35]attempttolearntextcomponentsbyCNNfeature  learning.Thecomponentsareeitherrepresentativepix-  els[49,44,8]orsegmentboxes[38,35].Thesemethods  canlearnfromwordannotations.Inaddition,textcompo-  nentisalsoabasicvisualelement,whichmayalso  acommontextdetectionengine.Nevertheless,ourmethod  takesadvantagesoverthesemethodsinthefollowingas-  pects:charactersprovidestrongercues,e.g.,character 4941 !"#$#%&'$( )'&'%&*$( !"#$#%&'$( +$*,-./0( 1./'(2345#4*/( #/6(7'%48%#4*/( 9*$6( :'-#$#4*/( ;/-,&( ;5#0'3  !"#$#%&'$( +$*,-./0( <*/=3'>,'/4#?( 7'%*0/.4*/( @'A&( 1./'3( 9*$63( @'A&(B?*%C3( 7'%*0/.D'6(@'A&3( @'A&(:&$,%&,$'(E/#?F3.3(( Figure3: Ourpipeline.Therearetwomodules,characterdetector andtextstructureanalysis.Thepipelineisxibleforvarious  scenariosascribedtotheatomicityanduniversalityofcharacters.  scalesandcenterlocations,forthesubsequenttextstruc-  tureanalysismodule;second,characterisasemanticele-  ment,whilecomponentnot.Thusourmethodisapplicable  toproblemswheredirectcharacterrecognitionisneeded,  e.g.matchexpression;third,ourmethodcanutilizeloose  wordannotationsfortraining,e.g.boundingboxannota-  tionsintheCOCO-Textdataset[39].Thisisbecauseour  methodcancharactercenterlabelsduringtraining.  Fortheabovecomponentbasedmethods,theirnoisylabels  areedwhichmayharmtraining.  2.OurApproach  2.1.Pipeline ThepipelineofourapproachisillustratedinFig.3. Givenanimage,wedetectcharactersonit.This  moduleissharedbyvariouslanguagesandscenarios.Its  performanceiscrucialforthewholepipeline.Instead  ofusingsyntheticcharacterdataalonefortraining,we  strengthenitbyexploitingwordannotationsfromrealscene  textdatasets.Thedetailsofourbasiccharacterdetectorand  thewordsupervisionmethodarepresentedinSection2.2  andSection2.3,respectively. Thenthedetectedcharactersarefedtoatextstructure analysismodule,whichisapplicationdependent.Wehan-  dleseveraltypicalscenarios.Firstisthesequentialline,a  widelyusedtextstructure.Weproposeamethodto  extractallofthehorizontal,orientatedanddeformedtext  lines.Englishtextlinesareoptionallyseparatedintowords  forwordbasedtextrecognitionmethods.Mathexpression  recognitionisanotherscenario,wherecharactersarenon-  sequential.Werecognizeallthedetectedcharacters  andthenrecoverstructuresconnectingcharacters/symbols  [11].DetailsfortextstructureanalysisarepresentedinSec-  tion2.4.  2.2.BasicCharacterDetector Thefullyconvolutionneuralnetworkisadopted,which hasseengoodperformanceongeneralobjectdetection,e.g.,  SSD[24]andDenseBox[12].Nevertheless,tobeapplied  forcharacters,severalfactorsneedtobetakenintoaccount.  First,charactersmayvaryalotinsizeondifferentimages. 128deconv +128conv 128deconv +128conv loss input patch 3x224x224 conv1_1~conv2_2 conv3_3256x56x56conv4_3512x28x28conv5_3512x14x14features 128x56x56predictions 5kx56x56labels 5kx56x56128x28x28128x56x56Figure4: Ourbasicdetectionnetwork.Thenetworkinheritsfrom theVGG16networkmodel[36].  Somecharactersmaybeverysmall,e.g., 10 10pixelsin an1Mpixelimage.Second,textsmayappearinverydif-  ferentscenarios,suchascaptureddocuments,streetscenes,  advertisingpostersandetc,whichmakesthebackgrounds  distributeinalargespace. Tocopewiththecharactersizeproblem,weusefea- turemapswithhigherresolutiontogeneratecharacterre-  sponses.,itis 1=4oftheoriginalimage, otherthan 1=16or1=32usedingeneralobjectdetec- tion[31,24].Cuesfromdeeperstageswithcoarserres-  olutionsaremergedforbetterrepresentationpower.We  adoptthemethodinFPN[23]forsuchpurpose,which  usesan eltsumlayertomergefeaturesfromtwostageswith 2 resolutiondifference.Itrequireslessparametersthan othermethods,e.g.,[26,32,21],forproducingthesame  numberoffeaturemaps.SeeFig.4asanillustration.  ThenetworkinheritsfromtheVGG16networkmodel[36].  conv5 featuresare 2 up-sampledbydeconvolutionand mergedwith conv4 featuresbyan eltsumlayer.The elt-sumedconv5 -conv4 featuresaremergedwith conv3 featuresinthesameway.Theproducedfeaturemapsareusedfor  bothtext/non-textonandboundingboxregres-  sion.5k =(1+4) k scoremapsaregenerated,with 1fortext/non-text 4forboundingboxregression, andk indicatingthenumberofanchors.Weuse k =3 anchors,representingcharacterswithdiagonallengthsof  24pixels, 16pixelsand 12pixels(onthe 224 224inputpatch),respectively.Thecharacterswithdiagonallengths  of0: 7˘ 1: 4againsttheanchor'sareregardedpositive. Toeasethebackgroundvariationproblem,weadopta two-levelhardnegativeexampleminingapproachfortrain-  ing.Firstisonlinehardnegativemining[24].Allposi-  tivesareusedforlosscomputation.Fornegatives,onlytop  scoredonesareusedinlosscomputationthattheratiobe-  tweennegativesandpositivesisatmost 3:1 .Secondis hardpatchmining.Duringtraining,wetestalltrainingim-  agesevery 10k iterationstofalsepositives(usingthe currentcharactermodel).Thesefalsepositiveswillbemore  likelysampledinthesuccessivemini-batchsamplingpro- 4942 cedure. Training 32224  224patchesarerandomlycropped fromtrainingimagestoformamini-batch. 50% ofthe patchesincludecharacters.Thesepositivepatchesare  croppedfromtrainingimagesaccordingtoarandomlyse-  lectedcharacterandanchor,withsomedegreeoftrans-  lation/scaleperturbation.Theother 50% arerandomly croppedbutwithnotexts.After 10k iterations,westart toapplythehardpatchminingprocedure.Fornegative  patches,halftrainingpatcheswillbehardoneswhich  shouldincludethecurrentdetectedfalsepositives.  Inference Weconductmulti-scaletestforanimage.The usedscalesare 2f 0 ;  1 ;  2 ;  3 ;  4 ;  5 g ,respectively.Since onlydown-samplingscalesareinvolved,thecomputation  overheadisafforded,atabout 1: 5times,comparedto single-scaletest.NMSwithIoUthresholdof 0: 5iscon- ductedtoproducethecharacters.Itshouldbenoted  thatmulti-scaletestingisindispensableforourbasicde-  tector,sinceweuseanchorswithonly 3scales.Explor- ingmoreefbasicdetectorwithouttheneedformulti-  scaletestingwillbeourfuturework.  2.3.LearningFromWordAnnotations  Overview AsillustratedinTable1,mostrealtextimage datasetsareannotatedinwordlevel,i.e.ICDAR15and  COCO-Text.Eachwordisattachedwithaquadrangle(e.g.  ICDAR15)oraboundingbox(e.g.COCO-Text)which  tightlysurroundsit,aswellasawordcategory.Inthispa-  per,wesupposeatleasttheboundingboxofeachwordis  available.Iffurtheraquadrangleorthewordcategoryis  given,weusethemtostrengthenourwordsupervisingpro-  cedure.Ourapproachisinspiredby[4]whichsuccessfully learnsobjectsegmentsfromboundingboxannotations.It  isillustratedasFig.2.Twoalternativestepsareconducted:  givenacharactermodel,automaticallygeneratethecharac-  termaskaccordingtoawordannotation;givenacharacter  mask,updatethecharacternetwork.Thesetwostepsare  alternativeineachnetworkiteration.Duringthetraining,  thecharactermasksandthenetworkarebothgraduallyim-  proved. Itisworthnotingthattheaboveprocedureisonlyin- volvedinnetworktraining.Theinferenceisthesameasin  Section2.2.  CharacterMaskGeneration Duringforwardandback- wardofeachmini-batch,thestepistogeneratecharac-  termasksusingthecurrentcharactermodelandwordanno-  tations,asillustratedinFig.5(bottom).First,wemakefor-  wardusingthecurrentcharactermodelandgetasetofcan-  didatecharactersinsidetheannotatedwordboundingbox. Weselectrealcharactersfromthesecandidatecharactersby  maximizingscore, s = w  s 1 +(1  w )  s 2 = w  area( B chars) area( B anno) +(1  w )  (1   2  1 ) ; (1)whereB charsrepresentstheboundingboxofselectedchar- acters;B annorepresentstheannotatedwordboundingbox; area(  ) denotestheareaoperator;  1 and 2 arethelargest andsecondlargesteigenvaluesofacovariancematrix C ,computedbycentercoordinatesofselectedcharacters; w isaweightbalancingthetwoscoreterms.Wethelearn-  ingisinsensitivetothechoiceof w ,anditissetas 0: 5bydefault.ThetermofEq.(1), s 1 ,favorslargercoverage ofselectedcharacterstotheannotatedwordboundingbox,  whilethesecondone, s 2 ,prefersthatallcharacterslocate onastraightline. Weuseasimilarapproachasin[45]toapproximately maximizeEq.(1).Firstly,amaximumspanningtree[1],  M ,isconstructedfromthecharactergraph G ,whichisbuilt bythe k -nnofallcandidatecharacterswithpairweights bytheirspatialdistancesandthecurrenttext/non-  textscores, w ij = exp (  d ( i;j ) d )  ( t i + t j ) ; (2)whered istheaverageofalldistancesbetween k -nnnodes; t i denotesthecurrenttext/non-textscoreofcandidate i .Itis obviousthateliminatinganedgein M equalstopartitioning thecharactersintotwogroups.Foreachpartitioning,we  choosethegroupwithlargerscoreaccordingto(1),andrun  thepartitioningproceduregreedilyandrecursivelyuntilthe  score(1)doesnotrise. Whenatightquadrangleorcharacternumberisgiven, wecanfurtherimprovegeneratedcharactermask:forthe  former,replacingcomputationof s 1 inEq.(1)byareara- tioofquadrangles;forthelatter,addingatermtoEq.(1)  thatthemaskprefersequalcharacternumbercomparedto  groundtruth.  CharacterNetworkUpdating Thegeneratedcharacter maskcanbeusedasgroundtruthtosupervisenetwork  training.Wealoss e L suchthatmorereliablemasks contributemore,as, e L = s L ; (3)whereL representsacombinationoftheloss andlocalizationlosscommonlyusedinmodernobjectde-  tectionframeworks[31,24]; s isthescorecomputedbyEq. (1).Fig.5showsthegraduallyupdatedcharactermasksdur- ingtraining.Duringtraining,thecharactermodelisgradu-  allyimproved. 4943 word annotation & initial chars initialepoch #1epoch #2epoch #10epoch #40 chars at  epoch #10, #40 Figure5: UpdatedcharacterresponsesandthecorrespondingcharactermasksduringwordsupervisedtrainingonICDAR15datasets.The initialmodelinthesecondcolumnistrainedby 5 k warmupiterationsonsyntheticdataalone.The 3 ˘ 6 th columnsareresponsesduring thewordsupervisedtraining,wheretheepochnumbermeansforICDAR15datasets.Forillustration,weuseboundingboxannotations  ratherthantheoriginalquadranglesintraining.Boththeresponsesandcharactercandidatesarecoloredbytheirscores(indexedby  colormapinMatlab). 2.4.TextStructureAnalysis GivencharactersextractedbythemethodsinSection2.2 and2.3,weconducttextstructureanalysisforvarioussce-  narios,e.g.,textlines,words,textblocks,mathexpressions,  andetc.Fig.3illustratesourtextstructureanalysismethods  forthesetypicaltextstructures.Fortextlinebasedappli-  cations,weproposeamethodwhichcanhandlearbitrarily  deformedlines.Thestepistogroupcharacters.Thena  linemodelisestimatedtodescribetheline.Withthemodel,  werectifythetextline,whichisusuallyrequiredbymod-  ernsequentialtextrecognitionsystems.Optionally,wesep-  aratelinesintowords.Thisisnotnecessary,butenables  wordbasedtextrecognitionmethods. Characterscanalsobeemployedfortextblockex- traction,e.g.,documentlayoutanalysis[28],andnon-  sequentialtextrecognition,e.g.,mathexpressionrecogni-  tion[10]. Inthefollowing,wedescribetechniquesusedfor extractingtextlines,whicharefrequentlyusedinourex-  periments.Moredetailscanbefoundinappendix.  CharacterGrouping Weadaptthemethodin[37]to groupcharactersintotextlinesorblocks.Givencharacters  withscorelargerthanathreshold,[37]buildsa k -nngraphwitheachnodedenotingacharactercandidate.Unary  andpairwisecostsareonthegraphtoachieveclus-  tering.Theunarycostsmodelrelationsbetweencharacters  andthetextcategory,e.g.characterscores.Thepairwise  costsmodelrelationsbetweentwocharacters,e.g.spatial  andscaledistances.Agreedymin-costwalgorithmis  thenconductedtoobtainallcharactergroups(see[37]for  details).Themethodin[37]isdesignedforhorizontaltextlines only.Tobeappliedinorientedanddeformedtextlines,  weintroduceahigher-ordercostwhichmodelsrelationsbe-  tweenthreecharacters.Toreservetheefencyofpairwise  graph,weusecharacterpairsinsteadofcharactersasgraph  nodes.Thecharacterpairsarespatiallyclosecharacters  withhighscoresandsmallspatial/scaledistances.Thenthe  unaryandpairwisecostsintheoldgraphcanbemodeledas  unarycostsinthenewgraph,whilethehigherordercosts,  e.g.angledistance,canbemodeledaspairwisecostsinthe  newgraph.Thesameasin[37],Wethenconductagreedy  min-costwalgorithmonthenewgraphtoachievechar-  actergrouping.Itcanhandleorientedanddeformedtext  lines,ascribedtotheintroductionofhigher-ordercosts.  LineModelEstimationand Foreach charactergroup,wethreetextlinemodelswithincreas-  ingcomplexity.Firstis 0-ordermodel .Textlinesareei- therhorizontalorvertical.Secondis 1-ordermodel .Text linescanbearbitrarilyorientated.Lastisa piecewiselinear model,wherearestrictedpolygonisadoptedtorepresenta textline. Amodelselectionapproachisconductedtochoosea modelwithbestbalancebetweenaccuracyandmodel  complexity.Giventheestimatedmodel,werectifythetext  lineusingthinplatespline(TPS)[2]method,wherethever-  texesofthetextlinemodelareusedascontrolpoints.  Wordpartition Sometextrecognitionsystemscanpro- cessonlywordinputs.Toenableusageofsuchsystems,  weoptionallyseparatetextlinesintowords.AnLSTM[6]  basedwordblankdetectionmethodisappliedontherecti-  textline.Wordsareseparatedaccordingly. 4944 3.Experiments Inthissection,wedoablationstudiesonsynthetic datawherecharacterlevelannotationsareprovided.Both  ourbasicdetectorandthewordsupervisionapproachare  evaluated.Thenweapplyourcharacterinducedtextdetec-  tionpipelineonscenetextbenchmarks.Finally,weshow  itsapplicationstovariousscenarios.  3.1.DatasetsandEvaluation Fourdatasetsareusedintheexperiments: VGGSynthText-part .TheVGGSynthText datasets[7]consistof800,000images,generatedbya  syntheticengineproposedin[7].Theimageshavede-  tailedcharacter-levelannotations.Forexperimentef  ciency,werandomlyselect50,000imagesfortraining  and5,000imagesforvalidation.Thissubsetisreferred  toas VGGSynthText-part .ICDAR13 .TheICDAR13datasets[20]arefrom theICDAR2013RobustReadingCompetition,with  229naturalimagesfortrainingand233fortesting.  Thetextsareannotatedwithcharacter-levelbound-  ingboxes,andtheyaremostlyhorizontalandwellfo-  cused.ICDAR15 .TheICDAR15datasets[20]arefromthe ICDAR2015RobustReadingCompetition,with 1000naturalimagesfortrainingand 500fortesting.The imagesareacquiredusingGoogleGlassandthetexts  accidentallyappearinthescenewithoutuser'sprior  intention.Allthetextsareannotatedwithword-level  quadrangles.COCO-Text .TheCOCO-Text[39]isalargescale datasetwith43,686imagesfortrainingand20,000  fortesting.TheoriginalimagesarefromMicrosoft  COCOdataset. TheVGGSynthText-partismainlyusedforablation experiments.Bothcharacter-levelandword-levelevalua-  tionsareconductedbyusingthePASCALVOCstylecri-  terion(  0: 5Intersection-over-Unionforapositivedetec- tion).ForbenchmarkexperimentsonICDAR13,ICDAR15  andCOCO-Text,theevaluationprotocolsprovidedbythe  datasetsthemselvesareadopted.,forICDAR13  andICDAR15,weusetheonlineevaluationsystempro-  videdwiththedatasets.ForCOCO-Text,theprotocolpro-  videdbythedatasetisusedforevaluation.  3.2.ImplementationDetails TheVGG16modelpretrainedontheILSVRCCLS- LOCdataset[33]isadoptedforallexperiments. Givendifferentdatasets,wetrainthreecharactermodels. Theistrainedbysyntheticcharacterdataalone,i.e.50k 0.40.50.60.70.80.91 precision0.20.40.60.81recallFigure6: Characterdetectionperformanceofourbasicdetec- tionnetwork,thefasterRCNNandSSDmethodsontheVGG  SynthText-partdatasets.Fourvariantsofourmethodarepre-  sented.Theterminbracketsindicatestheusedsupervision  source.Thesecondtermindicatestheusedhardnegativemining  strategy,withﬁoneﬂrepresentingone-levelhardnegativemining  andﬁtwoﬂrepresentingtwo-levelhardnegativemining.  trainingimagesfromVGGSynthText-partdatasets.Second  istrainedon1kICDAR15trainingimagesplus50kVGG  SynthText-part. 50% aresampledfromICDAR15andthe othersaresampledfromVGGSynthText-part.Thethird  istrainedonCOCOandVGGSynthText-part,withmini-  batchalsosampledhalf-halffromthetwodatasets.These  threemodelsaredubbedasﬁVGG16-synthﬂ,ﬁVGG16-  synth-icdarﬂandﬁVGG16-synth-cocoﬂ,respectively. WeuseSGDwithamini-batchsizeof 64on4GPUs( 16perGPU).Atotalof50kiterationsareperformedforall  models.FortheﬁVGG16-synthﬂmodel,30kareatalearn-  ingrateof 0: 001,andtheother20kat 0: 0001.Forother models,5kiterationswithVGGSynthText-partcharacter  supervisionalonearerunforwarmingup.Thelearn-  ingrateis 0: 001atthisstage.Then25kand20kiterations areconductedusingbothcharacterandwordsupervisionat  learningratesof 0: 001and0: 0001,respectively.Theweight decayissetas 0: 0005andthemomentumas 0: 9.ForexperimentsonICDAR13,ICDAR15andCOCO- text,thetextlinegenerationandwordpartitionapproaches  introducedinSection2.4areappliedtoproducewordlocal-  izations,whicharerequiredforevaluationofthesebench-  markdatasets.Forfaircomparison,wetunehyperparame-  tersofthelinegenerationalgorithmonasmallfractionof  trainingimages,i.e.50,forallcharactermodels.  3.3.ExperimentsonSyntheticData TheVGGSynthText-partdatasetsareused. 4945 EvaluationoftheBasicCharacterDetector Wet comparetheproposedbasicdetectionnetworkpresentedin  Section2.2withthestate-of-the-artalgorithmsinthe  ofgeneralobjectdetection,e.g.fasterRCNN[31]andSSD  [24].ForfasterRCNNandSSD,wedirectlyusethecodes  providedbytheauthors. Fig.6illustratestheprecision-recallsofourbasicnet- work,fasterRCNNandSSDoncharacterdetection,re-  spectively.Themaindifferencebetweenourcharacternet-  workwiththestate-of-the-artgeneralobjectdetectorsis  thatthefeaturemapsusedtoproducecharacterresponses  isthanthatofgeneralobjectdetectors( 1=4vs.1=16),whilemaintainingsufrepresentationpowerbymerg-  ingcuesfromdeeperstages.Thelargegapbetweenour  basicnetworkandgeneralobjectdetectordemonstratesthat  reservingresolutioniscrucialforcharacterdetection.The  two-levelhardnegativeminingduringtrainingisalsoaplus  thatthesecondlevelhardpatchminingcanbringamoder-  ategain,asshowninFig.6.  EvaluationofWordSupervisionApproach Threemod- elsaretrained.Theistrainedbyrandomlyselected  2,000imagesusingcharactersupervision.Thesecondis  trainedusingcharactersupervisionofallthe50kimages.  Thethirdistrainedusing2,000charactersupervisionim-  agesand48,000wordsupervisionimages.Thetrainingap-  proachesaresimilartothosein3.2. FromFig.6,itcanbeseenthattheword-supervised modelperformssuperiorto2kcharacterstrainedmodeland  theperformancedegradationagainstthefull50kcharacters  trainedmodelisdemonstratingtheeffective-  nessofourwordsupervisionapproachinexploitingweak  wordannotationsforcharactermodeltraining.  3.4.ExperimentsonSceneTextBenchmarks Weapplyourtextdetectionapproachonthreereal challengingscenetextbenchmarks:ICDAR13[20],IC-  DAR15[19]andCOCO-Text[39].Thesebenchmarksare  allbasedonword-levelevaluation.Hence,thetextline  generationandwordpartitionmethodsareinvolved.Inthe  linemodelestimationstep,weonlyuse 0-order and1-order modelsasnearlyalltextlineshaveuptoorientationdefor-  mation.Table2,3and4showtheperformancesofdiffer- entmethodsontheICDAR13,ICDAR15andCOCO-Text  datasets.Ourapproachoutperformspreviousstate-of-the-  artmethodsbyalargemargin. OnICDAR13,weachieve90.34%F-measure,whichis 2.65%higherthanthesecondbestone,i.e.CTPN[38]. OnthemorechallengingICDAR15datasets,imagesare morelikelytosufferfromblurry,perspectivedistortion,ex-  tremeillumination,andetc.Ourbestmodelachievesaf-  measureof78.16%,withalargemarginovertheprevious MethodRecallPrecisionF-measureMCLAB-FCN[49] 79.6588.4083.80Yaoetal.[44] 80.2288.8884.33Guptaetal.[7] 75.592.083.0Zhuetal.[51] 81.6493.4087.13CTPN[38] 82.9892.9887.69our( VGG16-synth )82.4191.9586.92our( VGG16-synth-icdar )87.5393.3490.34Table2: PerformancesofdifferentmethodsonICDAR13using theDetEvalcriterion(%). MethodRecallPrecisionF-measureMCLAB-FCN[49] 43.0970.8153.58CTPN[38] 51.5674.2260.85Yaoetal.[44] 58.6972.4064.77SCUT-DMPNet[25] 68.2273.2370.64RRPN-2[27] 72.6568.5370.53our( VGG16-synth )64.3774.7969.18our( VGG16-synth-icdar )77.0379.3378.16Table3: PerformancesofdifferentmethodsonICDAR15(%). bestmethod[25](78.16%vs.70.64%).Comparingourap-  proachusingdifferentcharactermodels,VGG-synth-icdar  performsmuchbetterthantheVGG-synthmodel(78.16%  vs.69.18%).VGG-synth-icdaronlyadds 1ktrainingimage comparedtotheVGG-synthmodel( 50ktrainingimages). Thisindicatesthatthegainisfrommore real data,other thanmoredata. OnCOCO,ourbestmodelachieves30.9%,45.2%and 36.8%inrecall,precisionandF-measure,respectively.It  takesoverYao'smethodby3.5%intotalF-measure.VGG-  synth-cocoalsoperformsmuchbetterthantheVGG-synth  model,demonstratingtheintroductionofrealtextimages  helpsalotintrainingbettercharactermodels. Fig.7illustratessomedetectionsamplesfromtheIC- DAR13,ICDAR15andCOCO-Texttestimages.Byex-  ploitingrichwordannotationsfromrealtextimagedatasets,  ourmodelbecomesmorerobustandcanthussuccessfully  detectvariouschallengingtexts,e.g.blurry,perspectivedis-  tortion,handwritten/artfonts,extremeilluminationandetc.,  whicharehardtobesynthesizedusingcomputers.  ComputationalTime Fora 500 500image,thecharac- ternetworktakesabout500msusinganNvidiaTeslaK40  GPU.Thetextlinegenerationandwordpartitionproce-  durestogethertakeabout20msusinga2GHzCPU.  3.5.AppliedtoVariousScenarios Weapplyourpipelinetovariouschallengingscenarios, includingadvertisingimages,deformeddocumenttextsand  mathexpressions.Acharactermodelistrainedbyapri-  vatelycollectedtextimagedatasetsaboutthesescenarios, 4946 Figure7: SamplequalitativeresultsusingtheVGG16-synthmodel(top)andthemodelstrainedbywordsupervision(bottom)onthe benchmarkscenetextdatasets.Yellowandredrectanglesillustratethecorrectlyandwronglydetectedtextlines,respectively. Figure8: Appliedtovariousscenarios.Thetoprowshowsdetectedcharacters,withcolorsindicatingcharacterscores(indexedby colormapinMatlab).Thebottomrowshowsresultsofstructureanalysis. MethodRecallPrecisionF-measureA[39] 23.383.7836.48B[39] 10.789.7319.14C[39] 4.718.567.47Yaoetal.[44] 27.143.233.3our( VGG16-synth )26.842.632.5our( VGG16-synth-coco )30.945.236.8Table4: PerformanceofdifferentmethodsontheCOCO-Text (%).Noticethattheannotationsareobtainedundertheparticipa-  tionofmethodA,BandC.Itisthusnotfairtobecomparedwith  thesemethods.Yet,theyarelistedhereforreference.  consistingof 2k character-levelannotatedimagesand 30k line-levelannotatedimages(onlyimageswithstraighttext  linesareinvolved).Thetrainingapproachissimilarasin  Section3.2.Textlinesaregeneratedbytheapproachin  Section2.4.Fig.8illustratesthecharacterdetection(top  row)andtextlinegeneration(bottomrow)resultsonsome  representativeimages.Ourapproachcanhandletextlines  withvariouslanguagesandextremedeformations.Itisalso  worthnotingthatChinesehasavastnumberofcharacter classes,wheresomeofthemmaynotbeseenbythe 2k character-levelannotatedimages.However,weempirically  foundthattheinitialmodelcanstillhelprecoveringcenter  masksofmanyunseencharactersgivenonlytextlinean-  notations.Onepossiblereasonisthattheunseenchracters  maysharesimilarsubstructuresorstrokeswiththecharac-  tersseenbytheinitialmodel. Wealsoshowapplicationformathexpressionrecogni- tion(seethelastcolumnofFig.8).Mathexpressionsare  non-sequential,andhencesequentialtextrecognitiontech-  niqueisnotapplicable.Givendetectedcharacters,wecan  recognizeeachofthem,producingasetofmathsymbols.  4.Conclusion Characterbasedtextdetectionmethodsarexibletobe appliedinvariousscenarios.Wepresentaweaklysuper-  visedapproachtoenabletheuseofrealword-levelanno-  tatedtextimagesfortraining.Weshowtherepresentation  powerofcharactermodelscanbestrength-  ened.Extensiveexperimentsdemonstratetheeffectiveness  ofourweaklysupervisedapproachandthexibilityofour  textdetectionpipeline. 4947 References [1]A.V.Aho,J.E.Hopcroft,andJ.D.Ullman. DataStructures andAlgorithms .Addison-Wesley,1983. [2]F.L.Bookstein.Principalwarps:Thin-platesplinesandthe decompositionofdeformations. IEEETPAMI ,11(6):567Œ 585,1989. [3]H.Chen,S.S.Tsai,G.Schroth,D.M.Chen,R.Grzeszczuk, andB.Girod.Robusttextdetectioninnaturalimageswith  edge-enhancedmaximallystableextremalregions.In ICIP,pages2609Œ2612,2011. [4]J.Dai,K.He,andJ.Sun.Boxsup:Exploitingbounding boxestosuperviseconvolutionalnetworksforsemanticseg-  mentation.In ProceedingsoftheIEEEInternationalCon- ferenceonComputerVision ,pages1635Œ1643,2015. [5]B.Epshtein,E.Ofek,andY.Wexler.Detectingtextinnatural sceneswithstrokewidthtransform.In CVPR,pages2963Œ 2970,2010. [6]F.A.Gers,J.Schmidhuber,andF.Cummins.Learningto forget:Continualpredictionwithlstm. Neuralcomputation ,12(10):2451Œ2471,2000. [7]A.Gupta,A.Vedaldi,andA.Zisserman.Syntheticdatafor textlocalisationinnaturalimages.In CVPR,2016. [8]T.He,W.Huang,Y.Qiao,andJ.Yao.Accuratetextlo- calizationinnaturalimagewithcascadedconvolutionaltext  network. CoRR,abs/1603.09423 ,2016. [9]T.He,W.Huang,Y.Qiao,andJ.Yao.Text-attentionalcon- volutionalneuralnetworkforscenetextdetection. IEEETIP ,25(6):2529Œ2541,2016. [10]W.He,Y.Luo,F.Yin,H.Hu,J.Han,E.Ding,andC.-L. Liu.Context-awaremathematicalexpressionrecognition:  Anend-to-endframeworkandabenchmark.In ICPR,2016. [11]W.He,Y.Luo,F.Yin,H.Hu,J.Han,E.Ding,andC.L. Liu.Context-awaremathematicalexpressionrecognition:  Anend-to-endframeworkandabenchmark.In Pattern Recognition(ICPR),201623rdInternationalConferenceon ,2017.[12]L.Huang,Y.Yang,Y.Deng,andY.Yu.DenseBox:Unify- inglandmarklocalizationwithendtoendobjectdetection.  CoRR,abs/1509.04874,2015. [13]W.Huang,Z.Lin,J.Yang,andJ.Wang.Textlocalization innaturalimagesusingstrokefeaturetransformandtextco-  variancedescriptors.In ICCV,pages1241Œ1248,2013. [14]W.Huang,Y.Qiao,andX.Tang.Robustscenetextdetec- tionwithconvolutionneuralnetworkinducedmsertrees.In  ECCV,pages497Œ511,2014. [15]M.Jaderberg,K.Simonyan,A.Vedaldi,andA.Zisserman. Syntheticdataandneuralnetworksfornaturalscene  textrecognition. ArXive-prints ,2014. [16]M.Jaderberg,K.Simonyan,A.Vedaldi,andA.Zisserman. Readingtextinthewildwithconvolutionalneuralnetworks.  IJCV,116(1):1Œ20,2016. [17]M.Jaderberg,A.Vedaldi,andA.Zisserman.Deepfeatures fortextspotting.In ECCV,pages512Œ528,2014. [18]L.Kang,Y.Li,andD.Doermann.Orientationrobusttext linedetectioninnaturalimages.In CVPR,pages4034Œ4041, 2014.[19]D.Karatzas,L.Gomez-Bigorda,A.Nicolaou,S.Ghosh, A.Bagdanov,M.Iwamura,J.Matas,L.Neumann,V.R.  Chandrasekhar,S.Lu,etal.Icdar2015competitiononro-  bustreading.In ICDAR ,pages1156Œ1160.IEEE,2015. [20]D.Karatzas,F.Shafait,S.Uchida,M.Iwamura,L.G.iBig- orda,S.R.Mestre,J.Mas,D.F.Mota,J.A.Almazan,and  L.P.delasHeras.Icdar2013robustreadingcompetition.In  ICDAR ,pages1484Œ1493.IEEE,2013. [21]T.Kong,A.Yao,Y.Chen,andF.Sun.Hypernet:towardsac- curateregionproposalgenerationandjointobjectdetection.  InCVPR,pages845Œ853,2016. [22]Y.Li,W.Jia,C.Shen,andA.vandenHengel.Characterness: Anindicatoroftextinthewild. IEEETIP ,23(4):1666Œ1677, 2014.[23]T.-Y.Lin,P.Doll ´ar,R.Girshick,K.He,B.Hariharan,and S.Belongie.Featurepyramidnetworksforobjectdetection.  InCVPR,2017. [24]W.Liu,D.Anguelov,D.Erhan,C.Szegedy,S.E.Reed, C.Fu,andA.C.Berg.SSD:singleshotmultiboxdetector.  InECCV,pages21Œ37,2016. [25]Y.LiuandL.Jin.Deepmatchingpriornetwork:To- wardtightermulti-orientedtextdetection. arXivpreprint arXiv:1703.01425,2017. [26]J.Long,E.Shelhamer,andT.Darrell.Fullyconvolutional networksforsemanticsegmentation.In CVPR,pages3431Œ 3440,2015. [27]J.Ma,W.Shao,H.Ye,L.Wang,H.Wang,Y.Zheng,and X.Xue.Arbitrary-orientedscenetextdetectionviarotation  proposals.In CoRR,abs/1603.09423 ,2017. [28]S.Mao,A.Rosenfeld,andT.Kanungo.Documentstructure analysisalgorithms:aliteraturesurvey.In ElectronicImag- ing2003 ,pages197Œ207.InternationalSocietyforOptics andPhotonics,2003. [29]G.Meng,Z.Huang,Y.Song,S.Xiang,andC.Pan.Extrac- tionofvirtualbaselinesfromdistorteddocumentimagesus-  ingcurvilinearprojection.In ICCV,pages3925Œ3933,2015. [30]A.Mishra,K.Alahari,andC.Jawahar.Scenetextrecogni- tionusinghigherorderlanguagepriors.In BMVC.BMVA, 2012.[31]S.Ren,K.He,R.B.Girshick,andJ.Sun.FasterR-CNN: towardsreal-timeobjectdetectionwithregionproposalnet-  works.In NIPS,pages91Œ99,2015. [32]O.Ronneberger,P.Fischer,andT.Brox.U-net:Convolu- tionalnetworksforbiomedicalimagesegmentation.In MIC-CAI,pages234Œ241.Springer,2015. [33]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh, S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.S.Bernstein,  A.C.Berg,andF.Li.Imagenetlargescalevisualrecognition  challenge.IJCV,115(3):211Œ252,2015. [34]H.F.Schantz. HistoryofOCR,opticalcharacterrecogni- tion.RecognitionTechnologiesUsersAssociation,1982. [35]B.Shi,X.Bai,andS.Belongie.Detectingorientedtext innaturalimagesbylinkingsegments. arXivpreprint arXiv:1703.06520,2017. [36]K.SimonyanandA.Zisserman.Verydeepconvolu- tionalnetworksforlarge-scaleimagerecognition. CoRR,abs/1409.1556,2014. 4948 [37]S.Tian,Y.Pan,C.Huang,S.Lu,K.Yu,andC.LimTan. Textw:Atextdetectionsysteminnaturalscene  images.In ICCV,pages4651Œ4659,2015. [38]Z.Tian,W.Huang,T.He,P.He,andY.Qiao.Detectingtext innaturalimagewithconnectionisttextproposalnetwork.In  ECCV,pages56Œ72,2016. [39]A.Veit,T.Matera,L.Neumann,J.Matas,andS.Be- longie.Coco-text:Datasetandbenchmarkfortextdetec-  tionandrecognitioninnaturalimages.In arXivpreprint arXiv:1601.07140,2016. [40]K.Wang,B.Babenko,andS.Belongie.End-to-Endscene textrecognition.In ICCV,pages1457Œ1464,2011. [41]K.WangandS.Belongie.Wordspottinginthewild.In ECCV,pages591Œ604,2010. [42]T.Wang,D.J.Wu,A.Coates,andA.Y.Ng.End-to-Endtext recognitionwithconvolutionalneuralnetworks.In ICPR,pages3304Œ3308,2012. [43]C.Yao,X.Bai,W.Liu,Y.Ma,andZ.Tu.Detectingtexts ofarbitraryorientationsinnaturalimages.In CVPR,pages 1083Œ1090,2012. [44]C.Yao,X.Bai,N.Sang,X.Zhou,S.Zhou,andZ.Cao. Scenetextdetectionviaholistic,multi-channelprediction.  CoRR,abs/1606.09002 ,2016. [45]F.YinandC.Liu.Handwrittenchinesetextlinesegmen- tationbyclusteringwithdistancemetriclearning. Pattern Recognition ,42(12):3146Œ3157,2009. [46]X.-C.Yin,W.-Y.Pei,J.Zhang,andH.-W.Hao.Multi- orientationscenetextdetectionwithadaptiveclustering.  IEEETPAMI ,37(9):1930Œ1937,2015. [47]X.-C.Yin,X.Yin,K.Huang,andH.-W.Hao.Robusttext detectioninnaturalsceneimages. IEEETPAMI ,36(5):970Œ 983,2014. [48]Z.Zhang,W.Shen,C.Yao,andX.Bai.Symmetry-basedtext linedetectioninnaturalscenes.In CVPR,pages2558Œ2567, 2015.[49]Z.Zhang,C.Zhang,W.Shen,C.Yao,W.Liu,andX.Bai. Multi-orientedtextdetectionwithfullyconvolutionalnet-  works.In CVPR,2016. [50]X.Zhou,C.Yao,H.Wen,Y.Wang,S.Zhou,W.He,and J.Liang.East:Anefandaccuratescenetextdetector.  arXivpreprintarXiv:1704.03155 ,2017. [51]S.ZhuandR.Zanibbi.Atextdetectionsystemfornatu- ralsceneswithconvolutionalfeaturelearningandcascaded  In CVPR,pages625Œ632,2016. 4949  
DeepVoting:ARobustandExplainableDeepNetwork forSemanticPartDetectionunderPartialOcclusion ZhishuaiZhang 1  CihangXie 1  JianyuWang 2  LingxiXie 1(  ) AlanL.Yuille 1 JohnsHopkinsUniversity 1 BaiduResearchUSA 2 f zhshuai.zhang,cihangxie306,wjyouch,198808xc,alan.l.yuille g @gmail.com Abstract Inthispaper,westudythetaskofdetectingsemantic partsofanobject,e.g.,awheelofacar,underpartialocclu- sion.Weproposethatallmodelsshouldbetrainedwithout seeingocclusionswhilebeingabletotransferthelearned knowledgetodealwithocclusions.Thissettingalleviates thedifincollectinganexponentiallylargedatasetto coverocclusionpatternsandismoreessential.Inthissce- nario,theproposal-baseddeepnetworks,likeRCNN-series, oftenproduceunsatisfactoryresults,becauseboththepro- posalextractionandstagesmaybeconfused bytheirrelevantoccluders.Toaddressthis,[ 25 ]proposeda votingmechanismthatcombinesmultiplelocalvisualcues todetectsemanticparts.Thesemanticpartscanstillbede- tectedeventhoughsomevisualcuesaremissingduetooc- clusions.However,thismethodismanually-designed,thus ishardtobeoptimizedinanend-to-endmanner. Inthispaper,wepresentDeepVoting,whichincorporates therobustnessshownby[ 25 ]intoadeepnetwork,sothat thewholepipelinecanbejointlyoptimized.,it addstwolayersaftertheintermediatefeaturesofadeep network,e.g.,thepool-4layerofVGGNet.Thestlayer extractstheevidenceoflocalvisualcues,andthesecond layerperformsavotingmechanismbyutilizingthespatial relationshipbetweenvisualcuesandsemanticparts.We alsoproposeanimprovedversionDeepVoting+bylearning visualcuesfromcontextoutsideobjects.Inexperiments, DeepVotingachievesbetterperformancethan severalbaselinemethods,includingFaster-RCNN,forse- manticpartdetectionunderocclusion.Inaddition,Deep- Votingenjoysexplainabilityasthedetectionresultscanbe diagnosedvialookingupthevotingcues. 1.Introduction Deepnetworkshavebeensuccessfullyappliedtoawide rangeofvisiontasks,inparticularobjectdetection[ 7 , 20 ,  Thethreeauthorscontributedequallytothiswork. 19 , 13 , 29 ].Recently,objectdetectionisdominatedbya familyofproposal-basedapproaches[ 7 , 20 ],which generatesasetofobjectproposalsforanimage,followed byatopredictobjects'scoreforeachproposal. However,semanticpartdetection,despiteitsimportance, hasbeenmuchlessstudied.A semanticpart isafractionof anobjectwhichhassemanticmeaningandcanbeverbally described,suchasa wheel ofa car ora chimney ofa train . Detectingsemanticpartsisahumanability,whichenables ustorecognizeorparseanobjectatascale. Intherealworld,semanticpartsofanobjectarefre- quentlyoccluded,whichmakesdetectionmuchharder.In thispaper,weinvestigatesemanticpartdetectionespecially whenthesesemanticpartsarepartiallyorfullyoccluded. Weusethesamedatasetsasin[ 25 ], i.e. ,theVehicleSeman- ticPartdatasetandtheVehicleOcclusiondataset.Sometyp- icalsemanticpartexamplesareshowninFigure 1 .Note that,theVehicleOcclusiondatasetisasyntheticocclusion dataset,wherethetargetobjectisrandomlysuperimposed withtwo,threeorfourirrelevantobjects(named occluders ) andtheocclusionratiosofthetargetobjectisconstrained. Tothebestofourknowledge,VehicleOcclusionisthe only publicocclusiondatasetthatprovidesaccurateocclusion annotationsofsemanticpartsliketheocclusionratioand numberofoccluders.Thisallowsustoevaluatedifferent methodsunderdifferentocclusiondiflevels. Oneintuitivesolutionofdealingwithocclusionisto trainamodelonthedatasetthatcoversdifferentocclusion cases.However,itisextremelydifyetcomputationally intractabletocollectadatasetthatcoversocclusionpatterns ofdifferentnumbers,appearancesandpositions.Toover- comethisdif,wesuggestamoreessentialsolution, i.e ,trainingdetectors only onocclusion-freeimages,butal- lowingthelearnedknowledge( e.g. ,thespatialrelationship betweensemanticparts, etc. )tobetransferredfromnon- occlusionimagestoocclusionimages.Thismotivatesusto designmodelsthatareinherentlyrobusttoocclusions.A relatedworkis[ 25 ],whichpointedoutthatproposal-based deepnetworksarelessrobusttoocclusion,andinsteadpro- posedavotingmechanismthataccumulatesevidencesfrom arXiv:1709.04577v2  [cs.CV]  29 Mar 2018Figure1.TypicalsemanticpartsonsixtypesofrigidobjectsfromtheVehicleSemanticPartdataset[ 25 ].Somesemanticparts( e.g. , wheel ) canappearindifferentobjectclasses,whilesomeothers( e.g. , chimney )onlyappearinoneclass( train ). multiplelocalvisualcues,andlocatethesemanticparts withthehelpofgeometricconstraints( i.e. ,thespatialre- lationshipbetweenthevisualcuesandthetargetsemantic part).However,thismanually-designedframeworkisbro- kendownintoseveralstages,andthusitisdiftoop- timizeitinanend-to-endmanner.Thismotivatesustosee iftherobustnessshownin[ 25 ]canbeincorporatedintoa deepnetworkwhichenablesend-to-endtrainingnaturally. Tothisend,wepropose DeepVoting ,anend-to-end frameworkforsemanticpartdetectionunderpartialocclu- sion.,weaddtwoconvolutionallayersafter theintermediatefeaturesofadeepneuralnetwork, e.g. ,the neuralresponsesatthe pool-4 layerofVGGNet.The convolutionallayerperformstemplatematchingandout- putslocalvisualcuesnamed visualconcepts ,whichwere vtobecapableofdetectingsemanticparts[ 26 ].This layerisfollowedbyaReLUactivation[ 16 ],whichsetsa thresholdforthematchedpatterns,andadropout layer[ 22 ],whichallowspartofevidencestobemissing. Afterthat,thesecondconvolutionlayerisaddedtoperform avotingmechanismbyutilizingthespatialrelationshipbe- tweenvisualcuesandsemanticparts.Thespatial/geometric relationsarestoredasconvolutionalweightsandvisual- izedas spatialheatmaps .Thevisualconceptsandspatial heatmapscanbelearnedeitheronforegroundobjectsonly oronwholeimagewithcontext.Wefollow[ 25 ]totrain ourmodelonforegroundobjectsonlybycroppingtheob- jectboundingboxes.Wefurthershowthatvisualconcepts andspatialheatmapscanalsoexploitcontextinformation byusingthewholeimagetotrainourmodel,andwecall thisimprovedversion DeepVoting+ . WeinvestigatebothDeepVotingandDeepVoting+inour experiments.Theversion,inwhichcontextsareex- cluded,outperforms[ 25 ]withthesameset- ting,arguablybecausetheend-to-endtrainingmannerpro- videsastrongermethodforjointoptimization.Thesec- ondversion,whichallowscontextualcuestobeincorpo- rated,thetrainingdatabetterandconsequentlyproduces higherdetectionaccuracies.Incomparisontothestate-of- the-artobjectdetectorssuchasFaster-RCNN[ 20 ],Deep- Votingenjoysaconsistentadvantage,andtheadvantage becomesmoreastheocclusionlevelgoesup. DeepVotingbringstwoadditionalapartfrombe- ingrobusttoocclusion:(i)DeepVotingenjoysmuchlower modelcomplexity, i.e. ,thenumberofparametersisoneor- derofmagnitudesmaller,andtheaveragetestingspeedis 2 : 5  faster;and(ii)DeepVotingprovidesthepossibilityto interpretthedetectionresultsvialookingupthevotingcues. 2.RelatedWork Deepconvolutionalneuralnetworkshavebeenapplied successfullytoawiderangeofcomputervisionprob- lems,includingimagerecognition[ 11 , 21 , 23 , 9 ],seman- ticsegmentation[ 14 , 2 , 30 ],objectdetection[ 7 , 20 , 19 , 13 , 29 ], etc .Forobjectdetection,oneofthemostpop- ularpipeline[ 7 , 20 ]involvedextractinganumberof regionsnamedobjectproposals[ 1 , 24 , 12 , 20 ],andthen determiningifeachofthembelongstothetargetclass. Bounding-boxregressionandnon-maximumsuppression wereattachedforpost-processing.Thisframeworksignif- icantlyoutperformsthedeformablepart-basedmodel[ 6 ] trainedontopofasetofhandcraftedfeatures[ 4 ]. Therearesomeworksusingsemanticpartstoassistob- jectdetection[ 3 , 31 ].Graphicalmodelwasusedtoassem- blepartsintoanobject.Also,partscanbeusedfor grainedobjectrecognition[ 28 , 27 ],beappliedasauxiliary cuestounderstand[ 10 ],orbetrainedforac- tionandattribute[ 8 ].Besides,[ 17 ]investi- gatedthetransferabilityofsemanticpartsacrossalargetar- getsetofvisuallydissimilarclassesinimageunderstanding. Detectingsemanticpartsunderocclusionisanimpor- tantproblembutwaslessstudiedbefore.[ 25 ]combined multiplevisualconceptsviathegeometricconstraints, i.e. , thespatialdistributionofthevisualconceptsrelatedtothe targetsemanticparts,toobtainastrongdetector.Different from[ 25 ],DeepVotingimplementsvisualconceptextrac- tionandthegeometricrelationshipsastwolayers,andat- tachthemdirectlytotheintermediateoutputsofadeepneu- ralnetworktoperformanend-to-endtraining.Thisyields muchbetterperformancecomparedto[ 25 ]. 3.TheDeepVotingFramework 3.1.Motivation Weaimatdetectingthesemanticpartsofanobjectun- derocclusion.Firstofall,wearguethat only occlusion-free imagesshouldbeusedinthetrainingphase.Thisisbe- causetheappearanceandpositionoftheoccluderscanbe arbitrary,thusitisalmostimpossibletocoverallofthem byalimitedtrainingset.Itisourgoaltodesignaframe- workwhichcantransferthelearnedknowledgefromthe occlusion-freedomaintotheocclusiondomain. Onepossiblesolutionistoadaptthestate-of-the-artob- jectdetectionmethods,suchasFaster-RCNN[ 20 ],tode- tectsemanticparts.,theadaptedmethods extractanumberofproposalsforsemanticpartsandthen computethescoresforeachofthem.But,we pointoutthatthisstrategymaymisssomepartiallyorfully occludedsemanticpartsbecauseoftwoimportantfactors: (1)occlusionmaydistracttheproposalgenerationnetwork fromextractinggoodproposalsforsemanticparts;(2)even withcorrectproposals,themaystillgowrong sincetheappearanceoftheoccludedsemanticpartscanbe totallydifferent.Weverifythatthesefactorsindeeddown- gradetheperformanceofFaster-RCNNinSection 4.3 . Thevotingmechanism[ 25 ]suggestsanalternativestrat- egy,whichaccumulatesmid-levelvisualcuestodetect high-levelsemanticparts.Thesemid-levelcuesarecalled visualconcepts[ 26 ], i.e. ,asetofintermediateCNNstates whicharecloselyrelatedtosemanticparts.Asemanticpart issupportedbymultiplevisualconceptsviathegeometric constraintsbetweenthem.Eveniftheevidencesfromsome visualconceptsaremissingduetoocclusion,itisstillpos- sibletoinferthepresenceofthesemanticpartviatheevi- dencesfromtheremainingones.However,itinvolvestoo manyhyper-parametersandthusishardtobeoptimized. Inthispaper,weproposeDeepVotingwhichincorpo- ratestherobustnessshownby[ 25 ]intoadeepnetwork. Following[ 26 ],thevisualconceptsarelearnedwhenthe objectsappearataedscalesinceeachneurononthein- termediatelayer, e.g. ,the pool-4 layer,hasaedrecep- tivesize[ 21 ].Therefore,weassumethattheobject scaleisapproximatelythesameinbothtrainingandtest- ingstages.Inthetrainingstage,weusedtheground-truth boundingboxtoresizetheobjectfortheDeepVoting,and computetheobject-to-imageratiototrainastandalonenet- work,ScaleNet[ 18 ],forscaleprediction(seeSection 3.4 fordetails).Inthetestingstage,thetrainedScaleNetwas usedtopredicttheresizingratio,andthenweresizethe testingimageaccordingtothepredictedratio. 3.2.Formulation Let I denoteanimagewithasizeof W  H .Follow- ing[ 25 ],wefeedthisimageintoa 16 -layerVGGNet[ 21 ], andextractthe pool-4 featuresasasetofintermediateneu- raloutputs.Denotetheoutputofthe pool-4 layeras X ,ora W 0  H 0  D cube,where W 0 and H 0 arethedown-sampled scalesof W and H ,and D is512forVGGNet.Thesefea- turescanbeconsideredas W 0  H 0 high-dimensionalvec- tors,andeachofthemrepresentstheappearanceofalocal region.Denoteeach D -dimensionalfeaturevectoras x i where i isanindexatthe W 0  H 0 grid.Thesefeature vectorsare ` 2 -normalizedsothat k x i k 2 =1 . 3.2.1VisualConceptExtraction In[ 25 ],asetofvisualconcepts V = f v 1 ;:::; v K g areob- tainedviaK-meansclustering,andeachvisualconceptis consideredintuitivelyasatemplatetocapturethemid-level semanticsfromtheseintermediateoutputs.,the responseofthevisualconcept v k atthe pool-4 featurevec- tor x i ismeasuredbythe ` 2 -distance, i.e. , k v k  x i k 2 2 . Wenotethat x i hasunitlength,andso k v k k 2 ˇ 1 as itisaveragedoverasetofneighboring x i 's,sowehave k v k  x i k 2 2 ˇ 2  2 h v k ; x i i where  ;  isthedotproduct operator.Thenthelog-likelihoodratiotestsareappliedto eliminatenegativeresponses.Thisisdrivenbytheideathat thepresenceofavisualconceptcanprovidepositivecues fortheexistenceofasemanticpart,buttheabsenceofa visualconceptshallnotgivetheoppositeinformation. Differentfrom[ 25 ],DeepVotingimplementsthismod- uleasaconvolutionallayer,namelyvisualconceptlayer, andattachesitdirectlyafterthenormalizedintermediate outputsofadeepneuralnetwork.Thekernelsizeofthis convolutionallayerissettobe 1  1 , i.e. ,each x i isconsid- eredindividually.TheReLUactivation[ 16 ]followstoset thenegativeresponsesas 0 'sandthusavoidsthemfrompro- vidingnegativecues.Weappendadropoutlayer[ 22 ]with adropratio 0 : 5 ,sothatarandomsubsetofthevisualcon- ceptresponsesarediscardedinthetrainingprocess.This strategyfacilitatesthemodeltoperformdetectionrobustly usingincompleteinformationand,consequently,improves thetestingaccuracywhenocclusionispresent. Theoutputofvisualconceptlayerisamap Y ofsize W 0  H 0 jVj ,where V isthesetofvisualconcepts.We set jVj =256 ,thoughalargersetmayleadtoslightlybetter performance.Althoughthesevisualconceptsaretrained fromscratchratherthanobtainedfromclustering[ 26 ],we showinSection 4.4 thattheyarealsocapableofcapturing repeatablevisualpatternsandsemanticallymeaningful. 3.2.2SemanticPartDetectionviatheVotingLayer Afterthepreviousstage,wecansomevisualcon- cepts, i.e. ,thosepositionswithpositiveresponsevalues. In[ 25 ],thevisualconceptsaredeterminedvialog- likelihoodratiotests.These ed visualconceptsarethen Figure2.TheoverallframeworkofDeepVoting(bestviewedincolor).A car imagewithtwo wheels (markedbyredframes,oneof themisoccluded)isfedintoVGGNet[ 21 ],andtheintermediateoutputsarepassedthroughavisualconceptextractionlayerandavoting layer.Weaggregatelocalcuesfromthevisualconceptmap(darkerblueindicatesmoresicues),considertheirspatialrelationship tothetargetsemanticpartviavoting,andobtainalow-resolutionmapofsemanticparts(darkerredoralargernumberindicateshigher Basedonthismap,weperformboundingboxregressionfollowedbynon-maximumsuppressiontoobtaintheresults. accumulatedtogetherforsemanticpartdetectionbycon- sideringthespatialconstraintsbetweeneachpairofvisual conceptandsemanticpart.Itismotivatedbythenaturethat avisualconceptcan,atleastweakly,suggesttheexistence ofasemanticpart.Forexample,asshowninFigure 2 ,ina car image,a headlight impliesthatthereisa wheel nearby,andthedistanceanddirectionfromthe headlight to the wheel areapproximatelythesameunderaedscale. Differentfrom[ 25 ],DeepVotingimplementsthespatial constraintsbetweenvisualconceptsandthesemanticparts asanotherconvolutionallayer,namedthevotinglayer,in whichwesetthereceptiveofeachconvolutionalkernel tobelarge, e.g. , 15  15 ,sothatavisualconceptcanvotefor thepresenceofasemanticpartatarelativelylongdistance. Thisstrategyhelpsparticularlywhentheobjectispartially occluded,aseffectivevisualcuesoftenemergeoutsidethe occluderandmaybefarfromthetarget. Thoughthespatialconstraintsarelearnedfromscratch andonlysemanticpartlevelsupervisionisimposedduring training,theycanstillrepresentthefrequencythatvisual conceptsappearatdifferentrelativepositions.Wereferto eachlearnedconvolutionalkernelatthislayerasaspatial heatmap ,andsomeofthemarevisualizedinSection 4.4 . Denotetheoutputofthevotinglayer, i.e. ,thesemantic partmap,as Z .Itisa W 0  H 0 jSj cubewhere S isthe setofsemanticparts.Eachlocalmaximumattheseman- ticpartmapcorrespondstoaregionontheimagelattice accordingtotheirreceptiveTogenerateabounding boxforsemanticpartdetection,wesetananchorbox, sized 100  100 andcenteredatthisregion,andthenlearn thespatialrescalingandtranslationtoregresstheanchor box(followingthesameregressionprocedurein[ 7 ])from thetrainingdata.Theanchorsize 100  100 istheaverage semanticpartscaleovertheentiretrainingdataset[ 25 ]. 3.3.TrainingandTesting Wetrainthenetworkonanocclusion-freeimagecorpus. Thishelpsusobtainclearrelationshipbetweenthevisual conceptsandthesemanticparts.Wediscardthebackground regionbycroppingtheobjectaccordingtotheground-truth boundingbox,tobeconsistentwith[ 25 ].Then,werescale thecroppedimagesothattheobjectshortedgehas 224 pix- els,whichismotivatedby[ 26 ]tocapturethevisualcon- ceptsataedscale.Theimageisfedintothe 16 -layer VGGNet,andwegetthefeaturevectorsatthe pool-4 layer. Thesefeaturevectorsarenormalizedandpassedthrough twolayersforvisualconceptextractionandvoting.We comparetheoutputsemanticpartmap Z withtheground- truthannotation L bycomputingdicecoefbetween predictionandground-truth[ 15 ].Togeneratetheground- truth,wethenearestgridpointatthe W 0  H 0 grid (down-sampledfromtheoriginalimagebythefactorof 16 ) basedonthecenterpixelofeachannotatedsemanticpart, andsetthelabelsofthesepositionsas 1 andothersas 0 . ThenweapplyGaussianonthebinaryground-truth annotation,togeneratethesmoothedground-truthannota- tion L .Thelabelcube L isalsoofsize W 0  H 0 jSj .The similaritybetween Z and L isas: D ( Z ; L )= 1 jSj jSj X s =1 2  P W 0 ;H 0 w =1 ;h =1 z w;h;s  l w;h;s P W 0 ;H 0 w =1 ;h =1  z 2 w;h;s + l 2 w;h;s  ; (1) Itisstraightforwardtocomputethegradientsbasedonthe lossfunction L ( Z ; L )=1 D ( Z ; L ) . Onthetestingstage,weuseScaleNet(seeSec- tion 3.4 )toobtaintheobjectscale.Then,werescalethe imagesothattheshortedgeoftheobjectroughlycontains 224 pixels.Wedonotcroptheobjectbecausewedonot knowitslocation.Then,theimageispassedthroughthe VGGNetfollowedbybothvisualconceptextractionand votinglayers,andweapplythespatialrescalingand translationtotheanchorbox( 100  100 )towardsmoreac- curatelocalization.Astandardnon-maximumsuppression isperformedtothedetectionresults. DeepVotingistrainedontheimagescroppedwith respecttotheobjectboundingboxestobeconsistent with[ 25 ].Moreover,visualconceptsandspatialheatmaps canalsoexploitcontextoutsideobjectboundingboxes.To Figure3.ExamplesofimagesinVehicleSemanticPartdatasetandVehicleOcclusiondataset.Theistheoriginalocclusion-freeimage fromVehicleSemanticPartdataset.Thesecond,thirdandforthimage(inrow-majororder)arefromVehicleOcclusiondataset.Thereare 2 , 3 and 4 occluders,andtheoccludedratioofobject,computedbypixels,is 0 : 2 Œ 0 : 4 , 0 : 4 Œ 0 : 6 and 0 : 6 Œ 0 : 8 ,respectively. verifythis,wetrainanimprovedversion,namedDeepVot- ing+,withoutcroppingtheboundingboxes.Wealsoresize theimagesothattheobjectshortedgecontains224pixels inthetrainingstage,andthetestingstageisthesameas DeepVoting.ExperimentsshowthatDeepVoting+achieves betterperformancethanDeepVoting. 3.4.TheScalePredictionNetwork Theaboveframeworkisbasedonanimportantassump- tion,thattheobjectsappearinapproximatelythesame scale.Thisisduetotworeasons.First,asshownin[ 26 ], thevisualconceptsemergewhentheobjectisrescaledto thesamescale, i.e. ,theshortedgeoftheobjectbounding boxcontains 224 pixels.Second,weexpectthevotinglayer tolearnedspatialoffsetswhichrelateavisualconceptto asemanticpart.Asanexample,theheatmapdeliversthe knowledgethatinthesideviewofa car ,the headlight of- tenappearsattheupperleftdirectionofa wheel ,andthe spatialoffseton x and y axesareabout 64 and 48 pixels( 4 and 3 atthe pool-4 grid),respectively.Suchinformationis notscale-invariant. Todealwiththeseissues,weintroduceanindividualnet- work,namelytheScaleNet[ 18 ],topredicttheobjectscale ineachimage.Themainideaistofeedaninputimage toa 16 -layerVGGNetforaregressiontask(the fc-8 layer isreplacedbya 1 -dimensionaloutput),andthelabelisthe ground-truthobjectsize.Eachinputimageisrescaled,so thatthelongedgecontains 224 pixels.Itisplacedatthe centerofan 224  224 squareandtheremainingpixelsare upwiththeaveragedintensity.Duringthetraining, weconsidertheshortedgeoftheobject,andaskthedeep networktopredicttheratiooftheobjectshortedgetothe imagelongedge( 224 pixels).Inthetestphase,animage ispreparedandfedintothenetworkinthesamewchart, andthepredictedratioisusedtonormalizetheobjectto thedesiredsize, i.e. ,itsshortedgecontains 224 pixels.We showinSection 4.2.1 thatthismethodworksverywell. 3.5.DiscussionsandRelationshiptoOtherWorks TheoverallframeworkofDeepVotingisquitedifferent fromtheconventionalproposal-baseddetectionmethods, suchasFaster-RCNN[ 20 ].Thisismainlyduetotheprob- lemsetting, i.e. ,whentheocclusionispresent,theaccuracy ofbothproposalandnetworksbecomeslower. However,DeepVotingisabletoinfertheoccludedsemantic partsviaaccumulatingthosenon-occludedvisualcues.We showmorecomparativeexperimentsinSection 4.3 . Wedecomposesemanticpartdetectionintotwosteps, i.e. ,centralpixeldetectionandboundingboxregression. Thestepisperformedlikesemanticsegmentation[ 14 ] inaverylow-resolutionsetting(down-sampledfromthe originalimagebythefactorof 16 ).Wealsoborrowthe ideafromsegmentation[ 15 ],whichusesalossfunctionre- latedtothedicecoefinoptimization.Astheseman- ticpartisoftenmuchsmallercomparedtotheentireimage, thisstrategyalleviatesthebiasofdataimbalance, i.e. ,the modelismorelikelytopredicteachpixelasbackgroundas itappearsdominantlyinthetrainingdata. 4.Experiments 4.1.DatasetandBaseline WeusetheVehicleSemanticPartdatasetandtheVehi- cleOcclusiondataset[ 25 ]forevaluation.TheVehicleSe- manticPartdatasetcontains 4549 trainingimagesand 4507 testingimagescoveringsixtypesofvehicles, i.e. , airplane , bicycle , bus , car , motorbike and train .Intotal, 133 se- manticpartsareannotated.ForeachtestimageinVehicle- SemanticPartdataset,somerandomly-positionedoccluders (irrelevanttothetargetobject)areplacedontothetarget object,andmakesurethattheocclusionratioofthetarget objectisconstrained.Figure 3 showsseveralexampleswith differentocclusionlevels. Wetrainsixmodels,oneforeachobjectclass.Allthe modelsaretrainedonanocclusion-freedataset,butevalu- atedoneithernon-occludedimages,ortheimageswithdif- ferentlevelsofocclusionsadded.Inthelatercase,wevary thediflevelbyoccludingdifferentfractionsofthe object.Weevaluateallthecompetitorsfollowingapopular criterion[ 5 ],whichcomputesthemeanaverageprecision (mAP)basedonthelistofdetectedsemanticparts.Ade- tectedboxisconsideredtobetrue-positiveifandonlyifits IoUratewithaground-truthboxisnotlowerthan 0 : 5 .Each semanticpartisevaluatedindividually,andthemAPofeach NoOcclusions L1 L2 L3 Category KVC DVC VT FR DV DV+ VT FR DV DV+ VT FR DV DV+ VT FR DV DV+ airplane 15.8 26.6 30.6 56.9 59.0 60.2 23.2 35.4 40.6 40.6 19.3 27.0 31.4 32.3 15.1 20.1 25.9 25.4 bicycle 58.0 52.3 77.8 90.6 89.8 90.8 71.7 77.0 83.5 85.2 66.3 62.0 78.7 79.6 54.3 41.1 63.0 62.5 bus 23.8 25.1 58.1 86.3 78.4 81.3 31.3 55.5 56.9 65.8 19.3 40.1 44.1 54.6 9.5 25.8 30.8 40.5 car 25.2 36.5 63.4 83.9 80.4 80.6 35.9 48.8 56.1 57.3 23.6 30.9 40.0 41.7 13.8 19.8 27.3 29.4 motorbike 32.7 29.2 53.4 63.7 65.2 69.7 44.1 42.2 51.7 55.5 34.7 32.4 41.4 43.4 24.1 20.1 29.4 31.2 train 12.3 12.8 35.5 59.9 59.4 61.2 21.7 30.6 33.6 43.7 8.4 17.7 19.8 29.8 3.7 10.9 13.3 22.2 mean 28.0 30.4 53.1 73.6 72.0 74.0 38.0 48.3 53.7 58.0 28.6 35.0 42.6 46.9 20.1 23.0 31.6 35.2 Table1.Left6columns:Comparisonofdetectionaccuracy(meanAP, % )of KVC , DVC , VT , FR , DV and DV+ withoutocclusion. Right12columns:Comparisonofdetectionaccuracy(meanAP, % )of VT , FR , DV and DV+ whentheobjectisoccludedatthreedifferent levels.Notethat DV+ isDeepVotingtrainedwithcontextoutsideobjectboundingboxes.Seethetextsfordetails. objectclassistheaveragemAPoverallthesemanticparts. DeepVotingandDeepVoting+(denotedby DV and DV+ , respectively,Section 3.3 )arecomparedwithfourbaselines:  KVC :Thesevisualconceptsareclusteredfromaset of pool-4 featuresusing K -Means[ 26 ].TheScaleNet (detailedinSection 3.4 )isusedtotacklescaleissue andtheextractedvisualconceptsaredirectlyusedto detectthesemanticparts.  DVC :ThesevisualconceptsareobtainedfromDeep- Voting, i.e. ,theweightsofthevisualconceptextrac- tionlayer.TheScaleNet(detailedinSection 3.4 )is usedtotacklescaleissueandtheextractedvisualcon- ceptsaredirectlyusedtodetectthesemanticparts.  VT :Thevotingmethodvisualconcepts vialog-likelihoodratiotests,andthenutilizesspatial constraintstocombinetheselocalvisualcues.  FR :Wetrainmodelsforeachcategoryindependently. Eachsemanticpartofacategoryisconsideredasa separateclassduringtraining, i.e. ,foreachcategory, wetrainamodelwith jSj +1 classes,corresponding to jSj semanticpartsandthebackground.Different fromotherbaselines,Faster-RCNNhereistrainedon fullimages,i.e.,objectcroppingisnotrequired.This enablesFaster-RCNNtousecontextforsemanticparts detectionandhandlescaleissuenaturallysinceseman- ticpartswithvariousscalesareusedintraining. 4.2.SemanticPartDetectionwithoutOcclusion Asatask,weevaluateouralgorithmindetect- ingsemanticpartsonnon-occludedobjects.Thisisalsoa baselineforlatercomparison.Intheleftsixcolumnsof Table 1 ,welistthedetectionaccuracyproducedbydif- ferentmethods.Theaveragedetectionaccuraciesbyboth votingandDeepVotingarehigherthanusing singlevisualconceptfordetection,regardlesswhetherthe visualconceptsareobtainedfrom K -Meansclusteringor DeepVoting.Thisindicatestheadvantageoftheapproaches Figure4.Thedistributionoftheratioofthepredictedscaletothe actualscale. whichaggregatesmultiplevisualcuesfordetection.Mean- while,DeepVotingismuchbetterthanvotingduetothe betterscalepredictionandtheend-to-endtrainingmanner. Eventherightscaleisprovidedforvoting(oraclescalere- sultsin[ 25 ]),DeepVotingstillbeatitbymorethan 20% in termsofaveragedmAPover6objects,whichindicatesthe broughtbythejointoptimizationofbothweightsfor visualconceptextractionlayerandvotinglayer. Ontheotherhand,DeepVotingproducesslightlylower detectionaccuracycomparedtoFaster-RCNN.Weargue thatFaster-RCNNfromthecontextoutsideobject boundingboxes,aswecansee,ifweimproveDeepVoting byaddingcontextduringthetraining( i.e. DeepVoting+), Faster-RCNNwillbelesscompetitivecomparedwithour method.Meanwhile,DeepVotingenjoyslowercomputa- tionaloverheads, i.e. ,itruns 2 : 5  faster. 4.2.1ScalePredictionAccuracy WeinvestigatetheaccuracyofScaleNet,whichisessen- tialforscalenormalization.Foreachtestingimage,we computetheratioofthepredictedobjectscaletotheac- tualscale,andplotthecontributionofthisratiooverthe entiretestingsetinFigure 4 .Onecanseethatinmorethan Recall atDifferentLevels mAP w/Addt'lProp. mAP byDeepVoting+ Category L0 L1 L2 L3 L1 L2 L3 L1 L2 L3 airplane 99 : 3 98 : 1 97 : 4 96 : 7 36 : 2 27 : 7 20 : 7 40 : 6 32 : 3 25 : 4 bicycle 99 : 5 99 : 0 98 : 0 96 : 5 77 : 9 64 : 0 44 : 7 85 : 2 79 : 6 62 : 5 bus 99 : 8 96 : 3 93 : 8 91 : 5 57 : 1 42 : 4 28 : 3 65 : 8 54 : 6 40 : 5 car 99 : 8 96 : 0 94 : 4 92 : 7 48 : 2 30 : 2 19 : 4 57 : 3 41 : 7 29 : 4 motorbike 99 : 0 96 : 5 95 : 7 93 : 3 43 : 6 33 : 1 21 : 3 55 : 5 43 : 4 31 : 2 train 98 : 3 93 : 5 90 : 6 85 : 6 32 : 0 19 : 4 11 : 3 43 : 7 29 : 8 22 : 2 mean 99 : 3 96 : 6 95 : 0 92 : 7 49 : 2 36 : 1 24 : 2 58 : 0 46 : 9 35 : 2 Table2.Left 4 columns:therecallrates( % )oftheproposalnetworkatdifferentocclusionlevels.Middle 3 andright 3 columns:detection mAPs( % )ofFaster-RCNN(ground-truthboundingboxesareaddedasadditionalproposals)andDeepVoting+atdifferentocclusionlevels. 75% cases,therelativeerrorofthepredictedscaledoesnot exceed 10% .Actually,thesepredictionresultsareaccurate enoughforDeepVoting.Evenifground-truthscaleispro- videdandwerescaletheimagesaccordingly,thedetection accuracyisslightlyimprovedfrom 72 : 0% to 74 : 5% . 4.3.SemanticPartDetectionunderOcclusion Wefurtherdetectsemanticpartswhentheobjectisoc- cludedinthreedifferentlevels.Sincethebaselines KVC and DVC performmuchworsethanothermethodseven whenocclusionisnotpresent,weignorethesetwomethods whenperformingsemanticpartdetectionunderocclusion. Inthelevel( i.e. L1 ),weplace 2 occludersoneachob- ject,andtheoccludedratio r oftheobject,computedbypix- els,satisfying 0 : 2 6 r< 0 : 4 .For L2 and L3 ,wehave 3 and 4 occluders,and 0 : 4 6 r< 0 : 6 and 0 : 6 6 r< 0 : 8 ,respec- tively(seeFigure 3 forexamples).Theoriginalocclusion- freetestingsetisdenotedas L0 .Thedetectionresultsare summarizedinTable 1 .OnecanseethatDeepVotingout- performsthevotingandtheFaster-RCNNin thesecases.FortheFaster-RCNN,theaccuracygainin- creasesastheocclusionlevelgoesup,suggestingtheadvan- tageofDeepVotingindetectingoccludedsemanticparts. Asasideevidence,weinvestigatetheimpactofthesize ofspatialheatmap(thekernelofthevotinglayer).Atthe heaviestocclusionlevel,whenweshrinkthedefault 15  15 to 11  11 ,themeandetectionaccuracydropsfrom 31 : 6% to 30 : 6% ,suggestingtheusefulnessoflong-distancevoting indetectingoccludedsemanticparts.Whenthekernelsize isincreasedto 19  19 ,theaccuracyisslightlyimprovedto 31 : 8% .Therefore,wekeepthekernelsizetobe 15  15 for alowermodelcomplexity. ToverifyourmotivationthatFaster-RCNNsuffers downgradedperformanceinboththeproposalnetworkand the,weinvestigateboththerecalloftheproposals andtheaccuracyofthe.Resultsaresummarized inTable 2 .First,wecanseethattherecallofthepropos- alsgoesdownastheocclusionlevelgoesup, sincetheobjectnessofthesemanticpartregionmaybe- comeweakerduetotherandomlyplacedoccluders.Thus thesecondstage, i.e. ,hastostartwitharela- tivelylow-qualitysetofcandidates.Inthesecondpart,we addtheground-truthboundingboxestotheexistingpro- posalssothattherecallis 100% ,feedthesecandidatesto the,andevaluateitsperformanceontheoccluded images.EvenwithsuchFaster-RCNNstillpro- ducesunsatisfyingdetectionaccuracy.Forexample,inde- tectingthesemanticpartsofa bicycle atthehighestocclu- sionlevel( L3 ),makinguseoftheadditionalproposalsfrom ground-truthboundingboxesmerelyimprovesthedetection accuracyfrom 41 : 1% to 44 : 7% ,whichisstillmuchlower thanthenumber 62 : 5% producedbyDeepVoting+.Thisim- pliesthatthemaybeconfusedsincetheoccluder changestheappearanceoftheproposals. 4.4.VisualizingVisualConceptsandHeatmaps InFigure 5 ,weshowsometypicalexamplesofthe learnedvisualconceptsandspatialheatmaps.Thevisual- izationofvisualconceptsfollowstheapproachusedin[ 26 ], which 10 mostresponsesoneachconvolu- tional, i.e. ,thematchingtemplate,tracesbacktothe originalimagelattice,andcropstheregioncorresponding totheneuronatthe pool-4 layer.Toshowdifferentspatial heatmaps,werandomlychoosesomerelevantpairsofvi- sualconceptandsemanticpart,andplottheconvolutional weightsofthevotinglayerforcomparison.Weseethatthe learnedvisualconceptsandspatialheatmapsaresemanti- callymeaningful,eventhoughthereisonlysemanticpart levelsupervisionduringtraining. 4.5.ExplainingtheDetectionResults Finally,weshowanintriguingofourapproach, whichallowsustoexplainthedetectionresults.InFig- ure 6 ,wedisplaythreeexamples,inwhichthetargetse- manticpartsarenotoccluded,partiallyoccludedandfully occluded,respectively.DeepVotingcaninfertheoccluded semanticparts,andisalsocapableoflookingupthevoting (supporting)visualconceptsfordiagnosis,todigintoerrors andunderstandtheworkingmechanismofourapproach. Figure5.Visualizationofvisualconceptsandspatialheatmaps(bestviewedincolor).Foreachvisualconcept,weshow 10 patcheswith thehighestresponses.Eachspatialheatmapillustratesthecuestodetectasemanticpart,inwhichyellow,cyananddarkblueindicate positive,zeroandnegativecues,respectively.Forexample, VC#073 ( windshield )oftenappearsabove SP#20 ( licenseplate ),and VC #170 ( carsidebottom )oftenappearsbelow SP#12 ( sidewindow ). Figure6.DeepVotingallowsustoexplainthedetectionresults.Intheexampleofheavyocclusion(thethirdcolumn),thetargetsemantic part, i.e. ,the licenceplate ona car ,isfullyoccludedbya bird .Withthehelpofsomevisualconcepts(bluedots),especiallythe 73 -rdVC (alsodisplayedinFigure 5 ),wecaninferthepositionoftheoccludedsemanticpart(markedinred).Notethatweonlyplotthe 3 VC's withthehighestscores,regardlessthenumberofvotingVC'scanbemuchlarger. 5.Conclusions Inthispaper,weproposearobustandexplainabledeep network,namedDeepVoting,forsemanticpartdetection underpartialocclusion.Theintermediatevisualrepresen- tations,named visualconcepts ,areextractedandusedto voteforsemanticpartsviatwoconvolutionallayers.The spatialrelationshipbetweenvisualconceptsandsemantic partsislearnedfromaocclusion-freedatasetandthentrans- ferredtotheoccludedtestingimages.DeepVotingiseval- uatedonboththeVehicleSemanticPartdatasetandtheVe- hicleOcclusiondataset,andshowscomparableperformance toFaster-RCNNinthenon-occlusionscenario,andsuperior performanceintheocclusionscenario.Ifcontextisutilized, i.e. ,DeepVoting+,thisframeworkoutperformsbothDeep- VotingandFaster-RCNNunderallscenarios. Moreover,ourapproachenjoystheadvantageofbeingex- plainable,whichallowsustodiagnosethesemanticparts detectionresultsbycheckingthecontributionofeachvot- ingvisualconcepts. Inthefuture,weplantoextendDeepVotingtodetect semanticpartsofnon-rigidandarticulatedobjectslikeani- mals.Also,weplantoperformobject-leveldetectionunder occlusionbycombiningthesesemanticcues. Acknowledgements ThisresearchwassupportedbyONRgrantN00014- 15-1-2356,andalsobytheCenterforBrains,Minds,and Machines(CBMM),fundedbyNSFSTCawardCCF- 1231216.WethankChenxiLiu,ZhuotunZhu,JunZhu, SiyuanQiao,YuyinZhouandWeiShenforinstructivedis- cussions. References [1] B.Alexe,T.Deselaers,andV.Ferrari.Measuringtheobject- nessofimagewindows. IEEETransactionsonPatternAnal- ysisandMachineIntelligence ,34(11):2189Œ2202,2012. 2 [2] L.-C.Chen,G.Papandreou,I.Kokkinos,K.Murphy,and A.L.Yuille.Deeplab:Semanticimagesegmentationwith deepconvolutionalnets,atrousconvolution,andfullycon- nectedcrfs. IEEETransactionsonPatternAnalysisandMa- chineIntelligence ,2017. 2 [3] X.Chen,R.Mottaghi,X.Liu,S.Fidler,R.Urtasun,and A.Yuille.Detectwhatyoucan:Detectingandrepresenting objectsusingholisticmodelsandbodyparts.In Proceed- ingsoftheIEEEConferenceonComputerVisionandPattern Recognition ,2014. 2 [4] N.DalalandB.Triggs.Histogramsoforientedgradientsfor humandetection.In ComputerVisionandPatternRecogni- tion .IEEE,2005. 2 [5] M.Everingham,L.VanGool,C.K.Williams,J.Winn,and A.Zisserman.Thepascalvisualobjectclasses(voc)chal- lenge. InternationalJournalofComputerVision ,88(2):303Œ 338,2010. 5 [6] P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ra- manan.Objectdetectionwithdiscriminativelytrainedpart- basedmodels. IEEETransactionsonPatternAnalysisand MachineIntelligence ,32(9):1627Œ1645,2010. 2 [7] R.Girshick.Fastr-cnn.In InternationalConferenceonCom- puterVision .IEEE,2015. 1 , 2 , 4 [8] G.Gkioxari,R.Girshick,andJ.Malik.Actionsandat- tributesfromwholesandparts.In InternationalConference onComputerVision .IEEE,2015. 2 [9] K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearn- ingforimagerecognition.In ComputerVisionandPattern Recognition .IEEE,2016. 2 [10] S.Huang,Z.Xu,D.Tao,andY.Zhang.Part-stackedcnnfor visualcategorization.In ComputerVisionand PatternRecognition .IEEE,2016. 2 [11] A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenet withdeepconvolutionalneuralnetworks.In AdvancesinNeuralInformationProcessingSystems ,2012. 2 [12] W.Kuo,B.Hariharan,andJ.Malik.Deepbox:Learningob- jectnesswithconvolutionalnetworks.In InternationalCon- ferenceonComputerVision .IEEE,2015. 2 [13] W.Liu,D.Anguelov,D.Erhan,C.Szegedy,S.Reed,C.-Y. Fu,andA.C.Berg.Ssd:Singleshotmultiboxdetector.In Europeanconferenceoncomputervision .Springer,2016. 1 , 2 [14] J.Long,E.Shelhamer,andT.Darrell.Fullyconvolutional networksforsemanticsegmentation.In ComputerVision andPatternRecognition .IEEE,2015. 2 , 5 [15] F.Milletari,N.Navab,andS.-A.Ahmadi.V-net:Fully convolutionalneuralnetworksforvolumetricmedicalim- agesegmentation.In InternationalConferenceon3DVision . IEEE,2016. 4 , 5 [16] V.NairandG.E.Hinton.linearunitsimprovere- strictedboltzmannmachines.In InternationalConference onMachineLearning ,2010. 2 , 3 [17] D.Novotn ´ y,D.Larlus,andA.Vedaldi.Ihaveseenenough: Transferringpartsacrosscategories.In BritishMachineVi- sionConference ,2016. 2 [18] S.Qiao,W.Shen,W.Qiu,C.Liu,andA.Yuille.Scalenet: Guidingobjectproposalgenerationinsupermarketsandbe- yond. arXivpreprintarXiv:1704.06752 ,2017. 3 , 5 [19] J.Redmon,S.Divvala,R.Girshick,andA.Farhadi.You onlylookonce:real-timeobjectdetection.In Com- puterVisionandPatternRecognition .IEEE,2016. 1 , 2 [20] S.Ren,K.He,R.Girshick,andJ.Sun.Fasterr-cnn:Towards real-timeobjectdetectionwithregionproposalnetworks.In AdvancesinNeuralInformationProcessingSystems ,2015. 1 , 2 , 3 , 5 [21] K.SimonyanandA.Zisserman.Verydeepconvolutional networksforlarge-scaleimagerecognition.In International ConferenceonLearningRepresentations ,2015. 2 , 3 , 4 [22] N.Srivastava,G.E.Hinton,A.Krizhevsky,I.Sutskever,and R.Salakhutdinov.Dropout:asimplewaytopreventneu- ralnetworksfromov JournalofMachineLearning Research ,15(1):1929Œ1958,2014. 2 , 3 [23] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed, D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich. Goingdeeperwithconvolutions.In ComputerVisionand PatternRecognition .IEEE,2015. 2 [24] J.R.Uijlings,K.E.VanDeSande,T.Gevers,andA.W. Smeulders.Selectivesearchforobjectrecognition. Interna- tionalJournalofComputerVision ,104(2):154Œ171,2013. 2 [25] J.Wang,C.Xie,Z.Zhang,J.Zhu,L.Xie,andA.Yuille. Detectingsemanticpartsonpartiallyoccludedobjects.In BritishMachineVisionConference ,2017. 1 , 2 , 3 , 4 , 5 , 6 [26] J.Wang,Z.Zhang,C.Xie,Y.Zhou,V.Premachandran, J.Zhu,X.Lingxi,andA.Yuille.Visualconceptsandcom- positionalvoting. AnnalsofMathematicalSciencesandAp- plications ,2018. 2 , 3 , 4 , 5 , 6 , 7 [27] H.Zhang,T.Xu,M.Elhoseiny,X.Huang,S.Zhang,A.El- gammal,andD.Metaxas.Spda-cnn:Unifyingsemanticpart detectionandabstractionforrecognition.In ComputerVisionandPatternRecognition .IEEE,2016. 2 [28] N.Zhang,J.Donahue,R.Girshick,andT.Darrell.Part- basedr-cnnsforcategorydetection.In Euro- peanConferenceonComputerVision .Springer,2014. 2 [29] Z.Zhang,S.Qiao,C.Xie,W.Shen,B.Wang,andA.L. Yuille.Single-shotobjectdetectionwithenrichedsemantics. arXivpreprintarXiv:1712.00433 ,2017. 1 , 2 [30] S.Zheng,S.Jayasumana,B.Romera-Paredes,V.Vineet, Z.Su,D.Du,C.Huang,andP.H.Torr.Conditionalran- domasrecurrentneuralnetworks.In International ConferenceonComputerVision .IEEE,2015. 2 [31] J.Zhu,X.Chen,andA.L.Yuille.Deepm:Adeeppart- basedmodelforobjectdetectionandsemanticpartlocaliza- tion. arXivpreprintarXiv:1511.07131 ,2015. 2  
HaveaLargerCakeandEatitFasterToo:AGuidelinetoTrain LargerModelsFaster NewshaArdalani BaiduSiliconValleyAILab ardalaninewsha@baidu.com JoelHestness BaiduSiliconValleyAILab hestness@baidu.com GregoryDiamos BaiduSiliconValleyAILab gregdiamos@baidu.com ABSTRACT Withtheincreasingprevalenceofdeepneuralnetworksandtheir growingdemandformorepowerfulhardware,understandingthe interplayofmodelarchitectureparameters,hardwarearchitecturepa- rameters,modelanddataparallelismonoverallmodelperformance (trainingtimeandaccuracy)becomesevermoreimportantinorder todesignnext-generationdeeplearning(DL)hardware.Toaidsuch understanding,thisworkstudiestheeffectofscalingmodelsize onoverallperformance,anddebunksalong-heldbeliefthatlarger modelsmusttakelongertotrain. Webreakthetotaltrainingtimeintonumberofstepsand time/step.Weanalyticallymodelthetrainingtimeperstepandem- piricallystudythenumberofstepstoconvergence.Weobservethat largermodelstakefewerstepstoreachtominimumvalidationloss (haltingpoint).Therefore,theburdenisonthehardwarecommu- nitytoimprovehardwaredesignsuchthatthegrowthintraining time/stepwouldbeslowerthanthedecreaseinthenumberofsteps asmodelsizescales.Ifsuccessful,largermodelswillconvergefaster, andthereforewecanhavealargercakeandeatitfastertoo. 1INTRODUCTION Therecentsuccessindeeplearninghasbeenadrivingforcein thehardwareindustryfordesigningmorepowerful,energy-efcient GPUs[ 3 ]andASICs[ 1 , 2 , 7 Œ 11 , 15 ]withspecialsupportfordeep learning.Recentstudiessuggestthatdeeplearningaccuracyscales withtrainingdata[ 4 , 12 , 14 ].Therefore,thereisanexpectationthat modelsizeandasaresultcomputationdemandstogrowrapidlywith datasetsize.Inthispaper,wemakeanobservationthatsuggeststhe growthinmodelsizeandcomputationdemandisexpectedtobeeven fasterthanthegrowthindatasetsize.Weobservethatlargermodels takefewerstepstoreachtosimilarlevelsofaccuracy.Figure1 showsthispatternforcharacter-levellanguagemodelstrainedon 0.01%oftheBillionWorddataset[ 6 ],withbatchsizeof128,and usingAdamoptimizerwithaninitiallearningrateof0.001.As depicted,largermodelstakefewerstepstoconverge,whiletraining timeperstepgrowswithmodelsize.Onthisparticulardesign, wealsogetbetteroveralltrainingtimeonantheexistingGPU hardware(NvidiaMaxwell).Thisoveralltrendoftotaltrainingtime droppingbymodelsizedependsontheunderlyinghardware,model architectureandalgorithmicpropertiesoftheimplementation.Note herethatthenumberofstepstoconvergenceisonlyafunctionof modelarchitecture,whilethetrainingtimeperstepisafunctionof hardwarearchitectureandtheefciencyoftheimplementation.If wecandesignourhardwareand/oralgorithmssuchthatthetraining timeperstepgrowswithalowerpacethanthedecreaseinthe numberofsteps,wecanpracticallytrainlargermodelsfasterthan smallerones. (a)NumberofSteps (b)TimeperStep (c)OverallTime Figure1:Largermodelstakefewerstepstoconverge Therearethreedifferentapproachestocontroltrainingtime/step asmodelsizegrows:Exploitingdataparallelism,modelparallelism andkernelparallelism,whichrequirenon-trivialchangesinimple- mentationandheavysupportinhardware.Recently,dataparallelism hasbeenexploredextensivelyinindustryandacademia[ 13 , 17 , 19 , 20 ],howevermodelparallelismhasbeenusedonlyforexotic traininganddeploymentscenarios[ 18 ].Wemaketheargumentthat modelparallelismmustbeconsideredasaparallelism requirementforfuturedeeplearningscaling.,asdataset sizegrows,thepotentialtogrowdataandmodelparallelismgrows comparably.However,therearehardupper-boundsonscalabiltyof dataparallelism[13,17,19,20]thatwillencouragegreaterempha- sisonwaystoimprovemodelparallelism.Further,weshowthat increasingmodelsizecanproceedwithoutincreasingtrainingtime. Themaincontributionsinclude: (1) Tothebestofourknowledge,wearethetoobservethat largermodelstakefewerstepstoconverge.Wepredictan increasingpressureonhardwarecommunitytosupportlarger modelsandmodelparallelism. (2) Ourresultssuggeststhatnumberofstepstoconvergencehas areciprocalrelationshiptomodelsizeandlinearrelationship todatasetsize(# Steps ˇ a : Data _ Size k 1 Model _ Size k 2 + b ).Meanwhile,the trainingtimeperstepgrowslinearlyinmodelsize(without changingparallelism). (3) Finally,weanalyzetheimplicationsoftheseon futurehardware/systemdesign. 2METHODOLOGY Wetime-to-convergenceasthenumberofstepstoreach within1%oftheminimumvalidationloss.Weuseearlystopping (atminimumvalidationloss)tocontrolovWeincreasethe modelsizebyincreasingthenumberofnodesperlayer,whilekeep- ingalltheotherarchitectureparametersthesame(samelearningrate, samebatchsize,etc.).Weevaluateouronthreeestablished DLmodels:character-levellanguagemodel(LM),word-levelLM andspeechrecognition. (a)CharacterLM (b)WordLM (c)Speech Figure2:#Stepsvs.ModelSizeAcrossDifferentApplicationDomains:X-axisrepresentsmodelsizeintermsofthenumberofparame- tersinlog-scale.Y-axisisthenumberofstepstominimumvalidationloss.Differentlinesrepresentdifferentdatasetsizes(percentage offulldataset).ForcharacterLM,wevarythedatasetsizefrom0.01%to0.4%ofthe1Bdataset.ForwordLM,wevarydatasetsize from0.1%to10%of1Bdataset.Forspeech,wevarythedatasetsizefrom1%to10%ofaninternal20000hourdataset. (a)CharacterLM (b)WordLM (c)Speech Figure3:MinimumValidationLossvs.ModelSize:X-axisrepresentsmodelsizeintermsofinlog-scale.Weuseearlystopping(at minimumvalidationloss)tocontrolov.Y-axisrepresentminimumvalidationloss(Lowerisbetter).  WordLM: WeimplementLSTM-basedwordLMsasde- scribedin[ 16 ].Werestrictthevocabularytothetop10,000 mostfrequentwordsintheBillionWordDataset[ 6 ].The networksare2-layerLSTMs,withsequencelengthof80,the samenumberofhiddennodesineachlayer.  CharacterLM: Weimplementchar-LMusingRecurrent HighwayNetworks(RHNs)[ 21 ].,wetraina1- layer,depth10RHN,sequencelength150.  Speechrecognition: WetrainmodelssimilartoDeepSpeech 2(DS2)[ 5 ]whichconsistoftwoconvolutionlayersfollowed byfourbidirectionalLSTMrecurrentlayers. 3RESULTSANDANALYSIS InthisSection,westudythetrade-offbetweenconvergencetime, accuracyandmodelsize.Ourpreliminaryresultssuggestthatthere isapotentialtoincreasemodelsizetoimprovetrainingtimewithout hurtingaccuracy. 3.1NumberofStepstoMinimumValidationLoss Figure2showsthenumberofstepstoconvergenceforthecharacter, wordandspeechmodel.X-axisandY-axisareinlogarithmic-scale. WeincreasethemodelsizeonX-axisbyincreasingthewidthofeach layer.Differentlinesrepresentdifferentdatasetsizes(percentageof afulldatasetsize).Asshown,thenumberofstepstoconvergence declineswithmodelsizeandgrowswithdatasetsize.Ingeneral, thenumberofstepstoconvergencecanbeapproximatedby D k 1 M k 2 , where D isthesizeoftrainingset, M ismodelsize,and k 1 and k 2 arecontrolledbydatasetcharacteristicsandmodelarchitecture parameters,respectively.Wealsoobservethatthispowerlawrela- tionshipeventuallyplateaus,i.ethereexistsamodelsizebeyond whichimprovingmodelsizedoesnotimprovetrainingtime. 3.2Accuracy Althoughtheoreticalresultssuggestover-parametrizingmodelsmay leadtoworsegeneralizationerror,ourempiricalresultsshowthat thechangeinaccuracyis(Figure3).Asshown,loss remainsalmostthesameafteritreachestoitsbestperformance. 3.3SensitivityAnalysis Wealsostudytheeffectofchangeindepthandlearningrateonthe numberofstepsandthepower-lawrelationship.Weobservethat thenumberofstepstominimumvalidationlossalsodeclinesaswe sweepthenumberoflayersfrom1to128.Wealsoobservethat adaptivelychanginglearningrate(usingAdamoptimizer)improves thenumberofstepsintwoways:Itshiftsdownthepower-lawcurve, andalsomakesitsteeper. 3.4System/HardwareImplications Movingforward,weexpectanincreasingdemandforlargermodels, notonlybecausetheyaremoreaccuratewithmoredatabutalso becausetheytrainfaster.Therefore,system/hardwaredesignersneed tofocusonlargermodelsbyprovidingsupportformodelparal- lelismthroughimprovinginter-devicebandwidthandcomputational throughputperdevice. 2 REFERENCES [1] 2017.Carebrase.https://www.cerebras.net.(2017).[Online;accessed20-Nov- 2017]. [2] 2017.GraphCore.https://www.graphcore.ai.(2017).[Online;accessed20-Nov- 2017]. [3] 2017.NVIDIATeslaVolta:TheNewGPUArchitectureDesignedtoBringAIto EveryIndustry.https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/. (2017).[Online;accessed20-Nov-2017]. [4] Shun-ichiAmari,NaotakeFujita,andShigeruShinomoto.1992.Fourtypesof learningcurves. NeuralComputation 4,4(1992),605Œ618. [5] DarioAmodei,RishitaAnubhai,EricBattenberg,CarlCase,JaredCasper,Bryan Catanzaro,JingDongChen,MikeChrzanowski,AdamCoates,GregDiamos,etal . 2016.DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin. In ProceedingsofTheInternationalConferenceonMachineLearning(ICML) . 173Œ182. [6] CiprianChelba,TomasMikolov,MikeSchuster,QiGe,ThorstenBrants,Phillipp Koehn,andTonyRobinson.2013.OneBillionWordBenchmarkforMeasuring ProgressinStatisticalLanguageModeling. arXivpreprintarXiv:1312.3005 (2013). [7] TianshiChen,ZidongDu,NinghuiSun,JiaWang,ChengyongWu,YunjiChen, andOlivierTemam.2014.Diannao:Asmall-footprinthigh-throughputaccelerator forubiquitousmachine-learning.In ACMSigplanNotices ,Vol.49.ACM,269Œ 284. [8] YunjiChen,TaoLuo,ShaoliLiu,ShijinZhang,LiqiangHe,JiaWang,LingLi, TianshiChen,ZhiweiXu,NinghuiSun,etal . 2014.Dadiannao:Amachine- learningsupercomputer.In Proceedingsofthe47thAnnualIEEE/ACMInterna- tionalSymposiumonMicroarchitecture .IEEEComputerSociety,609Œ622. [9] Yu-HsinChen,TusharKrishna,JoelSEmer,andVivienneSze.2017.Eyeriss:An energy-efcientacceleratorfordeepconvolutionalneuralnetworks. IEEEJournalofSolid-StateCircuits 52,1(2017),127Œ138. [10] TrishulMChilimbi,YutakaSuzue,JohnsonApacible,andKarthikKalyanaraman. 2014.ProjectAdam:BuildinganEfandScalableDeepLearningTraining System..In OSDI ,Vol.14.571Œ582. [11] ZidongDu,RobertFasthuber,TianshiChen,PaoloIenne,LingLi,TaoLuo, XiaobingFeng,YunjiChen,andOlivierTemam.2015.ShiDianNao:Shifting visionprocessingclosertothesensor.In ACMSIGARCHComputerArchitecture News ,Vol.43.ACM,92Œ104. [12] IanGoodfellow,YoshuaBengio,andAaronCourville.2016. Deeplearning .MIT press. [13] PriyaGoyal,PiotrDollàr,RossGirshick,PieterNoordhuis,LukaszWesolowski, AapoKyrola,AndrewTulloch,YangqingJia,andKaimingHe.2017.Accurate, LargeMinibatchSGD:TrainingImageNetin1Hour. FacebookAIResearch Publications (2017). [14] JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun, HassanKianinejad,MdPatwary,MostofaAli,YangYang,andYanqiZhou. 2017.DeepLearningScalingisPredictable,Empirically. arXivpreprint arXiv:1712.00409 (2017). [15] NormanPJouppi,CliffYoung,NishantPatil,DavidPatterson,GauravAgrawal, RaminderBajwa,SarahBates,SureshBhatia,NanBoden,AlBorchers,etal . 2017. In-datacenterperformanceanalysisofatensorprocessingunit.In Proceedings ofthe44thAnnualInternationalSymposiumonComputerArchitecture .ACM, 1Œ12. [16] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghui Wu.2016.ExploringtheLimitsofLanguageModeling. arXivpreprint arXiv:1602.02410v2 (2016). [17] NitishShirishKeskar,DheevatsaMudigere,JorgeNocedal,MikhailSmelyanskiy, andPingTakPeterTang.2016.OnLarge-BatchTrainingforDeepLearning: GeneralizationGapandSharpMinima. arXivpreprintarXiv:1609.04836 (2016). [18] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe, GeoffreyHinton,andJeffDean.2017.Outrageouslylargeneuralnetworks:The sparsely-gatedmixture-of-expertslayer. arXivpreprintarXiv:1701.06538 (2017). [19] Cho-JuiHsiehJamesDemmelKurtKeutzerYangYou,ZhaoZhang.2017.Ima- geNetTraininginMinutes. arXivpreprintarXiv:1709.05011 (2017). [20] YangYou,IgorGitman,andBorisGinsburg.2017.ScalingSGDBatchSizeto 32kforImageNetTraining. arXivpreprintarXiv:1708.03888 (2017). [21] JulianGeorgZilly,RupeshKumarSrivastava,JanKoutnik,andJurgenSchmid- huber.2017.RecurrentHighwayNetworks.In ProceedingsofTheInternational ConferenceonMachineLearning(ICML) . 3  
AreYouTalkingtoaMachine? DatasetandMethodsforMultilingualImageQuestionAnswering HaoyuanGao 1 JunhuaMao 2 JieZhou 1 ZhihengHuang 1 LeiWang 1 WeiXu 1 1 BaiduResearch 2 UniversityofCalifornia,LosAngeles gaohaoyuan@baidu.com , mjhustc@ucla.edu , f zhoujie01,huangzhiheng,wanglei22,wei.xu g @baidu.com Abstract Inthispaper,wepresentthe mQA model,whichisabletoanswerquestionsabout thecontentofanimage.Theanswercanbeasentence,aphraseorasingleword. Ourmodelcontainsfourcomponents:aLongShort-TermMemory(LSTM)to extractthequestionrepresentation,aConvolutionalNeuralNetwork(CNN)to extractthevisualrepresentation,anLSTMforstoringthelinguisticcontextinan answer,andafusingcomponenttocombinetheinformationfromthethree componentsandgeneratetheanswer.WeconstructaFreestyleMultilingualIm- ageQuestionAnswering( FM-IQA )datasettotrainandevaluateourmQAmodel. Itcontainsover150,000imagesand310,000freestyleChinesequestion-answer pairsandtheirEnglishtranslations.Thequalityofthegeneratedanswersofour mQAmodelonthisdatasetisevaluatedbyhumanjudgesthrougha TuringTest . ,wemixtheanswersprovidedbyhumansandourmodel.Thehuman judgesneedtodistinguishourmodelfromthehuman.Theywillalsoprovide ascore(i.e.0,1,2,thelargerthebetter)indicatingthequalityoftheanswer. Weproposestrategiestomonitorthequalityofthisevaluationprocess.Theex- perimentsshowthatin64.7%ofcases,thehumanjudgescannotdistinguishour modelfromhumans.Theaveragescoreis1.454(1.918forhuman).Thedetails ofthiswork,includingtheFM-IQAdataset,canbefoundontheprojectpage: http://idl.baidu.com/FM-IQA.html . 1Introduction Recently,thereisincreasinginterestintheofmultimodallearningforbothnaturallanguage andvision.Inparticular,manystudieshavemaderapidprogressonthetaskofimagecaptioning [26,15,14,40,6,8,4,19,16,42].Mostofthemarebuiltbasedondeepneuralnetworks(e.g. deepConvolutionalNeuralNetworks(CNN[17]),RecurrentNeuralNetwork(RNN[7])orLong Short-TermMemory(LSTM[12])).Thelarge-scaleimagedatasetswithsentenceannotations(e.g., [21,43,11])playacrucialroleinthisprogress.Despitethesuccessofthesemethods,therearestill manyissuestobediscussedandexplored.Inparticular,thetaskofimagecaptioningonlyrequires genericsentencedescriptionsofanimage.Butinmanycases,weonlycareaboutaparticularpart orobjectofanimage.Theimagecaptioningtasklackstheinteractionbetweenthecomputerand theuser(aswecannotinputourpreferenceandinterest). Inthispaper,wefocusonthetaskofvisualquestionanswering.Inthistask,themethodneeds toprovideananswertoafreestylequestionaboutthecontentofanimage.WeproposethemQA modeltoaddressthistask.Theinputsofthemodelareanimageandaquestion.Thismodelhasfour components(seeFigure2).ThecomponentisanLSTMnetworkthatencodesanaturallanguage sentenceintoadensevectorrepresentation.ThesecondcomponentisadeepConvolutionalNeural Network[36]thatextractedtheimagerepresentation.Thiscomponentwaspre-trainedonImageNet Task[33]andisedduringthetraining.ThethirdcomponentisanotherLSTM networkthatencodestheinformationofthecurrentwordandpreviouswordsintheanswerinto denserepresentations.Thefourthcomponentfusestheinformationfromthethreecomponents topredictthenextwordintheanswer.Wejointlytrainthethirdandfourthcomponentsby maximizingtheprobabilityofthegroundtruthanswersinthetrainingsetusingalog-likelihoodloss 1 Figure1:Sampleanswerstothevisualquestiongeneratedbyourmodelonthenewlyproposed FreestyleMultilingualImageQuestionAnswering(FM-IQA)dataset. function.Tolowerdowntheriskofovweallowtheweightsharingofthewordembedding layerbetweentheLSTMsintheandthirdcomponents.Wealsoadoptthetransposedweight sharingschemeasproposedin[25],whichallowstheweightsharingbetweenwordembeddinglayer andthefullyconnectedSoftmaxlayer. Totrainourmethod,weconstructalarge-scaleFreestyleMultilingualImageQuestionAnswering dataset 1 (FM-IQA,seedetailsinSection4)basedontheMSCOCOdataset[21].Thecurrent versionofthedatasetcontains158,392imageswith316,193Chinesequestion-answerpairsand theircorrespondingEnglishtranslations. 2 Todiversifytheannotations,theannotatorsareallowed toraiseanyquestionrelatedtothecontentoftheimage.Weproposestrategiestomonitorthe qualityoftheannotations.ThisdatasetcontainsawiderangeofAIrelatedquestions,suchasaction recognition(e.g.,ﬁIsthemantryingtobuyvegetables?ﬂ),objectrecognition(e.g.,ﬁWhatistherein yellow?ﬂ),positionsandinteractionsamongobjectsintheimage(e.g.ﬁWhereisthekitty?ﬂ)and reasoningbasedoncommonsenseandvisualcontent(e.g.ﬁWhydoesthebusparkhere?ﬂ,seelast columnofFigure3). Becauseofthevariabilityofthefreestylequestion-answerpairs,itishardtoaccuratelyevaluate themethodwithautomaticmetrics.WeconductaVisualTuringTest[38]usinghumanjudges. ,wemixthequestion-answerpairsgeneratedbyourmodelwiththesamesetofquestion- answerpairslabeledbyannotators.Thehumanjudgesneedtodeterminewhethertheansweris givenbyamodelorahuman.Inaddition,wealsoaskthemtogiveascoreof0(i.e.wrong),1(i.e. partiallycorrect),or2(i.e.correct).TheresultsshowthatourmQAmodelpasses64.7%ofthis test(treatedasanswersofahuman)andtheaveragescoreis1.454.Inthediscussion,weanalyze thefailurecasesofourmodelandshowthatcombinedwiththem-RNN[24]model,ourmodelcan automaticallyaskaquestionaboutanimageandanswerthatquestion. 2RelatedWork Recentworkhasmadeprogressusingdeepneuralnetworkmodelsinboththeof computervisionandnaturallanguage.Forcomputervision,methodsbasedonConvolutionalNeural Network(CNN[20])achievethestate-of-the-artperformanceinvarioustasks,suchasobjectclas- [17,34,17],detection[10,44]andsegmentation[3].Fornaturallanguage,theRecurrent NeuralNetwork(RNN[7,27])andtheLongShort-TermMemorynetwork(LSTM[12])arealso widelyusedinmachinetranslation[13,5,35]andspeechrecognition[28]. ThestructureofourmQAmodelisinspiredbythem-RNNmodel[24]fortheimagecaptioningand image-sentenceretrievaltasks.ItadoptsadeepCNNforvisionandaRNNforlanguage.Weextend themodeltohandletheinputofquestionandimagepairs,andgenerateanswers.Intheexperiments, wethatwecanlearnhowtoaskagoodquestionaboutanimageusingthem-RNNmodeland thisquestioncanbeansweredbyourmQAmodel. Therehasbeenrecenteffortonthevisualquestionansweringtask[9,2,22,37].However,mostof themuseaandrestrictedsetofquestions.Someofthesequestionsaregeneratedfroma template.Inaddition,ourFM-IQAdatasetismuchlargerthantheirs(e.g.,thereareonly2591and 1449imagesfor[9]and[22]respectively). 1 Weareactivelydevelopingandexpandingthedataset,pleasethelatestinformationontheprojectpage : http://idl.baidu.com/FM-IQA.html 2 Theresultsreportedinthispaperareobtainedfromamodeltrainedontheversionofthedataset(a subsetofthecurrentversion)whichcontains120,360imagesand250,569question-answerpairs. 2 Figure2:IllustrationofthemQAmodelarchitecture.Weinputanimageandaquestionaboutthe image(i.e.ﬁWhatisthecatdoing?ﬂ)tothemodel.Themodelistrainedtogeneratetheanswerto thequestion(i.e.ﬁSittingontheumbrellaﬂ).Theweightmatrixinthewordembeddinglayersof thetwoLSTMs(oneforthequestionandonefortheanswer)areshared.Inaddition,asin[25],this weightmatrixisalsoshared,inatransposedmanner,withtheweightmatrixintheSoftmaxlayer. Differentcolorsintherepresentdifferentcomponentsofthemodel.(Bestviewedincolor.) Therearesomeconcurrentandindependentworksonthistopic:[1,23,32].[1]proposealarge- scaledatasetalsobasedonMSCOCO.Theyalsoprovidesomesimplebaselinemethodsonthis dataset.Comparedtothem,weproposeastrongermodelforthistaskandevaluateourmethodusing humanjudges.Ourdatasetalsocontainstwodifferentkindsoflanguage,whichcanbeusefulfor othertasks,suchasmachinetranslation.Becauseweuseadifferentsetofannotatorsanddifferent requirementsoftheannotation,ourdatasetandthe[1]canbecomplementarytoeachother,andlead tosomeinterestingtopics,suchasdatasettransferringforvisualquestionanswering. Both[23]and[32]useamodelcontainingasingleLSTMandaCNN.Theyconcatenatethequestion andtheanswer(for[32],theanswerisasingleword.[23]alsopreferasinglewordastheanswer), andthenfeedthemtotheLSTM.Differentfromthem,weusetwoseparateLSTMsforquestions andanswersrespectivelyinconsiderationofthedifferentproperties(e.g.grammar)ofquestionsand answers,whileallowthesharingoftheword-embeddings.Forthedataset,[23]adoptthedataset proposedin[22],whichismuchsmallerthanourFM-IQAdataset.[32]utilizetheannotationsin MSCOCOandsynthesizeadatasetwithfourtypesofquestions(i.e.object,number, color,andlocation).Theyalsosynthesizetheanswerwithasingleword.Theirdatasetcanalsobe complementarytoours. 3TheMultimodalQA(mQA)Model WeshowthearchitectureofourmQAmodelinFigure2.Themodelhasfourcomponents:(I).a LongShort-TermMemory(LSTM[12])forextractingsemanticrepresentationofaquestion,(II).a deepConvolutionalNeuralNetwork(CNN)forextractingtheimagerepresentation,(III).anLSTM toextractrepresentationofthecurrentwordintheansweranditslinguisticcontext,and(IV).a fusingcomponentthatincorporatestheinformationfromthethreepartstogetherandgenerates thenextwordintheanswer.Thesefourcomponentscanbejointlytrainedtogether 3 .Thedetails ofthefourmodelcomponentsaredescribedinSection3.1.Theeffectivenessoftheimportant componentsandstrategiesareanalyzedinSection5.3. Theinputsofthemodelareaquestionandthereferenceimage.Themodelistrainedtogenerate theanswer.Thewordsinthequestionandanswerarerepresentedbyone-hotvectors(i.e.binary vectorswiththelengthofthedictionarysize N andhaveonlyonenon-zerovectorindicatingits indexintheworddictionary).Weadda h BOA i signanda h EOA i sign,astwospatialwordsin theworddictionary,atthebeginningandtheendofthetraininganswersrespectively.Theywillbe usedforgeneratingtheanswertothequestioninthetestingstage. Inthetestingstage,weinputanimageandaquestionabouttheimageintothemodelTo generatetheanswer,westartwiththestartsign h BOA i andusethemodeltocalculatetheprobability distributionofthenextword.Wethenuseabeamsearchschemethatkeepsthebest K candidates 3 Inpractice,wetheCNNpartbecausethegradientreturnedfromLSTMisverynoisy.Finetuningthe CNNtakesamuchlongertimethanjustit,anddoesnotimprovetheperformance. 3 withthemaximumprobabilitiesaccordingtotheSoftmaxlayer.Werepeattheprocessuntilthe modelgeneratestheendsignoftheanswer h BOA i . 3.1TheFourComponentsofthemQAModel (I).Thesemanticmeaningofthequestionisextractedbythecomponentofthemodel.It containsa512dimensionalwordembeddinglayerandanLSTMlayerwith400memorycells.The functionofthewordembeddinglayeristomaptheone-hotvectorofthewordintoadensesemantic space.WefeedthisdensewordrepresentationintotheLSTMlayer. LSTM[12]isaRecurrentNeuralNetwork[7]thatisdesignedforsolvingthegradientexplosionor vanishingproblem.TheLSTMlayerstoresthecontextinformationinitsmemorycellsandserves asthebridgeamongthewordsinasequence(e.g.aquestion).Tomodelthelongtermdependency inthedatamoreeffectively,LSTMaddthreegatenodestothetraditionalRNNstructure:theinput gate,theoutputgateandtheforgetgate.Theinputgateandoutputgateregulatethereadandwrite accesstotheLSTMmemorycells.Theforgetgateresetsthememorycellswhentheircontents areoutofdate.Differentfrom[23,32],theimagerepresentationdoesnotfeedintotheLSTMin thiscomponent.Webelievethisisreasonablebecausequestionsarejustanotherinputsourcefor themodel,soweshouldnotaddimagesasthesupervisionforthem.Theinformationstoredinthe LSTMmemorycellsofthelastwordinthequestion(i.e.thequestionmark)willbetreatedasthe representationofthesentence. (II).ThesecondcomponentisadeepConvolutionalNeuralNetwork(CNN)thatgeneratestherep- resentationofanimage.Inthispaper,weusetheGoogleNet[36].NotethatotherCNNmodels, suchasAlexNet[17]andVggNet[34],canalsobeusedasthecomponentinourmodel.Weremove theSoftMaxlayerofthedeepCNNandconnecttheremainingtoplayertoourmodel. (III).ThethirdcomponentalsocontainsawordembeddinglayerandanLSTM.Thestructureis similartothecomponent.Theactivationofthememorycellsforthewordsintheanswer,as wellasthewordembeddings,willbefedintothefusingcomponenttogeneratethenextwordsin theanswer. In[23,32],theyconcatenatethetrainingquestionandanswer,anduseasingleLSTM.Becauseof thedifferentproperties(i.e.grammar)ofquestionandanswer,inthispaper,weusetwoseparate LSTMsforquestionsandanswersrespectively.WedenotetheLSTMsforthequestionandthe answerasLSTM(Q)andLSTM(A)respectivelyintherestofthepaper.Theweightmatrixin LSTM(Q)isnotsharedwiththeLSTM(A)inthecomponents.Notethatthesemanticmeaning ofsinglewordsshouldbethesameforquestionsandanswerssothatwesharetheparametersinthe word-embeddinglayerfortheandthirdcomponent. (IV).Finally,thefourthcomponentfusestheinformationfromthethreelayers., theactivationofthefusinglayer f ( t ) forthe t th wordintheanswercanbecalculatedasfollows: f ( t )= g ( V r Q r Q + V I I + V r A r A ( t )+ V w w ( t )); (1) whereﬁ+ﬂdenoteselement-wiseaddition, r Q standsfortheactivationoftheLSTM(Q)memory cellsofthelastwordinthequestion, I denotestheimagerepresentation, r A ( t ) and w ( t ) denotes theactivationoftheLSTM(A)memorycellsandthewordembeddingofthe t th wordintheanswer respectively. V r Q , V I , V r A ,and V w aretheweightmatricesthatneedtobelearned. g ( : ) isan element-wisenon-linearfunction. Afterthefusinglayer,webuildanintermediatelayerthatmapsthedensemultimodalrepresentation inthefusinglayerbacktothedensewordrepresentation.WethenbuildafullyconnectedSoftmax layertopredicttheprobabilitydistributionofthenextwordintheanswer.Thisstrategyallowsthe weightsharingbetweenwordembeddinglayerandthefullyconnectedSoftmaxlayerasintroduced in[25](seedetailsinSection3.2). Similarto[25],weusethesigmoidfunctionastheactivationfunctionofthethreegatesandadopt ReLU[30]asthenon-linearfunctionfortheLSTMmemorycells.Thenon-linearactivationfunction forthewordembeddinglayer,thefusinglayerandtheintermediatelayeristhescaledhyperbolic tangentfunction[20]: g ( x )=1 : 7159  tanh( 2 3 x ) . 3.2TheWeightSharingStrategy AsmentionedinSection2,ourmodeladoptsdifferentLSTMsforthequestionandtheanswer becauseofthedifferentgrammarpropertiesofquestionsandanswers.However,themeaningof 4 singlewordsinbothquestionsandanswersshouldbethesame.Therefore,wesharetheweight matrixbetweentheword-embeddinglayersofthecomponentandthethirdcomponent. Inaddition,thisweightmatrixfortheword-embeddinglayersissharedwiththeweightmatrixin thefullyconnectedSoftmaxlayerinatransposedmanner.Intuitively,thefunctionoftheweight matrixintheword-embeddinglayeristoencodetheone-hotwordrepresentationintoadenseword representation.ThefunctionoftheweightmatrixintheSoftmaxlayeristodecodethedenseword representationintoapseudoone-wordrepresentation,whichistheinverseoperationoftheword- embedding.Thisstrategywillreducenearlyhalfoftheparametersinthemodelandisshownto havebetterperformanceinimagecaptioningandnovelvisualconceptlearningtasks[25]. 3.3TrainingDetails TheCNNweusedispre-trainedontheImageNetcationtask[33].Thiscomponentised duringtheQAtraining.Weadoptalog-likelihoodlossonthewordsequenceoftheanswer. Minimizingthislossfunctionisequivalenttomaximizingtheprobabilityofthemodeltogenerate thegroundtruthanswersinthetrainingset.Wejointlytrainthesecondandthefourthcom- ponentsusingstochasticgradientdecentmethod.Theinitiallearningrateis1andwedecreaseit byafactorof10foreveryepochofthedata.Westopthetrainingwhenthelossonthevalida- tionsetdoesnotdecreasewithinthreeepochs.Thehyperparametersofthemodelareselectedby cross-validation. FortheChinesequestionansweringtask,wesegmentthesentencesintoseveralwordphrases.These phrasescanbetreatedequivalentlytotheEnglishwords. 4TheFreestyleMultilingualImageQuestionAnswering(FM-IQA)Dataset Ourmethodistrainedandevaluatedonalarge-scalemultilingualvisualquestionansweringdataset. InSection4.1,wewilldescribetheprocesstocollectthedata,andthemethodtomonitorthequality ofannotations.SomestatisticsandexamplesofthedatasetwillbegiveninSection4.2.Thelatest datasetisavailableontheprojectpage: http://idl.baidu.com/FM-IQA.html 4.1TheDataCollection Westartwiththe158,392imagesfromthenewlyreleasedMSCOCO[21]training,validationand testingsetastheinitialimageset.TheannotationsarecollectedusingBaidu'sonlinecrowdsourc- ingserver 4 .Tomakethelabeledquestion-answerpairsdivtheannotatorsarefreetogive anytypeofquestions,aslongasthesequestionsarerelatedtothecontentoftheimage.Theques- tionshouldbeansweredbythevisualcontentandcommonsense(e.g.,wearenotexpectingtoget questionssuchasﬁWhatisthenameofthepersonintheimage?ﬂ).Theannotatorsneedtogivean answertothequestionthemselves. Ontheonehand,thefreedomwegivetotheannotatorsisinordertogetafreestyle, interestinganddivsetofquestions.Ontheotherhand,itmakesithardertocontrolthe qualityoftheannotationcomparedtoamoredetailedinstruction.Tomonitortheannotationquality, weconductaninitialqualitystage.,werandomlysampled1,000imagesas aqualitymonitoringdatasetfromtheMSCOCOdatasetasaninitialsetfortheannotators(they donotknowthisisatest).Wethensamplesomeannotationsandratetheirqualityaftereach annotatorsomelabelingonthisqualitymonitoringdataset(about20question-answerpairs perannotator).Weonlyselectasmallnumberofannotators(195individuals)whoseannotationsare satisfactory(i.e.thequestionsarerelatedtothecontentoftheimageandtheanswersarecorrect). Wealsogivepreferencetotheannotatorswhoprovideinterestingquestionsthatrequirehighlevel reasoningtogivetheanswer.Onlytheselectedannotatorsarepermittedtolabeltherestofthe images.Wepickasetofgoodandbadexamplesoftheannotatedquestion-answerpairsfromthe qualitymonitoringdataset,andshowthemtotheselectedannotatorsasreferences.Wealsoprovide reasonsforselectingtheseexamples.Aftertheannotationofalltheimagesiswefurther thedatasetandremoveasmallportionoftheimageswithbadlylabeledquestionsandanswers. 4.2TheStatisticsoftheDataset Currentlythereare158,392imageswith316,193Chinesequestion-answerpairsandtheirEnglish translations.Eachimagehasatleasttwoquestion-answerpairsasannotations.Theaveragelengths 4 http://test.baidu.com 5 Figure3:SampleimagesintheFM-IQAdataset.Thisdatasetcontains316,193Chinesequestion- answerpairswithcorrespondingEnglishtranslations. ofthequestionsandanswersare7.38and3.82respectivelymeasuredbyChinesewords.Some sampleimagesareshowninFigure3.Werandomlysampled1,000question-answerpairsandtheir correspondingimagesasthetestset. ThequestionsinthisdatasetaredivwhichrequiresavastsetofAIcapabilitiesinorder toanswerthem.Theycontainsomerelativelysimpleimageunderstandingquestionsof,e.g.,the actionsofobjects(e.g.,ﬁWhatistheboyingreencapdoing?ﬂ),theobjectclass(e.g.,ﬁIsthereany personintheimage?ﬂ),therelativepositionsandinteractionsamongobjects(e.g.,ﬁIsthecomputer ontherightorleftsideofthegentleman?ﬂ),andtheattributesoftheobjects(e.g.,ﬁWhatisthecolor ofthefrisbee?ﬂ).Inaddition,thedatasetcontainssomequestionsthatneedahigh-levelreasoning withcluesfromvision,languageandcommonsense.Forexample,toanswerthequestionofﬁWhy doesthebusparkthere?ﬂ,weshouldknowthatthisquestionisabouttheparkedbusintheimage withtwomenholdingtoolsattheback.Basedonourcommonsense,wecanguessthattheremight besomeproblemswiththebusandthetwomenintheimagearetryingtorepairit.Thesequestions arehardtoanswerbutwebelievetheyareactuallythemostinterestingpartofthequestionsinthe dataset.Wecategorizethequestionsinto8typesandshowthestatisticsofthemontheprojectpage. TheanswersarealsodivTheannotatorsareallowedtogiveasinglephraseorasingleword astheanswer(e.g.ﬁYellowﬂ)or,theycangiveacompletesentence(e.g.ﬁThefrisbeeisyellowﬂ). 5Experiments Fortheveryrecentworksforvisualquestionanswering([32,23]),theytesttheirmethodonthe datasetswheretheanswerofthequestionisasinglewordorashortphrase.Underthissetting, itisplausibletouseautomaticevaluationmetricsthatmeasurethesinglewordsimilarity,such asWu-Palmersimilaritymeasure(WUPS)[41].However,forournewlyproposeddataset,the answersinthedatasetarefreestyleandcanbecompletesentences.Formostofthecases,thereare numerouschoicesofanswersthatareallcorrect.ThepossiblealternativesareBLEUscore[31], METEOR[18],CIDEr[39]orothermetricsthatarewidelyusedintheimagecaptioningtask[24]. Theproblemofthesemetricsisthatthereareonlyafewwordsinananswerthataresemantically critical.Thesemetricstendtogiveequalweights(e.g.BLEUandMETEOR)ordifferentweights accordingtothetf-idffrequencyterm(e.g.CIDEr)ofthewordsinasentence,hencecannotfully showtheimportanceofthekeywords.Theevaluationoftheimagecaptioningtasksuffersfromthe sameproblem(notassevereasquestionansweringbecauseitonlyneedsageneraldescription). Toavoidtheseproblems,weconductareal VisualTuringTest usinghumanjudgesforourmodel, whichwillbedescribedindetailsinSection5.1.Inaddition,werateeachgeneratedsentences withascore(thelargerthebetter)inSection5.2,whichgivesamoreevaluationofour method.InSection5.3,weprovidetheperformancecomparisonsofdifferentvariantsofourmQA modelonthevalidationset. 6 VisualTuringTestHumanRatedScores PassFailPassRate(%)210Avg.Score Human9485294.89276491.918 blind-QA34066034.0---- mQA64735364.76281981741.454 Table1:TheresultsofourmQAmodelforourFM-IQAdataset. 5.1TheVisualTuringTest InthisVisualTuringTest,ahumanjudgewillbepresentedwithanimage,aquestionandtheanswer tothequestiongeneratedbythetestingmodelorbyhumanannotators.Heorsheneedtodetermine, basedontheanswer,whethertheanswerisgivenbyahuman(i.e.passthetest)oramachine(i.e. failthetest). Inpractice,weusetheimagesandquestionsfromthetestsetofourFM-IQAdataset.Weuseour mQAmodeltogeneratetheanswerforeachquestion.Wealsoimplementabaselinemodelofthe questionansweringwithoutvisualinformation.Thestructureofthisbaselinemodelissimilarto mQA,exceptthatwedonotfeedtheimageinformationextractedbytheCNNintothefusinglayer. Wedenoteitasblind-QA.TheanswersgeneratedbyourmQAmodel,theblind-QAmodeland thegroundtruthansweraremixedtogether.Thisleadsto3000questionansweringpairswiththe correspondingimages,whichwillberandomlyassignedto12humanjudges. TheresultsareshowninTable1.Itshowsthat64.7%oftheanswersgeneratedbyourmQAmodel aretreatedasanswersprovidedbyahuman.Theblind-QAperformsverybadlyinthistask.But someofthegeneratedanswerspassthetest.Becausesomeofthequestionsareactuallymulti-choice questions,itispossibletogetacorrectanswerbyrandomguessbasedonpurelinguisticclues. TostudythevarianceoftheVTTevaluationacrossdifferentsetsofhumanjudges,weconduct twoadditionalevaluationswithdifferentgroupsofjudgesunderthesamesetting.Thestandard deviationsofthepassingrateare0.013,0.019and0.024forhuman,theblind-mQAmodeland mQAmodelrespectively.ItshowsthatVTTisastableandreliableevaluationmetricforthistask. 5.2TheScoreoftheGeneratedAnswer TheVisualTuringTestonlygivesaroughevaluationofthegeneratedanswers.Wealsoconducta evaluationwithscoresofﬁ0ﬂ,ﬁ1ﬂ,orﬁ2ﬂ.ﬁ0ﬂandﬁ2ﬂmeanthattheansweristotally wrongandperfectlycorrectrespectively.ﬁ1ﬂmeansthattheanswerisonlypartiallycorrect(e.g., thegeneralcategoriesarerightbutthesub-categoriesarewrong)andmakessensetothehuman judges.ThehumanjudgesforthistaskarenotnecessarilythesamepeoplefortheVisualTuring Test.Aftercollectingtheresults,wethatsomehumanjudgesalsorateananswerwithﬁ1ﬂifthe questionisveryhardtoanswersothatevenahuman,withoutcarefullylookingattheimage,will possiblymakemistakes.Weshowrandomlysampledimageswhosescoresareﬁ1ﬂinFigure4. TheresultsareshowninTable1.Weshowthatamongtheanswersthatarenotperfectlycorrect(i.e. scoresarenot2),overhalfofthemarepartiallycorrect.SimilartotheVTTevaluationprocess,we alsoconductstwoadditionalgroupsofthisscoringevaluation.Thestandarddeviationsofhuman andourmQAmodelare0.020and0.041respectively.Inaddition,for88.3%and83.9%ofthe cases,thethreegroupsgivethesamescoreforhumanandourmQAmodelrespectively. 5.3PerformanceComparisonsoftheDifferentmQAVariants InordertoshowtheeffectivenessofthedifferentcomponentsandstrategiesofourmQAmodel,we implementthreevariantsofthemQAinFigure2.Forthevariant(i.e.ﬁmQA-avg-questionﬂ),we replacetheLSTMcomponentofthemodel(i.e.theLSTMtoextractthequestionembedding) Figure4:RandomexamplesoftheanswersgeneratedbythemQAmodelwithscoreﬁ1ﬂgivenby thehumanjudges. 7 Figure5:Thesamplegeneratedquestionsbyourmodelandtheiranswers. WordErrorLoss mQA-avg-question0.4422.17 mQA-same-LSTMs0.4392.09 mQA-noTWS0.4382.14 mQA-complete 0.3931.91 Table2:Performancecomparisonsofthe differentmQAvariants. withtheaverageembeddingofthewordsintheques- tionusingword2vec[29].Itisusedtoshowtheeffec- tivenessoftheLSTMasaquestionembeddinglearner andextractor.Forthesecondvariant(i.e.ﬁmQA- same-LSTMsﬂ),weusetwoshared-weightsLSTMsto modelquestionandanswer.Itisusedtoshowtheef- fectivenessofthedecouplingstrategyoftheweightsof theLSTM(Q)andtheLSTM(A)inourmodel.Forthe thirdvariant(i.e.ﬁmQA-noTWSﬂ),wedonotadopttheTransposedWeightSharing(TWS)strategy. ItisusedtoshowtheeffectivenessofTWS. TheworderrorratesandlossesofthethreevariantsandthecompletemQAmodel(i.e.mQA- complete)areshowninTable2.AllofthethreevariantsperformsworsethanourmQAmodel. 6Discussion Inthispaper,wepresentthemQAmodel,whichisabletogiveasentenceoraphraseastheanswer toafreestylequestionforanimage.Tovalidatetheeffectivenessofthemethod,weconstruct aFreestyleMultilingualImageQuestionAnswering(FM-IQA)datasetcontainingover310,000 question-answerpairs.WeevaluateourmethodusinghumanjudgesthrougharealTuringTest.It showsthat64.7%oftheanswersgivenbyourmQAmodelaretreatedastheanswersprovidedbya human.TheFM-IQAdatasetcanbeusedforothertasks,suchasvisualmachinetranslation,where thevisualinformationcanserveascontextinformationthathelpstoremoveambiguityofthewords inasentence. WealsotheLSTMinthecomponenttothemultimodalLSTMshownin[25].This allowsustogenerateafree-stylequestionaboutthecontentofimage,andprovidean answertothisquestion.WeshowsomesampleresultsinFigure5. WeshowsomefailurecasesofourmodelinFigure6.Themodelsometimesmakesmistakeswhen thecommonsensereasoningthroughbackgroundscenesisincorrect(e.g.,fortheimageinthe column,ourmethodsaysthatthemanisbutthesmallyellowfrisbeeintheimageindicates thatheisactuallytryingtocatchthefrisbee.Italsomakesmistakeswhenthetargetingobjectthatthe questionfocusesonistoosmallorlooksverysimilartootherobjects(e.g.imagesinthesecondand fourthcolumn).AnotherinterestingexampleistheimageandquestioninthecolumnofFigure 6.Answeringthisquestionisveryhardsinceitneedshighlevelreasoningbasedontheexperience fromeverydaylife.Ourmodeloutputsa h OOV i sign,whichisaspecialwordweusewhenthe modelmeetsawordwhichithasnotseenbefore(i.e.doesnotappearinitsworddictionary). Infuturework,wewilltrytoaddresstheseissuesbyincorporatingmorevisualandlinguisticinfor- mation(e.g.usingobjectdetectionorusingattentionmodels). Figure6:FailurecasesofourmQAmodelontheFM-IQAdataset. 8 References [1] S.Antol,A.Agrawal,J.Lu,M.Mitchell,D.Batra,C.L.Zitnick,andD.Parikh.Vqa:Visualquestionanswering. arXivpreprint arXiv:1505.00468 ,2015. [2] J.P.Bigham,C.Jayant,H.Ji,G.Little,A.Miller,R.C.Miller,R.Miller,A.Tatarowicz,B.White,S.White,etal.Vizwiz:nearly real-timeanswerstovisualquestions.In ACMsymposiumonUserinterfacesoftwareandtechnology ,pages333Œ342,2010. [3] L.-C.Chen,G.Papandreou,I.Kokkinos,K.Murphy,andA.L.Yuille.Semanticimagesegmentationwithdeepconvolutionalnetsand fullyconnectedcrfs. ICLR ,2015. [4] X.ChenandC.L.Zitnick.Learningarecurrentvisualrepresentationforimagecaptiongeneration.In CVPR ,2015. [5] K.Cho,B.vanMerrienboer,C.Gulcehre,F.Bougares,H.Schwenk,andY.Bengio.Learningphraserepresentationsusingrnnencoder- decoderforstatisticalmachinetranslation. arXivpreprintarXiv:1406.1078 ,2014. [6] J.Donahue,L.A.Hendricks,S.Guadarrama,M.Rohrbach,S.Venugopalan,K.Saenko,andT.Darrell.Long-termrecurrentconvolu- tionalnetworksforvisualrecognitionanddescription.In CVPR ,2015. [7] J.L.Elman.Findingstructureintime. Cognitivescience ,14(2):179Œ211,1990. [8] H.Fang,S.Gupta,F.Iandola,R.Srivastava,L.Deng,P.Doll ´ ar,J.Gao,X.He,M.Mitchell,J.Platt,etal.Fromcaptionstovisual conceptsandback.In CVPR ,2015. [9] D.Geman,S.Geman,N.Hallonquist,andL.Younes.Visualturingtestforcomputervisionsystems. PNAS ,112(12):3618Œ3623,2015. [10] R.Girshick,J.Donahue,T.Darrell,andJ.Malik.Richfeaturehierarchiesforaccurateobjectdetectionandsemanticsegmentation.In CVPR ,2014. [11] M.Grubinger,P.Clough,H.M ¨ uller,andT.Deselaers.Theiaprtc-12benchmark:Anewevaluationresourceforvisualinformation systems.In InternationalWorkshopOntoImage ,pages13Œ23,2006. [12] S.HochreiterandJ.Schmidhuber.Longshort-termmemory. Neuralcomputation ,9(8):1735Œ1780,1997. [13] N.KalchbrennerandP.Blunsom.Recurrentcontinuoustranslationmodels.In EMNLP ,pages1700Œ1709,2013. [14] A.KarpathyandL.Fei-Fei.Deepvisual-semanticalignmentsforgeneratingimagedescriptions.In CVPR ,2015. [15] R.Kiros,R.Salakhutdinov,andR.S.Zemel.Unifyingvisual-semanticembeddingswithmultimodalneurallanguagemodels. TACL , 2015. [16] B.Klein,G.Lev,G.Sadeh,andL.Wolf.Fishervectorsderivedfromhybridgaussian-laplacianmixturemodelsforimageannotation. arXivpreprintarXiv:1411.7399 ,2014. [17] A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenetwithdeepconvolutionalneuralnetworks.In NIPS ,2012. [18] A.LavieandA.Agarwal.Meteor:Anautomaticmetricformtevaluationwithhighlevelsofcorrelationwithhumanjudgements.In WorkshoponStatisticalMachineTranslation ,pages228Œ231.AssociationforComputationalLinguistics,2007. [19] R.Lebret,P.O.Pinheiro,andR.Collobert.Simpleimagedescriptiongeneratorviaalinearphrase-basedapproach. arXivpreprint arXiv:1412.8419 ,2014. [20] Y.A.LeCun,L.Bottou,G.B.Orr,andK.-R.M ¨ uller.Efbackprop.In Neuralnetworks:Tricksofthetrade ,pages9Œ48.2012. [21] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Doll ´ ar,andC.L.Zitnick.Microsoftcoco:Commonobjectsin context. arXivpreprintarXiv:1405.0312 ,2014. [22] M.MalinowskiandM.Fritz.Amulti-worldapproachtoquestionansweringaboutreal-worldscenesbasedonuncertaininput.In AdvancesinNeuralInformationProcessingSystems ,pages1682Œ1690,2014. [23] M.Malinowski,M.Rohrbach,andM.Fritz.Askyourneurons:Aneural-basedapproachtoansweringquestionsaboutimages. arXiv preprintarXiv:1505.01121 ,2015. [24] J.Mao,W.Xu,Y.Yang,J.Wang,Z.Huang,andA.Yuille.Deepcaptioningwithmultimodalrecurrentneuralnetworks(m-rnn).In ICLR ,2015. [25] J.Mao,W.Xu,Y.Yang,J.Wang,Z.Huang,andA.Yuille.Learninglikeachild:Fastnovelvisualconceptlearningfromsentence descriptionsofimages. arXivpreprintarXiv:1504.06692 ,2015. [26] J.Mao,W.Xu,Y.Yang,J.Wang,andA.L.Yuille.Explainimageswithmultimodalrecurrentneuralnetworks. NIPSDeepLearning Workshop ,2014. [27] T.Mikolov,A.Joulin,S.Chopra,M.Mathieu,andM.Ranzato.Learninglongermemoryinrecurrentneuralnetworks. arXivpreprint arXiv:1412.7753 ,2014. [28] T.Mikolov,M. ´ at,L.Burget,J.Cernock ˚ y,andS.Khudanpur.Recurrentneuralnetworkbasedlanguagemodel.In INTERSPEECH , pages1045Œ1048,2010. [29] T.Mikolov,I.Sutskever,K.Chen,G.S.Corrado,andJ.Dean.Distributedrepresentationsofwordsandphrasesandtheircomposition- ality.In NIPS ,pages3111Œ3119,2013. [30] V.NairandG.E.Hinton.linearunitsimproverestrictedboltzmannmachines.In ICML ,pages807Œ814,2010. [31] K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu.Bleu:amethodforautomaticevaluationofmachinetranslation.In ACL ,pages311Œ318, 2002. [32] M.Ren,R.Kiros,andR.Zemel.Imagequestionanswering:Avisualsemanticembeddingmodelandanewdataset. arXivpreprint arXiv:1505.02074 ,2015. [33] O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,A.C.Berg,and L.Fei-Fei.ImageNetLargeScaleVisualRecognitionChallenge,2014. [34] K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.In ICLR ,2015. [35] I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequencelearningwithneuralnetworks.In NIPS ,pages3104Œ3112,2014. [36] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.Goingdeeperwith convolutions. arXivpreprintarXiv:1409.4842 ,2014. [37] K.Tu,M.Meng,M.W.Lee,T.E.Choe,andS.-C.Zhu.Jointvideoandtextparsingforunderstandingeventsandansweringqueries. MultiMedia,IEEE ,21(2):42Œ70,2014. [38] A.M.Turing.Computingmachineryandintelligence. Mind ,pages433Œ460,1950. [39] R.Vedantam,C.L.Zitnick,andD.Parikh.Cider:Consensus-basedimagedescriptionevaluation.In CVPR ,2015. [40] O.Vinyals,A.Toshev,S.Bengio,andD.Erhan.Showandtell:Aneuralimagecaptiongenerator.In CVPR ,2015. [41] Z.WuandM.Palmer.Verbssemanticsandlexicalselection.In ACL ,pages133Œ138,1994. [42] K.Xu,J.Ba,R.Kiros,K.Cho,A.Courville,R.Salakhutdinov,R.Zemel,andY.Bengio.Show,attendandtell:Neuralimagecaption generationwithvisualattention. arXivpreprintarXiv:1502.03044 ,2015. [43] P.Young,A.Lai,M.Hodosh,andJ.Hockenmaier.Fromimagedescriptionstovisualdenotations:Newsimilaritymetricsforsemantic inferenceovereventdescriptions.In ACL ,pages479Œ488,2014. [44] J.Zhu,J.Mao,andA.L.Yuille.Learningfromweaklysuperviseddatabytheexpectationlosssvm(e-svm)algorithm.In NIPS ,pages 1125Œ1133,2014. 9  
ProceedingsofCOLING2012:TechnicalPapers ,pages215Œ230, COLING2012,Mumbai,December2012. 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230  
PublishedasaconferencepaperatICLR2018 I NTERACTIVE G ROUNDED L ANGUAGE A CQUISITION AND G ENERALIZATIONINA 2DW ORLD HaonanYu,HaichaoZhang&WeiXu BaiduResearch Sunnyvale,CA94089,USA f haonanyu,zhanghaichao,wei.xu g @baidu.com A BSTRACT Webuildavirtualagentforlearninglanguageina2Dmaze-likeworld.The agentseesimagesofthesurroundingenvironment,listenstoavirtualteacher,and takesactionstoreceiverewards.Itinteractivelylearnstheteacher'slanguagefrom scratchbasedontwolanguageusecases:sentence-directednavigationandques- tionanswering.Itlearnssimultaneouslythevisualrepresentationsoftheworld, thelanguage,andtheactioncontrol.Bydisentanglinglanguagegroundingfrom othercomputationalroutinesandsharingaconceptdetectionfunctionbetween languagegroundingandprediction,theagentreliablyinterpolatesandextrapo- latestointerpretsentencesthatcontainnewwordcombinationsornewwords missingfromtrainingsentences.Thenewwordsaretransferredfromtheanswers oflanguageprediction.Suchalanguageabilityistrainedandevaluatedonapopu- lationofover1.6milliondistinctsentencesconsistingof119objectwords,8color words,9spatial-relationwords,and50grammaticalwords.Theproposedmodel outperformsvecomparisonmethodsforinterpretingzero-shotsen- tences.Inaddition,wedemonstratehuman-interpretableintermediateoutputsof themodelintheappendix. 1I NTRODUCTION Someempiricistsarguethatlanguagemaybelearnedbasedonitsusage(Tomasello,2003).Skinner (1957)suggeststhatthesuccessfuluseofawordreinforcestheunderstandingofitsmeaningaswell astheprobabilityofitbeingusedagaininthefuture.Bruner(1985)emphasizestheroleofsocial interactioninhelpingachilddevelopthelanguage,andpositstheimportanceofthefeedbackand reinforcementfromtheparentsduringthelearningprocess.Thispapertakesapositiveviewofthe abovebehaviorismandtriestoexploresomeoftheideasbyinstantiatingthemina2Dvirtualworld where interactive languageacquisitionhappens.Thisinteractivesettingcontrastswithacommon learningsettinginthatlanguageislearnedfromdynamicinteractionswithenvironmentsinsteadof fromstaticlabeleddata. Languageacquisitioncangobeyondmappinglanguageasinputpatternstooutputlabelsformerely obtaininghighrewardsoraccomplishingtasks.Wetakeastepfurthertorequirethelanguagetobe grounded (Harnad,1990).,weconsulttheparadigmofproceduralsemantics(Woods, 2007)whichpositsthatwords,asabstractprocedures,shouldbeabletopickoutreferents.We willattempttoexplicitlylinkwordstoenvironmentconceptsinsteadoftreatingthewholemodel asablackbox.Suchacapabilityalsoimpliesthat,dependingontheinteractionswiththeworld, wordswouldhaveparticularmeaningsinaparticularcontextandsomecontentwordsintheusual sensemightnotevenhavemeaningsinourcase.Asaresult,thegoalofthispaperistoacquire ﬁin-contextﬂwordmeaningsregardlessoftheirsuitabilityinallscenarios. Ontheotherhand,ithasbeenarguedthatachild'sexposuretoadultlanguageprovidesinadequate evidenceforlanguagelearning(Chomsky,1991),butsomeinductionmechanismshouldexistto bridgethisgap(Landauer&Dumais,1997).ThispropertyiscriticalforanyAIsystemtolearn nitesentencesfromtrainingdata.Thistypeofgeneralizationproblemisspeciallyaddressedin ourproblemsetting.Aftertraining,wewanttheagenttogeneralizetointerpret zero-shot sentences oftwotypes: 1 arXiv:1802.01433v2  [cs.CL]  21 Feb 2018PublishedasaconferencepaperatICLR2018 Training Testing NAV ﬁMovetonorthof avocado .ﬂﬁGoto east ofrabbit.ﬂ ﬁGoto east of avocado .ﬂﬁCanyoureach watermelon ?ﬂ QA ﬁWhatisinnorthwest?ﬂ ﬁWhatis east of avocado ?ﬂﬁWhatisthecolorof watermelon ?ﬂ ( Answer :ﬁ Watermelon ﬂ)( Answer :ﬁNothingﬂ) ( Answer :ﬁRedﬂ) (a)(b) (c)(d) Figure1:Anillustrationof XWORLD andthetwolanguageusecases.(a)and(b):Amixedtraining ofNAVandQA.(c):TestingZS1sentencescontainanewcombinationofwords(ﬁeastﬂandﬁav- ocadoﬂ)thatneverappeartogetherinanytrainingsentence.(d):TestingZS2sentencescontaina newword(ﬁwatermelonﬂ)thatneverappearsinanytrainingsentencebutislearnedfromatraining answer.Thisisonlyaconceptualillustrationoflanguagegeneralization;inpracticeitmight takemanytrainingsessionsbeforetheagentcangeneralize.(Duetospacelimitations,themapsare onlypartiallyshown.) 1) interpolation ,newcombinationsofpreviouslyseenwordsforthesameusecase,or 2) extrapolation ,newwordstransferredfromotherusecasesandmodels. Inthefollowing,wewillcallthetypeZS1sentencesandthesecondtypeZS2sentences.Note thatsofarthezero-shotproblems,addressedbymostrecentwork(Hermannetal.,2017;Chaplot etal.,2018)ofinteractivelanguagelearning,belongtothecategoryofZS1.Incontrast,areliablein- terpretationofZS2sentences,whichisessentiallya transferlearning (Pan&Yang,2010)problem, willbeamajorcontributionofthiswork. Wecreateda2Dmaze-likeworldcalled XWORLD (Figure1),asatestbedforinteractivegrounded languageacquisitionandgeneralization. 1 Inthisworld,avirtualagenthastwolanguageusecases: navigation(NAV)andquestionanswering(QA).ForNAV,theagentneedstonavigatetocorrect placesindicatedbylanguagecommandsfromavirtualteacher.ForQA,theagentmustcorrectly generatesingle-wordanswerstotheteacher'squestions.NAVtestslanguagecomprehensionwhile QAadditionallytestslanguageprediction.Theyhappensimultaneously:Whentheagentisnavi- gating,theteachermightaskquestionsregardingitscurrentinteractionwiththeenvironment.Once theagentreachesthetargetorthetimeisup,thecurrent session endsandanewoneisrandomly generatedaccordingtoour(AppendixB).TheZS2sentencesinoursetting requirewordmeaningstobetransferredfromsingle-wordanswerstosentences,ormoreprecisely, fromlanguagepredictiontogrounding .Thisisachievedbyestablishinganexplicitlinkbetween groundingandpredictionviaacommonconceptdetectionfunction,whichconstitutesthemajor noveltyofourmodel.Withthistransferringability,theagentisabletocomprehendaquestioncon- taininganewobjectlearnedfromananswer,withoutretrainingtheQApipeline.Itisalsoableto navigatetoafreshlytaughtobjectwithoutretrainingtheNAVpipeline. Itisworthwhileemphasizingthatthisseeminglyﬁsimpleﬂworldinfactposesgreatchallengesfor languageacquisitionandgeneralization,because:  Thestatespaceishuge. Evenfora 7  7 mapwith15wallblocksand5objectsselectedfrom 119distinctclasses,therearealreadyoctillions( 10 27 )ofpossibledifferentnot tomentiontheintra-classvarianceofobjectinstances(seeFigure16intheappendix).Fortwo thatonlydifferinoneblock,theirsuccessfulnavigationpathscouldbecompletely different.Thisrequiresanaccurateperceptionoftheenvironment.Moreover,the constantlychangesfromsessiontosession,andfromtrainingtotesting.Inparticular,thetarget changesacrosssessionsinbothlocationandappearance. 1 https://github.com/PaddlePaddle/XWorld 2 PublishedasaconferencepaperatICLR2018  Thegoalspaceimpliedbythelanguagefornavigationishuge. Foravocabularycontainingonly 185words,thetotalnumberofdistinctcommandsthatcanbesaidbytheteacherconformingto ourgrammarisalreadyoverhalfamillion.Twocommandsthatdifferbyonlyoneword couldimplycompletelydifferentgoals.Thisrequiresanaccurategroundingoflanguage.  Theenvironmentdemandsastronglanguagegeneralizationabilityfromtheagent. Theagenthas tolearntointerpretzero-shotsentencesthatmightbeaslongas13words.Ithastoﬁplugﬂthe meaningofanewwordorwordcombinationintoafamiliarsententialcontextwhiletryingto stillmakesenseoftheunfamiliarwhole.Therecentwork(Hermannetal.,2017;Chaplotetal., 2018)addressesZS1(forshortsentenceswithseveralwords)butnotZS2sentences,whichisa keydifferencebetweenourlearningproblemandtheirs. Wedescribeanend-to-endmodelfortheagenttointeractivelyacquirelanguagefromscratchand generalizetounfamiliarsentences.Hereﬁscratchﬂmeansthatthemodeldoesnotholdanyassump- tionofthelanguagesemanticsorsyntax.Eachsentenceissimplyasequenceoftokenswitheach tokenbeingequallymeaninglessinthebeginningoflearning.Thisisunlikesomeearlypioneer- ingsystems(e.g.,SHRDLU(Winograd,1972)andA BIGAIL (Siskind,1994))thathard-codedthe syntaxorsemanticstolinklanguagetoasimulatedworldŒanapproachthatpresentsscalabilityis- sues.Therearetwoaspectsoftheinteraction:oneiswiththeteacher(i.e.,languageandrewards) andtheotheriswiththeenvironment(e.g.,steppingonobjectsorhittingwalls).Themodeltakes asinputRGBimages,sentences,andrewards.Itlearnssimultaneouslythevisualrepresentations oftheworld,thelanguage,andtheactioncontrol.Weevaluateourmodelonrandomlygenerated XWORLD mapswithrandomagentpositions,onapopulationofover1.6milliondistinctsentences consistingof119objectwords,8colorwords,9spatial-relationwords,and50grammaticalwords. Detailedanalysis(AppendixA)ofthetrainedmodelshowsthatthelanguageisgroundedinsuch awaythatthewordsarecapabletopickoutreferentsintheenvironment.Wespeciallytestthe generalizationabilityoftheagentforhandlingzero-shotsentences.TheaverageNAVsuccessrates are84.3%forZS1and85.2%forZS2whenthezero-shotportionishalf,comparabletotherate of90.5%inanormallanguagesetting.TheaverageQAaccuraciesare97.8%forZS1and97.7% forZS2whenthezero-shotportionishalf,almostasgoodastheaccuracyof99.7%inanormal languagesetting. 2M ODEL Ourmodelincorporatestwoobjectives.TheistomaximizethecumulativerewardofNAV andthesecondistominimizethecostofQA.Fortheformer,wefollowthestandard reinforcementlearning(RL)paradigm:theagentlearnstheactionateverystepfromrewardsignals. Itemploystheactor-critic(AC)algorithm(Sutton&Barto,1998)tolearnthecontrolpolicy(Ap- pendixE).Forthelatter,weadoptthestandardsupervisedsettingofVisualQA(Antoletal.,2015): thegroundtruthanswersareprovidedbytheteacherduringtraining.Thetrainingcostisformulated asthemulticlasscrossentropy. 2.1M OTIVATION Themodeltakestwostreamsofinputs:imagesandsentences.Thekeyishowtomodelthelanguage groundingproblem.Thatis,theagentmustlink(eitherimplicitlyorexplicitly)languageconcepts toenvironmententitiestocorrectlytakeanactionbyunderstandingtheinstructioninthecurrent visualcontext.Astraightforwardideawouldbetoencodethesentence s withanRNNandencode theperceivedimage e withaCNN,afterwhichthetwoencodedrepresentationsaremixedtogether. ,letthemultimodalmodulebe M ,theactionmodulebe A ,andthepredictionmodule be P ,thisideacanbeformulatedas: NAV: A  M p RNN p s q ; CNN p e qq  QA: P  M p RNN p s q ; CNN p e qq  : (1) Hermannetal.(2017);Misraetal.(2017);Chaplotetal.(2018)allemploytheaboveparadigm.In theirimplementations, M iseithervectorconcatenationorelement-wiseproduct.Foranyparticular wordinthesentence,fusionwiththeimagecouldhappenanywherestartingfrom M alltheway totheend,rightbeforealabelisoutput.ThisisduetothefactthattheRNNfoldsthestringof wordsintoacompactembeddingwhichthengoesthroughthesubsequentblackboxcomputations. 3 PublishedasaconferencepaperatICLR2018 Figure2:Anoverviewofthemodel.Weprocess e byalwaysplacingtheagentatthecentervia zeropadding.Thishelpstheagentlearnnavigationactionsbyreducingthevarietyoftargetrepre- sentations. c , a ,and v arethepredictedanswer,thenavigationaction,andthecriticvalueforpolicy gradient,respectively. ˚ denotestheconceptdetectionfunctionsharedbylanguagegroundingand prediction. M A generatesacompactrepresentationfrom x loc and h fornavigation(AppendixC). Therefore,languagegroundingandothercomputationalroutinesareentangled.Becauseofthis,we saythatthisparadigmhasan implicit languagegroundingstrategy.Suchastrategyposesagreat challengeforprocessingaZS2sentencebecauseitisalmostimpossibletopredicthowanewword learnedfromlanguagepredictionwouldperforminthecomplexentanglementinvolved.Thusa carefulinspectionofthegroundingprocessisneeded. 2.2A PPROACH Themainideabehindourapproachisto disentangle languagegroundingfromothercomputations inthemodel.Thisdisentanglementmakesitpossibleforustoexplicitlylanguagegrounding aroundacorefunctionthatisalsousedbylanguageprediction.,bothgroundingand predictionarecastasconceptdetectionproblems,whereeachword(embedding)istreatedasa detector.Thisopensupthepossibilityoftransferringwordmeaningsfromthelattertotheformer. TheoverallarchitectureofourmodelisshowninFigure2. 2.2.1E XPLICITGROUNDING Webeginwithourofﬁgrounding.ﬂWeasentenceasgenerallyastringofwords ofanylength.Asinglewordisaspecialcaseofasentence.Givenasentence s andanimage representation h  CNN p e q ,wesaythat s is grounded in h as x if I) h consistsof M entitieswhereanentityisasubsetofvisualfeatures,and II) x Pt 0 ; 1 u M witheachentry x r m s representingabinaryselectionofthe m thentityof h .Thus x isacombinatorialselectionover h . Furthermore, x is explicit if III)itisformedbythegroundingresultsof(some)individualwordsof s (i.e.,compositionality). Wesaythataframeworkhasanexplicitgroundingstrategyif IV) all language-visionfusionsintheframeworkareexplicitgroundings. Forourproblem,weproposeanewframeworkwithanexplicitgroundingstrategy: NAV: A  M A p x; CNN p e qq  QA: P  M P p x; CNN p e qq  ; (2) wherethesolelanguage-visionfusion x intheframeworkisanexplicitgrounding.Noticeinthe abovehowthegroundingprocess,asaﬁbottleneck,ﬂallowsonly x butnototherlinguisticinforma- tiontowtothedownstreamofthenetwork.Thatis, M A , M P , A ,and P all relyongrounded 4 PublishedasaconferencepaperatICLR2018 resultsbut not onothersentencerepresentations.Bydoingso,weexpect x tosummarizeallthe necessarylinguisticinformationforperformingthetasks. Theofthisframeworkaretwo-fold.First,theexplicitgroundingstrategyprovidesa con- ceptualabstraction (Garneloetal.,2016)thatmapshigh-dimensionallinguisticinputtoalower- dimensionalconceptualstatespaceandabstractsawayirrelevantinputsignals.Thisimprovesthe generalizationforsimilarlinguisticinputs.Given e ,allthatmattersforNAVandQAis x .Thisguar- anteesthattheagentwillperformexactlyinthesamewayonthesameimage e evengivendifferent sentencesaslongastheirgroundingresults x arethesame.Itdisentangleslanguagegrounding fromsubsequentcomputationssuchasobstacledetection,pathplanning,actionmaking,andfeature whichallshouldbeinherentlylanguage-independentroutines.Second,because x is explicit,therolesplayedbytheindividualwordsof s inthegroundingareinterpretable.Thisis incontrasttoEq.1wheretherolesofindividualwordsareunclear.Theinterpretabilityprovidesa possibilityofestablishingalinkbetweenlanguagegroundingandprediction,whichwewillperform intheremainderofthissection. 2.2.2I NSTANTIATIONOFEXPLICITGROUNDING Let h P R N  D beaspatiallyfeaturecube(originallyin3D,nowthe2Dspatialdomain collapsedinto1Dfornotationalsimplicity),where D isthenumberofchannelsand N isthenumber oflocationsinthespatialdomain.Weadoptthreeforanentity: 1) afeaturevectorataparticularimagelocation, 2) aparticularfeaturemapalongthechanneldimension,and 3) ascalarfeatureattheintersectionofafeaturevectorandafeaturemap. Theirgroundingresultsaredenotedas x loc p s;h qPt 0 ; 1 u N , x feat p s;h qPt 0 ; 1 u D ,and x cube p s;h qP t 0 ; 1 u N  D ,respectively.Intherestofthepaper,weremove s and h from x loc , x feat ,and x cube fornotationalsimplicitywhilealwaysassumingadependencyonthem.Weassumethat x cube isa low-rankmatrixthatcanbedecomposedintothetwo: x cube  x loc  x feat | : Tomakethemodelfullydifferentiable,inthefollowingwerelaxtheofgroundingsothat x loc Pr 0 ; 1 s N , x feat Pr 0 ; 1 s D ,and x cube Pr 0 ; 1 s N  D .Theattentionmap x loc isresponsiblefor imagespatialattention.Thechannelmask x feat isresponsibleforselectingimagefeaturemaps,and isassumedtobeindependentofthe h ,namely, x feat p s;h q x feat p s q .Intuitively, h canbe modulatedby x feat beforebeingsenttodownstreamprocessings.ArecentpaperbydeVriesetal. (2017)proposesanevenearliermodulationofthevisualprocessingbydirectlyconditioningsome oftheparametersofaCNNonthelinguisticinput. Finally,weemphasizethatourexplicitgrounding,eventhoughinstantiatedasasoftattentionmech- anism,isdifferentfromtheexistingvisualattentionmodels.SomeattentionmodelssuchasXuetal. (2015);deVriesetal.(2017)violateIIIandIV.Somework(Andreasetal.,2016a;b;Lu etal.,2016)violatesIVinawaythatlanguageisfusedwithvisionbyamultilayerper- ceptron(MLP)afterimageattention.Andersonetal.(2017)proposesapipelinesimilartoours butviolatesIIIinwhichtheimagespatialattentioniscomputedfromacompactquestion embeddingoutputbyanRNN. 2.2.3C ONCEPTDETECTION Withlanguagegroundingdisentangled,nowwerelateittolanguageprediction.Thisrelationisa common conceptdetection function.Weassumethateverywordinavocabulary,asaconcept,is detectableagainstentitiesoftype(1)asinSection2.2.1.Forameaningfuldetectionof spatial-relationwordsthatareirrelevanttoimagecontent,weincorporateparametricfeaturemaps into h tolearnspatialfeatures.Assumeaprecomputed x feat ,theconceptdetectionoperatesby slidingoverthespatialdomainofthefeaturecube h ,whichcanbewrittenasafunction ˚ : ˚ : h;x feat ;u ÞÑ ˜; where ˜ P R N isadetectionscoremapand u isawordembeddingvector.Thisfunctionscoresthe embedding u againsteachfeaturevectorof h ,modulatedby x feat thatselectswhichfeaturemapsto 5 PublishedasaconferencepaperatICLR2018 ﬁWhatisthecoloroftheobjectinthenortheast?ﬂ Figure3:Anillustrationoftheattentioncube x cube  x loc  x feat | ,where x loc attendstoimage regionsand x feat selectsfeaturemaps.Inthisexample, x loc iscomputedfromﬁnortheast.ﬂInorder fortheagenttocorrectlyanswerﬁredﬂ(color)insteadofﬁwatermelonﬂ(objectname), x feat hasto becomputedfromthesentencepatternﬁWhat...color...?ﬂ useforthescoring.Intuitively,eachscoreon ˜ indicatesthedetectionresponseofthefeaturevector inthatlocation.Ahigherscorerepresentsahigherdetectionresponse. Whiletherearemanypotentialformsfor ˚ ,weimplementitas ˚ p h;x feat ;u q h p x feat  u q ; (3) where  istheelement-wiseproduct.Todoso,wehavewordembedding u P R D where D isequal tothenumberofchannelsof h . 2.2.4P REDICTIONBYCONCEPTDETECTION Forprediction,wewanttooutputawordgivenaquestion s andanimage e .Supposethat x loc and x feat arethegroundingresultsof s .Basedonthedetectionfunction ˚ , M P outputsascore vector m P R K overtheentirelexicon,whereeachentryofthevectoris: m r k s x | loc ˚ p h;x feat ;u k q x | loc ˜ k ; (4) where u k isthe k thentryofthewordembeddingtable.Theabovesuggeststhat m r k s istheresult ofweightingthescoresonthemap ˜ k by x loc .Itrepresentsthecorrectnessofthe k thlexicalentry astheanswertothequestion s .Topredictananswer P p m q argmax k  softmax p m q  : Notethattheroleof x feat inthepredictionistoselectwhichfeaturemapsarerelevanttotheques- tion s .Otherwiseitwouldbeconfusingfortheagentaboutwhattopredict(e.g.,whethertopredict acolororanobjectname).Byusing x feat ,weexpectthatdifferentfeaturemapsencodedifferent imageattributes(seeanexampleinthecaptionofFigure3).Moreanalysisof x feat isperformedin AppendixA. 2.2.5G ROUNDINGBYCONCEPTDETECTION Tocompute x cube ,wecompute x loc and x feat separately. Wewant x loc tobebuiltonthedetectionfunction ˚ .Onecanexpecttocomputeaseriesofscore maps ˜ ofindividualwordsandmergetheminto x loc .Supposethat s consistsof L words t w l u with w l  u k beingsomeword k inthedictionary.Let ˝ p s q beasequenceofindices t l i u where 0 ¤ l i € L .Thissequencefunction ˝ decideswhichwordsofthesentenceareselectedand organizedinwhatorder.We x loc as x loc    ˚ p h; 1 ;w l 1 q ;:::;˚ p h; 1 ;w l i q ;:::;˚ p h; 1 ;w l I q    p ˜ l 1 ;:::;˜ l i ;:::;˜ l I q ; (5) 6 PublishedasaconferencepaperatICLR2018 Figure4:Asymbolicexampleofthe2Dconvolutionfortransformingattentionmaps.A2Dcon- volutioncanbedecomposedintotwosteps:andcrosscorrelation.Theattentionmapof ﬁnorthwestﬂistreatedasanoffsettotranslatethatofﬁapple.ﬂNotethatinpractice,theatten- tioniscontinuousandnoisy,andtheinterpreterhastolearntooutthewords(ifany)toperform thisconvolution. where 1 Pt 0 ; 1 u D isavectorofones,meaningthatitselectsallthefeaturemapsfordetecting w l i .  isanaggregationfunctionthatcombinesthesequenceofscoremaps ˜ l i ofindividualwords.As such, ˚ makesitpossibletotransfernewwordsfromEq.4toEq.5duringtesttime. Ifwewereprovidedwithanoraclethatisabletooutputaparsingtreeforanysentence,wecouldset ˝ and  accordingtothetreesemantics.Neuralmodulenetworks(NMNs)(Andreasetal.,2016a;b; Huetal.,2017)relyonsuchatreeforlanguagegrounding.Theygenerateanetworkofmodules whereeachmodulecorrespondstoatreenode.However,labeledtreesareneededfortraining. Belowweproposetolearn ˝ and  basedonwordattention(Bahdanauetal.,2015)tobypassthe needforlabeledstructureddata. Westartbyfeedingasentence s t w l u oflength L toabidirectionalRNN(Schuster&Paliwal, 1997).Itoutputsacompactsentenceembedding s emb andasequenceof L wordcontextvectors w l . Each w l summarizesthesententialpatternaroundthatword.Wethenemployametacontroller called interpreter inaniterativemanner.Forthe i thinterpretationstep,theinterpretercomputesthe wordattentionas: ˝  $ ' ' ' ' ' & ' ' ' ' ' % Wordattention: o i l 9 exp  S cos p p i  1 ; w l q ˘ Attendedcontext: w i  ¸ l o i l w l Attendedword: s i  ¸ l o i l w l Interpreterstate: p i  GRU p p i  1 ; w i q (6) where S cos iscosinesimilarityandGRUisthegatedrecurrentunit(Choetal.,2014).Hereweuse ˝  torepresentanapproximationof ˝ viasoftwordattention.Weset p 0 tothecompactsentence embedding s emb .Afterthis,theattendedword s i isfedtothedetectionfunction ˚ .Theinterpreter aggregatesthescoremapof s i by:  $ ' ' & ' ' % Detection: y 1  softmax  ˚ p h; 1 ;s i q  Maptransform: x i loc  y 1  y i  1 Mapupdategate: ˆ i  ˙ p Wp i  b q Mapupdate: y i  ˆ i x i loc p 1  ˆ i q y i  1 (7) where  denotesa2Dconvolution, ˙ issigmoid,and ˆ i isascalar. W and b areparameterstobe learned.Finally,theinterpreteroutputs x I loc as x loc ,where I isthemaximumstep. Notethatintheaboveweformulatethemaptransformasa2Dconvolution.Thisoperationenables theagenttoreasonaboutspatialrelations.Recallthateachattentionmap x loc isegocentric.When theagentneedstoattendtoaregionbyaspatialrelationreferringtoanobject,itcan translatetheobjectattentionwiththeattentionmapofthespatial-relationwordwhichservesasa 2Dconvolutionaloffset(Figure4).Forthisreason,weset y 0 asaone-hotmapwherethemap 7 PublishedasaconferencepaperatICLR2018 centerisone,torepresenttheidentitytranslation.Asimilarmechanismofspatialreasoningvia convolutionwasexploredbyKitaev&Klein(2017)foravoxel-grid3Drepresentation. Byassumption,thechannelmask x feat ismeanttobedeterminedsolelyfrom s ;namely,which featurestouseshouldonlydependonthesentenceitself,notonthevalueofthefeaturecube h . Thusitiscomputedas x feat  MLP  RNN p s q  ; (8) wheretheRNNreturnsanaveragestateofprocessing s ,followedbyanMLPwiththesigmoid activation. 2 3R ELATEDWORK Our XWORLD issimilartothe2DblockworldMaseBase(Sukhbaataretal.,2016).However,we emphasizetheproblemofgroundedlanguageacquisitionandgeneralization,whiletheydonot. Therehavebeenseveral3DsimulatedworldssuchasViZDoom(Kempkaetal.,2016),DeepMind Lab(Beattieetal.,2016),andMalmo(Johnsonetal.,2016).Still,theseothersettingsintendedfor visualperceptionandcontrol,withlessornolanguage. OurproblemsettingdrawsinspirationsfromtheAIroadmapdelineatedbyMikolovetal.(2015). Liketheirs,wehaveateacherintheenvironmentthatassignstasksandrewardstotheagent,poten- tiallywithacurriculum.Unliketheirproposalofusingonlythelinguisticchannel,wehavemultiple perceptualmodalities,thefusionofwhichisbelievedtobethebasisofmeaning(Kielaetal.,2016). Contemporarytoourwork,severalend-to-endsystemsalsoaddresslanguagegroundingproblems inasimulatedworldwithdeepRL.Misraetal.(2017)mapsinstructionsandvisualobservationsto actionsofmanipulatingblocksona2Dplane.Hermannetal.(2017);Chaplotetal.(2018)learnto navigatein3Dunderinstructions,andbothevaluateZS1generalization.Despitefallingshortofthe visionchallengesfoundintheseotherworlds,wehaveamuchlargerspaceofzero-shotsentences andadditionallyrequireZS2generalization,whichisinfactatransferlearning(Pan&Yang,2010) problem. Otherrecentwork(Andreasetal.,2017;Ohetal.,2017)onzero-shotmultitasklearningtreats languagetokensas(parsed)labelsforidentifyingskills.Inthesepapers,thezero-shotsettingsare notintendedforlanguageunderstanding. Theproblemofgroundinglanguageinperceptioncanperhapsbetracedbacktotheearlywork ofSiskind(1994;1999),althoughnostatisticallearningwasadoptedatthattime.Ourlanguage learningproblemisrelatedtosomerecentworkonlearningtogroundlanguageinimagesand videos(Yu&Siskind,2013;Gaoetal.,2016;Rohrbachetal.,2016).Thenavigationtaskisrelevant toroboticsnavigationunderlanguagecommands(Chen&Mooney,2011;Tellexetal.,2011;Barrett etal.,2017).Thequestion-answeringtaskisrelevanttoimagequestionanswering(VQA)(Antol etal.,2015;Gaoetal.,2015;Renetal.,2015;Luetal.,2016;Yangetal.,2016;Andersonetal., 2017;deVriesetal.,2017).Theinteractivesettingoflearningtoaccomplishtasksissimilartothat oflearningtoplayvideogamesfrompixels(Mnihetal.,2015). 4E XPERIMENTS Wedesignavarietyofexperimentstoevaluatetheagent'slanguageacquisitionandgeneralization ability.Ourmodeliscomparedwithseveralmethodstodemonstratethechallengesin XWORLD . Wethenevaluatetheagent'slanguagegeneralizationabilityintwodifferentzero-shotconditions. Finally,weconcludewithpreliminarythoughtsonhowtoscaleourmodeltoa3Dworld. 2 NotethatherewedroptheexplicitnessrequirementIII)forEq.8.Thischoiceofimplemen- tationsimplicityispurelybasedonourcurrentproblemthatrequireslittlelanguagecompositionalitywhen computing x feat (unlike x loc ).Onecouldimagineanalternativegroundingthatisexplicitwhereasingle-step wordattentionextractswordsfrom s tocompute x feat . 8 PublishedasaconferencepaperatICLR2018 Figure5:Thethreetypesoflanguagedataandtheirstatistics. 4.1G ENERALSETUP Foralltheexperiments,boththesentencesandtheenvironmentschangefromsessiontosession,and fromtrainingtotesting.Thesentencesaredrawnconformingtotheteacher'sgrammar.Thereare threetypesoflanguagedata:NAVcommand,QAquestion,andQAanswer,whichareillustrated inFigure5.Intotal,thereare  570kNAVcommands,  1mQAquestions,and136QAanswers (allthecontentwordsplusﬁnothingﬂandminusﬁbetweenﬂ).Theenvironmentare randomlygeneratedfromoctillionsofpossibilitiesofa 7  7 map,conformingtosomehigh-level suchasthenumbersofobjectsandwallblocks.ForNAV,ourmodelisevaluatedon fourtypesofnavigationcommands: nav obj :Navigatetoanobject. nav col obj :Navigatetoanobjectwithacolor. nav nr obj :Navigatetoalocationnearanobject. nav bw obj :Navigatetoalocationbetweentwoobjects. ForQA,ourmodelisevaluatedontwelvetypesofquestions( rec * inTable2).Wereferthereader toAppendixBforadetaileddescriptionoftheexperimentsettings. 4.2C OMPARISONMETHODS Fourcomparisonmethodsandoneablationmethodaredescribedbelow: ContextualAttention [CA] Avariantofourmodelthatreplacestheinterpreterwithacontextual attentionmodel.ThisattentionmodelusesagatedRNNtoconvertasentencetoarwhichis thenconvolvedwiththefeaturecube h toobtaintheattentionmap x loc .Theercoversthe 3  3 neighborhoodofeachfeaturevectorinthespatialdomain.Therestofthemodelisunchanged. StackedAttentionNet [SAN] AnadaptationofamodeldevisedbyYangetal.(2016)whichwasorig- inallyproposedforVQA.Wereplaceourinterpreterwiththeirstackedattentionmodeltocompute theattentionmap x loc .InsteadofemployingapretrainedCNNastheydid,wetrainaCNNfrom scratchtoaccommodateto XWORLD .TheCNNisastheoneemployedbyourmodel. Therestofourmodelisunchanged. VIS-LSTM [VL] AnadaptationofamodeldevisedbyRenetal.(2015)whichwasoriginallypro- posedforVQA.We h andprojectittothewordembeddingspace R D .Thenitisappended totheinputsentence s astheword.TheaugmentedsentencegoesthroughanLSTMwhoselast stateisusedforbothNAVandQA(Figure17,AppendixD). ConcatEmbed [CE] AnadaptationofamodelproposedbyMaoetal.(2015)whichwasoriginally proposedforimagecaptioning.Itinstantiates L asavanillaLSTMwhichoutputsasentenceembed- ding.Then h isprojectedandconcatenatedwiththeembedding.Theconcatenatedvectorisused forbothNAVandQA(Figure18AppendixD).Thisconcatenationmechanismisalsoemployed byHermannetal.(2017);Misraetal.(2017). NavAlone [NAVA] AnablationofourmodelthatdoesnothavetheQApipeline( M P and P )andis trainedonlyontheNAVtasks.Therestofthemodelisthesame. 9 PublishedasaconferencepaperatICLR2018 (a) (b)(c) Figure6:Thebasicevaluation.(a)Trainingrewardcurves.Theshownrewardistheaccumulated discountedrewardpersession,averagedoverevery8ktrainingtimesteps.Theshadedareaofeach curvedenotesthevarianceamong4randominitializationsofthemodelparameters.(b)Navigation successratesinthetest.(c)Theaccuraciesoftheanswersinthetest( NAVA isexcludedbecauseit doesnottrainQA). Inthefollowingexperiments,wetrainallsixapproaches(fourcomparisonmethods,oneablation, andourmodel)withasmalllearningrateof 1  10  5 andabatchsizeof16,foramaximumof 200kminibatches.AdditionaltrainingdetailsaredescribedinAppendixC.Aftertraining,wetest eachapproachon50ksessions.ForNAV,wecomputetheaveragesuccessrateofnavigationwhere asuccessisasreachingthecorrectlocationbeforethetimeoutofasession.ForQA,we computetheaverageaccuracyinansweringthequestions. 4.3B ASICEVALUATION Inthisexperiment,thetrainingandtestingsentences(includingNAVcommandsandQAquestions) aresampledfromthesamedistributionovertheentiresentencespace.Wecallitthenormallanguage setting. 3 ThetrainingrewardcurvesandtestingresultsareshowninFigure6. VL and CE havequitelow rewardswithoutconvergences.Thesetwoapproachesdonotusethespatialattention x loc ,andthus alwaysattendtowholeimageswithnofocus.Theregionofatargetlocationistinycompared tothewholeegocentricimage(aratioof 1: p 7  2  1 q 2  1:169 ).Itispossiblethatthis difdrivesthemodelstowardsovQAwithoutlearningusefulrepresentationsforNAV. 3 Althoughsometestsentencesmightnotbeseenintraining(i.e.,zero-shot)duetosampling,allthecontent wordsandtheircombinations(totalingadozenthousands)arehighlylikelytobeexhaustedbytraining.Thus weconsiderthisexperimentasanormalsetting,comparedtothezero-shotsettinginSection4.4. 10 PublishedasaconferencepaperatICLR2018 Both CA and SAN obtainrewardsandsuccessratesslightlyworsethanours.Thissuggeststhatin anormallanguagesetting,existingattentionmodelsareabletohandlepreviouslyseensentences. However,theirlanguagegeneralizationabilities,especiallyontheZS2sentences,areusuallyvery weak,aswewilldemonstrateinthenextsection.Theablation NAVA hasaverylargevariancein itsperformance.Dependingontherandomseed,itsrewardcanreachthatof SAN (  0 : 8 ),oritcan beaslowasthatof CE (  3 : 0 ).Comparingittoourfullmethod,weconcludethateventhough theQApipelineoperatesonacompletelydifferentsetofsentences,itlearnsusefullocalsentential knowledgethatresultsinaneffectivetrainingoftheNAVpipeline. NotethatallthefourcomparisonmethodsobtainedhighQAaccuracies( ¡ 70%,seeFigure6c), despitetheirdistinctNAVresults.ThissuggeststhatQA,asasupervisedlearningtask,iseasier thanNAVasanRLtaskinourscenario.OnereasonisthatthegroundtruthlabelinQAisamuch strongertrainingsignalthantherewardinNAV.AnotherreasonmightbethatNAVadditionally requireslearningthecontrolmodule,whichisabsentinQA. 4.4L ANGUAGEGENERALIZATION Ourmoreimportantquestioniswhethertheagenthastheabilityofinterpretingzero-shotsentences. Forcomparison,weuse CA and SAN fromtheprevioussection,astheyachievedgoodperformance inthenormallanguagesetting.Forazero-shotsetting,wesetuptwolanguageconditions: NewWordCombination [ZS1] SomewordpairsareexcludedfromboththeNAVcommandsandthe QAquestions.Weconsiderthreetypesofunorderedcombinationsofthecontentwords:( object , spatialrelation ),( object , color ),and( object , object ).Werandomlyholdout X %ofthewordpairs duringthetraining. NewWord [ZS2] SomesinglewordsareexcludedfromboththeNAVcommandsandtheQAques- tions,butcanappearintheQAanswers.Werandomlyholdout X %oftheobjectwordsduringthe training. Wevarythevalueof X (12.5,20.0,50.0,66.7,or90.0)inbothconditionstotesthowsensitivethe generalizationistotheamountoftheheld-outdata.Forevaluation,wereportthetestresultsonly forthezero-shotsentencesthatcontaintheheld-outwordpairsorwords.Theresultsareshownin Figure7. Wedrawthreeconclusionsfromtheresults.First,theZS1sentencesaremucheasiertointerpretthan theZS2sentences.Neuralnetworksseemtoinherentlyhavethiscapabilitytosomeextent.Thisis consistentwithwhathasbeenobservedinsomepreviouswork(Hermannetal.,2017;Chaplotetal., 2018)thataddressesthegeneralizationonnewwordcombinations.Second,theZS2sentencesare diffor CA and SAN .Evenwithaheld-outportionassmallas X %  12 : 5% ,theirnavigation successratesandQAaccuraciesdropupto80%and35%,respectively,comparedtothoseinthe normallanguagesetting.Incontrast,ourmodeltendstomaintainthesameresultsuntil X  90 : 0 . Impressively,itachievesanaveragesuccessrateof60%andanaverageaccuracyof83%evenwhen thenumberofnewobjectwordsis9timesthatofseenobjectwordsintheNAVcommandsand QAquestions,respectively!Third,in ZS2 ,ifwecomparetheslopesofthesuccess-ratecurveswith thoseoftheaccuracycurves(asshowninFigures7eand7f),wenoticethattheagentgeneralizes betteronQAthanonNAV.ThisfurthervourintheprevioussectionthatQAisin generalaneasiertaskthanNAVin XWORLD .ThisdemonstratesthenecessityofevaluatingNAVin additiontoQA,asNAVrequiresadditionallanguagegroundingtocontrol. Interestingly,wenoticethat nav bw obj isanoutliercommandtypeforwhich CA ismuchless sensitivetotheincreaseof X in ZS2 .Adeepanalysisrevealsthatfor nav bw obj , CA learnsto cheatbylookingfortheimageregionthatcontainsthespecialpatternofobjectpairsintheimage withouthavingtorecognizetheobjects.Thissuggeststhatneuralnetworkstendtoexploitdatain anunexpectedwaytoachievetasksifnoconstraintsareimposed(Kotturetal.,2017). Tosumup,ourmodelexhibitsastronggeneralizationabilityonbothZS1andZS2sentences,the latterofwhichposeagreatchallengefortraditionallanguagegroundingmodels.Althoughweusea particular2Dworldforevaluationinthiswork,thepromisingresultsimplythepotentialforscaling toanevenlargervocabularyandgrammarwithamuchlargerlanguagespace. 11 PublishedasaconferencepaperatICLR2018 (a) (b) (c) (d) (e) (f) Figure7:Thetestresultsoflanguagegeneralizationwithavaryingheld-outportionof X %,where X  0 representsthebasicevaluationinSection4.3.(aŒc) ZS1 .(dŒf) ZS2 .Foreither ZS1 or ZS2 ,fromtoptobottom,thethreerowsrepresenttheaveragenavigationrewardpersession,the averagenavigationsuccessratepersession,andtheaverageQAaccuracy,respectively.(Theplots of nav obj in(a)and(b)areemptybecausethereisnoZS1sentenceofthistypeby 4.5H OWDOESITADAPTTO 3D? Wediscussthepossibilityofadaptingourmodeltoanagentwithsimilarlanguageabilitiesina3D world(e.g.,Beattieetal.(2016);Johnsonetal.(2016)).Thisisourgoalforthefuture,buthere wewouldliketosharesomepreliminarythoughts.Generallyspeaking,a3Dworldwillposea 12 PublishedasaconferencepaperatICLR2018 greaterchallengeforvision-relatedcomputations.Thekeyelementofourmodelistheattention cube x cube thatisintendedforanexplicitlanguagegrounding,includingthechannelmask x feat and theattentionmap x loc .Thechannelmaskonlydependsonthesentence,andthusisexpectedtowork regardlessoftheworld'sdimensionality.Theinterpreterdependsonasequenceofscoremaps ˜ whichfornowarecomputedasmultiplyingawordembeddingwiththefeaturecube(Eq.3).Amore sophisticatedof ˚ willbeneededtodetectobjectsina3Denvironment.Additionally,the interpretermodelsthespatialtransformofattentionasa2Dconvolution(Eq.7).Thisassumption willbenotvalidforreasoning3Dspatialrelationson2Dimages,andabettertransformmethod thataccountsforperspectivedistortionisrequired.Lastly,thesurroundingenvironmentisonly partiallyobservabletoa3Dagent.Aworkingmemory,suchasanLSTMaddedtotheaction module A ,willbeimportantfornavigationinthiscase.Despitethesechangestobemade,we believethatourgeneralexplicitgroundingstrategyandthecommondetectionfunctionsharedby languagegroundingandpredictionhaveshedsomelightontheadaptation. 5C ONCLUSION Wehavepresentedanend-to-endmodelofavirtualagentforacquiringlanguagefroma2Dworld inaninteractivemanner,throughthevisualandlinguisticperceptionchannels.Afterlearning, theagentisabletobothinterpolateandextrapolatetointerpretzero-shotsentencesthatcontain newwordcombinationsorevennewwords.Thisgeneralizationabilityissupportedbyanex- plicitgroundingstrategythatdisentanglesthelanguagegroundingfromthesubsequentlanguage- independentcomputations.Italsodependsonsharingadetectionfunctionbetweenthelanguage groundingandpredictionasthecorecomputation.Thisfunctionenablesthewordmeaningsto transferfromthepredictiontothegroundingduringthetesttime.Promisinglanguageacquisition andgeneralizationresultshavebeenobtainedinthe2D XWORLD .Wehopethatthisworkcanshed somelightonacquiringandgeneralizinglanguageinasimilarwayina3Dworld. A CKNOWLEDGMENTS Wethanktheanonymousreviewersforprovidingvaluablecommentsandsuggestions.Wethankthe otherteammembers,YuanpengLi,LiangZhao,YiYang,ZihangDai,QingSun,JianyuWang,and XiaochenLian,forhelpfuldiscussions.WethankJackFeerickandRichardMarkforproofreading. Finally,wespeciallythankDhruvBatraforhisfeedbackonanearlyversionofthispaper. R EFERENCES PeterAnderson,XiaodongHe,ChrisBuehler,DamienTeney,MarkJohnson,StephenGould,and LeiZhang.Bottom-upandtop-downattentionforimagecaptioningandVQA. arXivpreprint arXiv:1707.07998 ,2017. JacobAndreas,MarcusRohrbach,TrevorDarrell,andDanKlein.Neuralmodulenetworks.In CVPR ,2016a. JacobAndreas,MarcusRohrbach,TrevorDarrell,andDanKlein.Learningtocomposeneural networksforquestionanswering.In ACL ,2016b. JacobAndreas,DanKlein,andSergeyLevine.Modularmultitaskreinforcementlearningwith policysketches.In ICML ,2017. StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZit- nick,andDeviParikh.VQA:Visualquestionanswering.In ICCV ,2015. DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointly learningtoalignandtranslate.In ICLR ,2015. DanielPaulBarrett,ScottAlanBronikowski,HaonanYu,andJeffreyMarkSiskind.Drivingunder the(oflanguage). IEEETransactionsonNeuralNetworksandLearningSystems ,2017. CharlesBeattie,JoelZ.Leibo,DenisTeplyashin,TomWard,MarcusWainwright,HeinrichK ¨ uttler, AndrewLefrancq,SimonGreen,V ´ Vald ´ es,AmirSadik,JulianSchrittwieser,KeithAnder- son,SarahYork,MaxCant,AdamCain,AdrianBolton,StephenGaffney,HelenKing,Demis 13 PublishedasaconferencepaperatICLR2018 Hassabis,ShaneLegg,andStigPetersen.DeepMindlab. arXivpreprintarXiv:1612.03801 , 2016. YoshuaBengio,J ´ er ‹ omeLouradour,RonanCollobert,andJasonWeston.Curriculumlearning.In ICML ,2009. JeromeBruner.Child'stalk:Learningtouselanguage. ChildLanguageTeachingandTherapy ,1 (1),1985. DevendraSinghChaplot,KanthashreeMysoreSathyendra,RamaKumarPasumarthi,DheerajRa- jagopal,andRuslanSalakhutdinov.Gated-attentionarchitecturesfortask-orientedlanguage grounding.In AAAI ,2018. DavidLChenandRaymondJMooney.Learningtointerpretnaturallanguagenavigationinstruc- tionsfromobservations.In AAAI ,2011. KyunghyunCho,BartvanMerrienboer,C¸aglarG ¨ ulc¸ehre,FethiBougares,HolgerSchwenk,and YoshuaBengio.LearningphraserepresentationsusingRNNencoder-decoderforstatisticalma- chinetranslation.In EMNLP ,2014. NoamChomsky.Linguisticsandcognitivescience:Problemsandmysteries.In TheChomskyan Turn .1991. HarmdeVries,FlorianStrub,J ´ er ´ emieMary,HugoLarochelle,OlivierPietquin,andAaronC. Courville.Modulatingearlyvisualprocessingbylanguage.In NIPS ,2017. JohnDuchi,EladHazan,andYoramSinger.Adaptivesubgradientmethodsforonlinelearningand stochasticoptimization. JMLR ,12(Jul),2011. HaoyuanGao,JunhuaMao,JieZhou,ZhihengHuang,LeiWang,andWeiXu.Areyoutalkingtoa machine?datasetandmethodsformultilingualimagequestion.In NIPS ,2015. QiaoziGao,MalcolmDoering,ShaohuaYang,andJoyceYChai.Physicalcausalityofactionverbs ingroundedlanguageunderstanding.In ACL ,2016. MartaGarnelo,KaiArulkumaran,andMurrayShanahan.Towardsdeepsymbolicreinforcement learning. arXivpreprintarXiv:1609.05518 ,2016. StevanHarnad.Thesymbolgroundingproblem. PhysicaD ,42,1990. KarlMoritzHermann,FelixHill,SimonGreen,FuminWang,RyanFaulkner,HubertSoyer,David Szepesvari,WojciechMarianCzarnecki,MaxJaderberg,DenisTeplyashin,MarcusWainwright, ChrisApps,DemisHassabis,andPhilBlunsom.Groundedlanguagelearninginasimulated3d world.In NIPSWorkshop ,2017. RonghangHu,JacobAndreas,MarcusRohrbach,TrevorDarrell,andKateSaenko.Learningto reason:End-to-endmodulenetworksforvisualquestionanswering.In ICCV ,2017. M.Johnson,K.Hofmann,T.Hutton,andD.Bignell.Themalmoplatformforintelligence experimentation.In IJCAI ,2016. I.T.Jolliffe. PrincipalComponentAnalysis .SpringerVerlag,1986. Kempka,MarekWydmuch,GrzegorzRunc,JakubToczek,andWojciechJa ´ skowski.ViZ- Doom:ADoom-basedAIresearchplatformforvisualreinforcementlearning.In IEEEConfer- enceonComputationalIntelligenceandGames ,2016. DouweKiela,LuanaBulat,AnitaL.Vero,andStephenClark.Virtualembodiment:Ascalable long-termstrategyforintelligenceresearch.In NIPSWorkshop ,2016. NikitaKitaevandDanKlein.Whereismisty?interpretingspatialdescriptorsbymodelingregions inspace.In EMNLP ,2017. SatwikKottur,Jos ´ eM.F.Moura,StefanLee,andDhruvBatra.Naturallanguagedoesnotemerge 'naturally'inmulti-agentdialog.In EMNLP ,2017. 14 PublishedasaconferencepaperatICLR2018 ThomasLandauerandSusanDumais.Asolutiontoplato'sproblem:Thelatentsemanticanalysis theoryofacquisition,induction,andrepresentationofknowledge. PsychologicalReview ,104, 1997. JiasenLu,JianweiYang,DhruvBatra,andDeviParikh.Hierarchicalquestion-imageco-attention forvisualquestionanswering.In NIPS ,2016. JunhuaMao,XuWei,YiYang,JiangWang,ZhihengHuang,andAlanLYuille.Learninglikea child:Fastnovelvisualconceptlearningfromsentencedescriptionsofimages.In ICCV ,2015. TomasMikolov,ArmandJoulin,andMarcoBaroni.Aroadmaptowardsmachineintelligence. arXiv preprintarXiv:1511.08130 ,2015. DipendraMisra,JohnLangford,andYoavArtzi.Mappinginstructionsandvisualobservationsto actionswithreinforcementlearning.In EMNLP ,2017. VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBelle- mare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-level controlthroughdeepreinforcementlearning. Nature ,518(7540),2015. JunhyukOh,SatinderP.Singh,HonglakLee,andPushmeetKohli.Zero-shottaskgeneralization withmulti-taskdeepreinforcementlearning.In ICML ,2017. SinnoJialinPanandQiangYang.Asurveyontransferlearning. IEEETrans.onKnowl.andData Eng. ,22(10),October2010. MengyeRen,RyanKiros,andRichardZemel.Exploringmodelsanddataforimagequestion answering.In NIPS ,2015. AnnaRohrbach,MarcusRohrbach,RonghangHu,TrevorDarrell,andBerntSchiele.Groundingof textualphrasesinimagesbyreconstruction.In ECCV ,2016. TomSchaul,JohnQuan,IoannisAntonoglou,andDavidSilver.Prioritizedexperiencereplay.In ICLR ,2016. M.SchusterandK.K.Paliwal.Bidirectionalrecurrentneuralnetworks. IEEETransactionson SignalProcessing ,45(11),1997. JeffreyMarkSiskind.Groundinglanguageinperception. IntelligenceReview ,1994. JeffreyMarkSiskind.Acomputationalstudyofcross-situationaltechniquesforlearningword-to- meaningmappings. Cognition ,1996. JeffreyMarkSiskind.Groundingthelexicalsemanticsofverbsinvisualperceptionusingforce dynamicsandeventlogic. JournalofIntelligenceResearch ,1999. BurrhusFredericSkinner. Verbalbehavior .CopleyPublishingGroup,1957. SainbayarSukhbaatar,ArthurSzlam,GabrielSynnaeve,SoumithChintala,andRobFergus.Maze- base:Asandboxforlearningfromgames. arXivpreprintarXiv:1511.07401 ,2016. RichardSSuttonandAndrewGBarto. Reinforcementlearning:Anintroduction ,volume1.MIT pressCambridge,1998. StefanieTellex,ThomasKollar,StevenDickerson,MatthewRWalter,AshisGopalBanerjee,Seth Teller,andNicholasRoy.Understandingnaturallanguagecommandsforroboticnavigationand mobilemanipulation.In AAAI ,2011. MichaelTomasello. ConstructingaLanguage:AUsage-BasedTheoryofLanguageAcquisition . HarvardUniversityPress,2003. DmitryUlyanov.Multicore-TSNE. https://github.com/DmitryUlyanov/ Multicore-TSNE ,2016. 15 PublishedasaconferencepaperatICLR2018 L.J.PvanderMaatenandG.E.Hinton.Visualizinghigh-dimensionaldatausingt-SNE. Journalof MachineLearningResearch ,9,2008. TerryWinograd.Understandingnaturallanguage. CognitivePsychology ,3(1),1972. WilliamA.Woods.Meaningandlinks. AIMagazine ,28(4),2007. KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCourville,RuslanSalakhudinov,Rich Zemel,andYoshuaBengio.Show,attendandtell:Neuralimagecaptiongenerationwithvisual attention.In ICML ,2015. ZichaoYang,XiaodongHe,JianfengGao,LiDeng,andAlexSmola.Stackedattentionnetworks forimagequestionanswering.In CVPR ,2016. HaonanYuandJeffreyMarkSiskind.Groundedlanguagelearningfromvideodescribedwithsen- tences.In ACL ,2013. 16 PublishedasaconferencepaperatICLR2018 Figure8:TheEuclideandistancematrixofthe134questiongroupswhereeachgroupisrepresented byawordlabel.Eachrow(column)representsthesampledquestionsthathavethewordlabelas theanswer.Amatrixentryindicatestheempiricalexpectationofthedistancebetweenthechannel masksofthesentencesfromtwoquestiongroups.Thelabelsarearrangedintothreetopics: color , object ,and spatialrelation .Asmalldistanceindicatesthatthetwochannelmasksaresimilar.(Zoom inonthescreenforabetterview.) A PPENDICES AV ISUALIZATIONANDANALYSIS Inthissection,wevisualizeandanalyzesomeintermediateresultsofourtrainedmodel. Channelmask x feat . Weinspectthechannelmask x feat whichallowsthemodeltoselectcertain featuremapsfromafeaturecube h andpredictananswertothequestion s .Werandomlysample 10kQAquestionsandcompute x feat foreachofthemusingthegroundingmodule L .Wedivide the10kquestionsinto134groups,whereeachgroupcorrespondstoadifferentanswer. 4 Then wecomputeanEuclideandistancematrix D whereentry D r i;j s istheaveragedistancebetween the x feat ofaquestionfromthe i thgroupandthatfromthe j thgroup(Figure8).Itisclearthat 4 Thewordﬁorangeﬂisbothacolorwordandanobjectword,whichiswhythenumberofgroupsisoneless than119(objects)  8(spatialrelationswithoutﬁbetweenﬂ)  8(colors)=135. 17 PublishedasaconferencepaperatICLR2018 (a)(b) Figure9:Visualizationsofthecomputationofwordattention.(a)Wordcontextvectors w l .(b)The wordattentions o l ofseveralexamplequestions.Eachattentionvector,representedbyacolorbar, showstheattentionaccumulatedover I interpretationsteps. therearethreetopics(object,color,andspatialrelation)inthematrix.Thedistancescomputed withinatopicaremuchsmallerthanthosecomputedacrosstopics.Thisdemonstratesthatwiththe channelmask,themodelisabletolookatdifferentsubsetsoffeaturesforquestionsofdifferent topics,whileusingthesamesubsetoffeaturesforquestionsofthesametopic.Italsoimpliesthat thefeaturecube h islearnedtoorganizefeaturemapsaccordingtoimageattributes. Wordcontext&attention. Tointuitivelydemonstratehowtheinterpreterworks,wevisualizethe wordcontextvectors w l inEq.6foratotalof20kwordlocations l (10kfromQAquestionsand theother10kfromNAVcommands).Eachwordcontextvectorisprojecteddowntoaspaceof50 dimensionsusingPCA(Jolliffe,1986),afterwhichweuset-SNE(vanderMaaten&Hinton,2008; Ulyanov,2016)tofurtherreducethedimensionalityto2.Thet-SNEmethodusesaperplexityof100 andalearningrateof200,andrunsfor1kiterations.Thevisualizationofthe20kpointsisshownin Figure9a.RecallthatinEq.6thewordattentioniscomputedbycomparingtheinterpreterstate p i  1 withthewordcontextvectors w l .Inordertoselectthecontentwordstogeneratemeaningfulscore mapsvia ˚ ,theinterpreterisexpectedtodifferentiatethemfromtheremaininggrammaticalwords basedonthecontexts.Thisexpectationisvbytheabovevisualizationinwhichthecontext vectorsofthecontentwords(inblue,green,andred)andthoseofthegrammaticalwords(inblack) aremostlyseparated.Figure9bshowssomeexamplequestionswhosewordattentionsarecomputed fromthewordcontextvectors.Itcanbeseenthatthecontentwordsaresuccessfullyselectedbythe interpreter. Attentionmap x loc . Finally,wevisualizethecomputationoftheattentionmap x loc .Ineachofthe followingsixexamples,theintermediateattentionmaps x i loc andwordattentions o i l (inEq.7)of thethreeinterpretationstepsareshownfromtoptobottom.Eachstepshowsthecurrentattention map x i loc overlaidontheenvironmentimage e .Thelastattentionmap x 3 loc istheoutputof theinterpreteratthecurrenttime.Notethatnotalltheresultsofthethreestepsareneededto generatetheoutput.Someresultsmightbediscardedaccordingtothevalueoftheupdategate ˆ i .Asaresult,sometimestheinterpretermayproduceﬁbogusﬂintermediateattentionmapswhich donotcontributeto x loc .Eachexamplealsovisualizestheenvironmentterrainmap x terr  inAppendixC)thatperfectlydetectsalltheobjectsandblocks,andthusprovidesagoodguide fortheagenttonavigatesuccessfullywithouthittingwallsorconfoundingtargets.Forabetter visualization,theegocentricimagesareconvertedbacktothenormalview. 18 PublishedasaconferencepaperatICLR2018 Figure10:Theexampleshowinghow x loc iscomputed. 19 PublishedasaconferencepaperatICLR2018 Figure11:Thesecondexampleshowinghow x loc iscomputed. 20 PublishedasaconferencepaperatICLR2018 Figure12:Thethirdexampleshowinghow x loc iscomputed. 21 PublishedasaconferencepaperatICLR2018 Figure13:Thefourthexampleshowinghow x loc iscomputed. 22 PublishedasaconferencepaperatICLR2018 Figure14:Theexampleshowinghow x loc iscomputed. 23 PublishedasaconferencepaperatICLR2018 Figure15:Thesixthexampleshowinghow x loc iscomputed. 24 PublishedasaconferencepaperatICLR2018 Figure16:Allthe 119  3  357 objectinstancesplustheagent(second-to-last)andthewall(last). B XWORLDSETUP XWORLD iswith 7  7 gridmaps.Oneachmap,theopenspacefortheagenthasasize rangingfrom 3  3 to 7  7 .Smalleropenspacesaresetupforcurriculumlearning(AppendixF), butnotfortesting.Tokeepthesizeoftheenvironmentimageed,wepadthemapwithwall blocksiftheopenspacehasasizelessthan 7 .Theagenthasfournavigationactionsintotal: left , right , up ,and down .Ineachsession, I Themaximumnumberoftimestepsisfourtimesthemapsize.Thatis,theagentonlyhas 7  4  28 stepstoreachatarget. II Thenumberofobjectsonthemaprangesfrom1to5. III Thenumberofwallblocksonthemaprangesfrom0to15. IV Thepositiverewardwhentheagentreachesthecorrectlocationis 1 : 0 . V Thenegativerewardsforhittingwallsandforsteppingonnon-targetobjectsare  0 : 2 and  1 : 0 , respectively. VI Thetimepenaltyofeachstepis  0 : 1 . 25 PublishedasaconferencepaperatICLR2018 ObjectSpatialrelationColorOther apple,armadillo,artichoke,avocado,banana,bat,between,blue,?,.,and, bathtub,beans,bear,bed,bee,beet,east,brown,block,by,can, beetle,bird,blueberry,bookshelf,broccoli,bull,north,gray,color,could,destination, b,cabbage,cactus,camel,carpet,carrot,northeast,green,direction,does, cat,centipede,chair,cherry,circle,clock,northwest,orange,go,goal,grid, coconut,corn,cow,crab,crocodile,cucumber,south,purple,have,identify,in, deer,desk,dinosaur,dog,donkey,dragon,southeast,red,is,locate,located, ,duck,eggplant,elephant,fan,southwest,yellowlocation,me,move, fox,frog,garlic,giraffe,westname,navigate,near, glove,goat,grape,greenonion,greenpepper,hedgehog,nothing,object,of, horse,kangaroo,knife,koala,ladybug,lemon,on,one,please, light,lion,lizard,microwave,mirror,monitor,property,reach,say, monkey,monster,mushroom,octopus,onion,orange,side,target,tell, ostrich,owl,panda,peacock,penguin,pepper,the,thing,three, pig,pineapple,plunger,potato,pumpkin,rabbit,to,two,what, racoon,rat,rhinoceros,rooster,seahorse,seashell,where,which,will, seaurchin,shrimp,snail,snake,sofa,spider,you,your square,squirrel,stairs,star,strawberry,tiger, toilet,tomato,triangle,turtle,vacuum,wardrobe, washingmachine,watermelon,whale,wheat,zebra Table1:Theteacher'slexicon. Theteacherhasavocabularysizeof185.Thereare9spatialrelations,8colors,119distinctobject classes,and50grammaticalwords.Everyobjectclasscontains3differentinstances.Allobject instancesareshowninFigure16.Everytimetheenvironmentisreset,anumberofobjectclasses arerandomlysampledandanobjectinstanceisrandomlysampledforeachclass.Therearein total16typesofsentencesthattheteachercanspeak,including4typesofNAVcommandsand12 typesofQAquestions.Eachsentencetypehasseveralnon-recursivetemplates,andcorresponds toaconcretetypeoftaskstheagentmustlearntoaccomplish.Intotalthereare1,639,015distinct sentenceswith567,579forNAVand1,071,436forQA.Thesentencelengthvariesbetween2and 13. Theobject,spatial-relation,andcolorwordsoftheteacher'slanguagearelistedinTable1.These arethecontentwordsthatcanbegroundedin XWORLD .Alltheothersaregrammaticalwords.Note thatthedifferentiationbetweenthecontentandthegrammaticalwordsisautomaticallylearnedby theagentaccordingtothetasks.Everywordisrepresentedbyanentryinthewordembeddingtable. ThesentencetypesthattheteachercanspeakarelistedinTable2.Eachtypehasatriggering conditionaboutwhentheteachersaysthattypeofsentences.Besidestheshownconditions,an extraconditionforNAVcommandsisthatthetargetmustbereachablefromthecurrentagent location.Anextraconditionforcolor-relatedquestionsisthattheobjectcolormustbeoneofthe eightcolors.Ifatanytimesteptherearemultipletypestriggered,werandomlysampleone forNAVandanotherforQA.Afterasentencetypeissampled,wegenerateasentenceaccordingto thecorrespondingsentencetemplates. CI MPLEMENTATIONDETAILS Theenvironmentimage e isa 156  156 egocentricRGBimage.TheCNNin F hasfourconvo- lutionallayers: p 3 ; 3 ; 64 q ; p 2 ; 2 ; 64 q ; p 2 ; 2 ; 256 q ; p 1 ; 1 ; 256 q ,where p a;b;c q representsalayer urationof c kernelsofsize a appliedatstridewidth b .AllthefourlayersareReLUactivated.To enabletheagenttoreasonaboutspatial-relationwords(e.g.,ﬁnorthﬂ),weappendanadditionalpara- metriccubetotheconvolutionaloutputtoobtain h .Thisparametriccubehasthesamenumberof channelswiththeCNNoutput,anditisinitializedwithazeromeanandazerostandarddeviation. Thewordembeddingtableisinitializedwithazeromeanandaunitstandarddeviation.Allthegated RNNs(includingthebidirectionalRNN)in L have 128 units.Allthelayersin L ,unlessotherwise stated,usetanhastheactivationfunction. 26 PublishedasaconferencepaperatICLR2018 SentenceTypeExampleTriggeringCondition nav obj ﬁPleasegototheapple.ﬂ[C0]Beginningofasession.& [C1]Thereferenceobjecthasaunique nameintheenvironment. nav col obj ﬁCouldyoupleasemovetotheredapple?ﬂ[C0]&[C2]Therearemultipleobjects thateitherhavethesamenamebut differentcolors,orhavedifferent namesbutthesamecolor. nav nr obj ﬁThenorthoftheappleisyourdestination.ﬂ[C0]&[C1] nav bw obj ﬁNavigatetothegridbetweenappleand[C0]&[C3]Bothreferenceobjects bananaplease.ﬂhaveuniquenamesintheenvironment andareseparatedbyoneblock. rec col2obj ﬁWhatistheredobject?ﬂ[C4]Thereisonlyoneobjectthathas thecolor. rec obj2col ﬁWhatisthecoloroftheapple?ﬂ[C1] rec loc2obj ﬁPleasetellthenameoftheobjectinthesouth.ﬂ[C5]Theagentisnearthereference object. rec obj2loc ﬁWhatisthelocationoftheapple?ﬂ[C1]&[C5] rec loc2col ﬁWhatcolordoestheobjectintheeasthave?ﬂ[C5] rec col2loc ﬁWhereistheredobjectlocated?ﬂ[C4]&[C5] rec loc obj2obj ﬁIdentifytheobjectwhichisintheeastoftheapple.ﬂ[C1]&[C6]Thereisanobjectnearthe referenceobject. rec loc obj2col ﬁWhatisthecoloroftheeasttotheapple?ﬂ[C1]&[C6] rec col obj2loc ﬁWhereistheredapple?ﬂ[C2]&[C5] rec bw obj2obj ﬁWhatistheobjectbetweenappleandbanana?ﬂ[C7]Bothreferenceobjectshaveunique namesintheenvironmentandare separatedbyablock. rec bw obj2loc ﬁWhereistheobjectbetweenappleandbanana?ﬂ[C7]&[C8]Theagentisneartheblock whichisbetweenthetworeference objects. rec bw obj2col ﬁWhatisthecoloroftheobjectbetweenapple[C7] andbanana?ﬂ Table2:Allthesixteensentencetypesoftheteacher. ForNAV, x loc isusedasthetargettoreachontheimageplane.However,knowingthisalonedoesnot sufTheagentmustalsobeawareofwallsandpossiblyconfoundingtargets(otherobjects)in theenvironment.Towardthisend, M A furthercomputesanenvironmentterrainmap x terr  ˙ p hf q where f P R D isaparametervectortobelearnedand ˙ issigmoid.Weexpectthat x terr detects anyblocksinformativefornavigation.Notethat x terr isunrelatedtothecommand;itsolely dependsonthecurrentenvironment.Afterstacking x loc and x terr together, M A feedsthemtoanother CNNfollowedbyanMLP.TheCNNhastwoconvolutionallayers p 3 ; 1 ; 64 q and p 3 ; 1 ; 4 q ,bothwith paddingsof 1 .Itisfollowedbyathree-layerMLPwhereeachlayerhas 512 unitsandisReLU activated. Theactionmodule A containsatwo-layerMLPofwhichthelayerhas 512 ReLUactivated unitsandthesecondlayerissoftmaxwhoseoutputdimensionisequaltothenumberofactions. Weuseadagrad(Duchietal.,2011)withalearningrateof 10  5 forstochasticgradientdescent (SGD).Therewarddiscountfactorissetto 0 : 99 .Alltheparametershaveadefaultweightdecay of 10  4  16 .Foreachlayer,itsparametershavezeromeanandastandarddeviationof 1 { ? K , where K isthenumberofparametersofthatlayer.Wesetthemaximuminterpretationstep I  3 . Thewholemodelistrainedendtoend. DB ASELINEDETAILS SomeadditionalimplementationdetailsofthebaselinesinSection4.3aredescribedbelow. [CA] ItsRNNhas 512 units. 27 PublishedasaconferencepaperatICLR2018 Figure17:Anoverviewofthebaseline VL .ThecomputationsofNAVandQAonlydifferinthe lastMLPs. Figure18:Anoverviewofthebaseline CE .ThecomputationsofNAVandQAonlydifferinthe lastMLPs. [VL] ItsCNNhasfourconvolutionallayers p 3 ; 2 ; 64 q , p 3 ; 2 ; 64 q , p 3 ; 2 ; 128 q ,and p 3 ; 1 ; 128 q .This isfollowedbyafully-connectedlayerofsize 512 ,whichprojectsthefeaturecubetotheword embeddingspace.TheRNNhas 512 units.ForeitherQAorNAV,theRNN'slaststategoes throughathree-layerMLPofwhicheachlayerhas 512 units(Figure17). [CE] Ithasthesamelayer-sizewith VL (Figure18). [SAN] ItsRNNhas 256 units.Followingtheoriginalapproach(Yangetal.,2016),weusetwo attentionlayers. AllthelayersoftheabovebaselinesareReLUactivated. EE XPLORATIONAND E XPERIENCE R EPLAY Theagenthasonemillionexplorationstepsintotal,andtheexplorationrate  decreaseslinearly from 1 to 0 : 1 .Ateachtimestep,theagenttakesanaction a Pt left , right , up , down u witha probabilityof   1 4 p 1   q ˇ  p a | s;e q ; where ˇ  isthecurrentpolicy,and s and e denotethecurrentcommandandenvironmentimage, respectively.Tostabilizethelearning,wealsoemployexperiencereplay(ER)(Mnihetal.,2015). Theenvironmentinputs,rewards,andtheactionstakenbytheagentinthemostrecent10ktimesteps arestoredinareplaybuffer.Duringtraining,eachtimeaminibatch t a i ;s i ;e i ;r i u N i  1 issampled fromthebuffer,usingtherank-basedsampler(Schauletal.,2016)whichhasproventoincreasethe trainingefybyprioritizingrareexperiences.Thenwecomputethegradientas:  N ¸ i  0  r  log ˇ  p a i | s i ;e i q r  v  p s i ;e i q  r i  v   p s 1 i ;e 1 i q v  p s i ;e i q  ; where i isthesampleindexinthebatch, s 1 i and e 1 i arethecommandandimageinthenexttimestep, v isthevaluefunction,  arethecurrentparameters,   arethetargetparametersthathaveanupdate delay,and  isthediscountfactor.Thisgradientmaximizestheexpectedrewardwhileminimizing thetemporal-difference(TD)error.NotethatbecauseofER,ourACmethodisoff-policy.Toavoid introducingbiasesintothegradient,importanceratiosareneeded.However,weignoredthemin theabovegradientforimplementationsimplicity.Wefoundthatthecurrentimplementationworked wellinpracticeforourproblem. 28 PublishedasaconferencepaperatICLR2018 FC URRICULUMLEARNING Weexploitcurriculumlearning(Bengioetal.,2009)tograduallyincreasetheenvironmentcomplex- itytohelptheagentlearn.Thefollowingquantitiesareincreasedinproportionalto min p 1 ;G 1 { G q , where G 1 isthenumberofsessionstrainedsofarand G isthetotalnumberofcurriculumsessions: I Thesizeoftheopenspaceontheenvironmentmap. II Thenumberofobjectsintheenvironment. III Thenumberofwallblocks. IV Thenumberofobjectclassesthatcanbesampledfrom. V ThelengthsoftheNAVcommandandtheQAquestion. Wefoundthatthiscurriculumisimportantforaneflearning.,thegradualchanges ofquantitiesIVandVaresupportedbytheofSiskind(1996)thatchildrenlearnnewwords inalinguisticcorpusmuchfasterafterpartialexposuretothecorpus.Intheexperiments,weset G  25kduringtrainingwhiledo not haveanycurriculumduringtest(i.e.,testingwiththemaximum dif 29  
InternationalJointConferenceonNaturalLanguageProcessing ,pages596Œ604, Nagoya,Japan,14-18October2013. 596 597 598 599 600 601 602 603 604  
De-BiasingCovariance-RegularizedDiscriminantAnalysis HaoyiXiong y ; + ,WeiCheng x ,YanjieFu z ; y ,WenqingHu z ,JiangBian z ; y ,ZhishanGuo z y BaiduInc.,Beijing,China + NationalEngineeringLaboratoryofDeepLearningTechnologyandApplication,Beijing,China z MissouriUniversityofScienceandTechnology,MO,UnitedStates x NECLaboratoriesAmerica,NJ,UnitedStates Abstract Fisher'sLinearDiscriminantAnalysis(FLD)isa well-knowntechniqueforlinearfea- tureextractionanddimensionreduction.Theem- piricalFLDreliesontwokeyestimationsfromthe dataŒthemeanvectorforeachclassandthe(in- verse)covariancematrix.Toimprovetheaccuracy ofFLDundertheHighDimensionLowSampleSize (HDLSS)settings, Covariance-RegularizedFLD (CRLD)hasbeenproposedtouseshrunkencovari- anceestimators,suchasGraphicalLasso,tostrike abalancebetweenbiasesandvariances.Though CRLDcouldobtainbetteraccuracy, itusuallyincursbiasandconvergestotheoptimal resultwithaslowerasymptoticrate.Inspiredby therecentprogressinde-biasedLasso,wepropose anovelFLD, DBLD ,whichimproves accuracyofCRLDthrough de-biasing . Theoreticalanalysisshowsthat DBLD possesses betterasymptoticpropertiesthanCRLD.Wecon- ductexperimentsonbothsyntheticdatasetsandreal applicationdatasetstothecorrectnessofour theoreticalanalysisanddemonstratethesuperior- ityof DBLD overclassicalFLD,CRLDandother downstreamcompetitorsunderHDLSSsettings. 1Introduction Fisher'sLinearDiscriminantAnalysis(FLD) [ Duda etal. , 2001 ] isawell-knowntechniqueforfeatureextractionand dimensionreduction [ Kulisandothers,2013 ] .Ithasbeen widelyusedinmanyapplications,suchasfacerecognition [ PeckandVanNess,1982 ] ,imageretrieval,etc.Anintrinsic limitationofclassicalFLDisthatitsobjectivefunctionrelies onthe well-estimated and non-singular covariancematrices. Formanyapplications,suchasthemicro-arraydataanalysis, allscattermatricescanbe singular or ill-posed sincethedatais oftenwithhighdimensionbutlowsamplesize(HDLSS) [ Cai etal. ,2016 ] . TheclassicalFLDreliesontwokeyparametersŒ themeanvectorofeachtypeandtheprecisionmatrix.Under theHDLSSsettings,thesampleprecisionmatrix(a.k.a.,the inverseofsamplecovariancematrix)usedinFLDisusually ill-estimatedandquitedifferentfromtheinverseofpopula- tion/truecovariancematrix [ Cai etal. ,2016 ] .Forexample, thelargesteigenvalueofthesamplecovariancematrixisnot aconsistentestimateofthelargesteigenvalueofthepopu- lationcovariancematrix,andtheeigenvectorsofthesample covariancematrixcanbenearlyorthogonaltothetruthwhen thenumberofdimensionsisgreaterthanthenumberofsam- ples [ Mar  cenkoandPastur,1967 ] .Suchinconsistencybe- tweenthetrueandtheestimatedprecisionmatricesdegrades theaccuracyofFLDundertheHDLSSsettings [ Zol- lanvariandDougherty,2013 ] . Aplethoraofexcellentworkhasbeenconductedtoad- dresssuchHDLSSdataproblemforFLD.For example,Krzanowskietal. [ Krzanowski etal. ,1995 ] sug- gestedtousepseudo-inversetoapproximatetheinverseco- variancematrix,whenthesamplecovariancematrixissin- gular.However,theprecisionofpseudo-inverseFLDisusu- allylowandnotwellguaranteed.Othertechniquesinclude thetwo-stagealgorithmPCA+FLD [ Ye etal. ,2004 ] ,FLD basedonKernels [ Zhangandothers,2003 ] and/orothernon- parametricstatistics [ KaskiandPeltonen,2003 ] .Toover- comethesingularityofthesamplecovariancematrices,in- steadofestimatinginversecovariancematrixandmeanvec- torsseparately, [ CaiandLiu,2011 ] proposedtoestimate theprojectionvectorfordiscriminationdirectly.Morepopu- larly,regularizedFLDapproaches [ Krzanowski etal. ,1995 ; WittenandTibshirani,2009 ] areproposedtosolvetheprob- lem.ThesemethodscanimprovetheperformanceofFLD eitherempiricallyortheoretically [ DurrantandKab ´ an,2015 ; Bickel etal. ,2004 ] ,whilefewofthemcandirectlyaddress theill-estimatedinversecovariancematrixestimationissue. Onerepresentativeregularizationapproachis Covariance- RegularizedFLD [ WittenandTibshirani,2009 ] thatreplaces theprecisionmatrixusedinFLDwithashrunkenestimator, suchasGraphicalLasso [ Friedman etal. ,2008 ] ,soasto achievea ﬁsuperiorpredictionﬂ .Intuitively,throughreplacing precisionmatrixusedinFLDwithasparseregularizedesti- mation,theill-posedproblemcausedbytheHDLSSsettings canbewelladdressed.Thesparseestimatorsusuallyconverge totheinverseoftrue/populationcovariancematrixfasterthan thesampleestimators [ Cai etal. ,2016 ] .Withtheasymptotic properties,thesparseFLDshouldbeclosetotheoptimalFLD. However,thewaythatthesparsityandtheconvergencerateof theprecisionmatrixestimatorwouldaffectthe accuracyisnotwellstudiedinliterature. Further,withinducedsparsity,theinversecovarianceesti- matorbecomesbiased [ ZhangandZhang,2014 ] .Theperfor- manceofsparseFLDisfrequentlybottleneckedduetothe biasofthesparseestimators.Recently,researcherstriedto de-biastheLassoestimator [ ZhangandZhang,2014 ] ,through adjustingthe ` 1 -penaltyfortheregularizedestimation,soas toachieveabetterregressionperformance.Inspiredbythis lineofresearch,weproposetoimprovesparseFLDthrough de-biasing(i.e.,inthispaper. OurContributions. Withrespecttotheaforementionedis- sues,inthispaper,wemadethefollowingcontributions. 1. InspiredbyDe-biasedLasso [ JavanmardandMonta- nari,2014 ] ,westudytheproblemofde-biasingthe Covariance-RegularizedFLD(CRLD),whichhasbeen widely-usedforempiricalsparseFLDestimation,forper- formanceimprovement.Tothebestofourknowledge, thisistheworkaimingatde-biasingCRLD. 2. Weproposeanovelalgorithm DBLD Œ D e- B iased Fisher's L inear D iscriminantAnalysisontopofCRLD. DBLD leveragesyet-anotherDe-BiasedEstimatorfor linear problem,tore-balancethevariances andbiasesoftheestimation,throughde-sparsifyingthe projectionvectorobtainedbyCRLD. 3. Ourtheoreticalanalysisshows,undercertainmildas- sumptions, DBLD convergesfasterthanCRLDwith sharpasymptoticrate.Wealsoconductextensiveex- perimentstodemonstratetheadvantageoftheproposed algorithmsoverothercompetitors.Theresultsvalidate thecorrectnessofourtheoreticalanalysis. Notations. Followingkeynotationsareusedintherestof thispaper:Givena p -dimensionalvector v 2 R p ,wede- notethe ` P vector-normas j v j P =( P m i =1 j v i j P ) 1 = P ( P is anon-negativeinteger)andthe ` 1 vector-normas j v j 1 = max 1  i  m fj v i jg .Givenamatrix A 2 R m 1  m 2 ,wedenote the ` P matrix-normas jj A jj P =max v 2 R p fj Av j P = j v j P g . Notethatthesymbol p referstothenumberofdimensions ofthedatawhile m referstothenumberofsamples.The operator O p (  ) referstothebig-O-notationinhighprobability. 2Preliminaries Inthissection,weintroducethebinaryclassi- usingFLD,thenpresentthepracticeofCRLDbasedon GraphicalLasso. 2.1FLDforBinary TousetheFisher'sLinearDiscriminantAnalysis(FLD),given the i:i:d: labeleddatapairs ( x 1 ;` 1 ) ::: ( x m ;` m ) ,we estimatethesamplecovariancematrix   usingthepooled samplecovariancematrixestimatorwithrespecttothetwo classes [ Duda etal. ,2001 ] ,thenestimatethesampleprecision matrixas  =    1 .Further,   + and    areestimatedasthe meanvectorsofthepositivesamplesandthenegativesamples inthe m trainingsamples,respectively. Givenallestimatedparameters   (and  =    1 ),   + and    ,theFLDmodelanewdatavector x astheresult of:  f ( x )=argmax `  ; + g  ( x;   ;   ` ;ˇ ` ) ; where  ( x;   ;   ` ;ˇ ` )= x >    `  1 2   > `    ` +log ˇ ` ; (1) where ˇ + and ˇ  refertothe(foreknown)frequenciesof positivesamplesandnegativesamplesinthewholepopulation, respectively. 2.2Covariance-RegularizedFLDviaGraphical Lasso Thisalgorithm,referredtoastheCovariance-Regularized FLD(CRLD)viaGraphicalLasso,wasderivedfromthe Scout family ofFLDintroducedby Wittenetal. in [ WittenandTib- shirani,2009 ] .ComparedtotheclassicalFLD,thisbaseline algorithmleveragesGraphicalLassoestimatortoreplacethe precisionmatrixestimatedusingsamplecovariancematrix. Theproposedalgorithmisimplementedusingthediscriminant functioninEq. 1 ,as: b f ( x )=argmax `  ; + g  ( x; b  ;   ` ;ˇ ` ) ; (2) where b  referstotheGraphicalLassoestimatorbasedonthe samplecovariancematrix   : b =argmin  > 0 0 @ tr(    logdet(+  X j 6 = k j  jk j 1 A : (3) Notethat,asalinear,theCRLDdecisionrule introducedinEq. 2 canbere-formulatedinalinearmodel, suchas: b f ( x )=sign   ( x; b  ;   + ;ˇ + )   ( x; b  ;    ;ˇ  )  =sign  x > b  G + c g  ; (4) where sign(  ) functionreturns +1 iftheinputisnon-negative, and  1 whentheinputisnegative.Thevector b  G = b   +     ) andthescalar c g =  1 2  (  + +   ) > b  G + log ( ˇ + =ˇ  ) . Obviously, b  G isthevectorofprojectioncoefforlinear  Inthispaper,wepresenttheanalyticalresults(i.e.,statisti- calrateofconvergencethat b  G approximatestotheoptimal projectionvectorwithvaryingnumberofsamples n anddi- mensions p )ofCRLDin Theorem1 . 3TheProposedAlgorithm Inthissection,weintroduceourproposedalgorithm DBLD Š D e- B iasedFisher's L inear D iscriminantAnalysis(viaGraph- icalLasso),thenpresentthetheoreticalanalysisonthetheo- reticalpropertiesoftheproposedalgorithms. 3.1 DBLD :TheDe-BiasedEstimationfor Covariance-RegularizedFLD Giventhe i:i:d: labeleddatapairs ( x 1 ;` 1 ) ::: ( x m ;` m ) drawn fromthetwoclasseswithcertainpriors,asshowninAlgo- rithm 1 .Thealgorithm (i) estimatesthesampleestimation ofcovariancematricesandthemeanvectors,then (ii) lever- agesCRLDtoestimatetheshrunkenprojectionvector b  G . Further, DBLD (iii) proposesade-biasedestimator(denoted as DeBias function)tode-bias b  G andobtaintheprojection vector b  D .Finally,weintroduceadecisionrulethatenables usingtheestimated b  D . Algorithm1 DBLD EstimationAlgorithm 1: procedure DBLD ( ( x 1 ;` 1 ) ::: ( x m ;` m ) ) 2: /*(i)SampleEstimatorsforMeanandCovariance*/ 3: X +   PositiveSampleSet(( x 1 ;` 1 ) :: ( x m ;` m )) ; 4: X    NegativeSampleSet(( x 1 ;` 1 ) :: ( x m ;` m )) ; 5:   +   1 j X + j  P x 2 X + x ,      1 j X  j  P x 2 X  x ; 6:   +   1 j X + j  P x 2 X + ( x    + )( x    + ) > ; 7:      1 j X  j  P x 2 X  ( x     )( x     ) > ; 8:     j X +    + + j X      j X + j + j X  j ,     j X +    + + j X      j X + j + j X  j ; 9: /*(ii)CRLDEstimator(toobtain b  G )*/ 10: b    GraphicalLasso(   ; ) ; 11: b  G   b   +     ) ; 12: /*(iii) DBLD Estimator(toobtain b  D )*/ 13: X   [ x 1 ;x 2 ;:::x m ] ; /* p  m matrix*/ 14: L   [ ` 1 ;` 2 ;:::` m ] > ; /* m  1 matrix*/ 15: U   [   :::   ] ; 16: /* U isan m  p matrix,everycolumnis   */ 17: c     > b  G ; 18: C   [ c;c;:::;c ] > ; 19: /* C isa m  1 matrix,everyrowis c */ 20: b  D   b  G + 1 m  b ( X  U )  2  L  X > b  G  C  21: return b  D ; Inthefollowingsection,wepresentthedesignoftheDe- BiasedEstimator(denotedas DeBiasing functioninAlgo- rithm 1 )toobtain b  D ,thenintroducethedecisionrulefor optimalLaterweanalyzethetheoreticalproper- tiesof b  D . TheDe-BiasedEstimator InspiredbytheDe-biasedLasso [ JavanmardandMonta- nari,2014 ] ,weproposetoimprovetheperformanceof CRLDthroughde-biasing  G .Given m labeledtraining data ( x 1 ;` 1 ) ; ( x 2 ;` 2 ) ;::: ( x m ;` m ) withbalancedlabels,the GraphicalLassoestimator b  onthedataandtheCRLDmodel (i.e., ^  G ),weproposeanovelde-biasedestimatorof ^  D that takestheformas b  D   b  G + 1 m  b ( X  U )  2  L  X > b  G  C  ; (5) wherewedenote X asan p  m matrixwhere 1  i  m andthe i th columnis x i ; L asan m  1 matrix(i.e.,vector) whose i th rowis ` i 2 1 g ; U isa p  m matrixwhereeach columnis   (asline7inAlgorithm 1 );and C isan m  1 matrixwhereeachrowis c (asline16inAlgorithm 1 ). The DBLD . Giventhede-biasedestimator b  D ,the DBLD the inputvector x usingthefollowingrule: b f D ( x )=sign    x >    + +   2  > b  D +log( ˇ + =ˇ  ) ! : (6) Inthefollowingsection,wepresenttheanalyticalresults of DBLD ,includingthecomputationalcomplexityofde- biasingandstatisticalrateofconvergence. 3.2ComplexityAnalysisof DBLD Inthissection,weanalyzethecomputationalcomplexityfor thethreestepsof Algorithm 1.Thestep(i)estimatesthesam- plecovariancematricesandmeanvectors,whichconsumesat most O ( p 2  m ) operations.Thestep(ii)performsGraphical Lassoandmatrixmultiplication,wherethecomplexitybased onstandardimplementation [ Friedman etal. ,2008 ] isupper- boundedby O ( p 3 ) .Thestep(iii)de-biasingisimplemented asanexactformulawith O ( p 2 ) complexity. Remark1. Allthreestepsof Algorithm 1arescalableon boththenumberofdimensions( p )andthenumberoftrain- ingsamples( m ).Theoverallcomplexityofthethreesteps is O ( p 3 + p 2  m ) .UndertheHDLSSsetting p>m ,the computationalcomplexityof DBLD isupper-boundedby O ( p 3 ) .Ontheotherhand,withlargesamplesettingwhere m  p ,theworstcasecomputationalcomplexityof DBLD isboundedby O ( p 2  m ) .Obviously,theproposedde-biasing estimator(i.e.,step(iii))withcomplexity O ( p 2 ) wouldnot boundtheperformance,whencomparedtothetwosteps. 3.3ConvergenceAnalysisof DBLD Inordertoanalyzetheperformanceof DBLD ,we thelinearprojectionvectoroftheoptimalFLDas   .Given m samples ( x 1 ;` 1 ) ;::: ( x m ;` m ) drawni.i.d.from N (   + ;   ) and N (    ;   ) withtheequalpriorsfortraining,theopti- malprojectionvectorshouldbe   =  (   +     ) and   =  1 .Weintendtounderstandhowclose b  G and b  D approximatetotheoptimalestimation   . Assumption1. Wefollowtheassumptionsmadein [ Rothman etal. ,2010 ] thatapositiveconstant K having 1 = K  min   )   max   ) K exists.Theoperators  min (  ) and  max (  ) denotethesmallest andlargesteigenvaluesrespectively.Inthisway,thereexists k   k 2 K and k   k 2 K . Assumption2. Wefurtherfollowtheassumptionthat,the datavectorsfortrainingareallrealizedfromarandomvector X andthereexistsanconstant B having j X j 2 B .Thus therehas j   + j 2 B and j    j 2 B . Theorem1. Withappropriatesettingoftuningparameter   p log p=m (inEq 3 ),the ` 2 -vector-normconvergence rateofCRLD b  G approximatingtotheoptimalestimation   is: j b  G    j 2 = O p   r ( p + d )log p m ! ; (7) where d = max 1  i  p jf j :   1 i;j 6 =0 gj referstothemaxi- maldegreeofthegraph(i.e.,populationinversecovariance matrix). Proof. Here,weprovetheupperboundof j b  G    j 1 . Aswas b  G = b   +     ) ,thenwehave: j b  G    j 2 = j b   +     )    (   +     ) j 2 : (8) Consideringtheinequities j x + y j 2 j x j 2 + j y j 2 and j Ax j 2  jj A jj 2 j x j 2 ,wehave j b  G    j 2 jj ( b     ) jj 2 j   +     j 2 + k   k 2  j   +    + j 2 + j        j 2  : (9) Accordingto [ Rothman etal. ,2010 ] ,when   p log p=m; weconsiderthespectral-normconvergencerate k b     k 2  k b     k F = O p ( p ( p + d )  log p=m ) ,theasymptoticrate ofsamplemeanvector [ DasGupta,2008 ] is j   +    + j 2 = O p ( p p=m ) and j        j 2 = O p ( p p=m ) ,withtheincreas- ingnumberofdimensions p andnumberofsamples m . Further,therehas k   k 2 K ( Assumption1 )and ` 2 - normsofallmeanvectorsareboundedby B .Inthisway,there mustexistpositiveconstants C 1 and C 2 having: j b  G    j 2  C 1  2 B r ( p + d )log p m + C 2 K r p m : (10) Thus,accordingtotheofasymptoticrate,wecon- cludetheconvergencerateas: j b  G    j 2 = O p   r ( p + d )log p m ! : (11) Theorem2. Withappropriatesettingoftuningparameter  (inEq 3 ),the ` 2 -vector-normconvergencerateof DBLD b  G approximatingtotheoptimalestimation   is: j b  D    j 2 = O p   r p log p m ! : (12) Proof. Here,weprovetheupperboundof j b  D    j 1 .Con- sidertheofthede-biasedFLDestimator b  D intro- ducedinEq. 5 ,wehave b  D = b  G + 2 m  b  XL  2 m  b  UL  1 m  b  X  U )( X  U ) > b  G : (13) Withtheassumptionofequalpriors( ˇ + = ˇ  =0 : 5 ), L isa m  1 labelmatrixthathalfofitselementsare +1 whilethe restareall  1 . X referstoa p  m matrix,whereeachcolumn isasampleofdatae.g., x 1 ;x 2 ;:::;x m .Aswas b  G = b   +     )= 2 m  b  XL .As U isamatrixinwhicheach columnisaconstantvector (  + +   ) = 2 and L isavector withhalfelementsas 1 andhalfelementsas  1 ,thus 2 m  b  UL = 2 m  b  UL )= 0 .Aseachcolumnof X refers toasampledrawnfromtheoriginaldatadistribution,thus 1 m ( X  U )( X  U ) > =   s isthesamplecovariancematrix estimator.Withallaboveinmind,wehave b  D = b  G +  I  b    s  b  G ; (14) where I referstoa p  p identitymatrix.Notethat  I  b     b  G canbeconsideredastheterm thatde-biases b  G .Thus,consideringtheasymptoticrate ofsamplemeanvector [ DasGupta,2008 ] is j   +    + j 2 = O p ( p p=m ) and j        j 2 = O p ( p p=m ) ,wehave j b  D    j 2         2  I  b    s  b           2 j   +     j 2 + j   (  +    +     +    ) j 2  2 B        2  I  b    s  b           2 + C 2 K r p m : (15) Accordingto [ Jankova etal. ,2015 ] ,withappropriatesetting of  ,thespectral-normconvergencerateofthe estimator b  D =  2  b   b    s b   undermildconditions shouldbe k b  D    k 1 = O p ( p log p=m ) ,thenthereexists k b  D    k 2 = O p ( p p log p=m ) ,withthevaryingnumber ofdimensions p andnumberofsamples m .Inthisway,with highprobability,weconcludetheconvergencerate: j b  D    j 2 = O p   r p log p m ! : (16) Remark2. ComparedtoCRLD'sprojectionvector b  G , ourmethod DBLD recoversthelinearprojectionvector b  D withafasterasymptoticrate,i.e., p p log p=m v.s. p ( p + d )log p=m inamildcondition.Thus,itwouldben- tosomeapplications,suchasdimensionalityreduction andfeatureselection.Ourlaterexperimentalresultsshow that DBLD outperformsCRLDwithhigher accuracy,duetothefasterstatisticalrateofconvergence. Remark3. Theproposedalgorithmprovidesa sub-optimal solution,whencomparedto [ CaiandLiu,2011 ] .Ourworkin- tendtoproposeanestimatorof   throughapproximating   ,   + and    separately,while [ CaiandLiu,2011 ] approximates b   straightforwardlyviaso-calledﬁdirectestimationﬂ. 4Experiments Inthissection,wevalidatedifferentpropertiesof DBLD onthesynthesizeddata.Then,weexperimentallyevaluate theperformanceof DBLD usingseveralreal-worlddatasets. Experimentsshowthesuperiorityof DBLD . 4.1SynthesizedDataEvaluation Tovalidateouralgorithms,weevaluateouralgorithmson asynthesizeddataset(importedfrom [ CaiandLiu,2011 ] ), whichisobtainedthroughapseudo-randomsimulation.The (a) DBLD vs.CRLD (b)  Tuning Figure1:Accuracyof DBLD vs.CRLDonPseudo-RandomSynthesizedData syntheticdataaregeneratedbytwoGaussiandis- tributions N (   + ;   ) and N (    ;   ) withequalpriors.The settingsof   + ,    and   areasfollows:   isa p  p symmet- ricandpositivmatrix,whereeachelement   i;j = 0 : 8 j i  j j , 1  i  p and 1  j  p .   + and    areboth p - dimensionalvectors,where   + = h 1 ; 1 ;:::; 1 ; 0 ; 0 ;:::; 0 i T (the 10 elementsareall1's,whiletherest p  10 elements are0's)and    = 0 .Inourexperiment,weset p =200 .To simulatetheHDLSSsettings,wetrainCRLDand DBLD , with20to200samplesrandomlydrawnfromthedistributions withequalpriors,andtestthetwoalgorithmsusing500ran- domlygeneratedsamples.Foreachsettings,werepeatthe experimentsfor100timesandreporttheaveragedresults,ina cross-validationmanner. Inthisexperiment,wecompare DBLD ,CRLDandFLD (withpseudoinverse).TheresultsofFLDisnotincluded here,asitperformsextremelyworsethanbothCRLDand DBLD undertheHDLSSsettings.Figure. 1 (a)presentsthe comparisonbetween DBLD andCRLD,intermsofaccuracy, whereeachalgorithmistunedwiththebestparameter  .Adetailedexampleofparametertuningisreportedin Figure. 1 (b),wherewerunbothalgorithms,withtrainingset sizeas160,whenvarying  from 1 to 70 .FromFigure. 1 (a),it isobviousthat DBLD outperformsCRLDmarginally.The  tuningcomparisonaddressedinFigure. 1 (b)showsthat,given asmall  ,bothCRLDand DBLD cannotperformwell,asthe sparseapproximationof b  G and b  D cannotbewellrecovered insuchcase [ WittenandTibshirani,2009 ] .When   6 , DBLD startsoutperformingCRLD,whiletheadvantageof DBLD toCRLDdecreaseswhenincreasing  .However, evenwithanextremelylarge  , DBLD stilloutperforms CRLD.InFigure 2 (a),wepresenttheevaluationresultsbased onunbalanceddatasets,wheretheaccuracyofalgorithms using m =160 trainingsamplesdrawnwithvaryingpriors isillustrated.Theproportionofpositivetrainingsamplesis varyingfrom10%to40%.Itisobviousthatallalgorithms achievetheirbestperformancewhentheproportionofpositive trainingsampleis10%(themostunbalancedcase). Tofurtherverifyouralgorithms,weproposetheoptimal FLD   =  (   +     ) ,whichisallbasedon thepopulationparameters.Wecomparethe b  D , b  G and   estimatedby DBLD ,CRLDandFLD(withpseudo-inverse) to   .Figure. 2 (b)presentsthecomparisonamong j b  D    j 1 , j b  G    j 1 and j      j 1 .Itisobviousthat b  D ismore closeto   than b  G and   .Thisobservationfurtherv the Theorem1 and 2 .Wealsocomparetheaccuracyof   to CRLD, DBLD andFLD.   outperformsthesealgorithms andtheaccuracyof   isaround84.4%Itisreasonableto concludethat DBLD outperformsCRLD,because b  D is morecloseto   . 4.2BenchmarkEvaluationResults InFigure. 3 (a),wecompare DBLD andotherFLDalgo- rithms,includingFLDwithpseudo-inverse,SparseFLDvia GraphicalLasso(CRLD)andYe-FLDderivedfrom [ Ye et al. ,2004 ] ,ontheWebdatasets [ Lin,2017 ] .Tosimulatethe HDLSSsettings( p ˛ m ),wevarythetrainingsamplesizes from30to120whileusing400samplesfortesting.Thenum- bersofdimensions p is300.Foreachalgorithm,reported resultisaveragedover100randomlyselectedsubsetsofthe training/testingdatawithequalpriors.CRLDand DBLD are withthebest  .Theexperimentalsettingsshow that DBLD consistentlyoutperformsothercompetitorsin differentsettings.Thenon-monotonictrendofFLDwiththe increasingtrainingsetsizeispartiallyduetothepoorperfor- manceofpseudoinverseusedinFLD. InadditiontoFLDwealsocompared DBLD withotherdownstreamalgorithmsincluding DecisionTree , RandomForest , LinearSupportVectorMachine(SVM) and KernelSVMwithGaussianKernel .Thecomparisonresults arelistedinFigure. 3 (b).Allalgorithmsarewith thebestparametersunderourexperimentsettings. 4.3EarlyDetectionofDiseasesonEHRDatasets Todemonstratetheeffectivenessof DBLD inhandlingthe realproblems,weevaluate DBLD onthereal-worldElec- tronicHealthRecords(EHR)dataforearlydetectionofdis- (a)AccuracyonUnbalancedDatasets( m =160 ) (b)Asymptoticity Figure2:MorePerformanceComparisonbasedonPseudo-RandomSynthesizedData (a) DBLD vs.FLDs (b) DBLD vs.Downstream Figure3:PerformanceComparisononBenchmarkDatasets( p =300 and p ˛ m ,D-Tree:DecisionTree,R-Forest:RandomForest,K-SVM: KernelSVM,andL-SVM:LinearSVM) TrainingSetSize Algorithm 100200300400500600700 DBLD 0.659  0.0220.677  0.0280.691  0.0240.692  0.0230.690  0.0210.696  0.0240.701  0.023 FLD0.543  0.0340.586  0.0330.616  0.0220.642  0.0290.642  0.0220.657  0.0250.658  0.026 Ye-FLD0.627  0.0500.620  0.0770.652  0.0630.620  0.0670.655  0.0620.637  0.0640.670  0.045 DecisionTree0.621  0.0460.649  0.0310.652  0.0410.655  0.0300.671  0.0280.665  0.0310.668  0.040 LinearSVM0.615  0.0260.628  0.0300.647  0.0230.666  0.0290.666  0.0210.670  0.0300.675  0.029 KernelSVM0.635  0.0320.669  0.0270.674  0.0390.678  0.0210.668  0.0380.688  0.0240.682  0.029 AdaBoost0.631  0.0350.630  0.0390.620  0.0280.622  0.0270.621  0.0220.617  0.0250.626  0.070 CRLD0.658  0.0230.676  0.0240.682  0.0280.686  0.0220.683  0.0210.692  0.0250.695  0.018 RandomForest0.590  0.0350.602  0.0350.653  0.0310.602  0.0400.674  0.0240.666  0.0260.658  0.032 Table1:EarlyDetectionofDiseasesAccuracyComparisonbetween DBLD andBaselines. eases [ Zhang etal. ,2015 ] .Inthisapplication,eachpatient's EHRdataisrepresentedbya p =295 dimensionalvector, referringtotheoutpatientrecordonthephysicaldisorders diagnosed.Patientsarelabeledwitheitherﬁpositiveﬂorﬁneg- TrainingSet Algorithm 100200300400500600700 DBLD 0.690  0.0280.708  0.027 0.722  0.0240.729  0.0180.727  0.01180.736  0.0180.734  0.022 FLD0.539  0.0480.580  0.0440.611  0.0300.646  0.0270.644  0.0250.662  0.0280.663  0.032 Ye-FLD0.644  0.1000.657  0.1240.688  0.0710.678  0.0570.698  0.0350.698  0.0350.712  0.027 DecisionTree0.626  0.1200.671  0.0740.675  0.0880.703  0.0320.695  0.0340.676  0.0780.690  0.097 LinearSVM0.616  0.0310.627  0.0410.651  0.0260.675  0.0310.675  0.0260.680  0.0350.690  0.031 KernelSVM 0.701  0.0630.723  0.022 0.702  0.1150.726  0.0160.681  0.1150.734  0.0190.715  0.071 AdaBoost0.560  0.0810.533  0.1070.498  0.0650.503  0.0780.500  0.0800.482  0.0660.503  0.070 CRLD0.696  0.0210.716  0.0210.719  0.0240.725  0.0180.721  0.0150.733  0.0210.734  0.016 RandomForest0.419  0.1260.509  0.1020.613  0.0670.509  0.1100.661  0.0360.640  0.0580.603  0.063 Table2:EarlyDetectionofDiseasesF1-ScoreComparisonbetween DBLD andotherBaselines. Datasets #Features#Samples Leukemia 7,12872(47/25) Colon 2,00062(40/22) Table3:DescriptionofDatasetsfor ativeﬂ,indicatingwhetherhe/shewasdiagnosedwithdepres- sion&anxietydisorders.Throughsupervisedlearningonthe datasets,thetrainedbinaryisexpectedtopredict whethera(new)patientisat-riskorwoulddeveloptothede- pression&anxietydisordersfromtheirhistoricaloutpatient records(physicaldisorderrecords) [ Zhang etal. ,2015 ] . Weevaluate DBLD andothercompetitors,includingLin- earSupportVectorMachine,NonlinearSVMwithGaussian Kernel,DecisionTree,AdaBoost,RandomForestandother FLDbaselines,withvaryingtrainingdatasetsize m from 100 to 700 .Table 1 presentsthecomparisonresults.Tosimplify thecomparison,weonlypresenttheresultsofthealgorithm withparameter,whichisselectedthrough10-fold cross-validation.Itisobviousthat DBLD andCRLDoutper- formotherbaselinealgorithms,while DBLD performsbetterthanCRLD.Theadvantageof DBLD over otheralgorithms,suchasSVM,isextremelyobviouswhen thesizeoftrainingdataset m issmall.Withtheincreasing samplesize,thoughthemarginsof DBLD overtherestofal- gorithmsdecrease, DBLD stilloutperformsotheralgorithms. WealsomeasuredtheF1-scoreofallalgorithms, DBLD still outperformsothercompetitorsinthemostcases.Pleaserefer toTable 2 fordetails. 4.4LeukemiaandColonCancerDatasets Weevaluate DBLD ,CRLDandotherbaselinealgorithms, includingDecisionTree,RandomForestandSVM,using leukemiaandcoloncancerdatasets(derivedfrom [ Lin,2017 ; Tibshirani etal. ,2002 ] )underHDLSSsettings(i.e., p = 7 ; 128 and 2 ; 000 vs. m =20 ). Table ?? presentsthedescriptionoftwodatasets [ Lin,2017 ; Tibshirani etal. ,2002 ] thatweusedtoevaluatetheproposed andbaselinealgorithms.ﬁLeukemiaﬂreferstotheleukemia cancerdataset [ Tibshirani etal. ,2002 ] thatincludes7,128 featuresandtotally72samples(fortrainingandtesting).In thisdatasets,47samplesarelabeledasﬁALLﬂclasswhile25 samplesareasﬁAMLﬂ.Ontheotherhand,ﬁColonﬂ referstothecoloncancerdatasets [ Lin,2017 ] thatarewith 2,000featuresandtotally62samples,where40samples arenegativeand22samplesareaspositive.Both datasetsarewithaultra-largenumberofdimensionsbutwith extremelylowsamplesizes(i.e., p ˛ m ). Toaccuratelyestimatetheperformanceofalgorithmsusing thesedatasetsunderHDLSSsettings,weusecross-validation tolimitthepotentialoverIneachroundofcross- validation,werandomlydrawn20sampleswithequal priorfromthedatasetsasthetrainingset,andrandomlydrawn 20sampleswithequalpriorfromthedisjointsetoftraining setasthetestingset.Foreachroundofcrossvalidation,there arenocommonsamplessharedbythetwosets.Weusethe trainingsettotraineacher(i.e., p =7 ; 128 or 2 ; 000 and m =20 ),soastosimulatetheextremelyHDLSSsettings, thentestthetrainedusingthetestingset.Foreach experiment,werepeatthecross-validationfor100rounds. Allalgorithms(includingbaselinesand DBLD )aretunedto havethebestaccuracy.Theexperimentresultsareshownin Table ?? .Allresultsshowthat DBLD improves CRLD,anditoutperformsallbaselinealgorithmswiththe highestaccuracyandF1-score.Pleasenotethatthoughwe trainedusinglesstrainingdata,baselinesinour experimentsperformcomparablywiththetesterrorsreported in [ Tibshirani etal. ,2002 ] . 4.5SummaryofExperimentResults Weevaluate DBLD withalimitednumberofsamplesfor trainingi.e., p>m or m 6˛ p ,tounderstanditsperformance underHDLSSsetting.Forlargesamplescenario,i.e.,when m ˛ p ,thesample-basedestimatorsmayprovidearobust estimationofLDA.Inthiscase,singularityissuesmightnot exist,thenregularizationandfurtherthede-biasingprocedures arenotmandatory. ColonLeukemia Algorithm AccuracyF1ScoreAccuracyF1Score DBLD0.8030.8020.9640.964 CRLD0.6330.6300.6900.690 DecisionTree0.6690.6580.8040.800 RandomForest0.8010.7980.9570.956 SVM0.7970.8120.9060.914 Table4:AccuracyandF1-ScoreComparisonbetween DBLD and otherBaselinesBasedonColonarandLeukCancerDatasets. 5RelatedWorkandDiscussion Inthissection,wereviewseveralmostrelevantstudiesofour research.ToaddresstheHDLSSissuesforFLD,alineof research [ Shao etal. ,2011 ; CaiandLiu,2011 ] proposedto directlyestimateasparseprojectionvectorwithoutestimat- ingtheinversecovariancematrix(samplecovariancematrix isnotinvertible)andmeanvectorsseparately.Ontheother hand, [ PeckandVanNess,1982 ; BickelandLevina,2008 ; WittenandTibshirani,2009 ] proposedtoestimatethe inversecovariancematrixthroughshrunkencovarianceestima- tors,thenestimatetheprojectionvectorwithsamplemeanvec- tors.Throughregularizingthe(inverse)covariancematrixesti- mation,thesealgorithmsareexpectedtoestimateasparsepro- jectionvectorwith(sub-)optimaldiscriminationpower [ Zol- lanvariandDougherty,2013 ] .Moreover,theperformanceof FLDhasbeenpreviouslystudiedin [ DurrantandKab ´ an,2015 ; Bickel etal. ,2004 ] . Inourpaper,wefocusonimprovingcovariance-regularized FLD [ WittenandTibshirani,2009 ] ,throughde-biasingthe projectionvectorestimatedwithGraphicalLasso [ Witten etal. ,2011 ] .Ourworkisdistinctduetothefollowing reasons: (1) Ourworkisthetostudytheproblemof de-biasingthesparseFLD [ ZhangandZhang,2014 ] ; (2) Comparedtotheexistingsolutiontothede-biasedlinear regressionmodels [ JavanmardandMontanari,2014 ] ,we proposedanovelde-biasedestimator(usingadifferentfor- mulateinEq 5 )forthe covariance-regularizedsparseLin- earDiscriminantAnalysis [ WittenandTibshirani,2009 ; Witten etal. ,2011 ] ; (3) Weanalyzedthede-biasedestimator andobtaineditsasymptoticproperties; (4) Wevalidateour algorithmsthroughcomparingawiderangeofbaselineson bothsynthesizedandreal-worlddatasets,wheretheevaluation resultendorsesourtheory(e.g.,asymptoticpropertiesproved in Theorem1 and 2 vs.thecurveshowninFig 2 (b)). DiscussionandFutureWork. Inthisresearch,wecom- pare DBLD withCRLD,andcommonFLD(sampleFLD, pseudo-inverseFLD,Ye-FLD).Wedonotmakefurthercom- parisonwithothersparseFLD [ CaiandLiu,2011 ] ,aswe focusonthecovariance-regularization.Infuturework,we plantostudythede-biasedestimatorsforthesesparseFLD. 6Conclusion Inthispaper,westudiedtheproblemofimprovingtheper- formanceofcovariance-regularizedFLD(CRLD)through re-balancingthebiasesandvariancesoftheprojectionvector estimation.Inspiredbythede-biasedestimatorofLasso [ Ja- vanmardandMontanari,2014 ] ,weproposed DBLD Œa novelDe-BiasedestimatorforCRLDthatlowerstheestima- tionerrorwithfasterasymptoticrate,throughde-biasingthe projectionvectorobtainedbyCRLD.Ouranalysisshowsthat DBLD iswithbetterasymptoticproperties,comparedto CRLD,andcanobtainhigheraccuracy,under HDLSSsettings.Theexperimentalresultsonsynthesizedand real-worlddatasetsshowthat DBLD outperformedallbase- linealgorithms.Further,theempiricalstudiesonestimator comparisonvalidateourtheoreticalanalysis. References [ BickelandLevina,2008 ] PeterJBickelandElizavetaLev- ina.Regularizedestimationoflargecovariancematrices. TheAnnalsofStatistics ,pages199Œ227,2008. [ Bickel etal. ,2004 ] PeterJBickel,ElizavetaLevina,etal. Sometheoryforslineardiscriminantfunction,naive bayes',andsomealternativeswhentherearemanymore variablesthanobservations. Bernoulli ,10(6):989Œ1010, 2004. [ CaiandLiu,2011 ] TonyCaiandWeidongLiu.Adirect estimationapproachtosparselineardiscriminantanal- ysis. JournaloftheAmericanStatisticalAssociation , 106(496):1566Œ1577,2011. [ Cai etal. ,2016 ] TTonyCai,ZhaoRen,HarrisonHZhou, etal.Estimatingstructuredhigh-dimensionalcovariance andprecisionmatrices:Optimalratesandadaptiveestima- tion. ElectronicJournalofStatistics ,10(1):1Œ59,2016. [ DasGupta,2008 ] AnirbanDasGupta. Asymptotictheoryof statisticsandprobability .SpringerScience&Business Media,2008. [ Duda etal. ,2001 ] RichardO.Duda,PeterE.Hart,and DavidG.Stork. Pattern(2ndEd) .Wiley, 2001. [ DurrantandKab ´ an,2015 ] RobertJDurrantandAtaKab ´ an. Randomprojectionsasregularizers:learningalineardis- criminantfromfewerobservationsthandimensions. Ma- chineLearning ,99(2):257Œ286,2015. [ Friedman etal. ,2008 ] JeromeFriedman,TrevorHastie,and RobertTibshirani.Sparseinversecovarianceestimation withthegraphicallasso. Biostatistics ,9(3):432Œ441,2008. [ Jankova etal. ,2015 ] JanaJankova,SaravandeGeer,etal. intervalsforhigh-dimensionalinversecovari- anceestimation. ElectronicJournalofStatistics ,9(1):1205Œ 1229,2015. [ JavanmardandMontanari,2014 ] AdelJavanmardandAn- dreaMontanari.intervalsandhypothesistest- ingforhigh-dimensionalregression. JournalofMachine LearningResearch ,15(1):2869Œ2909,2014. [ KaskiandPeltonen,2003 ] SamuelKaskiandJaakkoPelto- nen.Informativediscriminantanalysis.In ICML ,pages 329Œ336,2003. [ Krzanowski etal. ,1995 ] WJKrzanowski,PhilipJonathan, WVMcCarthy,andMRThomas.Discriminantanalysis withsingularcovariancematrices:methodsandapplica- tionstospectroscopicdata. AppliedStatistics ,pages101Œ 115,1995. [ Kulisandothers,2013 ] BrianKulisetal.Metriclearning: Asurvey. FoundationsandTrendsinMachineLearning , 5(4):287Œ364,2013. [ Lin,2017 ] Chih-JenLin.Libsvmdata: (binaryclass). https://www.csie.ntu.edu. tw/ Ÿ cjlin/libsvmtools/datasets/binary. html ,2017. [ Mar  cenkoandPastur,1967 ] VladimirAMar  cenkoand LeonidAPastur.Distributionofeigenvaluesforsomesets ofrandommatrices. MathematicsoftheUSSR-Sbornik , 1(4):457,1967. [ PeckandVanNess,1982 ] RogerPeckandJohnVanNess. Theuseofshrinkageestimatorsinlineardiscriminantanal- ysis. IEEETransactionsonPatternAnalysisandMachine Intelligence ,(5):530Œ537,1982. [ Rothman etal. ,2010 ] AdamJRothman,ElizavetaLevina, andJiZhu.Sparsemultivariateregressionwithcovari- anceestimation. JournalofComputationalandGraphical Statistics ,19(4):947Œ962,2010. [ Shao etal. ,2011 ] JunShao,YazhenWang,XinweiDeng, SijianWang,etal.Sparselineardiscriminantanalysisby thresholdingforhighdimensionaldata. TheAnnalsof Statistics ,39(2):1241Œ1265,2011. [ Tibshirani etal. ,2002 ] RobertTibshirani,TrevorHastie, BalasubramanianNarasimhan,andGilbertChu.Diagnosis ofmultiplecancertypesbyshrunkencentroidsofgeneex- pression. ProceedingsoftheNationalAcademyofSciences , 99(10):6567Œ6572,2002. [ WittenandTibshirani,2009 ] DanielaMWittenandRobert Tibshirani.Covariance-regularizedregressionandclassi- forhighdimensionalproblems. Journalofthe RoyalStatisticalSociety:SeriesB(StatisticalMethodol- ogy) ,71(3):615Œ636,2009. [ Witten etal. ,2011 ] DanielaMWitten,JeromeHFriedman, andNoahSimon.Newinsightsandfastercomputations forthegraphicallasso. JournalofComputationaland GraphicalStatistics ,20(4):892Œ900,2011. [ Ye etal. ,2004 ] JiepingYe,RaviJanardan,andQiLi.Two- dimensionallineardiscriminantanalysis.In NIPS ,pages 1569Œ1576,Cambridge,MA,USA,2004. [ Zhangandothers,2003 ] ZhihuaZhangetal.Learningmet- ricsviadiscriminantkernelsandmultidimensionalscaling: Towardexpectedeuclideanrepresentation.In ICML ,vol- ume2,pages872Œ879,2003. [ ZhangandZhang,2014 ] Cun-HuiZhangandStephanieS Zhang.intervalsforlowdimensionalparam- etersinhighdimensionallinearmodels. Journalofthe RoyalStatisticalSociety:SeriesB(StatisticalMethodol- ogy) ,76(1):217Œ242,2014. [ Zhang etal. ,2015 ] JingheZhang,HaoyiXiong,YuHuang, HaoWu,KevinLeach,andLauraE.Barnes.MSEQ:Early detectionofanxietyanddepressionviatemporalorders ofdiagnosesinelectronichealthdata.In BigData .IEEE, 2015. [ ZollanvariandDougherty,2013 ] AminZollanvariandEd- wardRDougherty.Randommatrixtheoryinpatternclas- Anapplicationtoerrorestimation.In 2013 AsilomarConferenceonSignals,SystemsandComputers , 2013.  
TDSS:ANewWordSenseRepresentationFramework forInformationRetrieval LiweiChen 12 ,YansongFeng 1 ,andDongyanZhao 1 1 InstituteofComputerScienceandTechnology,PekingUniversity,Beijing,China 2 BaiduInc.,Beijing,China chenliwei@baidu.com,fengyansong@pku.edu.cn, zhaodongyan@pku.edu.cn Abstract. Wordsenserepresentationisimportantinthetasksofinformationre- trieval(IR).Existinglexicaldatabases,e.g.,WordNet,andautomatedwordsense representingapproachesoftenuseonlyoneviewtorepresentaword,andmaynot workwellinthetaskswhicharesensitivetothecontexts,e.g.,queryrewriting.In thispaper,weproposeanewframeworktorepresentawordsensesimultaneous- lyintwoviews,explanationviewandcontextview.Wefurtherproposeannovel methodtoautomaticallylearnsuchrepresentationsfromlargescaleofquerylogs. Experimentalresultsshowthatournewsenserepresentationscanbetterhandle wordsubstitutionsinaqueryrewritingtask. Keywords: wordsenseinduction,graphclustering,queryrewriting 1Introduction AstheamountoftheinformationontheInternetincreasingexponentially,thereare growingdemandsintheinformationretrieval(IR)industrytounderstandthequeries better,soastoprovidetheuserswithmoreaccurateanddiverseresults.Suchquery understandingtasksrequiremodelingofwordmeaningstocapturesubtlesemantic differencesofwords.Forexample,queryrewriting,animportantIRtaskinmanyreal- isticsearchengines,shouldrewriteaquerywithsynonymousorsimilarwordswiththe onesinthequery,soastoobtainmoreretrievalresults.Inpractice,inordertoavoid semanticdriftofquerymeanings,wemainlyusesynonymstorewritethequery.Take thequeryﬁ Ê  š N C = p ¯ @ : (WherecanIhavebreakfastinWudaokou)ﬂasan example,wewouldliketouseﬁ @ ê (breakfast)ﬂtosubstituteﬁ @ : (breakfast)ﬂasa rewritingofthequery.However,manywordsinqueriesareambiguous,andweneedto usethecontexttodeterminewhichsenseawordwoulduse,andthesynonymouswords ofthissensewillbeusedtorewritethequery.Thismeansthatthetaskshouldexploit twoviewsofawordsense:theexplanationview,containingsynonymsofthesense,and thecontextview,representinginwhatcontextsthissenseisused.Unfortunately,most existingworksonwordsenserepresentationsarenotsuitableforthetask. Traditionallexicaldatabases,suchasWordNet[11,10],groupswordsintosetsof synonymscalledsynsets,providingshortandusageexamples.Itonlyuses theexplanationviewtorepresentthewordsenses,whilethecontextintheusageex- amplesisratherlimited.Furthermore,itrequireslotsofhumaneffortstoconstructand update,thusisdiftoadapttootherdomainsorlanguages. 2 Ontheotherhand,automatedwordsenseinduction(WSI)hasattractedmoreand moreattention.PreviousworksonWSImainlyfocusoncharacterizingwordmeanings bymodelingthecontextsordescriptionsoftheambiguousword,includingunsuper- visedclustering[15,14,5],ortopicmodels[2].Recentresearcheffortsalsoattemptto buildacontinuousvectortorepresentawordorasenseofaword[9,7,12,16,6,4,3], andthemodelsareusuallytrainedonthecontextsand/orthetextualdescriptionsofthe words.Thoseapproachesoftenuseonlyoneviewtorepresentaword,whichmakesthe twoaspectsofwordsenseinteractwitheachother,andmayconfusethequeryrewriting model. Inthispaper,weproposeanovelframework,two-dimensionalsemanticspace(TDSS), whichjointlyusetwovectors,theexplanationvectorandthecontextvector,torepresent awordsense.Theexplanationvectorisgeneratedusingsynonyms,servingasadescrip- tionofthissense.Forconvenience,wealsocallthosewordsasits explanationwords . Thecontextvectorisconstructedbasedonthecorrespondingcontextofthesense.We furtherproposeanapproachtoobtainsuchTDSSrepresentationsforwordsensesfrom largescalequerylogs.Theexplanationwordsandcontextwordsareextractedfromthe queryparaphrases,andfurthergroupedintomultiplesenses.Experimentalresultson 33Chinesewordsandaqueryrewritingcasestudyshowthatourapproachcanoutput reasonablewordsenses,whichcanbefurtherusedinthetaskofqueryrewriting. 2RelatedWork Inthissection,wereviewpreviousstudiesontraditionalcontext-orientedWSI methods. PreviousstudiesinWSIareoftencontext-oriented.Thoseapproachescanbedi- videdintothreecategories:unsupervisedclusteringapproaches,generativeapproaches andwordembeddingapproaches. Incluster-basedapproaches,WSIistreatedasaclusteringproblem.Thementions ofatargetwordaregroupedintoseveralclustersaccordingtothesimilarityoftheir contexts.Manydifferentclusteringalgorithmshavebeenusedsofar,e.g.,k-means [14],agglomerativeclustering[15],informationbottleneck[13]. Thegraph-basedclustermethodscanalsobeusedinthetaskofWSI[8],where wordsarethenodesandtheco-occurrencebetweenwordsaretheedges.Communi- tydetectionalgorithmscanbeemployedtodiscoverwordcommunitiesinthegraph, whichisusedtorepresentwordsenses. Generativeapproachesassumethatdifferentsenseshavedifferentlexicaldistribu- tions.Forexample,[2]utilizesaparametricBayesianmodel,i.e.,LDA,tosolvethe WSItask.Thewordsensesarecharacterizedasdistributionsoverwordsandanam- biguouswordisthendrawnfromadistributionofsenses.Inordertoautomatically decidethenumberofsenses,insteadofLDA,[17]proposetouseanonparametric Bayesianmodel,calledHierarchicalDirichletProcess(HDP). RecentlywordembeddingapproacheshaveattractedmoreandmoreattentionIn thoseapproaches,awordorthesenseofawordisoftenrepresentedbyacontinuous vectorwhichisbuiltbyneuralnetworkalgorithms[9,7,12,16,6,4,3].Mostapproach- esusethecontextsand/orthetextualdescriptionsofthewordstotrainthosemodels. 3 3ANewWordSenseRepresentationFramework InTDSS,werepresentthesenseofawordintwoviews,explanationviewandcontext view.Weusesynonymsofawordtogenerateitsexplanationview.Taketheword ﬁ w (see,look)ﬂasanexample,theexplanationviewofoneofitssensesmayconsist wordssuchasﬁ ˛ Ö (read)ﬂandﬁ è A (browse)ﬂ. Forthecontextview,weextractwordsfromthecontextswherethismeaningisused. Intheexampleofthewordﬁ w (see,look)ﬂ,wecanusethewordswhichoftenexistin itscontexts,suchasﬁ Ö (book)ﬂ,ﬁ  ™ (newspaper)ﬂ,etc.,togeneratethecontextview. Formally,givenaword w with k senses,its i thsensecanberepresentedasatuple S i = < E i ; C i > ,where E i isavectorrepresentingtheexplanationofthissense,and C i isavectorrepresentingthecontextofthissense.Wealsoassignapopularityfor E i , whichindicateshowpopular E i isamongallsenseexplanationsofthetargetword.We restrictthatthepopularityofallsenseexplanationsofawordshouldsumupto1: k X i =1 Pop ( E i )=1 (1) Generallyspeaking,thesensesusedinmorecommonanddiversecontextsareoften morepopular.Wecalculatethepopularityofasensebasedontheprobabilityofthe sense'scontexts: Pop ( S i )= P ( C i ) k P j =1 P ( C j ) (2) Inthispaperweconstructthevectorsasasimplebagofwords(BoW)model, whereeachdimensionofthevectorisawordinthesenseexplanationsorcontexts.It ispossibletousemoresophisticatedmodelssuchastopicmodelsorwordembedding, whichwillbeourfuturework. 4TDSSSenseExtraction InordertoautomaticallyobtaintheTDSSrepresentationsofwordsenses,weex- tractingparaphrasesfromthequerylogs,anddetectwordalignmentsbetweenthose paraphrasepairs.Thenweextractsubstitutionpairsaswellasthecorrespondingcon- textsbasedonthealignments.Thesubstitutionsofawordwillbeconsideredasits explanationwords.Finally,foragivenword,wecollectallitssubstitutionsaswellas thecorrespondingcontexts,adoptaclusteringalgorithmtogroupthesubstitutionsinto differentsensesandfurtherobtaintheTDSSrepresentationfromeachsense. 4.1ExplanationWordsandContextExtraction GeneratingParaphrases Giventhequerylogs,weadopttheapproachin[18]togen- eratesententialparaphrasesfromthem,whichwillbefurtherusedasthesourcesof miningexplanationwordsandcontextsofwordmeanings. 4 WordAlignment Foreachpairofsententialparaphrases,wealignthewordsinone paraphrasetotheircorrespondingsubstitutionwordsintheother.Weadoptarule-based aligningstrategyforthewordalignment.Twowordsarealignediftheyarethesame, synonyms(accordingtoanexistingthesaurus),orthewordsinsideawindowaround them(windowsizeissetto1)arethesameorsynonyms. SubstitutionandContextExtraction Generallyspeaking,twoalignedwords a and b inaparaphrasepaircanbesynonyms,co-hyponymsorhypernyms,etc.Thus,we canuseaword'ssubstitutionstoformtheexplanationviewsofitssenses.Foreach sense,wealsoneedacontextvector C .Thewordsinsideaedwindowsize(setto3 empirically)ofthequeryparaphrasesareextractedtogeneratethecontextvectors. Nowforeachsubstitutionpair,wewillcomputethesubstitutionprobabilityapprox- imatelybasedonallparaphrases.Givenaword a anditssubstitution b ,theprobability of a issubstitutedby b canbecalculatedas: p ( a ! b )= freq ( a ! b ) P b i 2 B freq ( a ! b i ) (3) where freq ( a ! b ) isthetimes a issubstitutedby b intotal, B isallwordswhichcan substitute a . Itisalsoimportanttoestimatetheprobabilityofthetargetword w substitutedbythe explanationword e giventhecontextword c .Formally, p ( w ! e j c ) canbecalculated as: p ( w ! e j c )= freq ( w ! e j c ) P e i 2 E c freq ( w ! e i j c ) (4) where freq ( w ! e j c ) isthetimesthat w issubstitutedby e giventhecontextword c , E c isthesetofallwordsthatcansubstitute w underthecontextword c . 4.2SenseGraphConstruction Inourapproach,thesensesofawordarecapturedfromthesensegraphofthisword. Thegraphisconstructedbasedonthesubstitutionpairsandthecontextwordsextract- edintheprevioussubsection.Foragivenword,wecollectallitssubstitutionwords, whichcanbeseenasitsexplanationwords.Thenweconnecttheobtainedexplanation wordsbytheirpairwiserelatednessestimatedbasedonthesubstitutionprobabilities andcontextwordsobtainedintheprevioussubsection. GraphPruning Sincetherearelotsofnoisesinthegeneratedsubstitutionpairs,we needtoprunethesensegraphtoreducethenoises.Thefollowingstrategiesareusedto prunethegraph. Thestrategyissimple:wepruneanyexplanationwordswithlowsubstitution probabilitiesorlowsubstitutionfrequencies.Thepruningthresholdsaresetempirically forthisstrategy. 5 Thesecondstrategyutilizestriangle-likesubstitutionstopruneunreliablegraph nodesandedges.Theassumptionisthatatriangle-likesubstitutionstructurearemore stableandreliable,andcanhelpustoprunethenoisesfromthegraph.Supposewe haveanode a inthesensegraphofaword w ,whichmeans w canbesubstitutedby a . Ifwecananothernode b inthegraphsatisfying a canbereplacedby b ,thenwe reserveboth a and b .Otherwise, a willbeprunedfromthegraph.Figure1illustratesa triangle-likesubstitutionstructure,where  (goonatrip)istheword w , i A (travel)is theword a and À 1 (takeatrip)istheword b . Fig.1. Illustrationofthetriangle-likesubstitutionstructure. EdgeWeighting Theweightsoftheedgesinthesensegraphindicatetherelatedness betweentheexplanationwords.Giventhesensegraphofaword w ,andtwonodes a and b inthisgraph,theedge e ab betweenthetwonodesisweightedas: weight ( e ab )= sub ( a;b )+(1   ) sim ( a;b ) (5) sub ( a;b ) isthesubstitutionrelatednessbetween a and b ,whichiscalculatedasthe averagevalueof p ( a ! b ) and p ( b ! a ) . sim ( a;b ) isthecontextsimilarity(cosine similarity)between a and b ,calculatedbasedonthecontextsinwhich w issubstituted by a and b respectively.Thecontextword c isweightedbytheconditionalprobability p ( a ! b j c ) .  isaparameterusedtoadjusttheimportanceofthetwocomponents.It willbeoptimizedonaheldoutset. Afterthepruningandedgeweightingstep,foreachtargetword,weobtainasense graphfromallremainingsubstitutionsaswellastheirpairwiseweightings.Figure2 illustratesanabridgedversionofasensegraphfortheword w .Themeaningsof w in Chineseincludes w " (visit), £  (cure)and *  (observe),etc. 4.3SenseGenerationandWeighting Afterweconstructthesensegraphforaword,weadoptaclusteringmethodtogroupthe explanationwordsintoseveralclusters,andfurthergeneratetheTDSSrepresentation forthesensesofthisword. 6 Fig.2. Anabridgedexampleofthesemanticgraph.Theweightsoftheedgesarealsoomittedfor thesakeofbrevity. GraphClustering Nowwewillclustertheexplanationwordsandcontextwordsinto differentsenses.Manyalgorithmscanbeusedforthistask,suchasKMeans,HAC,and density-basedclusteringalgorithms.Itismoresuitabletousethealgorithmswhichdo notneedtodeterminethenumberofclustersinadvance,sinceitisdifforusto knowtheexactnumbersofthesensesofeachword. Wechoosealabelpropagationalgorithmtoclusterthegraph.Allnodesofthegraph willbeinitializedwithadifferentsense.Ineachiteration,foreachnodeofthegraph, wecollectallitsneighbors,andthescoreofeachsenseiscalculatedbysummingthe weightsofallneighborswhichsupportit.Thesenseswhosescoresarehigherthan athresholdwillbethesensesofthisnode.Ifthesensesoflessthan10%nodesare inaniteration,theprocedurewillstop. SenseGeneration Aftertheclustering,wecanobtaintheTDSSrepresentationsfor sensesofthetargetword w .Eachclustercanbeusedtoconstructtheexplanationvector ofonesense.Thevalueofeachdimensioninanexplanationvector E isthesubstitution probabilityofthecorrespondingexplanationword: E =( p ( w ! e 1 ) ;p ( w ! e 2 ) ;:::;p ( w ! e m )) (6) where e i istheexplanationwordsinthecluster.Foreachsense,weaggregateallits sensewords'contextstogether,andtheoneswhosefrequenciesaretoolow.The remainingwordsformthe C vectorofthissense.Thevalueoftheelementsin C isthe conditionalprobabilitythatthetargetword w issubstitutedbytheexplanationwords ofthecurrentsensegiventhecorrespondingcontextword c .Thus, C iscomputedas: C =( p ( w ! E j c 1 ) ;p ( w ! E j c 2 ) ;:::;p ( w ! E j c n )) (7) 7 where E isthesetoftheexplanationwordsofthecorrespondingsense,and p ( w ! E j c i ) iscalculatedas: p ( w ! E j c i )= X e j 2 E p ( w ! e j j c i ) (8) AccordingtotheofthesensepopularityinSection3,givenaword w whichhas k senses,thepopularityof S i canbecomputedas: Pop ( S i )= P c 2 C i freq ( c ) k P j =1 P c 2 C j freq ( c ) (9) where freq ( c ) isthefrequencyofthecontextword c , C i isthesetofcontextwords of S i .Sincethepopularsensesareusuallymoreimportantfortheusers,wecollect thesenseswiththetop n popularityasourresults.Table1showssomesamplesofthe generatedsensesfortheword ‰ (beat). Table1. SamplesforInducedWordSenses Word Sensewordcluster S i andcontextcluster C i ‰ (beat) Sense 1 : ‰  (injection), 5  (injection) Context 1 : { x (whitening), ¯ _ (hepatitisB), æ ƒ (Anesthetic) Sense 2 : ‰ ã (towar), ô Ñ (strategy), L' (passgamelevel), ? ô (attack) Context 2 : ê ( (BladeSoul), C  (equipment),  ! (plug-in), E U (skills) Sense 3 : I Ñ (mark),  ¤ (written), – \ (keyboardinput), ‰ Ñ (show) Context 3 :WPS, ) Ò (parentheses), ² ’ (square), ©  (document) Sense 4 : Ó ‰ (beatcruelly), E ‰ (bebeaten), Ý § (Slap), ð ‰ (beatup) Context 4 : • 2 (wakeup), ?  (baby), P ú (husband),  S (guard) 5Experiments Weconstructadatasetwith33ambiguousChinesewordstoevaluatetheperformance ofourmodel.ThosewordsaremanuallyselectedfromtheambiguouswordsintheChi- nesequerylogs,andthedatasetisdesignedtoberepresentativefordifferentkindsof words,includingnouns,verbs,adjectives,adverbs,wordswithonlyonecharacter,and wordswithmultiplePOStags.Resolvingtheambiguityofthosewordsishighlyimpor- tantforthetaskofqueryunderstandingintheindustry.Thewordsweusearelistedin Table2:Eachword'sexplanationwordsarecollectedandclusteredintosensesmanual- lybythreevolunteers.Weintegratetheirannotationsbymajorityvote.Thisdatasetwill beourgoldstandardforevaluatingtheexplanationview.Wedonotmanuallygenerate thecontextviewsbecausetheyusuallycontaintoomanywords.Thecontextviewswill beevaluatedimplicitlyinthecasestudy. 8 Table2. Thewordsusedinourevaluation.TheEnglishwordsafterwardsillustratetworepresen- tativemeaningsoftheChinesewords. + (ring,giveup,etc.)  (whole,etc.)   (weak,thin,etc.) # m (fresh,novel,etc.) ‚ Ú (yellow,adult,etc.) & (letter,trust,etc.) • • (sill,threshold,etc.) š (wrap,bag,etc.) \ h (fuelcharging,cheerup,etc.) ƒ (straight,directly,etc.)  (leave,remove,etc.) Ÿ (put,release,etc.) 9 (hot,popular,etc.) ﬁ L (represent,representative,etc.) 1 (light,smooth,etc.) Ð (good,like,etc.) ß (through,completely,etc.) B ¨ (cheap,interest,etc.)  (hook,linkupwith,etc.) ” â (reckoning,getevenwith,etc.) Y © (humidity,exaggeration,etc.) Ó ﬁ (comrade,gay,etc.) w (look,read,etc.) š (mouth,opening,etc.) ° (wide,relieve,etc.) ( ¢ (solid,strong,etc.) ‰ (hit,from,etc.) © (branch,divide,etc.) ` (rushfor,catchupwith,etc.) s wer,spend,etc.) @ : (breakfast,abitearlier,etc.) ~ − (pimple,knot,etc.) K (negative,burden,etc.) ThequerylogsweusearefromonepopularChinesesearchengine.Wecollectthe queriesfromtheyear2012and2013,producingabout230,000,000paraphrasepairs. 5.1EvaluatingExplanationWordExtraction Itisalmostimpossibletocomputetheexactrecallofexplanationwordsextraction becausewecanonlycollecttheoneswhicharecommon.Thus,weonlyconsiderthe precisionaswellastherecallwithrespecttocommonexplanationwordsagainsthuman annotations.Theresultsshowthatourexplanationwordextractioncanachieve apre- cisionof62.9%,andarecallof79.1% .Inordertostudyhowtheincorrectextractions aregenerated,werandomlyselect200incorrectsubstitutions,andmanuallyinvestigate theparaphrasesfromwhichtheyareextracted.Wesummarizethemajorreasonswith thetop3proportionare:incorrectparaphrasemappings,incorrectwordsegmentations andtyposorgrammaticalerrorsinthequeries.Theproportionofeachreasonislisted inTable3. Table3. Themajorreasonsofincorrectsubstitutionextraction. Reasons Percentage(%) Incorrectparaphrasemappings 37.0 Incorrectwordsegmentations 32.0 Typosorgrammaticalerrors 35.0 9 Thetop1reasonisincorrectparaphrasemappings.Sincetheparaphrasesareauto- maticallygeneratedfromquerylogs,theyinevitablycontainsomenoisesandincorrect mappings.Sometimesausermayclickatitlewhichishighlyrelatedbutnotthesame asthequery.Forexample,thequeryisﬁ X Û w qq Ÿ m ì ¡ ﬂ(Howtoseethepictures intheQZone),butthetitlewhichtheuseractuallyclickedisﬁ X Û þ D qq Ÿ m ì ¡ ﬂ(HowtouploadthepicturestotheQZone).Thiswillmakeﬁ w ﬂ(see)bealignedto ﬁ þ D ﬂ(upload),andafterwardsanincorrectsubstitutionisextracted. Second,incorrectwordsegmentationresultsmayalsoinviteincorrectsubstitutions. InChinesenaturallanguageprocessing,wordsegmentationisanimportantandnec- essarypreprocessingstep.Theerrorsinwordsegmentationswouldcertainlyeffectthe taskofwordalignments,andfurtherbringnoisestosubstitutionextraction.Forin- stance,intheparaphrasesﬁ k Ÿ ⁄ <4 · Ð ” g ﬂ(ThereisapersonImisssomuch) andﬁ k Ÿ ⁄ å ¯ 4 · Ð ” g ﬂ(ThereisagirlImisssomuch),thecorrectsegmen- tationofthelastthreecharactersshouldbeﬁ Ð (somuch) ” g (miss)ﬂ,butthesecond sentenceiswronglysegmentedasﬁ Ð ” (misssomuch) g (miss)ﬂ.Itmakesﬁ Ð (so much)ﬂbealignedtoﬁ Ð ” (misssomuch)ﬂ,whichdoesnotmakesense. Finally,thequeriesinputbytheusers,andthetitlesoftheclickedwebpages,areof- tennotwellnormalized.Someofthemmaycontainwronglywrittenormispronounced characters,someofthemmayevencontainseriousgrammaticalerrors.Underthese circumstances,awordmaybealignedtoanotherwordwhichisfullyunrelatedwithit. Taketheparaphrasesﬁ N o 3Ô ˜ þ    Ë ﬂ(HowtocompleteWarcraftonBattle.net) andﬁ N o 3Ô ˜ þ ‰   Ë ﬂ(HowtoplayWarcraftonBattle.net)asanexample,the wordﬁ  ﬂ(complete)inthesentenceshouldactuallybethewordﬁ  ﬂ(play), whichisasynonymofthewordﬁ ‰ ﬂ(play)inthiscontext.Becauseﬁ  ﬂ(complete) andﬁ  ﬂ(play)pronouncethesameinChinese,sometimestheusersmayusethe wordbymistake.Thiswillgenerateanincorrectsubstitutionbetweenﬁ  ﬂ(complete) toﬁ ‰ ﬂ(play). WecanobservefromTable3thatthesumoftheproportionsofthethreereasonsis largerthan100%.Thisisbecausesomeincorrectsubstitutionsareextractedformultiple reasons. 5.2EvaluatingWordSenseGeneration WeusetheB-cubedcriterion[1]toevaluatetheperformanceoftheexplanationword clustering.Wealsomanuallyevaluatetheperformanceofgeneratedwordsensesac- cordingtotheexplanationview.Theresultsofthetop20sensesarearelistedinTable 4. Table4. Theperformanceofexplanationwordsclustering. P(%) R(%) FMeasure(%) Clustering 89.4 49.3 60.1 WordSense 67.0 59.7 63.1 10 Wecanobservethatthesenseswegeneratedachieveaprecisionof67%.Thecor- rectexplanationviewscanrepresentthesenseofawordwell.Forexample,onesense ofthewordﬁ w (see)ﬂconsistsofexplanationwordsﬁ £  (treatadisease)ﬂ,ﬁ £ (cure)ﬂ andﬁ ı £ (heal)ﬂ.Wecanalsoseethattheclusteringprecisionismuchhigherthanthe recall.Thisisbecausewetunetheparameterstomaketheclusterssmallerbutmore accurate,soastodecreasetheeffectofthenoisesintheexplanationwords.Further- more,inmosttasksofqueryunderstanding,asmallerbutmoreaccurateclusterwould bebetterthanalargerclusterwithnoises. 5.3CaseStudy:QueryRewriting NowwewillevaluatehowtheTDSSrepresentationsofwordsensesperforminthe queryrewritingtaskcomparedwithtwostrongcontinuouswordrepresentationap- proaches,CBOWandskip-gram[9].Werandomlycollect921queriescontainingthe 33Chinesewords,andmanuallylabeledthewordswhichcansubstitutethetargetword. ForCBOWandskip-grambaselines,weselectedthetop100similarwords fromthevocabularyascandidates.Thenwecomputethesimilaritybetweenthecandi- datesandthecontextsofthetargetwordsinthequeries,andtheonesaboveathreshold willbecollectedastherewritingwords.Weusetheextractedqueryparaphraseswhich containthe33wordstotraintheCBOWandskip-grammodel. ForourTDSSwordsenses,weselectthebestsenseforthetargetwordbasedonthe similaritiesbetweenthequeryandthecontextviews,andtheexplanationwordsofthe selectedsensewillbeusedastherewritingwords.Asfortheevaluatecriterion,weuse theprecisionofthegeneratedrewritingwords.Sinceitisdiftoobtaintheexact recalloftheresults,weusetheaveragenumberofcorrectrewritingwordsofallthe queriesinthedatasetinstead.TheresultsarelistedinTable5. Table5. TheperformanceofTDSSandthebaselinesinqueryrewriting. Approach Precision(%) AverageNumberofCorrectRewritingWords CBOW 19.0 1.8 Skip-Gram 6.4 3.2 TDSS 51.2 2.9 FromtheresultswecanobservethatourTDSSrepresentationscanobtainthebest precision.Thisismainlybecausethatitseparatestheessentialsofawordsense,expla- nationsandcontexts,intotwoviews,whiletheexistingapproaches,i.e.,CBOWand skip-gram,combinesthemintooneview,whichmayblurthedifferencesofthetwo typesofinformation.Skip-Gramcanobtainalittlemorecorrectrewritingwordsthan TDSS,butinpracticetheprecisionofthewordsaremoreimportantforussinceain- correctsubstitutionwouldinviteincorrectretrievalresults,whichmaydecreasetheuser experience.Theresultsalsoimplythatthecontextviewworkswellinqueryrewriting, whichimplicitlyprovesthatourapproachcangeneratereliablecontextviewforword senses. 11 6Conclusions Inthispaper,wedescribeanovelwordsenserepresentationframework,whichcaptures thesenseofawordintwoseparateviews,explanationandcontext,andfurtherpropose anapproachtoextractsuchrepresentationsfromlargescaleofquerylogs,withoutre- plyingonmuchhumaninvolvement.ExperimentalresultsonaChinesedatasetshow thatournewwordsenserepresentationframeworkcanhelpbetterhandleinformation retrievaltasks,suchasqueryrewriting,wheremodellingofwordmeaningsisde- sired.Forfurtherwork,wewilllookformorerobustrepresentationsforwordmeanings tobetterrepresentthetwoviews,andwillalsoattempttoapplytheobtainedwordsens- estomoreretrievalapplications. Acknowledgement.WewouldliketothankBenXu,WensongHe,ShuaixiangDai, XiaozhaoZhao,QiannanLv,andtheanonymousreviewersfortheirhelpfulfeedback. ThisworkissupportedbyNationalHighTechnologyR&DProgramofChina(Grant No.2015AA015403,2014AA015102)andNaturalScienceFoundationofChina(Grant No.61202233,61272344,61370055).Foranycorrespondence,pleasecontactLiwei Chen. References 1.Bagga,A.,Baldwin,B.:Entity-basedcross-documentcoreferencingusingthevectorspace model.InternationalConferenceonComputationalLinguistics-volumepp.79Œ85(1998) 2.Brody,S.,Lapata,M.:Bayesianwordsenseinduction.In:Proceedingsofthe12thConfer- enceoftheEuropeanChapteroftheAssociationforComputationalLinguistics.pp.103Œ111. AssociationforComputationalLinguistics(2009) 3.Chen,T.,Xu,R.,He,Y.,Wang,X.:Improvingdistributedrepresentationofwordsensevia wordnetglosscompositionandcontextclustering.In:Proceedingsofthe53rdAnnualMeet- ingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointCon- ferenceonNaturalLanguageProcessing(Volume2:ShortPapers).pp.15Œ20.Association forComputationalLinguistics,Beijing,China(July2015), http://www.aclweb.org/ anthology/P15-2003 4.Chen,X.,Liu,Z.,Sun,M.:Amodelforwordsenserepresentationanddisambigua- tion.In:Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage Processing(EMNLP).pp.1025Œ1035.AssociationforComputationalLinguistics,Doha, Qatar(October2014), http://www.aclweb.org/anthology/D14-1110 5.Dorow,B.,Widdows,D.:Discoveringwordsenses.In:Proceedingsofthe tenthconferenceonEuropeanchapteroftheAssociationforComputationalLinguistics- Volume2.pp.79Œ82.AssociationforComputationalLinguistics(2003) 6.Guo,J.,Che,W.,Wang,H.,Liu,T.:Learningsenswordembeddingsbyexploiting bilingualresources.In:ProceedingsofCOLING2014,the25thInternationalConference onComputationalLinguistics:TechnicalPapers.pp.497Œ507.DublinCityUniversityand AssociationforComputationalLinguistics,Dublin,Ireland(August2014), http://www. aclweb.org/anthology/C14-1048 7.Huang,E.,Socher,R.,Manning,C.,Ng,A.:Improvingwordrepresentationsviaglobalcon- textandmultiplewordprototypes.In:Proceedingsofthe50thAnnualMeetingoftheAs- sociationforComputationalLinguistics(Volume1:LongPapers).pp.873Œ882.Association forComputationalLinguistics,JejuIsland,Korea(July2012), http://www.aclweb. org/anthology/P12-1092 12 8.Jurgens,D.:Wordsenseinductionbycommunitydetection.In:ProceedingsofTextGraphs- 6:Graph-basedMethodsforNaturalLanguageProcessing.pp.24Œ28.AssociationforCom- putationalLinguistics(2011) 9.Mikolov,T.,Chen,K.,Corrado,G.,Dean,J.:Efestimationofwordrepresentation- sinvectorspace.CoRRabs/1301.3781(2013), http://dblp.uni-trier.de/db/ journals/corr/corr1301.html#abs-1301-3781 10.Miller,G.A.:Wordnet:alexicaldatabaseforenglish.CommunicationsoftheACM38(11), 39Œ41(1995) 11.Miller,G.A.,Beckwith,R.,Fellbaum,C.,Gross,D.,Miller,K.J.:Introductiontowordnet: Anon-linelexicaldatabase*.Internationaljournaloflexicography3(4),235Œ244(1990) 12.Neelakantan,A.,Shankar,J.,Passos,A.,McCallum,A.:Efnon-parametricestimation ofmultipleembeddingsperwordinvectorspace.In:Proceedingsofthe2014Conference onEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).pp.1059Œ1069.Associa- tionforComputationalLinguistics,Doha,Qatar(October2014), http://www.aclweb. org/anthology/D14-1113 13.Niu,Z.Y.,Ji,D.H.,Tan,C.L.:I2r:Threesystemsforwordsensediscrimination,chineseword sensedisambiguation,andenglishwordsensedisambiguation.In:Proceedingsofthe4thIn- ternationalWorkshoponSemanticEvaluations.pp.177Œ182.AssociationforComputational Linguistics(2007) 14.Purandare,A.,Pedersen,T.:Wordsensediscriminationbyclusteringcontextsinvectorand similarityspaces.In:ProceedingsoftheConferenceonComputationalNaturalLanguage Learning.vol.72.Boston(2004) 15.Sch ¨ utze,H.:Automaticwordsensediscrimination.Computationallinguistics24(1),97Œ123 (1998) 16.Tian,F.,Dai,H.,Bian,J.,Gao,B.,Zhang,R.,Chen,E.,Liu,T.Y.:Aprobabilisticmod- elforlearningmulti-prototypewordembeddings.In:ProceedingsofCOLING2014,the 25thInternationalConferenceonComputationalLinguistics:TechnicalPapers.pp.151Œ 160.DublinCityUniversityandAssociationforComputationalLinguistics,Dublin,Ireland (August2014), http://www.aclweb.org/anthology/C14-1016 17.Yao,X.,VanDurme,B.:Nonparametricbayesianwordsenseinduction.In:Proceedingsof TextGraphs-6:Graph-basedMethodsforNaturalLanguageProcessing.pp.10Œ14.Associa- tionforComputationalLinguistics(2011) 18.Zhao,S.,Wang,H.,Liu,T.:Paraphrasingwithsearchenginequerylogs.In:Proceedingsof the23rdInternationalConferenceonComputationalLinguistics.pp.1317Œ1325.Associa- tionforComputationalLinguistics(2010)  
DialogueBreakdownDetectionusingHierarchicalBi-DirectionalLSTMs ZeyingXie,GuangLing BaiduInc.,Beijing,China f xiezeying,lingguang01 g @baidu.com Abstract Inthispaper,wepresentahierarchicalBi-DirectionalLSTMs neuralnetworkdesignedforthedialoguebreakdowndetection challenge3(DBDC3).ThetaskofDBDC3istodetectinappro- priateutterancesthatleadtodialoguebreakdownsindialogue systems.Bymakinguseofthehierarchicalstructureofdia- loguesystems,ourmodeliscontext-awareandcanbetrained directlyfromwordsequencestobreakdownlabelsinanend- to-endmanner.Thusourmodelisgenericandrequiresnofea- tureengineering,makingitapplicabletodifferentdialoguesys- tems.Weevaluateourmodelon4dialoguesystemsprovided bytheDBDC3datasets.Experimentresultsshowthatourpro- posedmodelachieveshighaccuracy,andoutperformstheCRF baselineintermsofbothanddistribution- relatedmetricswithoutcareful IndexTerms :Bi-LSTM,hierarchicalarchitecture,end-to-end, dialoguebreakdowndetection,dialoguesystems 1.Introduction Withtheincreasingpervasivenessofsmartphonesandsmart devices,spokendialoguesystemsaregainingevergrowingat- tentionfrombothacademicandindustry.Spokendialoguesys- temisconsideredasacandidatefornextgenerationhuman- machineinterface.Alotofspokendialoguesystembasedassis- tantshaveemerged,includingSiri,GoogleAssistant,Amazon Echo,CortanaandXiaoice.Theliteratureofdialoguesystem researchcanbebroadlyintotwocategories,onethat isaimedathelpingusertogainknowledgeandprovidinguse- fulservicesandonethatcanchatwithuserswithoutcomplet- inganytasks.Theformeroneisusuallycalledtask- orientedorgoal-orienteddialoguesystem[1]andthelaterchat- orienteddialoguesystem[2,3].Althoughdialoguesystemsare improvingsubstantially,theuserexperienceofsuchsystemsare stillunsatisfactory.Thesystemsfailtounderstandtheintention oftheusers'utteranceandrespondinappropriatelyoccasion- ally.Wecallthisdialoguebreakdowns[4]anddetectionofthem iscrucialtoimproveuserexperience[5].Thispaperfocuseson chat-orienteddialoguebreakdowns. Wenowintroducechat-orienteddialoguesystem andpointoutthedifinherenttothedialoguesystem problemthatcouldresultinbreakdowns.Chat-orienteddi- aloguesystemprovidestheabilitytochatwithuser,mimic theconversationbetweentwopeople.Theobjectiveofchat- orienteddialoguesystemistorespondproperlyandconvinc- inglytousers'utterances.Recentchat-orienteddialoguesys- temsusuallyadoptneuralmachinetranslationarchitecture[6,7] andaretrainedinanend-to-endmanner. Thedesignandimplementationofdialoguesystemhas evolvedfromlabor-intensiverule-basedsystems[8]todata drivenapproaches[1,2,3,9].Recentadvanceofdeeplearn- inghasinspiredmanyapplicationsofneuralmodelstodialogue systems.Bothselectionbased[3,10]andgenerationbased methods[2,11]havebeenproposedtobuildchat-orienteddia- loguesystems. Theuserexperienceofspokendialoguesystemsisstillfar fromsatisfactory.Thesystemusuallyfailstounderstandthe intentionoftheuser,especiallyduringamulti-turnconversa- tion.Whenthesystemsfailtounderstandtheintentionofa user,itwillproduceresponsebasedonitsfalseunderstanding, whichcouldresultinobviouslyirrelevantandinappropriatere- sponses.Theuserexperiencewouldbeunpleasantatleastun- dersuchcircumstances.Ifwecoulddetectsuchsystembreak- downs,i.e.thatthesystemisproducingirrelevantorinappro- priateresponses,wecouldtakeprecautionsandasktheuserto reformulatehis/herquestions. Thecausesofbreakdownsincurrentdialoguesystemsare multi-aspects.Inchat-orienteddialoguesystems,takegenera- tionbasedsystemforexample,theencodercouldfailtoencode anutterancecorrectlyandthedecodercouldproduceirrelevant responses.Variabilityandambiguityofnaturallanguagealso causedifinunderstanding.Oftenthetruemeaningcan onlybeinferredinagivencontext.Inaddition,inspokendi- aloguesystem,theinputisoftenthetranscriptionproducedby automaticspeechrecognitionmodule,whichmayproduceer- roneoussentences. Detectthedialoguebreakdownsandhandlethemproperly couldbeavaluablealternativewaytobuildbetterdialoguesys- tembecausetheafore-mentionedcausesforunsatisfactorydi- aloguesystemperformanceisunlikelytobesolvedverysoon. Inthispaper,weproposeanovelhierarchicalBi-LSTMbased methodtodetectthesystembreakdownsinanend-to-endman- ner.Insection2wedescribethedialoguebreakdowndetection task.Wepresentthemodelindetailinsection3.Theemperical analysisisconductedinsection4andweconcludeourmethod insection5. 2.TaskDescription Thedialoguesystembreakdownisasasituationin adialoguewhereuserscannotproceedwiththeconversa- tion.ThetaskofDialogueBreakdownDetectionChallenge 3(DBDC3)[12,13]istodetectwhetherthesystemutterance causesdialoguebreakdowns.Thedevelopeddialoguebreak- downdetectorisrequiredtooutputbothadialoguebreakdown labelandadistributionoftheselabels.Duetothesubjectivena- tureofdecidingwhethertheusercanproceedwiththeconver- sation,thestatesofsystembreakdownsincludesthefollowing threelabels: Ł Notabreakdown(NB) :Theconversationcancontinue easily. Ł Possiblebreakdown(PB) :Itisdiftocontinuethe conversationsmoothly. Ł Breakdown(B) :Itisdiftocontinuetheconversa- tion. Table1: StatisticsofEnglishdatasets TKTK IRIS CIC YI train test train test train test train test No.ofsessions 100 50 100 50 115 50 100 50 No.ofannotators 30 30 30 30 30 30 30 30 NB(NotaBreakdown) 35.1% 44.3% 32.9% 34.5% 28.9% 29.1% 34.8% 35.4% PB(PossibleBreakdown) 27.6% 29.2% 27.8% 29.3% 29.8% 39.3% 36.1% 40.3% B(Breakdown) 37.3% 26.5% 39.4% 36.2% 41.3% 31.6% 29.1% 24.3% 2.1.Datasets DBDC3distributedmulti-turnhuman-systemdialoguesession datasetsalongwithhumanannotatedbreakdownlabels.Eight sessiondatasetsfromdifferentchat-orienteddialoguesystems areavailable,amongwhichfourareEnglishdatasetsandfour areJapanesedatasets.Inthispaper,weonlyfocusonEnglish datasets. ThefourEnglishdatasetsare:TKTK,IRIS,CICandYI. Eachdatasetisseparatedintotrainingdataformodeldevelop- mentandtestdataformodeltesting.Alldialoguesessionsare 20or21utteranceslongandinclude10systemresponses.Ta- ble1summarizesthestatisticsoftheEnglishdatasets. 2.2.EvaluationMetrics DBDC3usestwotypesofevaluationmetrics: relatedmetricsanddistribution-relatedmetrics. 2.2.1.elatedMetrics metricsareevaluatedonbreakdownlabels predictedbymodelagainstthegoldlabelsdeterminedbymajor- ityvotingofhumanannotations.ThemetricsusedbyDBDC3 areasfollows: ŁAccuracy:Thenumberofcorrectlyclassiedlabelsdi- videdbythetotalnumberoflabelstobeclassied. ŁPrecision,Recall,F-measure(B):Theprecision,recall, andF-measurefortheclassicationoftheBlabels. ŁPrecision,Recall,F-measure(PB+B):Theprecision,re- call,andF-measurefortheclassicationofPB+Blabels; thatis,PBandBlabelsaretreatedasasinglelabel. 2.2.2.Distribution-relatedMetrics Distribution-relatedmetricsevaluatethesimilarityofthepre- dictedandgoldbreakdowndistributions.Themetricsusedby DBDC3areasfollows: ŁJSDivergence(NB,PB,B):Distancebetweenthepre- dicteddistributionofthethreelabelsandthatofthegold labelscalculatedbyJensen-ShannonDivergence. ŁJSDivergence(NB,PB+B):JSdivergencewhenPBand Bareregardedasasinglelabel. ŁJSDivergence(NB+PB,B):JSdivergencewhenNBand PBareregardedasasinglelabel. ŁMeanSquaredError(NB,PB,B):Distancebetweenthe predicteddistributionofthethreelabelsandthatofthe goldlabelscalculatedbymeansquarederror. ŁMeanSquaredError(NB,PB+B):Meansquarederror whenPBandBareregardedasasinglelabel. Figure1: HierarchicalBi-DirectionalLSTMsmodelarchitec- ture. ŁMeanSquaredError(NB+PB,B):Meansquarederror whenNBandPBareregardedasasinglelabel. However,theresultsmaynotbeaseasilyinterpretableas theclassication-relatedmetricsbecausetheydonotdirectly translatetodetectionperformance. 3.ProposedModel Inthissection,wedescribethemotivationandcomponentsof HierarchicalBi-DirectionalLSTMs(H-Bi-LSTM)neuralnet- workindetail.TheoverallarchitectureofH-Bi-LSTMmodel isshowninFigure1. 3.1.Motivation Awell-knownandeffectiveneuralnetworkcalledLSTMisde- signedtomodelsequencedependenceandhasachievedstate- of-the-artinmanyNLPtasks.Asadialogueutteranceiscom- posedbysequenceofwords,weuseaBi-LSTMtoencodea utterancebyitscorrespondingwords.Dialoguesysteminter- actswithuserinsequentialorder,andeverysystemutterance isgeneratedbyconsideringthehistoryofthedialogueupthe Figure2: LongShort-TermMemoryUnitStructure. latestturn.Itisnaturaltoabstracttheprocessofdialoguein- teractionsasasequencemodel.ThusweuseanotherBi-LSTM overtheutteranceencoderfordialoguecontextencoding.The architectureofourmodelmimicsthehierarchicalstructureof dialogue,andresultsinthenameofthemodel.Besides,Our modelisdesignedtobetrulyend-to-endsothatitcanbeeasily appliedtodifferentdialoguesystemswithoutcation,no featureengineeringorextraresourcesarerequired. 3.2.Bi-DirectionalLSTM RecurrentNeuralNetwork(RNN)isapowerfulmodelthat makesuseofsequentialinformation.Unfortunately,inprac- ticestandardRNNoftenfailstocapturelongtermdependen- ciesduetogradientvanishing/exploding[14,15].Numerous variantsofRNNareproposedtoaddressthisproblemforRNN, amongwhichLSTM[16]isprovedtoworkamazinglywelland iswidelyappliedtovariousreal-worldproblems. 3.2.1.LSTMUnit Basically,aLSTMunitconsistofamemorycellandthreemul- tiplicativegates:inputgate,forgetgateandoutputgate,which controltheproportionsofinformationtowintooroutofthe memory.Figure2givesthebasicstructureofanLSTMunit. Formally,theformulastoupdateanLSTMunitattime t aregivenby: i t = ˙ ( W i h t  1 + U i x t + b i ) (1) f t = ˙ ( W f h t  1 + U f x t + b f ) (2) ~c t =tanh( W c h t  1 + U c x t + b c ) (3) c t = f t  c t  1 + i t  ~c t (4) o t = ˙ ( W o h t  1 + U o x t + b o ) (5) h t = o t  tanh( c t ) (6) where ˙ denotessigmoidfunctionand  denoteselement-wise product. x t istheinputvectorattime t , h t ishiddenstatevector attime t . U i ; U f ; U o ; U c denoteweightmatricesofinput x t , W i ; W f ; W o ; U c denoteweightmatricesofhiddenstate h t , b i ; b f ; b o ; b c denotethecorrespondingbiasvectors. 3.2.2.Bi-LSTM Formanysequencelabellingtasksitistohaveaccess tobothpastandfutureinformation.AstandardLSTMonly knowscontextofthepastandnothingaboutthefuture.Bi- directionalLSTM(Bi-LSTM)[17]offersanelegantsolution tothisproblem,andhasbeenproventooutperformunidirec- tionalLSTMconsistentlybypreviouswork.Thebasicideais topresenteachsequenceforwardsandbackwardstotwosepa- raterecurrenthiddenlayerstocapturepastandfutureinforma- tion,respectively.Thenthetwohiddenstatesareconcatenated toformtheoutput.Theoutputattime t ofBi-LSTMis calculatedasfollows: ! h t = ! LSTM ( x t ) (7)   h t =   LSTM ( x t ) (8) h t = ! h t    h t (9) where  denotesvectorconcatenation, ! LSTM ;   LSTMdenote theforwardLSTMandbackwardLSTM,respectively. ! h t ;   h t denotethecorrespondingforwardandbackwardoutputs. 3.3.ModelArchitecture H-Bi-LSTMconsistsofcomponentsthatextractsemanticrep- resentationsofdialoguesfromlow-leveltohigh-level:word embedding,utteranceencoderandcontextencoder.Context representationsarethenfedintoafully-connectedlayerand withsoftmaxactivationtooutputprobabilitiesoverpossible breakdownlabels. 3.3.1.WordRepresentation Eachwordinthevocabularyisrepresentedasaed-length semanticvectorthroughanembeddingmatrix W w .Duetothe limitedtrainingdataofDBDC3,Weusethepre-trainedGloVe embeddings[18]forwordrepresentationtopreventov 3.3.2.UtteranceRepresentation WeuseaBi-LSTMforutteranceencoding.Givenanutterance withNwords [ w 1 ; w 2 ;:::; w N ] ,weembedthewords through W w togetwordembeddings [ x 1 ; x 2 ; ::: ; x N ] .Word embeddingsoftheutterancearethenfedintotheBi-LSTMen- coderasinputsandtheoutput h N isregardedasthecor- respondingutteranceembedding uttr .Userutteranceembed- dingandsystemutteranceembeddingofthesamedialogueturn areconcatenatedtogether. 3.3.3.ContextRepresentation Similarly,weuseanotherBi-LSTMasdialoguecontexten- coder.Giventheutterancerepresentationsofadialoguewith Mturns [ uttr 1 ; uttr 2 ; ::: ; uttr M ] ,weapplytheencoder overthemtogetthecontextrepresentationsofeveryturn [ c 1 ; c 2 ; ::: ; c M ] . 3.3.4.BreakdownDetection Thecontextembedding c canbeusedashigh-levelfeatures tocalculatedialoguebreakdownscores, w b ; b b arethecorre- spondingweightmatrixandbias,thescoresarethentranslated toprobabilitiesviaasoftmaxnormalization: ^p = softmax ( W b c + b b ) (10) Table2: EvaluationresultsonDBDC3testdatasets(English). Accu- F1 F1 JSD(NB, JSD(NB, JSD(NB+ MSE(NB, MSE(NB, MSE(NB+ Model racy (B) (PB+B) PB,B) PB+B) PB,B) PB,B) PB+B) PB,B) CRFBaseline 0.4285 0.3543 0.7622 0.4409 0.2687 0.2985 0.2185 0.2171 0.2578 MajorityBaseline 0.3720 0.3343 0.8927 0.0393 0.0237 0.0257 0.0224 0.0278 0.0264 PLECOrun1 0.2950 0.3636 0.8744 0.0714 0.0427 0.0535 0.0415 0.0455 0.0632 RSL17BDrun2 0.4310 0.3201 0.8400 0.0412 0.0256 0.0225 0.0241 0.0301 0.0246 NCDSrun1 0.3605 0.2076 0.3458 0.0412 0.0248 0.0254 0.0237 0.0287 0.0270 KTHrun1 0.3375 0.3487 0.8423 0.4445 0.2343 0.2058 0.2240 0.1752 0.1476 SAM2017run1 0.4060 0.2413 0.2160 0.2823 0.2377 0.0805 0.1441 0.2652 0.0621 H-Bi-LSTM#1 (oursrun1) 0.4295 0.3210 0.7627 0.0807 0.0438 0.0444 0.0471 0.0501 0.0497 H-Bi-LSTM#2 (oftest) 0.4595 0.3631 0.8049 0.0393 0.0231 0.0250 0.0228 0.0270 0.0276 3.3.5.Loss Weusecrossentropyofthepredicteddistributions ^p withre- specttotruedistributions p asthetrainingloss: L =  X i p i log(^ p i ) (11) 4.Experiment Inthissection,weevaluatetheeffectivenessofourmodelonthe fourEnglishdialoguesessiondatasetsasdescribedinsection2. 4.1.DataPreparing Beforefeedingdatatoourmodel,weperformthefollowing preparations: ŁEachdialogueutteranceistokenizedintowordsusing StanfordsCoreNLPtoolkit[19]andwordsareconverted tolowercasetokeepconsistentwiththevocabularyof GloVewordembeddings.Out-of-vocabularywordsare replacedwithaspecialtoken h UNK i .Utteranceswith morethan50wordsaretruncated. ŁHumanannotatedbreakdownlabelsofeverysystemut- teranceareconvertedtoprobabilities,whichwillbere- gardedasthetruedistributionwhencalculatingmodel loss. ŁForeachdialoguedataset,werandomlysplitthetraining data:80%ofdataareusedfortrainingandtheremaining 20%areusedforvalidation. 4.2.ModelTraining Weusethesamehyper-parameterswhicharedeterminedbya roughgridsearchforalltheexperiments.Forwordembed- dings,weusethepublicavailableGloVe100-dimensionalem- beddingstrainedon6billionwordsfromWikipediaandweb text.ThedimensionofbothBi-LSTMhiddenstatesare50. Tomitigateovertting,weapplydropout[20]layersonword embeddings,utteranceembeddingsandcontextembeddingsre- spectivelywith0.2dropoutprobability.WechooseAdamopti- mizertominimizethetraininglosswithaninitiallearningrate of0.001andbatchsizeof1.Themodelistrained50epochsfor parameteroptimization. 4.2.1.TrueDistributionSetting Asmentionedabove,weusecrossentropybetweendistribution predictedbyourmodelandtruedistribution.Inourexperi- ments,therearetwosettingsforcalculatingtruelabeldistribu- tion: 1.Theprobabilityofthemostfrequentdialoguebreakdown labelissetto1.0,andprobabilitiesofotherlabelsareset to0.(usedinH-Bi-LSTM#1) 2.Thetruedistributioniscalculatedbyannotationcounts ofeachbreakdownlabeldividedbytotalannotation counts.(usedinH-Bi-LSTM#2) Forexample,supposeasystemdialogueturnisannotated withNB:PB:B=1:3:4.Insetting1,thetruedistributionis[0,0, 1];Insetting2,thetruedistributionis[0.125,0.375,0.5]. H-Bi-LSTMmodelsusingtruedistributionsetting1and2 arenamedH-Bi-LSTM#1andH-Bi-LSTM#2,respectively. TheresultswesubmittedwasgeneratedbyH-Bi-LSTM#1.Af- terthereferenceannotationsofthetestdatawerereleased,we usedH-Bi-LSTM#2todoextraofexperiments. 4.3.Baselines WecomparedH-Bi-LSTMwithtwobaselinesprovidedby DBDC3:OneisaCRF-basedmethod.Thedetectorlabelsut- terancesequenceswiththethreebreakdownlabels.Thefea- turesusedarewordsinthetargetutteranceanditspreviousut- terances.Theotheroneissimplyamajoritybaselinewhich outputsonlythemostfrequentdialoguebreakdownlabelinthe developmentsetwithaveragedprobabilitydistributions. 4.4.ResultsandDiscussion Theresultsofourproposedmodelalongwithtwobase- linesandveotherteamsareshowninTable2.Weonlysub- mittedonerunofourmodelresults(denotedasH-Bi-LSTM#1). Ascanbeseenfromthetable,H-Bi-LSTM#1achievesahigh accuracythatoutperformsthemajoritybaselineby15%,and improvesthedistributionmetricsbyahugemargincomparing totheCRFbaseline.However,H-Bi-LSTM#1isnotperform- inggoodenoughcomparingtootherteams,especiallyinturns ofF1scoresandalldistribution-relatedmetrics. AftercarefullyanalyzingtheresultsofH-Bi-LSTM#1,we outtheperformanceofH-Bi-LSTM#1isbadlyhurt byitstruedistributionsetting.Thus,wesetupextraof- experiments(H-Bi-LSTM#2),resultsshowthattheper- formanceofourmodelisimproved.H-Bi-LSTM #2improvesallthemetricscomparingtoH-Bi-LSTM#1bya hugemargin,andoutperformsallteamson4metrics.Besides, F1(B),JSD(NB+PB,B)andMSE(NB,PB,B)ofH-Bi-LSTM#2 areveryclosedtothebestresults.Asfromthetable,teamswith goodresultsintheclassication-relatedmetricsdidnotperform aswellindistribution-relatedmetricsandviceversa,whileH- Bi-LSTM#2canachievegoodresultsinbothmetriccategories. Theexperimentresultsdemonstratetheeffectivenessand robustnessofourproposedmodel.However,thereisstillsome roomforimprovementcomparingtothemajoritybaseline,ei- therbyhyper-parametersormodelensemble. 5.Conclusion Inthispaper,wepresentsourproposeddialoguebreakdownde- tectorforDBDC3.Themodelweusedisinanovelhierarchical Bi-LSTMneuralnetworkthatcancapturesemanticrepresen- tationsofdialoguesprogressivelyfromwordsandutterances. SubmittedresultsonDBDC3testdatasetsdemonstratethatour modelachievesahighaccuracy,andoutperformstheCRFbase- linesinseveralevaluationmetrics.Extraofexperiments showedthattheperformanceofourmodelcanbe improvedbyusinganothertruedistributionsetting.Inthefuture work,attentionmechanism[21]canbeintegratedintoH-Bi- LSTMtohelpgeneratebetterrepresentations,webelievethis willfurtherimprovetheperformanceofthemodel. 6.Acknowledgement ThispaperissupportedbyNationalBasicResearchProgramof China(973programNo.2014CB340505). 7.References [1]J.D.WilliamsandS.Young,ﬁPartiallyobservablemarkovdeci- sionprocessesforspokendialogsystems,ﬂ ComputerSpeech& Language ,vol.21,no.2,pp.393Œ422,2007. [2]O.VinyalsandQ.V.Le,ﬁAneuralconversational model,ﬂ CoRR ,vol.abs/1506.05869,2015.[Online].Avail- able:http://arxiv.org/abs/1506.05869 [3]X.Zhou,D.Dong,H.Wu,S.Zhao,D.Yu,H.Tian,X.Liu,and R.Yan,ﬁMulti-viewresponseselectionforhuman-computercon- versation,ﬂin Proceedingsofthe2016ConferenceonEmpirical MethodsinNaturalLanguageProcessing,EMNLP2016,Austin, Texas,USA,November1-4,2016 ,2016,pp.372Œ381.[Online]. Available:http://aclweb.org/anthology/D/D16/D16-1036.pdf [4]B.MartinovskyandD.Traum,ﬁTheerroristheclue:Breakdown inhuman-machineinteraction,ﬂUNIVERSITYOFSOUTHERN CALIFORNIAMARINADELREYCAINSTFORCREATIVE TECHNOLOGIES,Tech.Rep.,2006. [5]R.Higashinaka,K.Funakoshi,Y.Kobayashi,andM.In- aba,ﬁThedialoguebreakdowndetectionchallenge:Task description,datasets,andevaluationmetrics,ﬂin Proceed- ingsoftheTenthInternationalConferenceonLanguage ResourcesandEvaluationLREC2016,Portoro  z,Slovenia, May23-28,2016. ,2016.[Online].Available:http://www.lrec- conf.org/proceedings/lrec2016/summaries/525.html [6]D.Bahdanau,K.Cho,andY.Bengio,ﬁNeuralma- chinetranslationbyjointlylearningtoalignandtrans- late,ﬂ CoRR ,vol.abs/1409.0473,2014.[Online].Available: http://arxiv.org/abs/1409.0473 [7]T.Luong,H.Pham,andC.D.Manning,ﬁEffectiveapproaches toattention-basedneuralmachinetranslation,ﬂin Proceedings ofthe2015ConferenceonEmpiricalMethodsinNatural LanguageProcessing,EMNLP2015,Lisbon,Portugal,Septem- ber17-21,2015 ,2015,pp.1412Œ1421.[Online].Available: http://aclweb.org/anthology/D/D15/D15-1166.pdf [8]J.Cassell,C.Pelachaud,N.I.Badler,M.Steedman,B.Achorn, T.Becket,B.Douville,S.Prevost,andM.Stone,ﬁAnimated conversation:rule-basedgenerationoffacialexpression,gesture &spokenintonationformultipleconversationalagents,ﬂ in Proceedingsofthe21thAnnualConferenceonComputer GraphicsandInteractiveTechniques,SIGGRAPH1994,Orlando, FL,USA,July24-29,1994 ,1994,pp.413Œ420.[Online]. Available:http://doi.acm.org/10.1145/192161.192272 [9]M.Henderson,ﬁDiscriminativeMethodsforStatisticalSpoken DialogueSystems,ﬂPh.D.dissertation,UniversityofCambridge, 2015. [10]Y.Wu,W.Wu,C.Xing,M.Zhou,andZ.Li,ﬁSequential matchingnetwork:Anewarchitectureformulti-turnresponse selectioninretrieval-basedchatbots,ﬂin Proceedingsofthe 55thAnnualMeetingoftheAssociationforComputational Linguistics,ACL2017,Vancouver,Canada,July30-August4, Volume1:LongPapers ,2017,pp.496Œ505.[Online].Available: https://doi.org/10.18653/v1/P17-1046 [11]J.Li,M.Galley,C.Brockett,J.Gao,andB.Dolan,ﬁApersona- basedneuralconversationmodel,ﬂ CoRR ,vol.abs/1603.06155, 2016.[Online].Available:http://arxiv.org/abs/1603.06155 [12]R.Higashinaka,K.Funakoshi,M.Inaba,Y.Tsunomori,T.Taka- hashi,andN.Kaji,ﬁOverviewofdialoguebreakdowndetection challenge3,ﬂin ProceedingsofDialogSystemTechnologyChal- lenge6(DSTC6)Workshop ,2017. [13]R.Higashinaka,K.Funakoshi,Y.Kobayashi,andM.Inaba, ﬁThedialoguebreakdowndetectionchallenge:Taskdescription, datasets,andevaluationmetrics.ﬂin LREC ,2016. [14]Y.Bengio,P.Simard,andP.Frasconi,ﬁLearninglong-termde- pendencieswithgradientdescentisdifﬂ IEEEtransactions onneuralnetworks ,vol.5,no.2,pp.157Œ166,1994. [15]S.Hochreiter,Y.Bengio,P.Frasconi,J.Schmidhuber etal. ,ﬁGra- dientwinrecurrentnets:thedifoflearninglong-term dependencies.ﬂ [16]S.HochreiterandJ.Schmidhuber,ﬁLongshort-termmemory,ﬂ Neuralcomputation ,vol.9,no.8,pp.1735Œ1780,1997. [17]M.SchusterandK.K.Paliwal,ﬁBidirectionalrecurrentneu- ralnetworks,ﬂ IEEETransactionsonSignalProcessing ,vol.45, no.11,pp.2673Œ2681,1997. [18]J.Pennington,R.Socher,andC.D.Manning,ﬁGlove:Global vectorsforwordrepresentation,ﬂin EmpiricalMethodsinNatural LanguageProcessing(EMNLP) ,2014,pp.1532Œ1543.[Online]. Available:http://www.aclweb.org/anthology/D14-1162 [19]C.D.Manning,M.Surdeanu,J.Bauer,J.Finkel,S.J.Bethard, andD.McClosky,ﬁTheStanfordCoreNLPnaturallanguage processingtoolkit,ﬂin AssociationforComputationalLinguistics (ACL)SystemDemonstrations ,2014,pp.55Œ60.[Online]. Available:http://www.aclweb.org/anthology/P/P14/P14-5010 [20]N.Srivastava,G.E.Hinton,A.Krizhevsky,I.Sutskever,and R.Salakhutdinov,ﬁDropout:asimplewaytopreventneuralnet- worksfromovﬂ Journalofmachinelearningresearch , vol.15,no.1,pp.1929Œ1958,2014. [21]D.Bahdanau,K.Cho,andY.Bengio,ﬁNeuralmachinetrans- lationbyjointlylearningtoalignandtranslate,ﬂ arXivpreprint arXiv:1409.0473 ,2014.  
UsingaChunk-basedDependencyParserto MineCompoundWordsfromTweets XianchaoWu Baidu(Japan)Inc. Roppongi-HillsMori-Tower34F,6-10-1RoppongiMinato-ku,Tokyo106-0032 {wuxianchao}@baidu.com1Introduction Newwordsareappearingeverydayinonlinecom- municationapplications,suchasTwitter 1.Twit- teristheworld’smostfamousonlinesocialnet-  workingandmicrobloggingservicethatenablesits  userstosend/readtext-basedmessagesofupto140  characters,knownas“tweets”.Duetothefacts  thattweetsareonlinetyped(asfastaspossible)  withinalimitednumberofcharacters,tweetsare  fullofhand-madeabbreviationsandinformalwords.  Thesefactsmakeadifferencebetweentweetsand  frequentlyusedtextsinregularwebpages,suchas  news,blogs.Consequently,traditionalhand-made  corpora(indomainssuchasnews)fornaturallan-  guageprocessing,suchaswordsegmentation,part-  of-speech(POS)tagging,parsing,needtobe“do-  mainadapted”tobewellsuitabletotweets.That  is,ifoneJapanesenew(compound)wordisnotsuc-  cessfullyrecognizedbyawordsegmentationtoolkit,  wecanhardlyensurethewordbeenwellcoveredby  aJapaneseInputMethodEditor(IME)orwelltrans-  latedbyastatisticalmachinetranslationsystem. Inthispaper,wefocusonnovelcompound worddetectionfromJapanesetweets.wepro-  poseamethodforminingcontiguouscompound  wordsfromsingle/doubleBensetsusgeneratedby  astate-of-the-artchunk-baseddependencyparser,  Cabocha2(KudoandMatsumoto,2002)which makesuseofMecab 3withIPAdictionary 4forJapanesewordsegmentation,POStagging,and 1http://twitter.com/ 2http://code.google.com/p/cabocha/3http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html 4http://code.google.com/p/mecab/downloads/detail?name=mecab- ipadic-2.7.0-20070801.tar.gz pronunciationannotating.Inthispaper,weuse  BensetsutorepresentoneJapanese“chunk”,i.e.,  onecentralwordsuchasverbornoun,followed  byzeroormanyassistantwordssuchasparticles.  BensetsuisspecializedforJapanese,andcorre-  spondstowordssuchas“chunk,phrase,clause”  inEnglish.Theminedcompoundwordswiththeir  kanapronunciationsandPOStagscanbeeasilyap-  pliedton-posmodelbasedJapaneseIMEsystems,  suchasthefreelyavailableBaiduJapaneseIME 5(Chenetal.,2012). Thispaperisorganizedasfollows:wedescribe thedetailedminingalgorithminSection2;experi-  mentsandconclusionaregivenrespectivelyinSec-  tion3andSection4. 2CompoundWordMining 2.1MiningsingleBensetsu IncaseofsingleBensetsu,compoundwordsare minedbysimplyremovetheparticlesinthe  left-hand-sideandright-hand-sideofthecentral  word(s).Specially,theparticlethatconnectstwo  centralwords(suchas“wo/ ”in“yasai/  //vegetables itameru///cooking”)willnotbetrimmed. ThisminingideaisbasedonthefactthatMecab tendstosplitoneout-of-vocabulary(OOV)word  whichcontainsseveralJapaneseKanjiindividually  intoseveralwordsinwhicheachKanjicharacter  foroneword.Yet,forCabocha,ittendstoin-  cludethesesingle-Kanji-characterwordsintoone  Bensetsu.Thus,wecanre-combinethewrongly  separatedpiecesintoone(compound)word.This 5http://ime.baidu.jp/type/?source=pstop  0š'¨$Î/²1=ˆe ˙v >Ì                                                                            Figure1:Theminingprocess. considerationalsosuitableforothertypesofcom- poundwords,suchaspersonalnames.InMecab,  onepersonalnameisfrequentlyseparatedintotwo  individualwords,familynameandgivenname.  InCabocha,familynameandgivennamearefre-  quentlyre-combinedintooneBensetsu.Thus,we  canre-combinethesetwopartsintoonecomplete  personalname. TheconsequentproblemisthattheKanapro- nunciationofthenewcombinedwordisnotnec-  essarytobethecombinationoftheKanapro-  nunciationsoftheoldindividualwords.Forex-  ample,whentwowords“kabushiki/ /”(stock)and“kaisya/ /”(com- pany)arecombinedtogether,theresultpronunci-  ationis“kabushiki-gaisya/ ”,where“ka/ ”ischangedinto“ga/ ”.Another  categoryisthat,thenewpronunciationhasnodi-  rectrelationtotheoldindividualpronunciationsany  more.Forexample,when“ichi/ /”(one)and “niti//”(day)arecombinedtogether,there- sultpronunciationcanbe“ichiniti/ ”(one day)or“tuitati/ ”(speciallyrefertotheﬁrst  dayofeverymonth). 2.2MiningdoubleBensetsus IncaseofdoubleBensetsus,weonlyextractcom- poundwordsfromtwoBensetsuswithdependency  relations.Thatis,oneBensetsutakesasthehead (node)andtheothertakesasthechild(node)inthe  dependencytree.Notethatthisstrategydoesnot  limitthepositionoftheheadnode,i.e.,notmatter  beingtheleft-hand-sideorright-hand-sideBensetsu.  Throughthisminingmethod,wecaneasilyobtain  relativelylongdistancedependencies,suchasdeter-  miningtheverbbygivenitsargument. RecallthatJapaneseisatypicalSubject-Object- Verb(SOV)language.Thus,thedirectob-  jectphraseappearsbeforetheverb.Forex-  ample,fortwoinputKanasequences“yasai-  woitameru/ ”(for“yasai/ //vegetableswo/ /particleitameru/ //cooking”,i.e.,stir-friedvegeta- bles)and“atamawoitameru/ ”(atama//headwo/ /particleitameru/ //pain,i.e.,gotaheadache),even “itameru/”takesthesimilarkeyboardtyp- ing,theﬁrst-chooseKanjiformsaretotallydiffer-  ent.Thepre-verbobjectsdeterminesthiskindofdy-  namicchoosingofKanjicharactersduringJapanese  IMEtyping. 2.3Filteringthelexicons Theoriginalentriesminedfromsing/doubleBenset- susarenotguaranteedtobewell-formedcompound  words.Wefurtherusethefollowingstrategiesfor  ﬁlteringtheoriginalentries: •removecompoundwordsstartwithastopchar-  acter/word/POSlist,thelistincludescharacters  suchas“ ,,,”;wordssuchas“ ˘ˇ,”;andPOSssuchas“ ,,”;•removecompoundwordsendwithastopchar-  acter/word/POSlist,thelistincludescharacters  suchas“ ,”;wordssuchas“ ,ˆ”;andPOSssuchas“ ”;•compoundwordsareallowedtocontainnum-  bersandEnglishlettersforcompoundwords  suchas“YouTube &'(,AKB 48”.2.4Theminingprocess Figure1showsthemajorminingprocess.Here, weusethetwitter4jpackage 6,aJavalibraryforthe TwitterAPI,especiallythetwitterStreamingAPI 7,6http://twitter4j.org/ja/index.html 7https://dev.twitter.com/docs/streaming-apis Figure2:Thedistributionofthenumberofwordsper chunkintweets. todownloadtweets.Sincethedownloadedtweets startswithuseraccountsandtheirtweetsentences,  wefurthergetridoftheuseraccountinformation  andonlykeeptheJapanesesentences.Here,weuse  agreedystrategytocollectJapanesetweets:ifat  leastonekatakanaorhirakanaappearsinthetweet,  thenitislegal.Then,weuseCabochawhichin-  tegratedMecabandIPAdictionaryforchunk-level  Japanesedependencyparsing.Thesingle/double  Bensetsusinthedependencytreesareusedtomine  compoundwords.Duringthemining/generatingof  ﬁnallexicons,theﬁlteringstrategiesareperformed. 3Experiments Usingthetwitter4jpackage,wedownloaded  44,700,736Japanesesentences(wecallthiscorpus  “tweet”hereafter).Therearetotally1,287,800,193  wordsinthesesentences,averagely28.8wordsfor  eachsentence.Figure2showsthedistributionofthe  numberofwordsperchunkintheseJapanesesen-  tences.Fromtheﬁgure,wecanobservethat32.2%  chunkscontaintwoJapanesewords.Chunksthat  containfromtwotofourwordstakeacoverageof  62.8%ofthetotalchunks. Inordertoverifythenoveltyofthecompound wordsminedfromtweets,wealsoapplythesimi-  larsingle/doubleBensetsusminingalgorithmtoan-  other200Gdata(wecallthiscorpus“200G”here-  after)whichareautomaticallycrawledfromthe  JapaneseWeb(otherthanthosetweets). Tabel1showsthenumberofcompoundwords minedfromtweets’single/doubleBensetsus.Inor-  dertocontrolthequalityofthelexicons,werespec- cut.1cut.20cut.500single(200G) -9,823,176685,363double(200G) -20,698,683794,605single(tweets) 16,497,474337,72715,044+ﬁltered 9,048,185156,5066,131+ﬁltered(-200G) -21,370(13.7%) 492(8.0%) double(tweets) 40,030,048295,5414,791+ﬁltered 19,671,721160,9682,446+ﬁltered(-200G) -35,474(22.0%) 443(18.1%) Table1:Thenumberofcompoundwordsminedfrom single/doubleBensetsus(ofthe“tweets”dataandthe  “200G”webdata),usingathresholdof1,20,and500.  Here,“ﬁltered”standsforusingthepost-ﬁlteringstrate-  gies,“-200G”standsfortheentriesthatarenotexisting  inthecorrespondinglexiconsminedusingthe200Gweb  data.Table2:Examplesofcompoundwordsextractedfrom  singleBensetsu. tivelyused1,20,and500asthefrequencythresh- oldsforlexiconﬁltering.Fromthetable,wecan  observethat: •theﬁlteringstrategiescanremovenearlyahalf  oftheentries; •therearestill8%to22%oftheﬁlteredentries  thatdonotappearinthe200G’slexicons; •wecanaveragelymine  16,497,474/44,700,736=0.369single  Bensetsuentriespersentenceand  40,030,048/44,700,736=0.896double  Bensetsuentriespersentence.Thesenumbers  reﬂectthelargevarianceoftheinformation  containedintweets. Table2listsseveralexamplesofcompoundwords extractedfromsingleBensetsu.Wecanobserve Table3:Highfrequencyexamples(top-10)ofcompound wordsextractedfromsingle/doubleBensetsus. cut.20cut.500single/double(tweets) 38.71%18.06%+ﬁltered 30.97%13.55%+ﬁltered(-200G) 12.26%9.03%Table4:Thecoverageratesofthecompoundwordlexi-  constoanexistingtwitterlexicon. thatmostofthesecompoundwordsareabbrevia- tions.Also,thecompoundwordscanbrieﬂybesep-  aratedintotwocategories.Onecategoryincludes  compoundwordsthatarestronglyrelatedtotwitter  service,suchas“ )*,RT, ”.Theother categoryincludescompoundwordsthatarestrongly  relatedtoaspecialperiod,suchas“ ()/girlspowerful(face-stylecharacters)” 8.Easyto saythatthese hotcompoundwordscanbedynam- icallyminedfromtweetsandsenttotheIMEusers  everyday. Wefurtherliststhetop-10(sortedbyfrequency) compoundwordsminedfromsingle/doubleBenset-  susinTable3.Sincewedistinguishfromuppercases  tolowercases,wordsof“ RT”and“ rt”aretaken  asdifferentcompoundwords.Oneinterestingthing  inthistableisthat,mosthighfrequencywordscon-  tainbothkana/kanjiandEnglishabbreviations,such  as“RT,rt,SJ”. Besidestheseclosedtests,wealsouseanexisting twitterlexicontotestifythelexiconsmined.Theex- 8http://ime.baidu.jp/type/lp/girlspower kaomoji/Top1 Top3 Top5 baselineIME 38.93%63.76%70.47%+single/doubleBensetsus 48.99%65.77%70.47%Table5:Thetop1/3/5precisionchangesofappending theminedsingle/doubleBensetsulexiconstoabaseline  IMEsystem. istingtwitterlexicon 9contains155entries.Table4 showsthecoveragerates.Evenweremovednearly halfoftheentriesusingtheﬁlteringstrategies,the  coverageratesdonotdropthatmuch(nearly5%to  8%).Thehighestcoverageratebelongstothesin-  gle/doubleBensetsulexiconwithaﬁlteringthresh-  oldof20. Finally,weappendtheminedsingle/double Bensetsulexicons(cut.20)totheBaiduJapanese  IMEsystem(Chenetal.,2012)bytakingthe155en-  triesasatestset.Thetop1/3/5precisionchangesare  listedinTable5.Theprecisionofthetop-1candi-  datesigniﬁcantlyimprovesfrom38.93%to48.99%  (+10.06%).Throughthesenumbers,wecansaythat  theproposedapproachishelpfulforimprovingreal  NLPapplications,suchastheJapaneseIMEsystem. 4Conclusion Wehaveproposedanalgorithmforminingnew  compoundwordsfromsingle/doubleBensetsus.Ex-  perimentsshowthatthealgorithmcanefﬁciently  collectnovelcompoundwordsfromtweetsand  large-scalemonolingualJapanesesentences.One  naturalextensionistominecompoundwordsfrom  morethantwo,ornon-contiguousBensetsus,such  as....References LongChen,XianchaoWu,andJingzhouHe.2012.Us- ingcollocationsandk-meansclusteringtoimprove then-posmodelforjapaneseime.In Proceedings ofSecondWorkshoponAdvancesinTextInputMeth-  ods(WTIM2)(COLING2012Post-ConferenceWork-  shops).TakuKudoandYujiMatsumoto.2002.Japanesedepen- dencyanalysisusingcascadedchunking.In CoNLL2002:Proceedingsofthe6thConferenceonNatu-  ralLanguageLearning2002(COLING2002Post-  ConferenceWorkshops) ,pages63–69. 9canbedownloadedfromhttp://netyougo.com/  
AnalysisofLearningfrom PositiveandUnlabeledData MarthinusC.duPlessis TheUniversityofTokyo Tokyo,113-0033,Japan christo@ms.k.u-tokyo.ac.jp GangNiu BaiduInc. Beijing,100085,China niugang@baidu.com MasashiSugiyama TheUniversityofTokyo Tokyo,113-0033,Japan sugi@k.u-tokyo.ac.jp Abstract Learningafrompositiveandunlabeleddata isanimportantclassof problemsthatareconceivableinmanypracticalapplications.Inthis paper,weshowthatthisproblemcanbesolvedbycost-sensitivelearning betweenpositiveandunlabeleddata.Wethenshowthatconvexsurrogateloss functionssuchasthehingelossmayleadtoawrongboundarydue toanintrinsicbias,buttheproblemcanbeavoidedbyusingnon-convexlossfunc- tionssuchastheramploss.Wenextanalyzetheexcessriskwhentheclassprior isestimatedfromdata,andshowthattheaccuracyisnotsensitiveto classpriorestimationiftheunlabeleddataisdominatedbythepositivedata(this isnaturallyininlier-basedoutlierdetectionbecauseinliersaredominant intheunlabeleddataset).Finally,weprovidegeneralizationerrorboundsand showthat,foranequalnumberoflabeledandunlabeledsamples,thegeneraliza- tionerroroflearningonlyfrompositiveandunlabeledsamplesisnoworsethan 2 p 2 timesthefullysupervisedcase.Thesetheoreticalarealsovalidated throughexperiments. 1Introduction Letusconsidertheproblemof learningafrompositiveandunlabeleddata (PU tion),whichisaimedatassigninglabelstotheunlabeleddataset[1].PUisconceivable invariousapplicationssuchasland-cover[2],wherepositivesamples(built-upurban areas)canbeeasilyobtained,butnegativesamples(ruralareas)aretoodiversetobelabeled.Outlier detectioninunlabeleddatabasedoninlierdatacanalsoberegardedasPU[3,4]. Inthispaper,weexplainthat,iftheclasspriorintheunlabeleddatasetisknown,PU tioncanbereducedtotheproblemof cost-sensitive [5]betweenpositiveandunlabeled data.Thus,inprinciple,thePUproblemcanbesolvedbyastandardcost-sensitive suchastheweightedsupportvectormachine[6].Thegoalofthispaperistogivenew insightintothisPUalgorithm.Ourcontributionsarethreefolds:  Theuseofconvexsurrogatelossfunctionssuchasthe hingeloss maypotentiallylead toawrongboundarybeingselected,evenwhentheunderlyingclassesare completelyseparable.Toobtainthecorrectboundary,theuseofnon-convex lossfunctionssuchasthe ramploss isessential. 1  Whentheclasspriorintheunlabeleddatasetisestimatedfromdata,theerror isgovernedbywhatwecallthe effectiveclassprior thatdependsbothonthetrueclassprior andtheestimatedclassprior.Inadditiontogainingintuitionbehindtheerror incurredinPUapracticaloutcomeofthisanalysisisthatthe errorisnotsensitivetoclass-priorestimationerroriftheunlabeleddataisdominatedby positivedata.Thiswouldbeusefulin,e.g.,inlier-basedoutlierdetectionscenarioswhere inliersamplesaredominantintheunlabeleddataset[3,4].Thisanalysiscanberegardedas anextensionoftraditionalanalysisofclasspriorsinordinaryscenarios[7,8] toPU  WeestablishgeneralizationerrorboundsforPUForanequalnumberof positiveandunlabeledsamples,theconvergencerateisnoworsethan 2 p 2 timesthefully supervisedcase. Finally,wenumericallyillustratetheabovetheoreticalthroughexperiments. 2PUascost-sensitive Inthissection,weshowthattheproblemofPUcanbecastascost-sensitive cation. Ordinary TheBayesoptimalcorrespondstothedecisionfunction f ( X ) 2f 1 ;  1 g thatminimizesthe expectedrate w.r.t.aclasspriorof ˇ : R ( f ):= ˇR 1 ( f )+(1  ˇ ) R  1 ( f ) ; where R  1 ( f ) and R 1 ( f ) denotetheexpected falsepositiverate andexpected falsenegativerate : R  1 ( f )= P  1 ( f ( X ) 6 =  1) and R 1 ( f )= P 1 ( f ( X ) 6 =1) ; and P 1 and P  1 denotethemarginalprobabilitiesofpositiveandnegativesamples. Intheempiricalriskminimizationframework,theaboveriskisreplacedwiththeirempiricalver- sionsobtainedfromfullylabeleddata,leadingtopractical[9]. Cost-sensitive Acost-sensitiveselectsafunction f ( X ) 2f 1 ;  1 g in ordertominimizetheweightedexpectedrate: R ( f ):= ˇc 1 R 1 ( f )+(1  ˇ ) c  1 R  1 ( f ) ; (1) where c 1 and c  1 aretheper-classcosts[5]. Sincescalingdoesnotmatterin(1),itisoftenusefultointerprettheper-classcostsasreweighting theproblemaccordingtonewclasspriorsproportionalto ˇc 1 and (1  ˇ ) c  1 . PU InPUaislearnedusinglabeleddatadrawnfromthe positiveclass P 1 andunlabeleddatathatisamixtureofpositiveandnegativesampleswithunknown classprior ˇ : P X = ˇP 1 +(1  ˇ ) P  1 : Sincenegativesamplesarenotavailable,letustrainatominimizetheexpectedmisclassi- ratebetweenpositiveandunlabeledsamples.Sincewedonothavenegativesamplesinthe PUsetup,wecannotdirectlyestimate R  1 ( f ) andthuswerewritetherisk R ( f ) not toinclude R  1 ( f ) .More,let R X ( f ) betheprobabilitythatthefunction f ( X ) givesthe positivelabelover P X [10]: R X ( f )= P X ( f ( X )=1) = ˇP 1 ( f ( X )=1)+(1  ˇ ) P  1 ( f ( X )=1) = ˇ (1  R 1 ( f ))+(1  ˇ ) R  1 ( f ) : (2) 2 Thentherisk R ( f ) canbewrittenas R ( f )= ˇR 1 ( f )+(1  ˇ ) R  1 ( f ) = ˇR 1 ( f )  ˇ (1  R 1 ( f ))+ R X ( f ) =2 ˇR 1 ( f )+ R X ( f )  ˇ: (3) Let  betheproportionofsamplesfrom P 1 comparedto P X ,whichisempiricallyestimatedby n n + n 0 where n and n 0 denotethenumbersofpositiveandunlabeledsamples,respectively.Therisk R ( f ) canthenbeexpressedas R ( f )= c 1 R 1 ( f )+ c X (1   ) R X ( f )  ˇ; where c 1 = 2 ˇ  and c X = 1 1   : Comparingthisexpressionwith(1),wecanthatthePUproblemissolved bycost-sensitivebetweenpositiveandunlabeleddatawithcosts c 1 and c X .Some implementationsofsupportvectormachines,suchas libsvm [6],allowforassigningweights toclasses.Inpractice,theunknownclassprior ˇ maybeestimatedbythemethodsproposedin [10,1,11]. Inthefollowingsections,weanalyzethisalgorithm. 3Necessityofnon-convexlossfunctionsinPU Inthissection,weshowthatsolvingthePUproblemwithaconvexlossfunctionmay leadtoabiasedsolution,andtheuseofanon-convexlossfunctionisessentialtoavoidthisproblem. Lossfunctionsinordinary Weconsiderordinaryproblems wheresamplesfrombothclassesareavailable.Insteadofabinarydecisionfunction f ( X ) 2  1 ; 1 g ,acontinuousdecisionfunction g ( X ) 2 R suchthat sign( g ( X ))= f ( X ) islearned.The lossfunctionthenbecomes J 0-1 ( g )= ˇ E 1 [ ` 0-1 ( g ( X ))]+(1  ˇ ) E  1 [ ` 0-1 (  g ( X ))] ; where E y istheexpectationover P y and ` 0-1 ( z ) isthezero-oneloss: ` 0-1 ( z )= ˆ 0 z> 0 ; 1 z  0 : Sincethezero-onelossishardtooptimizeinpracticeduetoitsdiscontinuousnature,itmaybe replacedwitha ramploss (asillustratedinFigure1): ` R ( z )= 1 2 max(0 ; min(2 ; 1  z )) ; givinganobjectivefunctionof J R ( g )= ˇ E 1 [ ` R ( g ( X ))]+(1  ˇ ) E  1 [ ` R (  g ( X ))] : (4) Toavoidthenon-convexityoftheramploss,the hingeloss isoftenpreferredinpractice: ` H ( z )= 1 2 max(1  z; 0) ; givinganobjectiveof J H ( g )= ˇ E 1 [ ` H ( g ( X ))]+(1  ˇ ) E  1 [ ` H (  g ( X ))] : (5) Onepracticalmotivationtousetheconvexhingelossinsteadofthenon-convexramplossisthat separability(i.e., min g J R ( g )=0 )implies ` R ( z )=0 everywhere,andforallvaluesof z forwhich ` R ( z )=0 ,wehave ` H ( z )=0 .Therefore,theconvexhingelosswillgivethesamedecision boundaryasthenon-convexramplossintheordinarysetup,undertheassumptionthat thepositiveandnegativesamplesarenon-overlapping. 3 ` H ( z )= 1 2 max(0 ; 1  z ) 1 1 2 1  1 ` H ( z ) ` R ( z ) ` R ( z )= 1 2 max(0 ; min(2 ; 1  z )) (a)Lossfunctions ` H ( z )+ ` H (  z ) 1 1  1 ` R ( z )+ ` R (  z ) (b)Resultingpenalties Figure1: ` R ( z ) denotesthe ramploss ,and ` H ( z ) denotesthe hingeloss . ` R ( z )+ ` R (  z ) isconstant but ` H ( z )+ ` H (  z ) isnotandthereforecausesapenalty. RamplossfunctioninPU Animportantquestioniswhetherthesameinterpreta- tionwillholdforPUcanthePUproblembesolvedbyusingtheconvex hingeloss?Asweshowbelow,theanswertothisquestionisunfortunatelyﬁnoﬂ. InPUtheriskisgivenby(3),anditsramp-lossversionisgivenby J PU-R ( g )=2 ˇR 1 ( f )+ R X ( f )  ˇ (6) =2 ˇ E 1 [ ` R ( g ( X ))]+[ ˇ E 1 [ ` R (  g ( X ))]+(1  ˇ ) E  1 [ ` R (  g ( X ))]]  ˇ (7) = ˇ E 1 [ ` R ( g ( X ))]+ ˇ E 1 [ ` R ( g ( X ))+ ` R (  g ( X ))] +(1  ˇ ) E  1 [ ` R (  g ( X ))]  ˇ; (8) where(6)comesfrom(3)and(7)isduetothesubstitutionof(2).Sincetheramplossissymmetric inthesenseof ` R (  z )+ ` R ( z )=1 ; (8)yields J PU-R ( g )= ˇ E 1 [ ` R ( g ( X ))]+(1  ˇ ) E  1 [ ` R (  g ( X ))] : (9) (9)isessentiallythesameas(4),meaningthatlearningwiththeramplossinthePU settingwillgivethesameboundaryasintheordinarysetting. Fornon-convexoptimizationwiththeramploss,see[12,13]. HingelossfunctioninPU Ontheotherhand,usingthehingelosstominimize(3) forPUlearninggives J PU-H ( g )=2 ˇ E 1 [ ` H ( g ( X ))]+[ ˇ E 1 [ ` H (  g ( X ))]+(1  ˇ ) E  1 [ ` H (  g ( X ))]]  ˇ; (10) = ˇ E 1 [ ` H ( g ( X ))]+(1  ˇ ) E  1 [ ` H (  g ( X ))] | {z } Ordinaryerrorterm,cf.(5) + ˇ E 1 [ ` H ( g ( X ))+ ` H (  g ( X ))] | {z } penalty  ˇ: Weseethatthehingelosshasatermthatcorrespondsto(5),butitalsohasapenalty term(seealsoFigure1).Thispenaltytermmaycauseanincorrectboundarytobe selected.Indeed,evenif g ( X ) perfectlyseparatesthedata,itmaynotminimize J PU-H ( g ) duetothe penalty.Toobtainthecorrectdecisionboundary,thelossfunctionshouldbesymmetric (andthereforenon-convex).Alternatively,sincethepenaltytermcanbeevaluated,it canbesubtractedfromtheobjectivefunction.Notethat,fortheproblemoflabelnoise,anidentical symmetryconditionhasbeenobtained[14]. Illustration: WeillustratethefailureofthehingelossonatoyPUproblemwith classconditionaldensitiesof: p ( x j y =1)= N   3 ; 1 2  and p ( x j y =1)= N  3 ; 1 2  ; where N ( ˙ 2 ) isanormaldistributionwithmean  andvariance ˙ 2 .Thehinge-lossobjective functionforPU J PU-H ( g ) ,isminimizedwithamodelof g ( x )= wx + b (theex- pectationsintheobjectivefunctioniscomputedvianumericalintegration).Theoptimaldecision 4 (a)Class-conditionaldensitiesof theproblem (b)Optimalthresholdandthreshold usingthehingeloss (c)Theratefor theoptimalandhingelosscase Figure2:IllustrationofthefailureofthehingelossforPUTheoptimalthreshold andthethresholdestimatedbythehingelossdiffer(Figure2(b)),causingadifference intherates(Figure2(c)).Thethresholdfortheramplossagreeswiththeoptimal threshold. thresholdandthethresholdforthehingelossisplottedinFigure2(b)forarangeofclasspriors. Notethatthethresholdfortheramplosswillcorrespondtotheoptimalthreshold.Fromthis wenotethatthehinge-lossthresholddiffersfromtheoptimalthreshold.Thedifferenceisespecially severeforlargerclasspriors,duetothefactthatthepenaltyisweightedbytheclass prior.Whentheclass-priorislargeenough,thelargehinge-lossthresholdcausesallsamplestobe positivelylabeled.Insuchacase,thefalsenegativerateis R 1 =0 butthefalsepositiverateis R  1 =1 .Therefore,theoverallrateforthehingelosswillbe 1  ˇ . 4Effectofinaccurateclass-priorestimation TosolvethePUproblembycost-sensitivelearningdescribedinSection2,thetrue classprior ˇ isneeded.However,sinceitisoftenunknowninpractice,itneedstobeestimated,e.g., bythemethodsproposedin[10,1,11].Sincemanyoftheestimationmethodsarebiased[1,11], itisimportanttounderstandtheofinaccurateclass-priorestimationonthe performance.Inthissection,weelucidatehowtheerrorintheestimatedclassprior b ˇ affectsthe accuracyinthePUsetting. Riskwithtrueclasspriorinordinary Intheordinaryscenarios withpositiveandnegativesamples,theriskfora f onadatasetwithclassprior ˇ isgiven asfollows([8,pp.26Œ29]and[7]): R ( f;ˇ )= ˇR 1 ( f )+(1  ˇ ) R  1 ( f ) : Theriskfortheoptimalaccordingtotheclassprior ˇ istherefore, R  ( ˇ )=min f 2F R ( f;ˇ ) Notethat R  ( ˇ ) is concave ,sinceitistheminimumofasetoffunctionsthatarelinearw.r.t. ˇ .This isillustratedinFigure3(a). Excessriskwithclasspriorestimationinordinary Supposewehavea b f thatminimizestheriskforanestimatedclassprior b ˇ : b f :=argmin f 2F R ( f; b ˇ ) : Theriskwhenapplyingthe b f onadatasetwithtrueclassprior ˇ isthenonthelinetangent totheconcavefunction R  ( ˇ ) at ˇ = b ˇ ,asillustratedinFigure3(a): b R ( ˇ )= ˇR 1 ( b f )+(1  ˇ ) R  1 ( b f ) : Thefunction b f issuboptimalat ˇ ,andresultsintheexcessrisk[8]: E ˇ = b R ( ˇ )  R ( ˇ ) : 5 e ˇ E ˇ 0 : 2 0 : 1 ˇ 1 R  ( ˇ )= b R ( ˇ ) b R ( ˇ ) Classprior Risk (a)Selectingatominimize(11)andapply- ingittoadatasetwithclassprior ˇ leadstoanexcess riskof E ˇ . (b)Theeffectiveclassprior e ˇ vs.theestimatedclass prior b ˇ fordifferenttrueclasspriors ˇ . Figure3:LearninginthePUframeworkwithanestimatedclassprior b ˇ isequivalenttoselecting awhichminimizestheriskaccordingtoaneffectiveclassprior e ˇ .(a)Thedifference betweentheeffectiveclassprior e ˇ andthetrueclassprior ˇ causesanexcessrisk E ˇ .(b)The effectiveclassprior e ˇ dependsonthetrueclassprior ˇ andtheestimatedclassprior b ˇ . ExcessriskwithclasspriorestimationinPU Wewishtoselectathat minimizestheriskin(3).Inpractice,however,weonlyknowanestimatedclassprior b ˇ .Therefore, aisselectedtominimize R ( f )=2 b ˇR 1 ( f )+ R X ( f )  b ˇ: (11) Expandingtheaboveriskbasedon(2)gives R ( f )=2 b ˇR 1 ( f )+ ˇ (1  R 1 ( f ))+(1  ˇ ) R  1 ( f )  b ˇ =(2 b ˇ  ˇ ) R 1 ( f )+(1  ˇ ) R  1 ( f )+ ˇ  b ˇ: Thus,theestimatedclassprioraffectstheriskwithrespectto 2 b ˇ  ˇ and 1  ˇ .Thisresult immediatelyshowsthatPUcannotbeperformedwhentheestimatedclassprioris lessthanhalfofthetrueclassprior: b ˇ  1 2 ˇ . Wethe effectiveclassprior e ˇ sothat 2 b ˇ  ˇ and 1  ˇ arenormalizedtosumtoone: e ˇ = 2 b ˇ  ˇ 2 b ˇ  ˇ +1  ˇ = 2 b ˇ  ˇ 2 b ˇ  2 ˇ +1 : Figure3(b)showstheoftheeffectiveclassprior e ˇ fordifferent ˇ .Thegraphshowsthat whenthetrueclassprior ˇ islarge, e ˇ tendstobearound ˇ .Whenthetrueclasspriorisknown tobelarge(suchastheproportionofinliersininlier-basedoutlierdetection),aroughclass-prior estimatorissuftohaveagoodperformance.Ontheotherhand,ifthetrueclass priorissmall,PUtendstobehardandanaccurateclass-priorestimatorisnecessary. Wealsoseethatwhenthetrueclasspriorislarge,overestimationoftheclasspriorismoreattenu- ated.Thismayexplainwhysomeclass-priorestimationmethods[1,11]stillgiveagoodpractical performanceinspiteofhavingapositivebias. 5GeneralizationerrorboundsforPU Inthissection,weanalyzethegeneralizationerrorforPUwhentrainingsamplesare clearly notidenticallydistributed . More,wederiveerrorboundsforthefunction f ( x ) ofform f ( x )= n X i =1  i k ( x i ; x )+ n 0 X j =1  0 j k ( x 0 j ; x ) ; where x 1 ;:::; x n arepositivetrainingdataand x 0 1 ;:::; x 0 n 0 arepositiveandnegativetestdata.Let A = f (  1 ;:::; n ; 0 1 ;:::; 0 n 0 ) j x 1 ;:::; x n ˘ p ( x j y =+1) ; x 0 1 ;:::; x 0 n 0 ˘ p ( x ) g 6 bethesetofallpossibleoptimalsolutionsreturnedbythealgorithmgivensometrainingdataand testdataaccordingto p ( x j y =+1) and p ( x ) .Thentheconstants C  =sup  2A ; x 1 ;:::; x n ˘ p ( x j y =+1) ; x 0 1 ;:::; x 0 n 0 ˘ p ( x )  P n i;i 0 =1  i  i 0 k ( x i ; x i 0 )+2 P n i =1 P n 0 j =1  i  0 j k ( x i ; x 0 j )+ P n 0 j;j 0 =1  0 j  0 j 0 k ( x 0 j ; x 0 j 0 )  1 = 2 ; C k =sup x 2 R d p k ( x ; x ) ; andthefunctionclass F = f f : x 7! n X i =1  i k ( x i ; x )+ n 0 X j =1  0 j k ( x 0 j ; x ) j  2A ; x 1 ;:::; x n ˘ p ( x j y =+1) ; x 0 1 ;:::; x 0 n 0 ˘ p ( x ) g : (12) Let `  ( z ) bea surrogateloss forthezero-oneloss `  ( z )= 8 < : 0 if z>; 1  z if 0 <z  ; 1 if z  0 : Forany > 0 , `  ( z ) islowerboundedby ` 0-1 ( z ) andapproaches ` 0-1 ( z ) as  approacheszero. Moreover,let e ` ( yf ( x ))= 2 y +3 ` 0-1 ( yf ( x )) and e `  ( yf ( x ))= 2 y +3 `  ( yf ( x )) : Thenwehavethefollowingtheorems(proofsareprovidedinAppendixA).Ourkeyideaisto decomposethegeneralizationerroras E p ( x ;y ) [ ` 0-1 ( yf ( x ))]= ˇ  E p ( x j y =+1) h e ` ( f ( x )) i + E p ( x ;y ) h e ` ( yf ( x )) i ; where ˇ  := p ( y =1) isthetrueclasspriorofthepositiveclass. Theorem1. Fix f 2F ,then,forany 0 << 1 ,withprobabilityatleast 1   overtherepeated samplingof f x 1 ;:::; x n g and f ( x 0 1 ;y 0 1 ) ;:::; ( x 0 n 0 ;y 0 n 0 ) g forevaluatingtheempiricalerror, 1 E p ( x ;y ) [ ` 0-1 ( yf ( x ))]  1 n 0 n 0 X j =1 e ` ( y 0 j f ( x 0 j ))  ˇ  n n X i =1 e ` ( f ( x i ))+  ˇ  2 p n + 1 p n 0  r ln(2  ) 2 : (13) Theorem2. Fix > 0 ,then,forany 0 << 1 withprobabilityatleast 1   overtherepeated samplingof f x 1 ;:::; x n g and f ( x 0 1 ;y 0 1 ) ;:::; ( x 0 n 0 ;y 0 n 0 ) g forevaluatingtheempiricalerror,every f 2F  E p ( x ;y ) [ ` 0-1 ( yf ( x ))]  1 n 0 n 0 X j =1 e `  ( y 0 j f ( x 0 j ))  ˇ  n n X i =1 e `  ( f ( x i ))+  ˇ  p n + 2 p n 0  C  C k  +  ˇ  2 p n + 1 p n 0  r ln(2  ) 2 : Inboththeorems,thegeneralizationerrorboundsareoforder O (1 = p n +1 = p n 0 ) .Thisorderis optimalforPUwherewehave n i.i.d.datafromadistributionand n 0 i.i.d.datafrom anotherdistribution.Theerrorboundsforfullysupervisedbyassumingthese n + n 0 dataarealli.i.d.,wouldbeoforder O (1 = p n + n 0 ) .However,thisassumptionisunreasonable forPUandwecannottrainfullysupervisedusingthese n + n 0 samples. Althoughtheorders(andthelosses)differslightly, O (1 = p n +1 = p n 0 ) forPUisno worsethan 2 p 2 times O (1 = p n + n 0 ) forfullysupervised(assuming n and n 0 are equal).Tothebestofourknowledge,nopreviousworkhasprovidedsuchgeneralizationerror boundsforPU 1 Theempiricalerrorthatwecannotevaluateinpracticeisintheleft-handsideof(13),andtheempirical errorandtermsthatwecanevaluateinpracticeareintheright-handsideof(13). 7 Table1:rate(inpercent)forPUontheUSPSdataset.Thebest,and equivalentby 95% t-test,isindicatedinbold. ˇ 0.2 0.4 0.6 0.8 0.9 0.95 RampHingeRampHingeRampHingeRampHingeRampHingeRampHinge 0vs1 3.36 4.40 4.854.78 5.48 5.18 4.16 4.002.68 9.86 1.71 4.94 0vs2 5.15 6.20 6.96 8.67 7.22 8.79 5.90 14.60 4.12 9.92 2.80 4.94 0vs3 3.49 5.52 4.72 8.08 5.02 8.52 4.06 16.51 2.89 9.92 2.12 4.94 0vs4 1.68 2.83 2.05 4.00 2.21 3.99 2.00 3.03 1.70 9.92 1.42 4.94 0vs5 5.21 7.42 7.22 11.16 7.46 12.04 6.16 19.78 4.36 9.92 3.21 4.94 0vs6 11.4711.61 19.87 19.5922.58 22.94 15.13 19.83 8.86 9.925.29 4.94 0vs7 1.89 3.55 2.55 4.61 2.64 3.70 2.312.491.78 9.92 1.39 4.94 0vs8 3.98 5.09 4.81 7.00 4.75 6.85 3.74 11.34 2.79 9.92 2.11 4.94 0vs9 1.22 2.76 1.60 3.86 1.73 3.56 1.61 2.24 1.38 9.92 1.13 4.94 (a)Lossfunctions (b)Classprioris ˇ =0 : 2 (c)Classprioris ˇ =0 : 6 (d)Classprioris ˇ =0 : 9 . . Figure4:Examplesoftheboundaryfortheﬁ0ﬂvs.ﬁ7ﬂdigits,obtainedbyPUlearning. Theunlabeleddatasetandtheunderlying(latent)classlabelsaregiven.Sincediscriminantfunction forthehingelosscaseisconstant 1 when ˇ =0 : 9 ,nodecisionboundarycanbedrawnandall negativesamplesare 6Experiments Inthissection,theexperimentallycomparetheperformanceoftheramplossandthehingelossinPU (weightingwasperformedw.r.t.thetrueclasspriorandtheramplosswasoptimized with[12]).WeusedtheUSPSdataset,withthedimensionalityreducedto 2 viaprincipalcomponent analysistoenableillustration. 550 sampleswereusedforthepositiveandmixturedatasets.Fromthe resultsinTable1,itisclearthattheramplossgivesamuchhigheraccuracythanthe hingeloss,especiallyforlargeclasspriors.Thisisduetothefactthattheeffectofthe penaltytermin(10)becomeslargersinceitscaleswith ˇ . Whentheclasspriorislarge,theaccuracyforthehingelossisoftencloseto 1  ˇ . Thiscanbeexplainedby(10):collectingthetermsforthepositiveexpectation,wegetaneffective lossfunctionforthepositivesamples(illustratedinFigure4(a)).When ˇ islargeenough,the positivelossisminimized,givingaconstant 1 .Theratebecomes 1  ˇ sinceitis acombinationofthefalsenegativerateandthefalsepositiverateaccordingtotheclassprior. Examplesofthediscriminationboundaryfordigitsﬁ0ﬂvs.ﬁ7ﬂaregiveninFigure4.Whenthe class-priorislow(Figure4(b)andFigure4(c))therateofthehingelossisslightly higher.Forlargeclass-priors(Figure4(d)),thehingelosscausesallsamplestobeas positive(inspectionshowedthat w = 0 and b =1 ). 7Conclusion Inthispaperwediscussedtheproblemoflearningafrompositiveandunlabeleddata. WeshowedthatPUlearningcanbesolvedusingacost-sensitiveiftheclasspriorofthe unlabeleddatasetisknown.Weshowed,however,thatanon-convexlossmustbeusedinorderto preventapenaltytermintheobjectivefunction. Inpractice,theclasspriorisunknownandestimatedfromdata.Weshowedthattheexcessriskis actuallycontrolledbyaneffectiveclasspriorwhichdependsonboththeestimatedclasspriorand thetrueclassprior.Finally,generalizationerrorboundsfortheproblemwereprovided. Acknowledgments MCdPissupportedbytheJSTCRESTprogram,GNwassupportedbythe973ProgramNo. 2014CB340505andMSissupportedbyKAKENHI23120004. 8 References [1] C.ElkanandK.Noto.Learningfromonlypositiveandunlabeleddata.In Proceedingsofthe 14thACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining(KDD2008) , pages213Œ220,2008. [2] W.Li,Q.Guo,andC.Elkan.Apositiveandunlabeledlearningalgorithmforone-classof remote-sensingdata. IEEETransactionsonGeoscienceandRemoteSensing ,49(2):717Œ725,2011. [3] S.Hido,Y.Tsuboi,H.Kashima,M.Sugiyama,andT.Kanamori.Inlier-basedoutlierdetectionviadirect densityratioestimation.InF.Giannotti,D.Gunopulos,F.Turini,C.Zaniolo,N.Ramakrishnan,and X.Wu,editors, ProceedingsofIEEEInternationalConferenceonDataMining(ICDM2008) ,pages223Œ 232,Pisa,Italy,Dec.15Œ192008. [4] C.ScottandG.Blanchard.Noveltydetection:Unlabeleddatahelp.In Proceedingsofthe TwelfthInternationalConferenceonIntelligenceandStatistics(AISTATS2009) ,pages464Œ471, ClearwaterBeach,FloridaUSA,Apr.16-182009. [5] C.Elkan.Thefoundationsofcost-sensitivelearning.In ProceedingsoftheSeventeenthInternational JointConferenceonIntelligence(IJCAI2001) ,pages973Œ978,2001. [6] C.C.ChangandC.J.Lin.LIBSVM:Alibraryforsupportvectormachines. ACMTransactionsonIntel- ligentSystemsandTechnology ,2:27:1Œ27:27,2011. [7] H.L.VanTrees. Detection,Estimation,andModulationTheory,PartI .Detection,Estimation,and ModulationTheory.JohnWileyandSons,NewYork,NY,USA,1968. [8] R.Duda,P.Hart,andD.Stork. Pattern .JohnWiley&Sons,2ndedition,2001. [9] V.Vapnik. TheNatureofStatisticalLearningTheory .Springer,2000. [10] G.Blanchard,G.Lee,andC.Scott.Semi-supervisednoveltydetection. TheJournalofMachineLearning Research ,11:2973Œ3009,2010. [11] M.C.duPlessisandM.Sugiyama.Classpriorestimationfrompositiveandunlabeleddata. IEICE TransactionsonInformationandSystems ,E97-D:1358Œ1362,2014. [12] R.Collobert,F.H.Sinz,J.Weston,andL.Bottou.Tradingconvexityforscalability.In Proceedingsofthe 23rdInternationalConferenceonMachinelearning(ICML2006) ,pages201Œ208,2006. [13] S.Suzumura,K.Ogawa,M.Sugiyama,andI.Takeuchi.Outlierpath:Ahomotopyalgorithmforrobust SVM.In Proceedingsof31stInternationalConferenceonMachineLearning(ICML2014) ,pages1098Œ 1106,Beijing,China,Jun.21Œ262014. [14] A.Ghosh,N.Manwani,andP.S.Sastry.Makingriskminimizationtoleranttolabelnoise. CoRR , abs/1403.3610,2014. [15] M.Mohri,A.Rostamizadeh,andA.Talwalkar. FoundationsofMachineLearning .MITPress,2012. 9  
PublishedasaconferencepaperatICLR2017 DSD:D ENSE -S PARSE -D ENSE T RAININGFOR D EEP N EURAL N ETWORKS SongHan  ,HuiziMao,EnhaoGong,ShijianTang,WilliamJ.Dally y StanfordUniversity {songhan,huizi,enhaog,sjtang,dally}@stanford.edu JeffPool  ,JohnTran,BryanCatanzaro NVIDIA {jpool,johntran,bcatanzaro}@nvidia.com SharanNarang  ,ErichElsen z BaiduResearch sharan@baidu.com PeterVajda,ManoharPaluri Facebook {vajdap,mano}@fb.com A BSTRACT Moderndeepneuralnetworkshavealargenumberofparameters,makingthem veryhardtotrain.WeproposeDSD,adense-sparse-densetrainingw,for regularizingdeepneuralnetworksandachievingbetteroptimizationperformance. IntheD(Dense)step,wetrainadensenetworktolearnconnectionweights andimportance.IntheS(Sparse)step,weregularizethenetworkbypruningthe unimportantconnectionswithsmallweightsandretrainingthenetworkgiventhe sparsityconstraint.IntheD(re-Dense)step,weincreasethemodelcapacity byremovingthesparsityconstraint,re-initializetheprunedparametersfromzero andretrainthewholedensenetwork.ExperimentsshowthatDSDtrainingcan improvetheperformanceforawiderangeofCNNs,RNNsandLSTMsonthe tasksofimagecaptiongenerationandspeechrecognition.On ImageNet,DSDimprovedtheTop1accuracyofGoogLeNetby1.1%,VGG-16by 4.3%,ResNet-18by1.2%andResNet-50by1.1%,respectively.OntheWSJ'93 dataset,DSDimprovedDeepSpeechandDeepSpeech2WERby2.0%and1.1%. OntheFlickr-8Kdataset,DSDimprovedtheNeuralTalkBLEUscorebyover 1.7.DSDiseasytouseinpractice:attrainingtime,DSDincursonlyoneextra hyper-parameter:thesparsityratiointheSstep.Attestingtime,DSDdoesn't changethenetworkarchitectureorincuranyinferenceoverhead.Theconsistent andperformancegainofDSDexperimentsshowstheinadequacyofthe currenttrainingmethodsforthebestlocaloptimum,whileDSDeffectively achievessuperioroptimizationperformanceforabettersolution.DSD modelsareavailabletodownloadathttps://songhan.github.io/DSD. 1I NTRODUCTION Deepneuralnetworks(DNNs)haveshownimprovementsinmanyapplicationdomains, rangingfromcomputervision(Heetal.(2015))tonaturallanguageprocessing(Luongetal.(2015)) andspeechrecognition(Amodeietal.(2015)).Theabundanceofmorepowerfulhardwaremakesit easiertotraincomplicatedDNNmodelswithlargecapacities.Theupsideofcomplicatedmodelsis thattheyareveryexpressiveandcancapturethehighlynon-linearrelationshipbetweenfeaturesand output.Thedownsideofsuchlargemodelsisthattheyarepronetocapturingthenoise,ratherthan theintendedpattern,inthetrainingdataset.Thisnoisedoesnotgeneralizetonewdatasets,leadingto overandahighvariance.  Indicatesequalcontribution y AlsoatNVIDIA z NowatGoogleBrain.eriche@google.com 1 arXiv:1607.04381v2  [cs.CV]  21 Feb 2017PublishedasaconferencepaperatICLR2017 Figure1:Dense-Sparse-DenseTrainingFlow.Thesparsetrainingregularizesthemodel,andthe densetrainingrestorestheprunedweights(red),increasingthemodelcapacitywithoutov Algorithm1: WwofDSDtraining Initialization: W (0) with W (0) ˘ N (0 ;  Output: W ( t ) . ŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŒ InitialDensePhase ŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŒ while notconverged do W ( t ) = W ( t  1)   ( t ) r f ( W ( t  1) ; x ( t  1) ) ; t = t +1 ; end ŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠ SparsePhase ŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠ- // initializethemaskbysortingandkeepingtheTop-kweights. S = sort ( j W ( t  1) j ) ;  = S k i ; Mask = 1 ( j W ( t  1) j > ) ; while notconverged do W ( t ) = W ( t  1)   ( t ) r f ( W ( t  1) ; x ( t  1) ) ; W ( t ) = W ( t )  Mask ; t = t +1 ; end ŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠ- FinalDensePhase ŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŠŒ while notconverged do W ( t ) = W ( t  1)   ( t ) r f ( W ( t  1) ; x ( t  1) ) ; t = t +1 ; end goto SparsePhase foriterativeDSD; Incontrast,simplyreducingthemodelcapacitywouldleadtotheotherextreme,causingamachine learningsystemtomisstherelevantrelationshipsbetweenfeaturesandtargetoutputs,leadingto underandahighbias.Biasandvariancearehardtooptimizeatthesametime. Tosolvethisproblem,weproposeadense-sparse-densetrainingw(DSD),anoveltrainingstrategy thatstartsfromadensemodelfromconventionaltraining,thenregularizesthemodelwithsparsity- constrainedoptimization,andincreasesthemodelcapacitybyrestoringandretrainingthe prunedweights.Attestingtime,themodelproducedbyDSDstillhasthesamearchitecture anddimensionastheoriginaldensemodel,andDSDtrainingdoesn'tincuranyinferenceoverhead. WeexperimentedDSDtrainingon7mainstreamCNN/RNN/LSTMsandfoundconsistent performancegainsoveritscomparablecounterpartforimageimagecaptioningand speechrecognition. 2DSDT RAINING F LOW OurDSDtrainingemploysathree-stepprocess:dense,sparse,re-dense.Eachstepisillustratedin Figure1andAlgorithm1.TheprogressionofweightdistributionisplottedinFigure2. InitialDenseTraining: TheDsteplearnstheconnectionweightsandimportancevianormal networktrainingonthedensenetwork.Unlikeconventionaltraining,however,thegoalofthisDstep isnotonlytolearnthevaluesoftheweights;wearealsolearningwhichconnectionsareimportant. Weusethesimpleheuristictoquantifytheimportanceoftheweightsusingtheirabsolutevalue. 2 PublishedasaconferencepaperatICLR2017 (a) (b) (c) (d) (e) Figure2:WeightdistributionoftheoriginalGoogLeNet(a),prunedGoogLeNet(b),afterretraining thesparsity-constrainedGoogLeNet(c),ignoringthesparistyconstraintandrecoveringthezero weights(d),andafterretrainingthedensenetwork(e). SparseTraining: TheSstepprunesthelow-weightconnectionsandtrainsasparsenetwork.We appliedthe same sparsitytoallthelayers,thusthere'sasinglehyperparameter:the sparsity ,the percentageofweightsthatareprunedto0.Foreachlayer W with N parameters,wesortedthe parameters,pickedthek-thlargestone  = S k asthethresholdwhere k = N  (1  sparsity ) ,and generatedabinarymasktoremovealltheweightssmallerthan  .DetailsareshowninAlgorithm1. WeremovesmallweightsbecauseoftheTaylorexpansion.ThelossfunctionanditsTaylorexpansion areshowninEquation(1)(2).Wewanttominimizetheincreasein Loss whenconductingahard thresholdingontheweights,soweneedtominimizetheandsecondtermsinEquation2. Sincewearezeroingoutparameters,  W i isactually W i  0= W i .Atthelocalminimumwhere @Loss=@W i ˇ 0 and @ 2 Loss @W 2 i > 0 ,onlythesecondordertermmatters.Sincesecondordergradient @ 2 Loss=@W 2 i isexpensivetocalculateand W i hasapowerof2,weuse j W i j asthemetricofpruning. Smaller j W i j meansasmallerincreasetothelossfunction. Loss = f ( x;W 1 ;W 2 ;W 3 ::: ) (1)  Loss = @Loss @W i  W i + 1 2 @ 2 Loss @W 2 i  W i 2 + ::: (2) Retrainingwhileenforcingthebinarymaskineachiteration,weconvertedadensenetworkintoa sparsenetworkthathasaknownsparsitysupportandcanfullyrecoverorevenincreasetheoriginal accuracyofinitialdensemodelunderthesparsityconstraint.The sparsity isthesameforallthe layersandcanbetunedusingvalidation.Weasparsityvaluebetween25%and50%generally workswellinourexperiments. FinalDenseTraining: TheDsteprecoverstheprunedconnections,makingthenetworkdense again.Thesepreviously-prunedconnectionsareinitializedtozeroandtheentirenetworkisretrained with1/10theoriginallearningrate(sincethesparsenetworkisalreadyatagoodlocalminima). Hyperparameterslikedropoutratiosandweightdecayremainedunchanged.Byrestoringthepruned connections,theDstepincreasesthemodelcapacityofthenetworkandmakesitpossibleto arriveatabetterlocalminimacomparedwiththesparsemodelfromSstep. TovisualizetheDSDtrainingw,weplottedtheprogressionofweightdistributioninFigure2.The isplottedusingGoogLeNetinception_5b3x3layer,andwefoundthisprogressionofweight distributionveryrepresentativeforVGGNetandResNetaswell.Theoriginaldistributionofweight iscenteredonzerowithtailsdroppingoffquickly.Pruningisbasedonabsolutevaluesoafterpruning thelargecenterregionistruncatedaway.Theun-prunednetworkparametersadjustthemselves duringtheretrainingphase,soin(c),theboundarybecomessoftandformsabimodaldistribution. In(d),atthebeginningofthere-densetrainingstep,alltheprunedweightscomebackagainand arereinitializedtozero.Finally,in(e),theprunedweightsareretrainedtogetherwiththeun-pruned weights.Inthisstep,wekeptthesamelearninghyper-parameters(weightdecay,learningrate,etc.) forprunedweightsandun-prunedweights.ComparingFigure(d)and(e),theun-prunedweights' distributionalmostremainedthesame,whiletheprunedweightsbecamedistributedfurtheraround zero.Theoverallmeanabsolutevalueoftheweightdistributionismuchsmaller.Thisisagood phenomenon:choosingthe smallest vectorthatsolvesthelearningproblemsuppressesirrelevant componentsoftheweightvectorMoodyetal.(1995). 3 PublishedasaconferencepaperatICLR2017 Table1:Overviewoftheneuralnetworks,datasetsandperformanceimprovementsfromDSD. NeuralNetworkDomainDatasetTypeBaselineDSDAbs.Imp.Rel.Imp. GoogLeNetVisionImageNetCNN31.1% 1 30.0% 1.1%3.6% VGG-16VisionImageNetCNN31.5% 1 27.2% 4.3%13.7% ResNet-18VisionImageNetCNN30.4% 1 29.2% 1.2%4.1% ResNet-50VisionImageNetCNN24.0% 1 22.9% 1.1%4.6% NeuralTalkCaptionFlickr-8KLSTM16.8 2 18.5 1.710.1% DeepSpeechSpeechWSJ'93RNN33.6% 3 31.6% 2.0%5.8% DeepSpeech-2SpeechWSJ'93RNN14.5% 3 13.4% 1.1%7.4% 1 Top-1error.VGG/GoogLeNetbaselinesfromCaffemodelzoo,ResNetfromFacebook. 2 BLEUscorebaselinefromNeuralTalkmodelzoo,higherthebetter. 3 Worderrorrate:DeepSpeech2istrainedwithaportionofBaiduinternaldatasetwithonlymax decodingtoshowtheeffectofDNNimprovement. 3R ELATED W ORK DropoutandDropConnect: DSD,Dropout(Srivastavaetal.(2014))andDropConnnect(Wanetal. (2013))canallregularizeneuralnetworksandpreventoverThedifferenceisthatDropoutand DropConnectusea random sparsitypatternateachSGDiteration,whileDSDtraininglearnswitha deterministic datadrivensparsitypatternthroughoutsparsetraining.OurexperimentsonVGG16, GoogLeNetandNeuralTalkshowthatDSDtrainingcanworktogetherwithDropout. Distillation: Modeldistillation(Hintonetal.(2015))isamethodthatcantransferthelearned knowledgefromalargemodeltoasmallmodel,whichismoreefcientfordeployment.Thisis anothermethodthatallowsforperformanceimprovementsinneuralnetworkswithoutarchitectural changes. ModelCompression: Bothmodelcompression(Hanetal.(2016;2015))andDSDtraininguse networkpruning(LeCunetal.(1990);Hassibietal.(1993)).Thedifferenceisthatthefocusof DSDtraininggoesbeyondmaintainingtheaccuracy.DSDisabletofurtherimprovetheaccuracyby considerablemargins.AnotherdifferenceisthatDSDtrainingdoesn'trequireaggressivepruning.A modestlyprunednetwork(50%-60%sparse)canworkwell.However,modelcompressionrequires aggressivelypruningthenetworktoachievehighcompressionrate. SparsityRegularizationandHardThresholding: thetruncation-basedsparsenetworkhasbeen theoreticallyanalyzedforlearningabroadrangeofstatisticalmodelsinhighdimensions(Langford etal.(2009);Yuan&Zhang(2013);Wangetal.(2014)).Similartrainingstrategywithiterative hardthresholdingandconnectionrestorationisproposedbyJinetal.(2016)assameperiodbut independentwork.SparsityregularizedoptimizationisheavilyappliedinCompressedSensing (Candes&Romberg(2007))tooptimalsolutionstotheinverseproblemsinhighlyunder- determinedsystemsbasedonthesparsityassumption. 4E XPERIMENTS WeappliedDSDtrainingtodifferentkindsofneuralnetworksindifferentdomains.Wefoundthat DSDtrainingimprovedtheaccuracyfor ALL thesenetworkscomparedtothebaselinenetworks thatwerenottrainedwithDSD.TheneuralnetworksarechosenfromCNN,RNNandLSTMs; thedatasetscoveredimagespeechrecognition,andcaptiongeneration.Fornetworks trainedforImageNet,wefocusonGoogLeNet,VGGandResNet,whicharewidelyusedinresearch andproduction.Anoverviewofthenetworks,datasetandaccuracyresultsareshowninTable1.For theconvolutionalnetworks,wedonotprunethelayerduringthesparsephase,sinceithasonly3 channelsandisverysensitivetopruning.Thesparsityisthe same foralltheotherlayers,including convolutionalandfully-connectedlayers.Wedonotchangeanyothertraininghyper-parameters,and theinitiallearningrateateachstageisdecayedthesameasconventionaltraining.Theepochsare decidedbywhenthelossconverges.Whenthelossnolongerdecreases,westopthetraining. 4 PublishedasaconferencepaperatICLR2017 4.1G OOG L E N ET WeexperimentedwiththeBVLCGoogLeNet(Szegedyetal.(2015))modelobtainedfromtheCaffe ModelZoo(Jia(2013)).Ithas13millionparametersand57convolutionallayers.Weprunedeach layer(excepttheto30%sparsity.Retrainingthesparsenetworkgavesomeimprovementin accuracyduetoregularization,asshowninTable2.Afterthedensetrainingstep,GoogLeNet's errorrateswerereducedby1.12%(Top-1)and0.62%(Top-5)overthebaseline. WecomparedDSDv.s.conventionaltrainingforthe samenumberofepochs bydroppingthelearning rateupon"convergence"andcontinuingtolearn.TheresultisshownasLLR(lowerthelearning rate).ThetrainingepochsforLLRisequaltothatofSparse+re-Denseasafaircomparison.LLRcan notachievethesameaccuracyasDSD. Table2:DSDresultsonGoogLeNet GoogLeNet Top-1ErrTop-5Err Sparsity Epochs LR Baseline 31.14%10.96% 0% 250 1e-2 Sparse 30.58%10.58% 30% 11 1e-3 DSD 30.02%10.34% 0% 22 1e-4 LLR 30.20%10.41% 0% 33 1e-5 Improve(abs) 1.12%0.62% - - - Improve(rel) 3.6%5.7% - - - 4.2VGGN ET WeexploredDSDtrainingonVGG-16(Simonyan&Zisserman(2014)),whichiswidelyusedin detection,segmentationandtransferlearning.ThebaselinemodelisobtainedfromtheCaffeModel Zoo(Jia(2013)).SimilartoGoogLeNet,eachlayerisprunedto30%sparsity.DSDtraininggreatly reducedtheerrorby4.31%(Top-1)and2.65%(Top-5),detailedinTable3.DSDalsowinsoverthe LLRresultbyalargemargin. Table3:DSDresultsonVGG-16 VGG-16 Top-1ErrTop-5Err Sparsity Epochs LR Baseline 31.50%11.32% 0% 74 1e-2 Sparse 28.19%9.23% 30% 1.25 1e-4 DSD 27.19%8.67% 0% 18 1e-5 LLR 29.33%10.00% 0% 20 1e-7 Improve(abs) 4.31%2.65% - - - Improve(rel) 13.7%23.4% - - - 4.3R ES N ET DeepResidualNetworks(ResNets,Heetal.(2015))werethetopperformerinthe2015ImageNet challenge.ThebaselineResNet-18andResNet-50modelsareprovidedbyFacebook(2016).We pruneto30%sparsityuniformly,andasingleDSDpassforthesenetworksreducedtop-1errorby 1.13%(ResNet-18)and0.85%(ResNet-50),showninTable4.AsecondDSDiterationcanfurther improvetheaccuracy.Asafaircomparison,wecontinuetraintheoriginalmodelbyloweringthe learningratebyanotherdecade,butcan'treachthesameaccuracyasDSD,asshownintheLLRrow. Table4:DSDresultsonResNet-18andResNet-50 ResNet-18 ResNet-50 Top-1ErrTop-5Err Top-1ErrTop-5Err Sparsity Epochs LR Baseline 30.43%10.76% 24.01%7.02% 0% 90 1e-1 Sparse 30.15%10.56% 23.55%6.88% 30% 45 1e-2 DSD 29.17 % 10.13 % 22.89 % 6.47 % 0% 45 1e-3 LLR 30.04%10.49% 23.58%6.84% 0% 90 1e-5 Improve(abs) 1.26%0.63% 1.12%0.55% - - - Improve(rel) 4.14%5.86% 4.66%7.83% - - - 5 PublishedasaconferencepaperatICLR2017 Figure3:VisualizationofDSDtrainingimprovingtheperformanceofimagecaptioning. Table5:DSDresultsonNeuralTalk NeuralTalk BLEU-1BLEU-2BLEU-3BLEU-4 Sparsity Epochs LR Baseline 57.238.625.416.8 0 19 1e-2 Sparse 58.439.726.317.5 80% 10 1e-3 DSD 59.240.727.418.5 0 6 1e-4 Improve(abs) 2.02.12.01.7 - - - Improve(rel) 3.5%5.4%7.9%10.1% - - - 4.4N EURAL T ALK WeevaluatedDSDtrainingonRNNandLSTMbeyondCNN.WeappliedDSDtoNeuralTalk (Karpathy&Fei-Fei(2015)),anLSTMforgeneratingimagedescriptions.ItusesaCNNasanimage featureextractorandanLSTMtogeneratecaptions.ToverifyDSDtrainingonLSTMs,weed theCNNweightsandonlytraintheLSTMweights.ThebaselineNeuralTalkmodelweusedisthe downloadedfromNeuralTalkModelZoo. Inthepruningstep,weprunedalllayersexcept W s ,thewordembeddinglookuptable,to80% sparse.WeusedahighersparsitythanCNN'sexperimentsbasedonthevalidationsetofWe retrainedtheremainingsparsenetworkusingthesameweightdecayandbatchsizeastheoriginal paper.Thelearningrateistunedbasedonthevalidationset,showninTable5.Retrainingthesparse networkimprovedtheBLUEscoreby[1.2,1.1,0.9,0.7].Aftergettingridofthesparsityconstraint andretrainingthedensenetwork,theresultsofDSDfurtherimprovedtheBLEUscoreby[2.0, 2.1,2.0,1.7]overbaseline. TheBLEUscoreisnotthesolecriteriameasuringauto-captionsystem.Wevisualizedthecaptions generatedbyDSDtraininginFigure3.Intheimage,thebaselinemodelmistakesthegirlwitha boyandthegirl'shairwitharockwall;thesparsemodelcantellthatit'sagirl;andtheDSDmodel canfurtheridentifytheswing.Inthethesecondimage,DSDtrainingcanmoreaccuratelytellthe playerisinawhiteuniformandtryingtomakeashot,ratherthanthebaselinejustsayinghe'sin areduniformandplayingwithaball.TheperformanceofDSDtraininggeneralizesbeyondthese examples;moreimagecaptionresultsgeneratedbyDSDtrainingareprovidedintheAppendix. 4.5D EEP S PEECH WeexploreDSDtrainingonspeechrecognitiontasksusingbothDeepSpeech1(DS1)andDeep Speech2(DS2)networks(Hannunetal.(2014);Amodeietal.(2015)). TheDS1modelisa5layernetworkwith1BidirectionalRecurrentlayer,asdescribedinTable6. ThetrainingdatasetusedforthismodelistheWallStreetJournal(WSJ),whichcontains81hoursof 6 PublishedasaconferencepaperatICLR2017 Table6:DeepSpeech1Architecture LayerID 0 1 2 3 4 5 Type Conv FC FC BidirectionalRecurrent FC CTCCost #Params 1814528 1049600 1049600 3146752 1049600 29725 Table7:DSDresultsonDeepSpeech1:WordErrorRate(WER) DeepSpeech1 WSJ'92WSJ'93 Sparsity Epochs LR DenseIter0 29.8234.57 0% 50 8e-4 SparseIter1 27.9032.99 50% 50 5e-4 DenseIter1 27.9032.20 0% 50 3e-4 SparseIter2 27.4532.99 25% 50 1e-4 DenseIter2 27.4531.59 0% 50 3e-5 Baseline 28.0333.55 0% 150 8e-4 Improve(abs) 0.581.96 - - - Improve(rel) 2.07%5.84% - - - speech.Thevalidationsetconsistsof1hourofspeech.ThetestsetsarefromWSJ'92andWSJ'93 andcontain1hourofspeechcombined.TheWordErrorRate(WER)reportedonthetestsetsforthe baselinemodelsisdifferentfromAmodeietal.(2015)duetotwofactors.First,inDeepSpeech2, themodelsweretrainedusingmuchlargerdatasetscontainingapproximately12,000hoursof multi-speakerspeechdata.Secondly,WERwasevaluatedwithbeamsearchandalanguagemodelin DeepSpeech2;herethenetworkoutputisobtainedusingonlymaxdecodingtoshowimprovementin theneuralnetworkaccuracy,andouttheotherparts. Thedensephasewastrainedfor50epochs.Inthesparsephase,weightsareprunedinthe FullyConnectedlayersandtheBidirectionalRecurrentlayeronly(theyarethemajorityofthe weights).Eachlayerisprunedtoachievethesame50%sparsityandtrainedfor50epochs.Inthe densephase,theprunedweightsareinitializedtozeroandtrainedforanother50epochs.For afaircomparisonofbaseline,weusedNesterovSGDtotrain,reducethelearningratewitheach re-training,andkeepallotherhyperparametersunchanged.Thelearningrateispickedusingour validationset. WewantedtocomparetheDSDresultswithabaselinemodeltrainedforthe same numberof epochs.The3rowsofTable7showstheWERwhentheDSDmodelistrainedfor50+50+50=150 epochs,andthe6thlineshowsthebaselinemodeltrainedby150epochs(theSame#Epochsas DSD).DSDtrainingimprovesWERby0.13(WSJ'92)and1.35(WSJ'93)giventhe samenumber ofepochs astheconventionaltraining. GivenasecondDSDiteration,accuracycanbefurtherimproved.InthesecondDSDiteration, eachlayerisprunedaway25%oftheweights.Similartotheiteration,thesparsemodeland subsequentdensemodelarefurtherretrainedfor50epochs.Thelearningrateisscaleddownforeach re-trainingstep.TheresultsareshowninTable7.Comparedwiththefullytrainedandconverged baseline,thesecondDSDiterationimprovesWERby0.58(WSJ'92)and1.96(WSJ'93),arelative improvementof2.07%(WSJ'92)and5.84%(WSJ'93).So,wecandomoreDSDiterations (DSDSD)tofurtherimprovetheperformance.AddingmoreDSDiterationshasadiminishingreturn. 4.6D EEP S PEECH 2 ToshowhowDSDworksondeepernetworks,weevaluatedDSDontheDeepSpeech2(DS2) network,describedinTable8.Thisnetworkhas7BidirectionalRecurrentlayerswithapproximately 67millionparameters,around8timeslargerthantheDS1model.AsubsetoftheinternalEnglish trainingsetisused.Thetrainingsetiscomprisedof2,100hoursofspeech.Thevalidationsetis comprisedof3.46hoursofspeech.ThetestsetsarefromWSJ'92andWSJ'93,whichcontain1hour ofspeechcombined. Table9showstheresultsofthetwoiterationsofDSDtraining.Forthesparsere-training, similartoDS1,50%oftheparametersfromtheBidirectionalRecurrentLayersandFullyConnected 7 PublishedasaconferencepaperatICLR2017 Table8:DeepSpeech2Architecture LayerID 0 1 2 3-8 9 10 Type 2DConv 2DConv BR BR FC CTCCost #Params 19616 239168 8507840 9296320 3101120 95054 Table9:DSDresultsonDeepSpeech2(WER) DeepSpeech2 WSJ'92WSJ'93 Sparsity Epochs LR DenseIter0 11.8317.42 0% 20 3e-4 SparseIter1 10.6514.84 50% 20 3e-4 DenseIter1 9.1113.96 0% 20 3e-5 SparseIter2 8.9414.02 25% 20 3e-5 DenseIter2 9.0213.44 0% 20 6e-6 Baseline 9.5514.52 0% 60 3e-4 Improve(abs) 0.531.08 - - - Improve(rel) 5.55%7.44% - - - Layersarepruned.TheBaselinemodelistrainedfor60epochstoprovideafaircomparisonwith DSDtraining.Thebaselinemodelshowsnoimprovementafter40epochs.Withoneiterationof DSDtraining,WERimprovesby0.44(WSJ'92)and0.56(WSJ'93)comparedtothefullytrained baseline. HereweshowagainthatDSDcanbeappliedmultipletimesoriterativelyforfurtherperformance gain.AseconditerationofDSDtrainingachievesbetteraccuracyasshowninTable9.Forthesecond sparseiteration,25%ofparametersintheFullyConnectedlayerandBidirectionalRecurrentlayers arepruned.OverallDSDtrainingachievesrelativeimprovementof5.55%(WSJ'92)and7.44% (WSJ'93)ontheDS2architecture.TheseresultsareinlinewithDSDexperimentsonthesmaller DS1network.WecanconcludethatDSDre-trainingcontinuestoshowimprovementinaccuracy withlargerlayersanddeepernetworks. 5D ISCUSSION Dense-Sparse-Densetrainingchangestheoptimizationprocessandimprovestheoptimizationperfor- mancewithmarginsbynudgingthenetworkwithpruningandre-densing.Weconjecture thatthefollowingaspectscontributetotheefyofDSDtraining. EscapeSaddlePoint: Basedonpreviousstudies,oneofthemostprofounddifcultiesofoptimizing deepnetworksistheproliferationofsaddlepoints(Dauphinetal.(2014)).Advancedoptimization methodshavebeenproposedtoovercomesaddlepoints.Forasimilarpurposebutwithadifferentap- proach,theproposedDSDmethodovercomesthesaddlepointsbypruningandre-densingframework. Pruningtheconvergedmodelperturbsthelearningdynamicsandallowsthenetworktojumpaway fromsaddlepoints,whichgivesthenetworkachancetoconvergeatabetterlocalorglobalminimum. ThisideaisalsosimilartoSimulatedAnnealing(Hwang(1988)).WhileSimulatedAnnealing randomlyjumpswithdecreasingprobabilityonthesearchgraph,DSDdeterministicallydeviates fromtheconvergedsolutionachievedinthedensetrainingphasebyremovingthesmallweights andenforcingasparsitysupport.SimilartoSimulatedAnnealing,whichcanescapesub-optimal solutionsmultipletimesintheentireoptimizationprocess,DSDcanalsobeappliediterativelyto achievefurtherperformancegains,asshownintheDeepSpeechresults. BetterMinima: Afterescapingsaddlepoint,DSDachievedbetterminima.We measuredboththetraininglossandvalidationloss,DSDtrainingdecreasedthelossanderroron boththetrainingandthevalidationsetsonImageNet.Wehavealsovalidatedtheofthe improvementscomparedwithconventionalbyt-test,shownintheappendix. RegularizedandSparseTraining: Thesparsityregularizationinthesparsetrainingstepmovesthe optimizationtoalower-dimensionalspacewherethelosssurfaceissmootherandtendtobemore robusttonoise.MorenumericalexperimentsvthatbothsparsetrainingandtheDSD reducethevarianceandleadtolowererror(shownintheappendix). 8 PublishedasaconferencepaperatICLR2017 Robustre-initialization: Weightinitializationplaysabigroleindeeplearning(Mishkin&Matas (2015)).Conventionaltraininghasonlyonechanceofinitialization.DSDgivestheoptimizationa second(ormore)chanceduringthetrainingprocesstore-initializefrommorerobustsparsetraining solution.Were-densethenetworkfromthesparsesolutionwhichcanbeseenasazeroinitialization forprunedweights.Otherinitializationmethodsarealsoworthtrying. BreakSymmetry: Thepermutationsymmetryofthehiddenunitsmakestheweightssymmetrical, thuspronetoco-adaptationintraining.InDSD,pruningtheweightsbreaksthesymmetryofthe hiddenunitsassociatedwiththeweights,andtheweightsareasymmetricalinthedensephase. 6C ONCLUSION WeintroduceDSD,adense-sparse-densetrainingframeworkthatregularizesneuralnetworksby pruningandthenrestoringconnections.Ourmethodlearnswhichconnectionsareimportantduring theinitialdensesolution.Thenitregularizesthenetworkbypruningtheunimportantconnections andretrainingtoasparserandmorerobustsolutionwithsameorbetteraccuracy.Finally,thepruned connectionsarerestoredandtheentirenetworkisretrainedagain.Thisincreasesthedimensionality ofparameters,andthusmodelcapacity,fromthesparsermodel. DSDtrainingachievessuperioroptimizationperformance.Wehighlightourexperimentsusing GoogLeNet,VGGNet,andResNetonImageNet;NeuralTalkonFlickr-8K;andDeepSpeech-1&2 ontheWSJdataset.ThisshowsthattheaccuracyofCNNs,RNNs,andLSTMscanbe fromDSDtraining.Ournumericalresultsandempiricaltestsshowtheinadequacyofcurrent trainingmethodsforwhichwehaveprovidedaneffectivesolution. 9 PublishedasaconferencepaperatICLR2017 R EFERENCES DarioAmodei,RishitaAnubhai,EricBattenberg,CarlCase,JaredCasper,BryanCatanzaro,JingdongChen, MikeChrzanowski,AdamCoates,GregDiamos,etal.Deepspeech2:End-to-endspeechrecognitionin englishandmandarin. arXivpreprintarXiv:1512.02595 ,2015. EmmanuelCandesandJustinRomberg.Sparsityandincoherenceincompressivesampling. Inverseproblems , 23(3):969,2007. YannNDauphin,RazvanPascanu,CaglarGulcehre,KyunghyunCho,SuryaGanguli,andYoshuaBengio. Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convexoptimization.In Advances inneuralinformationprocessingsystems ,pp.2933Œ2941,2014. Facebook.Facebook.ResNet.Torch.https://github.com/facebook/fb.resnet.torch,2016. SongHan,JeffPool,JohnTran,andWilliamDally.Learningbothweightsandconnectionsforefneural network.In AdvancesinNeuralInformationProcessingSystems ,pp.1135Œ1143,2015. SongHan,HuiziMao,andWilliamJDally.Deepcompression:Compressingdeepneuralnetworkswithpruning, trainedquantizationandhuffmancoding. InternationalConferenceonLearningRepresentations ,2016. AwniHannun,CarlCase,JaredCasper,BryanCatanzaro,GregDiamos,ErichElsen,RyanPrenger,Sanjeev Satheesh,ShubhoSengupta,AdamCoates,andAndrewNg.Deepspeech:Scalingupend-to-endspeech recognition. arXiv,preprintarXiv:1412.5567 ,2014. BabakHassibi,DavidGStork,etal.Secondorderderivativesfornetworkpruning:Optimalbrainsurgeon. Advancesinneuralinformationprocessingsystems ,pp.164Œ164,1993. KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforimagerecognition. arXiv preprintarXiv:1512.03385 ,2015. GeoffreyHinton,OriolVinyals,andJeffDean.Distillingtheknowledgeinaneuralnetwork. arXivpreprint arXiv:1503.02531 ,2015. Chii-RueyHwang.Simulatedannealing:theoryandapplications. ActaApplicandaeMathematicae ,12(1): 108Œ111,1988. YangqingJia.BVLCcaffemodelzoo.http://caffe.berkeleyvision.org/model_zoo,2013. XiaojieJin,XiaotongYuan,JiashiFeng,andShuichengYan.Trainingskinnydeepneuralnetworkswithiterative hardthresholdingmethods. arXivpreprintarXiv:1607.05423 ,2016. AndrejKarpathyandLiFei-Fei.Deepvisual-semanticalignmentsforgeneratingimagedescriptions.In ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition ,2015. JohnLangford,LihongLi,andTongZhang.Sparseonlinelearningviatruncatedgradient.In Advancesinneural informationprocessingsystems ,pp.905Œ912,2009. YannLeCun,JohnS.Denker,andSaraA.Solla.Optimalbraindamage.In AdvancesinNeuralInformation ProcessingSystems ,pp.598Œ605.MorganKaufmann,1990. Minh-ThangLuong,HieuPham,andChristopherDManning.Effectiveapproachestoattention-basedneural machinetranslation. arXivpreprintarXiv:1508.04025 ,2015. DmytroMishkinandJiriMatas.Allyouneedisagoodinit. arXivpreprintarXiv:1511.06422 ,2015. JMoody,SHanson,AndersKrogh,andJohnAHertz.Asimpleweightdecaycanimprovegeneralization. Advancesinneuralinformationprocessingsystems ,4:950Œ957,1995. KarenSimonyanandAndrewZisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition. arXivpreprintarXiv:1409.1556 ,2014. NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.Dropout:A simplewaytopreventneuralnetworksfromov JMLR ,15:1929Œ1958,2014. ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,ScottReed,DragomirAnguelov,DumitruErhan, VincentVanhoucke,andAndrewRabinovich.Goingdeeperwithconvolutions.In ProceedingsoftheIEEE ConferenceonComputerVisionandPatternRecognition ,pp.1Œ9,2015. LiWan,MatthewZeiler,SixinZhang,YannLCun,andRobFergus.Regularizationofneuralnetworksusing dropconnect.In ICML ,pp.1058Œ1066,2013. ZhaoranWang,QuanquanGu,YangNing,andHanLiu.Highdimensionalexpectation-maximizationalgorithm: Statisticaloptimizationandasymptoticnormality. arXivpreprintarXiv:1412.8729 ,2014. Xiao-TongYuanandTongZhang.Truncatedpowermethodforsparseeigenvalueproblems. TheJournalof MachineLearningResearch ,14(1):899Œ925,2013. 10 PublishedasaconferencepaperatICLR2017 A.A PPENDIX :S IGNIFICANCEOF DSD IMPROVEMENTS DSDtrainingimprovesthebaselinemodelperformancebyconsecutivelypruningandre-densingthenetwork weights.Weconductedmoreintensiveexperimentstovalidatethattheimprovementsareandnotdue toanyrandomnessintheoptimization.Inordertoevaluatethewerepeatedthebaselinetraining, DSDtraining(retrainingonbaseline)andconventional(retrainingonthesamebaseline)multiple times.ThestatisticalofDSDimprovementsareontheCifar-10datasetusingResNet. 1.S IGNIFICANTIMPROVEMENTSON C IFAR -10 USING R ES N ET -20 Cifar-10isasmallerimagerecognitionbenchmarkwith50,00032x32colorimagesfortrainingand10,000 fortesting.TrainingonCifar-10isfastenoughthusitisapplicabletoconductintensiveexperimentswithin reasonabletimetoevaluateDSDperformance.Thebaselinemodelsweretrainedwiththestandard164epochs andinitialLRof0.1asrecommendedinthereleasedcode(Facebook,2016).After164epochs,weobtained themodelwitha8.26%top-1testingerrorthatisconsistentwiththeFacebookresult.Initializedfromthis baselinemodel,werepeated16timesofre-trainingusingDSDtrainingand16timesusingconventional TheDSDusedsparsityof50%and90epochs(45forsparsetrainingand45forre-densingtraining). Asafaircomparison,theconventionalisalsobasedonthe same baselinemodelwiththe same hyper-parametersandsettings(90epochs,45LRof0.001and45LRof0.0001). Detailedresultsarelistedbelow.OnCifar-10andusingResNet-20architecture,theDSDtrainingonaverage achievedTop-1testingerrorof 7 : 89% ,whichisa0.37%absoluteimprovement(4.5%relativeimprovement) overthebaselinemodelandrelatively1.1%betterthanwhattheconventionalTheexperimentalso showsthatDSDtrainingcanreducethevarianceoflearning:thetrainedmodelsafterthesparsetrainingand theDSDtrainingbothhavelowerstandarddeviationoferrorscomparedwiththeircounterpartsusing conventional Table10:ValidationofDSDonCifar10datausingResNet-20 ResNet-20 Avg.Top-1ErrSD.Top-1Err Sparsity Epochs LR Baseline 8.26%- 0% 164 1e-1 DirectFinetune(Firsthalf) 8.16%0.08% 0% 45 1e-3 DirectFinetune(Secondhalf) 7.97%0.04% 0% 45 1e-4 DSD(Fisthalf,Sparse) 8.12%0.05% 50% 45 1e-3 DSD(Secondhalf,Dense) 7.89%0.03% 0% 45 1e-4 Improvefrombaseline(abs) 0.37%- - - - Improvefrombaseline(rel) 4.5% - - - - Weusedt-test(unpaired)tocomparethetop-1testingerrorrateofthemodelstrainedusingDSDandconventional methods.TheresultsdemonstratetheDSDtrainingachievesimprovementsfromboththebaseline model(p<0.001)andconventionaltuning(p<0.001). Figure4:ofDSDimprovementsoverbaselineand Basedontheresultsabove,DSDimprovesconventionalbaselinetrainingandisalso betterandmorerobustthanconventional 11 PublishedasaconferencepaperatICLR2017 B.A PPENDIX :M ORE E XAMPLESOF DSDT RAINING I MPROVESTHE C APTIONS G ENERATEDBY N EURAL T ALK (I MAGESFROM F LICKR -8KT EST S ET ) 12 PublishedasaconferencepaperatICLR2017 13  
DeepVoice:Real-timeNeuralText-to-Speech Sercan ¨ O. y SERCANARIK @ BAIDU . COM MikeChrzanowski y MIKECHRZANOWSKI @ BAIDU . COM AdamCoates y ADAMCOATES @ BAIDU . COM GregoryDiamos y GREGDIAMOS @ BAIDU . COM AndrewGibiansky y GIBIANSKYANDREW @ BAIDU . COM YongguoKang y KANGYONGGUO @ BAIDU . COM XianLi y LIXIAN 05@ BAIDU . COM JohnMiller y MILLERJOHN @ BAIDU . COM AndrewNg y ANDREWNG @ BAIDU . COM JonathanRaiman y JONATHANRAIMAN @ BAIDU . COM ShubhoSengupta y SSENGUPTA @ BAIDU . COM MohammadShoeybi y MOHAMMAD @ BAIDU . COM BaiduSiliconValleyIntelligenceLab,1195BordeauxDr.Sunnyvale,CA94089 Abstract WepresentDeepVoice,aproduction-quality text-to-speechsystemconstructedentirelyfrom deepneuralnetworks.DeepVoicelaysthe groundworkfortrulyend-to-endneuralspeech synthesis.Thesystemcomprisesvema- jorbuildingblocks:asegmentationmodelfor locatingphonemeboundaries,agrapheme-to- phonemeconversionmodel,aphonemeduration predictionmodel,afundamentalfrequencypre- dictionmodel,andanaudiosynthesismodel. Forthesegmentationmodel,weproposeanovel wayofperformingphonemeboundarydetection withdeepneuralnetworksusingconnectionist temporalon(CTC)loss.Fortheau- diosynthesismodel,weimplementavariant ofWaveNetthatrequiresfewerparametersand trainsfasterthantheoriginal.Byusinganeu- ralnetworkforeachcomponent,oursystemis simplerandmorexiblethantraditionaltext-to- speechsystems,whereeachcomponentrequires laboriousfeatureengineeringandextensivedo- mainexpertise.Finally,weshowthatinference withoursystemcanbeperformedfasterthanreal timeanddescribeoptimizedWaveNetinference kernelsonbothCPUandGPUthatachieveupto 400xspeedupsoverexistingimplementations. y Authorsarelistedalphabeticallybylastname. SubmittedFebruary24,2017forreviewfortheInternationalCon- ferenceonMachineLearning(ICML)2017.Copyright2017by theauthor(s). 1.Introduction Synthesizinghumanspeechfromtext,commonly knownastext-to-speech(TTS),isanessentialcomponent inmanyapplicationssuchasspeech-enableddevices,navi- gationsystems,andaccessibilityforthevisually-impaired. Fundamentally,itallowshuman-technologyinteraction withoutrequiringvisualinterfaces.ModernTTSsystems arebasedoncomplex,multi-stageprocessingpipelines, eachofwhichmayrelyonhand-engineeredfeaturesand heuristics.Duetothiscomplexity,developingnewTTS systemscanbeverylaborintensiveanddif DeepVoiceisinspiredbytraditionaltext-to-speech pipelinesandadoptsthesamestructure,whilereplacingall componentswithneuralnetworksandusingsimplerfea- tures:weconverttexttophonemeandthenusean audiosynthesismodeltoconvertlinguisticfeaturesinto speech( Taylor , 2009 ).Unlikepriorwork(whichuses hand-engineeredfeaturessuchasspectralenvelope,spec- tralparameters,aperiodicparameters,etc.),ouronlyfea- turesarephonemeswithstressannotations,phonemedu- rations,andfundamentalfrequency(F0).Thischoiceof featuresmakesoursystemmorereadilyapplicabletonew datasets,voices,anddomainswithoutanymanualdataan- notationoradditionalfeatureengineering.Wedemonstrate thisclaimbyretrainingourentirepipelinewithoutanyhy- perparameterchangesonanentirelynewdatasetthatcon- tainssolelyaudioandunalignedtextualtranscriptionsand generatingrelativelyhighqualityspeech.Inaconventional TTSsystemthisadaptationrequiresdaystoweeksoftun- ing,whereasDeepVoiceallowsyoutodoitinonlyafew hoursofmanualeffortandthetimeittakesmodelstotrain. arXiv:1702.07825v2  [cs.CL]  7 Mar 2017DeepVoice:Real-timeNeuralTTS Real-timeinferenceisarequirementforaproduction- qualityTTSsystem;withoutit,thesystemisunusablefor mostapplicationsofTTS.Priorworkhasdemonstratedthat aWaveNet( vandenOordetal. , 2016 )cangeneratecloseto human-levelspeech.However,WaveNetinferenceposesa dauntingcomputationalproblemduetothehigh-frequency, autoregressivenatureofthemodel,andithasbeenhitherto unknownwhethersuchmodelscanbeusedinaproduc- tionsystem.Weanswerthisquestionintheafveand demonstrateeffaster-than-real-timeWaveNetinfer- encekernelsthatproducehigh-quality16kHzaudioand realizea400XspeedupoverpreviousWaveNetinference implementations( Paineetal. , 2016 ). 2.RelatedWork Previousworkusesneuralnetworksassubstitutesfor severalTTSsystemcomponents,includinggrapheme-to- phonemeconversionmodels( Raoetal. , 2015 ; Yao& Zweig , 2015 ),phonemedurationpredictionmodels( Zen &Sak , 2015 ),fundamentalfrequencypredictionmodels ( Pascual&Bonafonte , 2016 ; Ronankietal. , 2016 ),and audiosynthesismodels( vandenOordetal. , 2016 ; Mehri etal. , 2016 ).UnlikeDeepVoice,however,noneofthese systemssolvetheentireproblemofTTSandmanyofthem usespecializedhand-engineeredfeaturesdevelopedspecif- icallyfortheirdomain. Mostrecently,therehasbeenalotofworkinparamet- ricaudiosynthesis,notablyWaveNet,SampleRNN,and Char2Wav( vandenOordetal. , 2016 ; Mehrietal. , 2016 ; Soteloetal. , 2017 ).WhileWaveNetcanbeusedfor bothconditionalandunconditionalaudiogeneration,Sam- pleRNNisonlyusedforunconditionalaudiogeneration. Char2WavextendsSampleRNNwithanattention-based phonemedurationmodelandtheequivalentofanF0pre- dictionmodel,effectivelyprovidinglocalconditioningin- formationtoaSampleRNN-basedvocoder. DeepVoicediffersfromthesesystemsinseveralkeyas- pectsthatnotablyincreasethescopeoftheproblem.First, DeepVoiceiscompletelystandalone;traininganewDeep Voicesystemdoesnotrequireapre-existingTTSsystem, andcanbedonefromscratchusingadatasetofshortau- dioclipsandcorrespondingtextualtranscripts.Incontrast, reproducingeitheroftheaforementionedsystemsrequires accessandunderstandingofapre-existingTTSsystem,be- causetheyusefeaturesfromanotherTTSsystemeitherat trainingorinferencetime. Second,DeepVoiceminimizestheuseofhand-engineered features;itusesone-hotencodedcharactersforgrapheme tophonemeconversion,one-hotencodedphonemesand stresses,phonemedurationsinmilliseconds,andnormal- izedlogfundamentalfrequencythatcanbecomputedfrom waveformsusinganyF0estimationalgorithm.Allofthese caneasilybeobtainedfromaudioandtranscriptswithmin- imaleffort.Incontrast,priorworksuseamuchmorecom- plexfeaturerepresentation,thateffectivelymakesrepro- ducingthesystemimpossiblewithoutapre-existingTTS system.WaveNetusesseveralfeaturesfromaTTSsystem ( Zenetal. , 2013 ),thatincludevaluessuchasthenumber ofsyllablesinaword,positionofsyllablesinthephrase, positionofthecurrentframeinthephoneme,anddynamic featuresofthespeechspectrumlikespectralandexcitation parameters,aswellastheirtimederivatives.Char2Wav reliesonvocoderfeaturesfromtheWORLDTTSsystem ( Moriseetal. , 2016 )forpre-trainingtheiralignmentmod- ulewhichincludeF0,spectralenvelope,andaperiodicpa- rameters. Finally,wefocusoncreatingaproduction-readysystem, which requires thatourmodelsruninreal-timeforinfer- ence.DeepVoicecansynthesizeaudioinfractionsofa second,andoffersatunabletrade-offbetweensynthesis speedandaudioquality.Incontrast,previousresultswith WaveNetrequireseveralminutesofruntimetosynthesize onesecondofaudio.Weareunawareofsimilarbench- marksforSampleRNN,butthe3-tierarchitectureasde- scribedintheoriginalpublicationrequiresapproximately 4-5Xasmuchcomputeduringinferenceasourlargest WaveNetmodels,sorunningthemodelinreal-timemay provechallenging. 3.TTSSystemComponents AsshowninFig. 1 ,theTTSsystemconsistsofvemajor buildingblocks:  The grapheme-to-phonememodel convertsfrom writtentext(Englishcharacters)tophonemes(en- codedusingaphonemicalphabetsuchasARPABET).  The segmentationmodel locatesphonemebound- ariesinthevoicedataset.Givenanaudioanda phoneme-by-phonemetranscriptionoftheaudio,the segmentationmodelwhereintheaudioeach phonemebeginsandends.  The phonemedurationmodel predictsthetemporal durationofeveryphonemeinaphonemesequence(an utterance).  The fundamentalfrequencymodel predictswhether aphonemeisvoiced.Ifitis,themodelpre- dictsthefundamentalfrequency(F0)throughoutthe phoneme'sduration.  The audiosynthesismodel combinestheoutputs ofthegrapheme-to-phoneme,phonemeduration,and fundamentalfrequencypredictionmodelsandsynthe- sizesaudioatahighsamplingrate,correspondingto DeepVoice:Real-timeNeuralTTS Figure1. Systemdiagramdepicting(a)trainingprocedureand(b)inferenceprocedure,withinputsontheleftandoutputsontheright. Inoursystem,thedurationpredictionmodelandtheF0predictionmodelareperformedbyasingleneuralnetworktrainedwithajoint loss.Thegrapheme-to-phonememodelisusedasafallbackforwordsthatarenotpresentinaphonemedictionary,suchasCMUDict. Dottedlinesdenotenon-learnedcomponents. thedesiredtext. Duringinference,textisfedthroughthegrapheme-to- phonememodeloraphonemedictionarytogenerate phonemes.Next,thephonemesareprovidedasinputsto thephonemedurationmodelandF0predictionmodelto assigndurationstoeachphonemeandgenerateanF0con- tour.Finally,thephonemes,phonemedurations,andF0 areusedaslocalconditioninginputfeaturestotheaudio synthesismodel,whichgeneratestheutterance. Unliketheothermodels,thesegmentationmodelisnot usedduringinference.Instead,itisusedtoannotate thetrainingvoicedatawithphonemeboundaries.The phonemeboundariesimplydurations,whichcanbeused totrainthephonemedurationmodel.Theaudio,anno- tatedwithphonemesandphonemedurationsaswellas fundamentalfrequency,isusedtotraintheaudiosynthe- sismodel. Inthefollowingsections,wedescribeallthebuilding blocksindetail. 3.1.Grapheme-to-PhonemeModel Ourgrapheme-to-phonememodelisbasedontheencoder- decoderarchitecturedevelopedby( Yao&Zweig , 2015 ). However,weuseamulti-layerbidirectionalencoderwith agatedrecurrentunit(GRU)nonlinearityandanequally deepunidirectionalGRUdecoder( Chungetal. , 2014 ).The initialstateofeverydecoderlayerisinitializedtothe hiddenstateofthecorrespondingencoderforwardlayer. Thearchitectureistrainedwithteacherforcinganddecod- ingisperformedusingbeamsearch.Weuse3bidirectional layerswith1024unitseachintheencoderand3unidirec- tionallayersofthesamesizeinthedecoderandabeam searchwithawidthof5candidates.Duringtraining,we usedropoutwithprobability0.95aftereachrecurrentlayer. Fortraining,weusetheAdamoptimizationalgorithmwith  1 =0 : 9 ; 2 =0 : 999 ;" =10  8 ,abatchsizeof64,a learningrateof 10  3 ,andanannealingrateof0.85applied every1000iterations( Kingma&Ba , 2014 ). 3.2.SegmentationModel Oursegmentationmodelistrainedtooutputthealign- mentbetweenagivenutteranceandasequenceoftarget phonemes.Thistaskissimilartotheproblemofaligning speechtowrittenoutputinspeechrecognition.Inthatdo- main,theconnectionisttemporalcla(CTC)loss functionhasbeenshowntofocusoncharacteralignments tolearnamappingbetweensoundandtext( Gravesetal. , 2006 ).Weadapttheconvolutionalrecurrentneuralnet- workarchitecturefromastate-of-the-artspeechrecogni- tionsystem( Amodeietal. , 2015 )forphonemeboundary detection. AnetworktrainedwithCTCtogeneratesequencesof phonemeswillproducebriefpeaksforeveryoutput phoneme.Althoughthisissuftoroughlyalignthe phonemestotheaudio,itisinsuftodetectprecise DeepVoice:Real-timeNeuralTTS phonemeboundaries.Toovercomethis,wetraintopredict sequencesofphoneme pairs ratherthansinglephonemes. Thenetworkwillthentendtooutputphonemepairsat timestepsclosetotheboundarybetweentwophonemesin apair. Toillustrateourlabelencoding,considerthestring ﬁHello!ﬂ.Toconvertthistoasequenceofphonemepair labels,converttheutterancetophonemes(usingapro- nunciationdictionarysuchasCMUDictoragrapheme-to- phonememodel)andpadthephonemesequenceoneither endwiththesilencephonemetogetﬁsilHHEHLOWsilﬂ. Finally,constructconsecutivephonemepairsandgetﬁ(sil, HH),(HH,EH),(EH,L),(L,OW),(OW,sil)ﬂ. Inputaudioisfeaturizedbycomputing20Mel-frequency cepstralcoef(MFCCs)withatenmillisecondstride. Ontopoftheinputlayer,therearetwoconvolutionlay- ers(2Dconvolutionsintimeandfrequency),threebidirec- tionalrecurrentGRUlayers,andasoftmaxoutput layer.Theconvolutionlayersusekernelswithunitstride, heightnine(infrequencybins),andwidthve(intime) andtherecurrentlayersuse512GRUcells(foreachdi- rection).Dropoutwithaprobabilityof0.95isapplied afterthelastconvolutionandrecurrentlayers.Tocom- putethephoneme-pairerrorrate(PPER),wedecodeusing beamsearch.Todecodephonemeboundaries,weperform abeamsearchwithwidth50withtheconstraintthatneigh- boringphonemepairsoverlapbyatleastonephonemeand keeptrackofthepositionsintheutteranceofeachphoneme pair. Fortraining,weusetheAdamoptimizationalgorithmwith  1 =0 : 9 ; 2 =0 : 999 ;" =10  8 ,abatchsizeof128,a learningrateof 10  4 ,andanannealingrateof0.95applied every500iterations( Kingma&Ba , 2014 ). 3.3.PhonemeDurationandFundamentalFrequency Model Weuseasinglearchitecturetojointlypredictphonemedu- rationandtime-dependentfundamentalfrequency.Thein- puttothemodelisasequenceofphonemeswithstresses, witheachphonemeandstressbeingencodedasaone-hot vector.Thearchitecturecomprisestwofullyconnectedlay- erswith256unitseachfollowedbytwounidirectionalre- currentlayerswith128GRUcellseachandafully- connectedoutputlayer.Dropoutwithaprobabilityof0.8is appliedaftertheinitialfully-connectedlayersandthelast recurrentlayer. Thelayerproducesthreeestimationsforeveryinput phoneme:thephonemeduration,theprobabilitythatthe phonemeisvoiced(i.e.hasafundamentalfrequency),and 20time-dependentF0values,whicharesampleduniformly overthepredictedduration. Themodelisoptimizedbyminimizingajointlossthat combinesphonemedurationerror,fundamentalfrequency error,thenegativeloglikelihoodoftheprobabilitythat thephonemeisvoiced,andapenaltytermproportionalto theabsolutechangeofF0withrespecttotimetoimpose smoothness.Thefunctionalformofthelossfunc- tionisdescribedinAppendix B . Fortraining,weusetheAdamoptimizationalgorithmwith  1 =0 : 9 ; 2 =0 : 999 ;" =10  8 ,abatchsizeof128,a learningrateof 3  10  4 ,andanannealingrateof0.9886 appliedevery400iterations( Kingma&Ba , 2014 ). 3.4.AudioSynthesisModel OuraudiosynthesismodelisavariantofWaveNet. WaveNetconsistsofaconditioningnetwork,whichup- sampleslinguisticfeaturestothedesiredfrequency,and anautoregressivenetwork,whichgeneratesaprobabil- itydistribution P ( y ) overdiscretizedaudiosamples y 2 f 0 ; 1 ;:::; 255 g .Wevarythenumberoflayers ` ,thenum- berofresidualchannels r (dimensionofthehiddenstateof everylayer),andthenumberofskipchannels s (thedimen- siontowhichlayeroutputsareprojectedpriortotheoutput layer). WaveNetconsistsofanupsamplingandconditioningnet- work,followedby ` 2  1 convolutionlayerswith r residual outputchannelsandgated tanh nonlinearities.Webreak theconvolutionintotwomatrixmultipliespertimestep with W prev and W cur .Theselayersareconnectedwith residualconnections.Thehiddenstateofeverylayeris concatenatedtoan `r vectorandprojectedto s skipchan- nelswith W skip ,followedbytwolayersof 1  1 convolu- tions(withweights W relu and W out )with relu nonlineari- ties. WaveNetusestransposedconvolutionsforupsamplingand conditioning.Wethatourmodelsperformbetter,train faster,andrequirefewerparametersifweinsteaden- codetheinputswithastackofbidirectionalquasi-RNN (QRNN)layers( Bradburyetal. , 2016 )andthenperform upsamplingbyrepetitiontothedesiredfrequency. Ourhighest-qualitymodeluses ` =40 layers, r =64 residualchannels,and s =256 skipchannels.Fortrain- ing,weusetheAdamoptimizationalgorithmwith  1 = 0 : 9 ; 2 =0 : 999 ;" =10  8 ,abatchsizeof8,alearning rateof 10  3 ,andanannealingrateof0.9886appliedevery 1,000iterations( Kingma&Ba , 2014 ). PleaserefertoAppendix A forfulldetailsofourWaveNet architectureandtheQRNNlayersweuse. DeepVoice:Real-timeNeuralTTS 4.Results WetrainourmodelsonaninternalEnglishspeechdatabase containingapproximately20hoursofspeechdataseg- mentedinto13,079utterances.Inaddition,wepresent audiosynthesisresultsforourmodelstrainedonasubset oftheBlizzard2013data( Prahalladetal. , 2013 ).Both datasetsarespokenbyaprofessionalfemalespeaker. AllofourmodelsareimplementedusingtheTensorFlow framework( Abadietal. , 2015 ). 4.1.SegmentationResults Wetrainon8TitanXMaxwellGPUs,splittingeachbatch equallyamongtheGPUsandusingaringall-reducetoav- eragegradientscomputedondifferentGPUs,witheach iterationtakingapproximately1300milliseconds.After approximately14,000iterations,themodelconvergestoa phonemepairerrorrateof7%.Wealsothatphoneme boundariesdonothavetobeprecise,andrandomlyshift- ingphonemeboundariesby10-30millisecondsmakesno differenceintheaudioquality,andsosuspectthataudio qualityisinsensitivetothephonemepairerrorratepasta certainpoint. 4.2.Grapheme-to-PhonemeResults Wetrainagrapheme-to-phonememodelondataobtained fromCMUDict( Weide , 2008 ).Westripoutallwordsthat donotstartwithaletter,containnumbers,orhavemultiple pronunciations,whichleaves124,978outoftheoriginal 133,854grapheme-phonemesequencepairs. WetrainonasingleTitanXMaxwellGPUwitheachit- erationtakingapproximately150milliseconds.Afterap- proximately20,000iterations,themodelconvergestoa phonemeerrorrateof5.8%andaworderrorrateof28.7%, whichareonparwithpreviousreportedresults( Yao& Zweig , 2015 ).Unlikepriorwork,wedonotusealanguage modelduringdecodinganddonotincludewordswithmul- tiplepronunciationsinourdataset. 4.3.PhonemeDurationandFundamentalFrequency Results WetrainonasingleTitanXMaxwellGPUwitheachitera- tiontakingapproximately120milliseconds.Afterapprox- imately20,000iterations,themodelconvergestoamean absoluteerrorof38milliseconds(forphonemeduration) and29.4Hz(forfundamentalfrequency). 4.4.AudioSynthesisResults Wedividetheutterancesinouraudiodatasetintoone secondchunkswithaquartersecondofcontextforeach chunk,paddingeachutterancewithaquartersecondofsi- lenceatthebeginning.Weoutchunksthatarepre- dominantlysilenceandendupwith74,348totalchunks. Wetrainedmodelswithvaryingdepth,including10,20, 30,and40layersintheresiduallayerstack.Wethat modelsbelow20layersresultinpoorqualityaudio.The 20,30,and40layermodelsallproducehighqualityrec- ognizablespeech,butthe40layermodelshavelessnoise thanthe20layermodels,whichcanbedetectedwithhigh- qualityover-earheadphones. Previousworkhasemphasizedtheimportanceofreceptive sizeindeterminingmodelquality.Indeed,the20layer modelshavehalfthereceptiveasthe40layermod- els.However,whenrunat48kHz,modelswith40layers haveonly83millisecondsofreceptivebutstillgen- eratehighqualityaudio.Thissuggeststhereceptive ofthe20layermodelsissufandweconjecturethe differenceinaudioqualityisduetosomeotherfactorthan receptivesize. Wetrainon8TitanXMaxwellGPUswithonechunkper GPU,usingaringallreducetoaveragegradientscomputed ondifferentGPUs.Eachiterationtakesapproximately450 milliseconds.Ourmodelconvergesafterapproximately 300,000iterations.Wethatasingle1.25schunkissuf- tosaturatethecomputeontheGPUandthatbatch- ingdoesnotincreasetrainingefy. Asiscommonwithhigh-dimensionalgenerativemodels ( Theisetal. , 2015 ),modellossissomewhatuncorrelated withperceptualqualityofindividualsamples.Whilemod- elswithunusuallyhighlosssounddistinctlynoisy,models thatoptimizebelowacertainthresholddonothavealoss indicativeoftheirquality.Inaddition,changesinmodel architecture(suchasdepthandoutputfrequency)canhave aimpactonmodellosswhilehavingasmallef- fectonaudioquality. Toestimateperceptualqualityoftheindividualstagesof ourTTSpipeline,wecrowdsourcedmeanopinionscore (MOS)ratings(ratingsbetweenoneandve,highervalues beingbetter)fromMechanicalTurkusingtheCrowdMOS toolkitandmethodology( Ribeiroetal. , 2011 ).Inorderto separatetheeffectoftheaudiopreprocessing,theWaveNet modelquality,andthephonemedurationandfundamental frequencymodelquality,wepresentMOSscoresforava- rietyofutterancetypes,includingsynthesisresultswhere theWaveNetinputs(durationandF0)areextractedfrom groundtruthaudioratherthansynthesizedbyothermod- els.TheresultsarepresentedinTable 1 .Wepurposefully includegroundtruthsamplesineverybatchofsamplesthat ratersevaluatetohighlightthedeltafromhumanspeech andallowraterstodistinguishgraineddifferencesbe- tweenmodels;thedownsideofthisapproachisthatthere- sultingMOSscoreswillbelowerthanifraters DeepVoice:Real-timeNeuralTTS arepresented only withsynthesizedaudiosamples. Firstofall,weadropinMOSwhensimply downsamplingtheaudiostreamfrom48kHzto16kHz,es- peciallyincombinationwith  -lawcompandingandquan- tization,likelybecausea48kHzsampleispresentedtothe ratersasabaselinefora5score,andalowqualitynoisy synthesisresultispresentedasa1.Whenusedwithground truthdurationsandF0,ourmodelsscorehighly,withthe 95%intervalsofourmodelsintersectingthose ofthegroundtruthsamples.However,usingsynthesized frequencyreducestheMOS,andfurtherincludingsynthe- sizeddurationsreducesit.Weconcludethat themainbarriertoprogresstowardsnaturalTTSlieswith durationandfundamentalfrequencyprediction,andour systemshavenotmeaningfullyprogressedpastthestateof theartinthatregard.Finally,ourbestmodelsrunslightly slowerthanreal-time(seeTable 2 ),sowedemonstratethat synthesisqualitycanbetradedforinferencespeedbyad- justingmodelsizebyobtainingscoresformodelsthatrun 1Xand2Xfasterthanreal-time. WealsotestedWaveNetmodelstrainedonthefullsetof featuresfromtheoriginalWaveNetpublication,butfound noperceptualdifferencebetweenthosemodelsandmodels trainedonourreducedfeatureset. 4.5.BlizzardResults Todemonstratethexibilityofoursystem,weretrained allofourmodelswithidenticalhyperparametersonthe Blizzard2013dataset( Prahalladetal. , 2013 ).Forourex- periments,weuseda20.5hoursubsetofthedatasetseg- mentedinto9,741utterances.Weevaluatedthemodelus- ingtheproceduredescribedinSection 4.4 ,whichencour- agesraterstocomparesynthesizedaudiodirectlywiththe groundtruth.Ontheheldoutset,16kHzcompandedand expandedaudioreceivesaMOSscoreof 4 : 65  0 : 13 ,while oursynthesizedaudioreceivedaMOSscoreof 2 : 67  0 : 37 . 5.OptimizingInference AlthoughWaveNethasshownpromiseingeneratinghigh- qualitysynthesizedspeech,initialexperimentsreported generationtimesofmanyminutesorhoursforshortut- terances.WaveNetinferenceposesanincrediblychalleng- ingcomputationalproblemduetothehigh-frequency,au- toregressivenatureofthemodel,whichrequiresordersof magnitudemoretimestepsthantraditionalrecurrentneural networks.Whengeneratingaudio,asinglesamplemust begeneratedinapproximately60  s(for16kHzaudio)or 20  s(for48kHzaudio).Forour40layermodels,this meansthatasinglelayer(consistingofseveralmatrixmul- tipliesandnonlinearities)mustcompleteinapproximately 1.5  s.Forcomparison,accessingavaluethatresides inmainmemoryonaCPUcantake0.1  s.Inorderto performinferenceatreal-time,wemusttakegreatcareto neverrecomputeanyresults,storetheentiremodelinthe processorcache(asopposedtomainmemory),andopti- mallyutilizetheavailablecomputationalunits.Thesesame techniquescouldbeusedtoaccelerateimagesynthesiswith PixelCNN( Oordetal. , 2016 )tofractionsofasecondper image. Synthesizingonesecondofaudiowithour40layer WaveNetmodeltakesapproximately 55  10 9 point operations(FLOPs).Theactivationsinanygivenlayerde- pendontheactivationsinthepreviouslayerandthepre- vioustimestep,soinferencemustbedoneonetimestep andonelayeratatime.Asinglelayerrequiresonly 42  10 3 FLOPs,whichmakesachievingmeaningfulpar- allelismdifInadditiontothecomputerequirements, themodelhasapproximately 1 : 6  10 6 parameters,which equatetoabout6.4MBifrepresentedinsingleprecision. (SeeAppendix E foracompleteperformancemodel.) OnCPU,asingleHaswellorBroadwellcorehasapeak single-precisionthroughputofapproximately 77  10 9 FLOPsandanL2-to-L1cachebandwidthofapproximately 140GB/s 1 .Themodelmustbeloadedfromcacheonceper timestep,whichrequiresabandwidthof100GB/s.Even ifthemodelweretoinL2cache,theimplementation wouldneedtoutilize70%ofthemaximumbandwidthand 70%ofthepeakFLOPSinordertodoinferenceinreal- timeonasinglecore.Splittingthecalculationsacross multiplecoresreducesthedifoftheproblem,but nonethelessitremainschallengingasinferencemustop- erateatafractionofmaximummemoryband- widthandpeakFLOPsandwhilekeepingthreadssynchro- nized. AGPUhashighermemorybandwidthandpeakFLOPs thanaCPUbutprovidesamorespecializedandhence restrictivecomputationalmodel.Anaiveimplementation thatlaunchesasinglekernelforeverylayerortimestepis untenable,butanimplementationbasedonthepersistent RNNtechnique( Diamosetal. , 2016 )maybeabletotake advantageofthethroughputofferedbyGPUs. Weimplementhigh-speedoptimizedinferencekernelsfor bothCPUandGPUanddemonstratethatWaveNetinfer- enceatfaster-than-real-timespeedsisachievable.Table 2 liststheCPUandGPUinferencespeedsfordifferentmod- els.Inbothcases,thebenchmarksincludeonlytheau- toregressive,high-frequencyaudiogenerationanddo not includethegenerationoflinguisticconditioningfeatures (whichcanbedoneinparallelfortheentireutterance).Our CPUkernelsrunatreal-timeorfaster-than-real-timefora 1 Assumingtwo8-wideAVXFMAinstructionseverycycle andanL2-to-L1bandwidthof64bytespercycle. DeepVoice:Real-timeNeuralTTS Type ModelSize MOS  CI GroundTruth(48kHz) None 4 : 75  0 : 12 GroundTruth None 4 : 45  0 : 16 GroundTruth(compandedandexpanded) None 4 : 34  0 : 18 Synthesized ` =40 ;r =64 ;s =256 3 : 94  0 : 26 Synthesized(48kHz) ` =40 ;r =64 ;s =256 3 : 84  0 : 24 Synthesized(SynthesizedF0) ` =40 ;r =64 ;s =256 2 : 76  0 : 31 Synthesized(SynthesizedDurationandF0) ` =40 ;r =64 ;s =256 2 : 00  0 : 23 Synthesized(2Xreal-timeinference) ` =20 ;r =32 ;s =128 2 : 74  0 : 32 Synthesized(1Xreal-timeinference) ` =20 ;r =64 ;s =128 3 : 35  0 : 31 Table1. MeanOpinionScores(MOS)and95%intervals(CIs)forutterances.ThisMOSscoreisarelativeMOSscore obtainedbyshowingratersthesameutteranceacrossallthemodeltypes(whichencouragescomparativeratingandallowstheraters todistinguishgraineddifferences).Everybatchofsamplesalsoincludesthegroundtruth48kHzrecording,whichmakesallour ratingscomparativetonaturalhumanvoices.474ratingswerecollectedforeverysample.Unlessotherwisementioned,modelsused phonemedurationsandF0extractedfromthegroundtruth,ratherthansynthesizedbythedurationpredictionandfrequencyprediction models,aswellasa16384Hzaudiosamplingrate. Model Platform DataType NumberofThreads Speed-upOverReal-time ` =20 ;r =32 ;s =128 CPU float32 6 2.7 ` =20 ;r =32 ;s =128 CPU float32 2 2.05 ` =20 ;r =64 ;s =128 CPU int16 2 1.2 ` =20 ;r =64 ;s =128 CPU float32 6 1.11 ` =20 ;r =64 ;s =128 CPU float32 2 0.79 ` =40 ;r =64 ;s =256 CPU int16 2 0.67 ` =40 ;r =64 ;s =256 CPU float32 6 0.61 ` =40 ;r =64 ;s =256 CPU float32 2 0.35 ` =20 ;r =32 ;s =128 GPU float32 N/A 0.39 ` =20 ;r =64 ;s =128 GPU float32 N/A 0.29 ` =40 ;r =32 ;s =128 GPU float32 N/A 0.23 ` =40 ;r =64 ;s =128 GPU float32 N/A 0.17 Table2. CPUandGPUinferencekernelbenchmarksfordifferentmodelsinandint16.Atleastonemainandoneauxiliary threadwereusedforallCPUkernels.Thesekernelsoperateonasingleutterancewithnobatching.CPUresultsarefromaIntelXeon E5-2660v3Haswellprocessorclockedat2.6GHzandGPUresultsarefromaGeForceGTXTitanXMaxwellGPU. subsetofmodels,whiletheGPUmodelsdonotyetmatch thisperformance. 5.1.CPUImplementation Weachievereal-timeCPUinferencebyavoidinganyre- computation,doingcache-friendlymemoryaccesses,par- allelizingworkviamultithreadingwithefsynchro- nization,minimizingnonlinearityFLOPs,avoidingcache thrashingandthreadcontentionviathreadpinning,andus- ingcustomhardware-optimizedroutinesformatrixmulti- plicationandconvolution. FortheCPUimplementation,wesplitthecomputationinto thefollowingsteps: 1. SampleEmbedding: ComputetheWaveNetinput causalconvolutionbydoingtwosampleembeddings, oneforthecurrenttimestepandonefortheprevious timestep,andsummingthemwithabias.Thatis, x (0) = W emb,prev  y i  1 + W emb,cur  y i + B embed (1) DeepVoice:Real-timeNeuralTTS MainThreads AuxiliaryThreads SyncPoints a (1) cur x (1) a (2) cur x (1) ::: a ( `  1) cur x ( `  1) a ( ` ) cur x ( ` ) q (1) q (2) ::: q ( `  1) q ( ` ) a (1) prev ! a (2) prev ! ::: ! a ( `  1) prev ! a ( ` ) prev z s ! z a ! p x (0) a (1) cur ! a (2) cur ! ::: ! a ( `  1) cur ! a ( ` ) cur x (1) ! x (2) ! ::: ! x ( `  1) ! x ( ` ) timestep t timestep t +1 Figure2. Twogroupsofthreadsruninparallel.Computationofthe W skip isoftotheauxiliarythreadswhilethemainthreads progressthroughthestackofWaveNetlayers.Whilethemainthreadsarecomputingtheoutputlayer,theauxiliarythreadspreparethe left W prev halfoftheWaveNetlayerconvolutionsfortheupcomingtimestep.Arrowsindicatewhereonethreadgroupwaitsonresults fromtheotherthreadgroup,andareimplementedasspinlocks. 2. LayerInference: Foreverylayer j from j =1 to ` withdilationwidth d : (a) Computethelefthalfofthewidth-twodilated convolutionviaamatrix-vectormultiply: a ( j ) prev = W ( j ) prev  x ( j  1) i  d (2) (b) Computetherighthalfofthedilatedconvolution: a ( j ) cur = W ( j ) cur  x ( j  1) i (3) (c) Computethehiddenstate h ( j ) giventhecondi- tioningvector L ( j ) h : a ( j ) = a ( j ) prev + a ( j ) cur + B ( j ) h + L ( j ) h (4) h ( j ) =tanh  a ( j ) 0: r   ˙  a ( j ) r :2 r  ; (5) where v 0: r denotesthe r elementsofthevec- tor v and v r :2 r denotesthenext r elements.Then, computetheinputtothenextlayerviaamatrix- vectormultiply: x ( j ) = W ( j ) res  h ( j ) + B ( j ) res (6) (d) Computethecontributiontotheskip-channel matrixmultiplyfromthislayer,accumulating overalllayers,with q (0) = B skip : q ( j ) = q ( j  1) + W ( j ) skip  h ( j ) (7) 3. Output: Computethetwooutput 1  1 convolutions: z s = relu  q ( ` )  (8) z a = relu ( W relu  z s + B relu ) (9) p = softmax ( W out  z a + B out ) (10) Finally,sample y i +1 randomlyfromthedistribution p . Weparallelizetheseacrosstwogroupsofthreadsasde- pictedinFigure 2 .Agroupofmainthreadscomputes x (0) , a ( j ) cur , h ( j ) ,and x ( j ) , z a ,and p .Agroupofauxiliary threadscomputes a ( j ) prev , q ( j ) ,and z s ,withthe a ( j ) prev being computedforthenextupcomingtimestepwhilethemain threadscompute z a and p .Eachofthesegroupscancon- sistofasinglethreadorofmultiplethreads;ifthereare multiplethreads,eachthreadcomputesoneblockofeach matrix-vectormultiply,binaryoperation,orunaryopera- tion,andthreadbarriersareinsertedasneeded.Splitting themodelacrossmultiplethreadsbothsplitsupthecom- puteandcanalsobeusedtoensurethatthemodelweights intotheprocessorL2cache. Pinningthreadstophysicalcores(ordisablinghyper- threading)isimportantforavoidingthreadcontentionand cachethrashingandincreasesperformancebyapproxi- mately30%. Dependingonmodelsize,thenonlinearities( tanh , sigmoid ,and softmax )canalsotakeafrac- tionofinferencetime,sowereplaceallnonlinearitieswith high-accuracyapproximations,whicharedetailedinAp- pendix C .Themaximumabsoluteerrorarisingfromthese approximationsis 1 : 5  10  3 for tanh , 2 : 5  10  3 for sigmoid ,and 2 : 4  10  5 for e x .Withapproximateinstead ofexactnonlinearities,performanceincreasesbyroughly 30% . Wealsoimplementinferencewithweightmatricesquan- tizedto int16 andnochangeinperceptualquality whenusingquantization.Forlargermodels,quantization offersaspeedupwhenusingfewerthreads,but overheadofthreadsynchronizationpreventsitfrombeing usefulwithalargernumberofthreads. Finally,wewritecustomAVXassemblykernelsformatrix- vectormultiplicationusingPeachPy( Dukhan , 2015 )spe- cializedtoourmatrixsizes.Inferenceusingourcustom assemblykernelsisupto1.5XfasterthanIntelMKLand 3.5XfasterthanOpenBLASwhenusing float32 .Nei- DeepVoice:Real-timeNeuralTTS therlibraryprovidestheequivalent int16 operations. 5.2.GPUImplementation Duetotheircomputationalintensity,manyneuralmodels areultimatelydeployedonGPUs,whichcanhaveamuch highercomputationalthroughputthanCPUs.Sinceour modelismemorybandwidthandFLOPbound,itmayseem likeanaturalchoicetoruninferenceonaGPU,butitturns outthatcomeswithadifferentsetofchallenges. Usually,codeisrunontheGPUinasequenceofkernel invocations,witheverymatrixmultiplyorvectoroperation beingitsownkernel.However,thelatencyforaCUDA kernellaunch(whichmaybeupto50  s)combinedwith thetimeneededtoloadtheentiremodelfromGPUmem- oryareprohibitivelylargeforanapproachlikethis.An inferencekernelinthisstyleendsupbeingapproximately 1000Xslowerthanreal-time. Togetclosetoreal-timeonaGPU,weinsteadbuildaker- nelusingthetechniquesofpersistentRNNs( Diamosetal. , 2016 )whichgeneratesallsamplesintheoutputaudioina singlekernellaunch.Theweightsforthemodelareloaded toregistersonceandthenusedwithoutunloadingthemfor theentiredurationofinference.Duetothemismatchbe- tweentheCUDAprogrammingmodelandsuchpersistent kernels,theresultingkernelsarespecializedtoparticular modelsizesandareincrediblylabor-intensivetowrite.Al- thoughourGPUinferencespeedsarenotquitereal-time (Table 2 ),webelievethatwiththesetechniquesandabet- terimplementationwecanachievereal-timeWaveNetin- ferenceonGPUsaswellasCPUs.Implementationdetails forthepersistentGPUkernelsareavailableinAppendix D . 6.Conclusion Inthiswork,wedemonstratethatcurrentDeepLearning approachesareviableforallthecomponentsofahigh- qualitytext-to-speechenginebybuildingafullyneuralsys- tem.Weoptimizeinferencetofaster-than-real-timespeeds, showingthatthesetechniquescanbeappliedtogener- ateaudioinreal-timeinastreamingfashion.Oursystem istrainablewithoutanyhumaninvolvement,dramatically simplifyingtheprocessofcreatingTTSsystems. Ourworkopensmanynewpossibledirectionsforexplo- ration.Inferenceperformancecanbefurtherimproved throughcarefuloptimization,modelquantizationonGPU, and int8 quantizationonCPU,aswellasexperiment- ingwithotherarchitecturessuchastheXeonPhi.An- othernaturaldirectionisremovingtheseparationbetween stagesandmergingthesegmentation,durationprediction, andfundamentalfrequencypredictionmodelsdirectlyinto theaudiosynthesismodel,therebyturningtheprobleminto afullsequence-to-sequencemodel,creatingasingleend- to-endtrainableTTSsystem,andallowingustotrainthe entiresystemwithnointermediatesupervision.Inlieuof fusingthemodels,improvingthedurationandfrequency modelsvialargertrainingdatasetsorgenerativemodeling techniquesmayhaveanimpactonvoicenaturalness. DeepVoice:Real-timeNeuralTTS References Abadi,Mart ´ Agarwal,Ashish,Barham,Paul,Brevdo, Eugene,Chen,Zhifeng,Citro,Craig,Corrado,GregS., Davis,Andy,Dean,Jeffrey,Devin,Matthieu,Ghe- mawat,Sanjay,Goodfellow,Ian,Harp,Andrew,Irv- ing,Geoffrey,Isard,Michael,Jia,Yangqing,Jozefowicz, Rafal,Kaiser,Lukasz,Kudlur,Manjunath,Levenberg, Josh,Man ´ e,Dan,Monga,Rajat,Moore,Sherry,Murray, Derek,Olah,Chris,Schuster,Mike,Shlens,Jonathon, Steiner,Benoit,Sutskever,Ilya,Talwar,Kunal,Tucker, Paul,Vanhoucke,Vincent,Vasudevan,Vijay,Vi ´ egas, Fernanda,Vinyals,Oriol,Warden,Pete,Wattenberg, Martin,Wicke,Martin,Yu,Yuan,andZheng,Xiaoqiang. TensorFlow:Large-scalemachinelearningonheteroge- neoussystems,2015.URL http://tensorflow. org/ .Softwareavailablefromw.org. Amodei,Dario,Anubhai,Rishita,Battenberg,Eric,Case, Carl,Casper,Jared,Catanzaro,Bryan,Chen,Jingdong, Chrzanowski,Mike,Coates,Adam,Diamos,Greg,etal. Deepspeech2:End-to-endspeechrecognitioninenglish andmandarin. arXivpreprintarXiv:1512.02595 ,2015. Boersma,PaulusPetrusGerardusetal.Praat,asystem fordoingphoneticsbycomputer. Glotinternational ,5, 2002. Bradbury,James,Merity,Stephen,Xiong,Caiming,and Socher,Richard.Quasi-recurrentneuralnetworks. arXiv preprintarXiv:1611.01576 ,2016. Chung,Junyoung,Gulcehre,Caglar,Cho,KyungHyun, andBengio,Yoshua.Empiricalevaluationofgatedre- currentneuralnetworksonsequencemodeling. arXiv preprintarXiv:1412.3555 ,2014. Diamos,Greg,Sengupta,Shubho,Catanzaro,Bryan, Chrzanowski,Mike,Coates,Adam,Elsen,Erich,Engel, Jesse,Hannun,Awni,andSatheesh,Sanjeev.Persistent rnns:Stashingrecurrentweightson-chip.In Proceed- ingsofThe33rdInternationalConferenceonMachine Learning ,pp.2024Œ2033,2016. Dukhan,Marat.Peachpymeetsopcodes:directmachine codegenerationfrompython.In Proceedingsofthe5th WorkshoponPythonforHigh-PerformanceandScien- Computing ,pp.3.ACM,2015. Graves,Alex,Fern ´ andez,Santiago,Gomez,Faustino,and Schmidhuber,J ¨ urgen.Connectionisttemporal tion:Labellingunsegmentedsequencedatawithrecur- rentneuralnetworks.In Proceedingsofthe23rdInter- nationalConferenceonMachineLearning ,ICML'06, pp.369Œ376,NewYork,NY,USA,2006.ACM. Kingma,D.andBa,J.Adam:Amethodforstochastic optimization. arXivpreprintarXiv:1412.6980 ,2014. Mehri,Soroush,Kumar,Kundan,Gulrajani,Ishaan,Ku- mar,Rithesh,Jain,Shubham,Sotelo,Jose,Courville, Aaron,andBengio,Yoshua.Samplernn:Anuncondi- tionalend-to-endneuralaudiogenerationmodel. arXiv preprintarXiv:1612.07837 ,2016. Morise,Masanori,Yokomori,Fumiya,andOzawa,Kenji. World:avocoder-basedhigh-qualityspeechsynthesis systemforreal-timeapplications. IEICETRANSAC- TIONSonInformationandSystems ,99(7):1877Œ1884, 2016. Oord,Aaronvanden,Kalchbrenner,Nal,and Kavukcuoglu,Koray.Pixelrecurrentneuralnetworks. arXivpreprintarXiv:1601.06759 ,2016. Paine,TomLe,Khorrami,Pooya,Chang,Shiyu,Zhang, Yang,Ramachandran,Prajit,Hasegawa-Johnson, MarkA,andHuang,ThomasS.Fastwavenetgen- erationalgorithm. arXivpreprintarXiv:1611.09482 , 2016. Pascual,SantiagoandBonafonte,Antonio.Multi-output rnn-lstmformultiplespeakerspeechsynthesiswith  - interpolationmodel. way ,1000:2,2016. Prahallad,Kishore,Vadapalli,Anandaswarup,Elluru, Naresh,etal.Theblizzardchallenge2013indianlan- guagetask.In InBlizzardChallengeWorkshop2013 , 2013. Rao,Kanishka,Peng,Fuchun,Sak,Has¸im,andBeau- fays,Franc¸oise.Grapheme-to-phonemeconversionus- inglongshort-termmemoryrecurrentneuralnetworks. In Acoustics,SpeechandSignalProcessing(ICASSP), 2015IEEEInternationalConferenceon ,pp.4225Œ4229. IEEE,2015. Ribeiro,Fl ´ avio,Flor ‹ encio,Dinei,Zhang,Cha,andSeltzer, Michael.Crowdmos:Anapproachforcrowdsourcing meanopinionscorestudies.In Acoustics,Speechand SignalProcessing(ICASSP),2011IEEEInternational Conferenceon ,pp.2416Œ2419.IEEE,2011. Ronanki,Srikanth,Henter,GustavEje,Wu,Zhizheng,and King,Simon.Atemplate-basedapproachforspeech synthesisintonationgenerationusinglstms. Interspeech 2016 ,pp.2463Œ2467,2016. Sotelo,Jose,Mehri,Soroush,Kumar,Kundan,Santos, JoaoFelipe,Kastner,Kyle,Courville,Aaron,andBen- gio,Yoshua.Char2wav:End-to-endspeechsynthesis.In ICLR2017workshopsubmission ,2017.URL https: //openreview.net/forum?id=B1VWyySKx . Stephenson,Ian. ProductionRendering,DesignandIm- plementation .Springer,2005. DeepVoice:Real-timeNeuralTTS Taylor,Paul. Text-to-SpeechSynthesis .CambridgeUniver- sityPress,NewYork,NY,USA,1stedition,2009.ISBN 0521899273,9780521899277. Theis,Lucas,Oord,A ¨ aronvanden,andBethge,Matthias. Anoteontheevaluationofgenerativemodels. arXiv preprintarXiv:1511.01844 ,2015. vandenOord,A ¨ aron,Dieleman,Sander,Zen,Heiga,Si- monyan,Karen,Vinyals,Oriol,Graves,Alex,Kalch- brenner,Nal,Senior,Andrew,andKavukcuoglu,Ko- ray.Wavenet:Agenerativemodelforrawaudio. CoRR abs/1609.03499 ,2016. Weide,R. TheCMUpronunciationdictionary0.7 . CarnegieMellonUniversity,2008. Yao,KaishengandZweig,Geoffrey.Sequence-to- sequenceneuralnetmodelsforgrapheme-to-phoneme conversion. arXivpreprintarXiv:1506.00196 ,2015. Zen,HeigaandSak,Has¸im.Unidirectionallongshort-term memoryrecurrentneuralnetworkwithrecurrentoutput layerforlow-latencyspeechsynthesis.In Acoustics, SpeechandSignalProcessing(ICASSP),2015IEEEIn- ternationalConferenceon ,pp.4470Œ4474.IEEE,2015. Zen,Heiga,Senior,Andrew,andSchuster,Mike.Statistical parametricspeechsynthesisusingdeepneuralnetworks. In ProceedingsoftheIEEEInternationalConferenceon Acoustics,Speech,andSignalProcessing(ICASSP) ,pp. 7962Œ7966,2013. DeepVoice:Real-timeNeuralTTS Appendices A.WaveNetArchitectureandDetails TheWaveNetconsistsofaconditioningnetwork c = C ( v ) ,whichconvertslow-frequencylinguisticfeatures v tothe nativeaudiofrequency,andanauto-regressiveprocess P ( y i j c;y i  1 ;:::;y i  R ) whichpredictsthenextaudiosamplegiven theconditioningforthecurrenttimestep c andacontextof R audiosamples. R isthereceptivesize,andisaproperty determinedbythestructureofthenetwork.AsketchoftheWaveNetarchitectureisshowninFigure 3 .Thenetworkdetails aredescribedinthefollowingsubsections. Figure3. TheWaveNetarchitecture.Componentsarecoloredaccordingtofunction:tealinputs,greenconvolutionsand QRNNs,yellowunaryoperationsandsoftmax,pinkbinaryoperations,andindigoreshapes,transposes,andslices. A.1.Auto-regressiveWaveNet Thestructureoftheauto-regressivenetworkisparameterizedbythenumberoflayers ` ,thenumberofskipchannels s , andthenumberofresidualchannels r . Audioisquantizedto a =256 valuesusing  -lawcompanding,asdescribedinSection2.2ofWaveNet.Theone-hot encodedvaluesgothroughaninitial2x1convolutionwhichgeneratestheinput x (0) 2 R r forthelayerintheresidual stack: x (0) = W embed  y + B embed ; (11) DeepVoice:Real-timeNeuralTTS where  istheone-dimensionalconvolutionoperator.Sincetheinputaudio y isaone-hotvector,thisconvolutioncanbe doneviaembeddingsinsteadofmatrixmultiplies.Eachsubsequentlayercomputesahiddenstatevector h ( i ) andthen(due totheresidualconnectionsbetweenlayers)addstoitsinput x ( i  1) togenerateitsoutput x ( i ) : h ( i ) =tanh  W ( i ) h  x ( i  1) + B ( i ) h + L ( i ) h   ˙  W ( i ) g  x ( i  1) + B ( i ) g + L ( i ) g  (12) x ( i ) = x ( i  1) + W ( i ) r  h ( i ) + B ( i ) r ; (13) where L ( i ) istheoutputforthatlayeroftheconditioningnetwork.Sinceeachlayeraddsitsoutputtoitsinput,the dimensionalityofthelayersmustremainedtothenumberofresidualchannels, r .Althoughherethisiswrittenastwo convolutions,onefor W h andonefor W g ,itisactuallydonemoreefwithasingleconvolutionwith r inputand 2 r outputchannels.Duringinference,thisconvolutionisreplacedwithtwomatrix-vectormultiplieswithmatrices W prev (thelefthalfoftheconvolution)and W cur (therighthalf).Thuswecanreformulatethecomputationof h ( i ) fora timestep t asfollows: h 0 ( i ) = W ( i ) prev  x ( i  1) t  d + W ( i ) cur  x ( i  1) t + B ( i ) + L ( i ) (14) h ( i ) =tanh  h 0 ( i ) 0: r   ˙  h 0 ( i ) r :2 r  ; (15) where L ( i ) andisaconcatenationof L ( i ) h and L ( i ) g and B ( i ) andisaconcatenationof B ( i ) h and B ( i ) g . Thehiddenstate h ( i ) fromeachofthelayers 1 through ` isconcatenatedandprojectedwithalearned W skip downtothe numberofskipchannels s : h = 2 6 6 6 4 h (1) h (2) . . . h ( ` ) 3 7 7 7 5 h 2 R `r (16) z s = relu ( W skip  h + B skip ) ;z s 2 R s (17) whererelu ( x )=max(0 ;x ) . z s isthenfedthroughtwofullyconnectedrelulayerstogeneratetheoutputdistribution p 2 R a : z a = relu ( W relu  z s + B relu ) ;z a 2 R a (18) p = softmax ( W out  z a + B out ) (19) A.2.ConditioningNetwork Whentrainedwithoutconditioninginformation,WaveNetmodelsproducehuman-likeﬁbabblingsoundsﬂ,astheylacksuf- long-rangeinformationtoreproducewords.Inordertogeneraterecognizablespeech,everytimestepisconditioned byanassociatedsetoflinguisticfeatures.Thisisdonebybiasingeverylayerwithaper-timestepconditioningvector generatedfromalower-frequencyinputsignalcontainingphoneme,stress,andfundamentalfrequencyfeatures. Thefrequencyoftheaudioishigherthanthefrequencyofthelinguisticconditioninginformation,soan upsamplingprocedureisusedtoconvertfromlower-frequencylinguisticfeaturestohigher-frequencyconditioningvectors foreachWaveNetlayer. TheoriginalWaveNetdoesupsamplingbyrepetitionorthroughatransposedconvolution.Instead,wepassourinput featuresthroughtwobidirectionalquasi-RNNlayers( Bradburyetal. , 2016 )with fo -poolingand2x1convolutions.A unidirectionalQRNNlayerwith fo -poolingisbythefollowingequations: ~ h =tanh( W h  x + B h ) (20) o = ˙ ( W o  x + B o ) (21) f = ˙ ( W f  x + B f ) (22) h t = f t  h t  1 +(1  f t )  ~ h t (23) z t = o t  h t (24) DeepVoice:Real-timeNeuralTTS AbidirectionalQRNNlayeriscomputedbyrunningtwounidirectionalQRNNs,oneontheinputsequenceandoneon areversedcopyoftheinputsequence,andthenstackingtheiroutputchannels.AfterbothQRNNlayers,weinterleave thechannels,sothatthe tanh andthe sigmoid intheWaveNetbothgetchannelsgeneratedbytheforwardQRNNand backwardQRNN. FollowingthebidirectionalQRNNlayers,weupsampletothenativeaudiofrequencybyrepetition 2 . Wethatthemodelisverysensitivetotheupsamplingprocedure:althoughmanyvariationsoftheconditioningnetwork converge,theyregularlyproducephonememispronunciations. A.3.InputFeaturization OurWaveNetistrainedwith8-bit  -lawcompandedaudiowhichisdownsampledto16384Hzfrom16-bitdual-channel PCMaudioat48000Hz.Itisconditionedona256Hzphonemesignal.Theconditioningfeaturevectorhas227dimen- sions.Ofthesetwoareforfundamentalfrequency.Oneoftheseindicateswhetherthecurrentphonemeisvoiced(andthus hasanF0)andtheotherisnormalizedlog-frequency,computedbynormalizingthelogofF0tominimumobservedF0to beapproximatelybetween-1and1.Therestofthefeaturesdescribethecurrentphoneme,thetwopreviousphonemes, andthetwonextphonemes,witheachphonemebeingencodedviaa40-dimensionalone-hotvectorforphonemeidentity (with39phonemesforARPABETphonemesand1forsilence)anda5-dimensionalone-hotvectorforphonemestress(no stress,primarystress,secondarystress,tertiarystress,andquaternarystress).Notallofthedatasetsweworkwithhave tertiaryorquaternarystress,andthosefeaturesarealwayszeroforthedatasetsthatdonothavethosestresslevels. Inourexperiments,wefoundthatincludingthephonemecontext(twopreviousandtwonextphonemes)iscrucialfor upsamplingviatransposedconvolutionandlesscriticalbutstillimportantforourQRNN-basedupsampling.Although soundqualitywithoutthephonemecontextremainshigh,mispronunciationofasubsetoftheutterancesbecomesanissue. Wealsofoundthatincludingextraprosodyfeaturessuchaswordandsyllablebreaks,pauses,phonemeandsyllablecounts, framepositionrelativetophoneme,etc,wereunhelpfulanddidnotresultinhigherqualitysynthesizedsamples. Inordertoconvertfromphonemesannotatedwithdurationstoaed-frequencyphonemesignal,wesamplethephonemes atregularintervals,effectivelyrepeatingeachphoneme(withcontextandF0)anumberproportionaltoitsduration.Asa result,phonemedurationiseffectivelyquantizedto 1 = 256 sec ˇ 4 ms. WeusePraat( Boersmaetal. , 2002 )inbatchmodetocomputeF0attheappropriatefrequency,withaminimumF0 of75andamaximumF0of500.ThePraatbatchscriptusedtogenerateF0isavailableat https://github.com/baidu- research/deep-voice/blob/master/scripts/f0-script.praat andcanberunwith praat--runf0-script.praat . A.4.SamplingfromOutputDistribution Ateverytimestep,thesynthesismodelproducesadistributionoversamples, P ( s ) ,conditionedontheprevioussamples andthelinguisticfeatures.Inordertoproducethesamples,thereareavarietyofwaysyoucouldchoosetousethis distribution:  DirectSampling: Samplerandomlyfrom P ( y ) .  TemperatureSampling: Samplerandomlyfromadistributionadjustedbyatemperature t ~ P t ( y )= 1 Z P ( y ) 1 = t ; (25) where Z isanormalizingconstant.  Mean: Takethemeanofthedistribution E P [ y ] .  Mode: Takethemostlikelysample,argmax P ( y ) : 2 Upsamplingusingbilinearinterpolationslowedconvergenceandreducedgenerationqualitybyaddingnoiseorcausingmispro- nunciations,whilebicubicupsamplingledtomufsounds.Upsamplingbyrepetitionisdonebycomputingtheratiooftheoutput frequencytotheinputfrequencyandrepeatingeveryelementintheinputsignalanappropriatenumberoftimes. DeepVoice:Real-timeNeuralTTS  Top k : Samplefromanadjusteddistributionthatonlypermitsthetop k samples ~ P k ( y )= ( 0 if y<k th ( P ( y )) P ( y ) =Z otherwise ; (26) where Z isanormalizingconstant. Wethatoutofthesedifferentsamplingmethods,onlydirectsamplingproduceshighqualityoutputs.Temperaturesam- plingproducesacceptablequalityresults,andindeedoutperformsdirectsamplingearlyonintraining,butforconverged modelsisworse.Thisobservationindicatesthatthegenerativeaudiomodelaccuratelylearnsaconditional sampledistributionandthatmodifyingthisdistributionthroughtheaboveheuristicsisworsethanjustusingthelearned distribution. A.5.Training Weobservedseveraltendenciesofthemodelsduringtraining.Asexpected,therandomlyinitializedmodelproduceswhite noise.Throughouttraining,themodelgraduallyincreasesthesignaltonoiseratio,andthevolumeofthewhitenoise diesdownwhilethevolumeofthespeechsignalincreases.Thespeechsignalcanbeinaudiblefortensofthousandsof iterationsbeforeitdominatesthewhitenoise. Inaddition,becausethemodelisautoregressive,raremistakescanproduceveryaudibledisturbances.Forexample,a commonfailuremodeistoproduceasmallnumberofincorrectsamplesduringsampling,whichthenresultsinalarge numberincorrectsamplesduetocompoundingerrors.Thisisaudibleasabriefperiodofloudnoisebeforethemodel stabilizes.Thelikelihoodofthishappeningishigherearlyonintraining,anddoesnothappeninconvergedmodels. B.PhonemeModelLoss Thelossforthe n th phonemeis L n = j ^ t n  t n j +  1 CE (^ p n ;p n )+  2 T  1 X t =0 j c F 0 n;t  F 0 n;t j +  3 T  2 X t =0 j c F 0 n;t +1  c F 0 n;t j ; (27) where  i 'saretradeoffconstants, b t n and t n aretheestimatedandground-truthdurationsofthe n th phoneme, ^ p n and p n aretheestimatedandground-truthprobabilitiesthatthe n th phonemeisvoiced,CEisthecross-entropyfunction, b F n;t and F n;t aretheestimatedandground-truthvaluesofthefundamentalfrequencyofthe n th phonemeattime t . T timesamples areequallyspacedalongthephonemeduration. C.NonlinearityApproximationDetails Duringinference,wereplaceexactimplementationsoftheneuralnetworknonlinearitieswithhigh-accuracyrationalap- proximations.Inthisappendix,wedetailthederivationoftheseapproximations. C.1.tanhandsigmoidapproximation Denoting ~ e ( x ) asanapproximationto e j x j ,weusethefollowingapproximationsfor tanh and ˙ : tanh( x ) ˇ sign ( x ) ~ e ( x )  1 = ~ e ( x ) ~ e ( x )+ 1 = ~ e ( x ) (28) ˙ ( x ) ˇ ( ~ e ( x ) 1+~ e ( x ) x  0 1 1+~ e ( x ) x  0 (29) Wechooseaforth-orderpolynomialtorepresent ~ e ( x ) .Thefollowingproducesaccuratevaluesforboth tanh( x ) and ˙ ( x ) : ~ e ( x )=1+ j x j +0 : 5658 x 2 +0 : 143 x 4 (30) Byitself, ~ e ( x ) isnotaverygoodapproximatefunctionfor e j x j ,butityieldsgoodapproximationswhenusedtoapproximate tanh and ˙ asdescribedinEquations 28 and 29 . DeepVoice:Real-timeNeuralTTS C.2. e x approximation Wefollowtheapproachof( Stephenson , 2005 )tocalculateanapproximate e x function.Insteadofapproximating e x directly,weapproximate 2 x andusetheidentity e x =2 x = ln2 . Let b x c tobetheof x 2 R .Then, 2 x =2 b x c  2 x  x c =2 b x c   1+(2 x  x c  1)  where 0  2 x  x c  1 < 1 since 0  x b x c < 1 .Ifweusea 32 -bittorepresent 2 x ,then b x c +127 and 2 x  x c  1 arerepresentedbytheexponentandfractionbitsof 2 x .Therefore,ifweinterpretthebytespatternof 2 x asa 32 -bitsinteger (representedby I 2 x ),wehave I 2 x =( b x c +127)  2 23 +(2 x  x c  1)  2 23 : (31) RearrangingtheEquation 31 andusing z = x b x c resultsto I 2 x =( x +126+ f 2 z  z g )  2 23 (32) Ifwecanaccuratelyapproximate g ( z )=2 z  z over z 2 [0 ; 1) ,theninterpretingbackthebyterepresentationof I 2 x in Equation 32 asa 32 -bitswecanaccuratelyapproximate 2 x .Weusearationalapproximationas g ( z ) ˇ 4 : 7259162+ 27 : 7280233 4 : 84252568  z  1 : 49012907 z; (33) whichgivesaremaximumerror 2 : 4  10  5 for x 2 (  ; 0] . D.PersistentGPUKernels ANVIDIAGPUhasmultipleStreamingMultiprocessors(SMs),eachofwhichhasaregisterandaL1cache.There isalsoacoherentL2cachethatissharedbyallSMs.Theinferenceprocessneedstogenerateonesampleevery61  s. DuetothehighlatencyofaCUDAkernellaunchandofreadingsmallmatricesfromGPUmemory,theentireaudio generationprocessmustbedonebyasinglekernelwiththeweightsloadedintotheregisteracrossallSMs.Thisraises twochallengesŠhowtosplitthemodelacrossregistersinawaytominimizecommunicationbetweenSMsandhowto communicatebetweenSMsgiventherestrictionsimposedbytheCUDAprogrammingmodel. Wesplitthemodelacrosstheregisterof24SMs,numberedSM1  SM24,ofaTitanXGPU.WedonotuseSM24. SM1toSM20storetwoadjacentlayersoftheresidualstack.ThismeansSM1storeslayers1and2,SM2storeslayers 3and4andsoonandsoforth.EachlayerhasthreematricesandthreebiasvectorsŠ W prev , B prev , W cur , B cur ,thatare forthedilatedconvolutionsand W r , B r .ThusSM i generatestwohiddenstates h (2 i ) and h (2 i +1) andanoutput x (2 i ) . EachSMalsostorestherowsofthe W skip matrixthatwillinteractwiththegeneratedhiddenstatevectors.Thus W skip is partitionedacross20SMs.OnlySM20needstostore B skip .SM21stores W relu and B relu .Finally, W out issplitacross twoSMsŠSM22andSM23becauseofregisterlimitationsandSM23stores B out . ThenextchallengeistocoordinatethedatatransferbetweenSMs,sincetheCUDAprogrammingmodelexecutesone kernelacrossallSMsinparallel.HoweverwewantexecutiontogosequentiallyinaroundrobinfashionfromSM1to SM23andbackagainfromSM1aswegenerateoneaudiosampleatatime.WelaunchourCUDAkernelwith23thread blocksandsimulatesuchsequentialexecutionbyspinningonlocks,oneforeachSM,thatarestoredinglobalmemoryand cachedinL2.FirstSM1executestwolayersoftheWaveNetmodeltogenerate h (1) , h (2) and x (2) .Itthenunlocksthelock thatSM2isspinningonandsetsitsownlock.ItdoesthisbybypassingtheL1cachetowritetoglobalmemorysothatall SMshaveacoherentviewofthelocks.ThenSM2doesthesameforSM3andthissequentiallockingandunlockingchain continuesforeachSM.FinallySM23generatestheoutputdistribution p fortimestep t andunlocksSM1sothatentire processcanrepeattogenerate p fortimestep t +1 . Justlikelocks,wepassdatabetweenSMs,byreadingandwritingtoglobalmemorybybypassingtheL1cache.Since NVIDIAGPUshaveacoherentL2cache,aglobalmemorywritebypassingtheL1,followedbyamemoryfenceresults inacoherentviewofmemoryacrossSMs. DeepVoice:Real-timeNeuralTTS Thispartitioningschemehoweverisquitexibleandonlyworksforvaluesof l , r and s showninTable 2 .This isbecauseeachSMhasaedsizedregisterandcombinedwiththerelativelyxibleandexpensivecommunica- tionmechanismbetweenSMsimpliesthatsplittingweightmatricesbetweenSMsischallenging.Anychangeinthose parametersmeansanewkernelhastobewritten,whichisaverytimeconsumingprocess. TherearetwomainreasonswhytheGPUkernelsareslowerthanCPUkernels.Firstly,synchronizationbetweenSMsina GPUisexpensivesinceitisdonebybusywaitingonlocksinL2cache.Secondlyeventhoughwedividethemodelina waythatwillintheregisterofeachSM,theCUDAcompilerstillspillstoL1cache.Wehopethatwithhandcrafted assemblycode,wewillbeabletomatchtheperformanceofCPUkernels.However,thelackofparallelisminWaveNet inferencemakesitdiftohidethelatenciesinherentinreadingandwritingsmallmatricesfromGPUmemorywhich areexposedintheabsenceofarichcachehierarchyinGPUs. E.Performancemodel WepresentaperformancemodelfortheautoregressiveWaveNetarchitecturedescribedinAppendix A.1 .Inourmodela dotproductbetweentwovectorsofdimension r takes 2 r FLOPSŠ r multiplicationsand r additions.Thismeansthata matrix-vectormultiplybetween W ,an r  r matrixand x ,a r  1 vectortakes 2 r  r =2 r 2 FLOPs.Thuscalculating h 0 ( i ) uses Cost  h 0 ( i )  =(2 r  2 r )+(2 r  2 r )+2 r +2 r +2 r FLOPs (34) Letdivisionandexponentiatontake f d and f e FLOPsrespectively.Thismeans tanh and ˙ takes ( f d +2 f e +1) FLOPs. Thuscalculating h ( i ) takes 2 r  ( f d +2 f e +1)+ r FLOPs.Finallycalculating x ( i ) foreachlayertakes r +(2 r  r )+ r FLOPs.ThisbringsthetotalFLOPsforcalculatingonelayerto Cost ( layer )=10 r 2 +11 r +2 r ( f d + f e )FLOPs (35) Underthesamemodel,calculating z s takes ( `  2 r )  s + s + s FLOPs,whereweassumethat relu takes1FLOP.Similarly, calculating z a takes 2 s  a + a + a FLOPsand W out  z a + B out takes 2 a  a + a FLOPs. Calculatingthenumericallystable softmax takesone max ,onesubtract,oneexponentiation,onesumandonedivisionper elementofavector.Hencecalculating p takes 3 a + a ( f d + f e ) FLOPs. Addingitallup,ourperformancemodeltogenerateeachaudiosampleisasfollows: Cost ( sample )= `  10 r 2 +11 r +2 r ( f d + f e )  + s (2 r  ` +2)+ a (2 s +2 a +3)+ a (3+ f d + f e )FLOPS (36) Ifwelet ` =40 , r =64 ,and s = a =256 ,andassumethat f d =10 and f e =10 ,withasamplingfrequencyof16384Hz, wehaveapproximately 55  10 9 FLOPsforeverysecondofsynthesis.  
KernelPoolingforConvolutionalNeuralNetworks YinCui 1 ; 2  FengZhou 3 JiangWang 4 XiaoLiu 3 YuanqingLin 3 SergeBelongie 1 ; 2 1 DepartmentofComputerScience,CornellUniversity 2 CornellTech 3 BaiduResearch 4 GoogleResearch f ycui,sjb g @cs.cornell.eduwww.f-zhou.com wangjiangb@gmail.com f liuxiao12,linyuanqing g @baidu.com Abstract ConvolutionalNeuralNetworks(CNNs)withBilinear Pooling,initiallyintheirfullformandlaterusingcompact representations,haveyieldedimpressiveperformancegains onawiderangeofvisualtasks,includingainedvi- sualcategorization,visualquestionanswering,facerecog- nition,anddescriptionoftextureandstyle.Thekeytotheir successliesinthespatiallyinvariantmodelingofpairwise (2 nd order)featureinteractions.Inthiswork,wepropose ageneralpoolingframeworkthatcaptureshigherorderin- teractionsoffeaturesintheformofkernels.Wedemon- stratehowtoapproximatekernelssuchasGaussianRBF uptoagivenorderusingcompactexplicitfeaturemapsin aparameter-freemanner.CombinedwithCNNs,thecom- positionofthekernelcanbelearnedfromdatainanend- to-endfashionviaerrorback-propagation.Theproposed kernelpoolingschemeisevaluatedintermsofbothkernel approximationerrorandvisualrecognitionaccuracy.Ex- perimentalevaluationsdemonstratestate-of-the-artperfor- manceoncommonlyusedainedrecognitiondatasets. 1.Introduction Theideaofinteractionsbetweenfeatureshasbeenused extensivelyasahigherorderrepresentationinlearningtasks recently[ 24 , 34 , 3 , 23 ].Themotivationbehindistomake thesubsequentlinearoperatesonhigherdimen- sionalfeaturemapsothatitbecomesmorediscriminative. Therearetwowaysingeneraltocreatehigherorderinter- actions.Themostcommonlyusedoneisto implicitly map thefeatureviathekerneltrick,likeinthecaseofkernel SVM[ 41 ].Thedisadvantagesaretwofold.Thestorage neededandtheevaluationtimearebothproportionaltothe numberoftrainingdata,whichmakesitinefonlarge datasets.Inaddition,theconstructionofthekernelmakesit  PartofthisworkwasdoneduringtheinternshipatBaiduResearch. Figure1.TheproposedKernelPoolingmethod.Forafeaturevec- tor(i.e.,theactivationataspatiallocationonthefeaturemap,in thecaseofaCNN),weuseCountSketch[ 6 ]togenerateacompact explicitfeaturemapupto p th order.Afterapplyingkernelpool- ing,theinnerproductbetweentwofeaturescancapturehighorder featureinteractionsasinEqn. 1 .Thismakesthesubsequentlin- earhighlydiscriminative.Theproposedkernelpooling schemeisend-to-endtrainableandthecompositionofthekernel canbelearnedthroughtheupdateofcoef f  i g p i =0 .The vanillacompactbilinearpooling[ 11 , 10 ]onlyusethe2 nd order informationasthefeaturevector. hardtousestochasticlearningmethods,includingStochas- ticGradientDescent(SGD)inthetrainingofCNNs.The otherwayisto explicitly mapthefeaturevectorintohigh dimensionalspacewithproductsoffeatures(monomials). Thedrawbackofthismethodisobvious.Ifwewantupto p th orderinteractionsona d dimensionalfeaturevector,the dimensionoftheexplicitfeaturemapwillbe O ( d p ) ,which makesitimpracticaltouseinrealworldapplications.A commonwaytoaddresstheseissuesistocompactlyap- proximateeitherkernelfunctions[ 37 , 44 ]orfeaturemaps [ 17 , 31 , 2 ]. BeforetheremarkablesuccessofusingConvolutional NeuralNetworks(CNNs)onvisualdata[ 20 , 38 , 39 , 15 ], low-levelhand-craftedfeatures( e.g .,SIFT[ 25 ],HOG[ 8 ], 1 Figure2.End-to-endtrainingwiththeproposedpoolingmethod.Aninputimageisfedintoaseriesoffullyconvolutionallayerstoget theoutputfeaturemapofsize h  w  c .Forthe c dimensionalfeaturevectoroneverysinglespatiallocation( e.g .,theredorbluebar onthefeaturemap),weapplytheproposedkernelpoolingmethodillustratedinFig. 1 .Thefeaturevectorisaveragepooledoverall locations h  w .ThenalinearlayerwithsoftmaxisusedtodotheThekernelisbytheorder p andcoef f  i g p i =0 ,whichcanbelearnedfromdatathroughback-propagation. Gist[ 28 ])combinedwithmid-levelfeatureaggregationor poolingmethods( e.g .,Bag-of-visual-words,SpatialPyra- midMatching[ 21 ],SparseCoding[ 45 ],FisherVector[ 30 ]) werewidelyadoptedasthestandardschemeforfeatureex- traction.Whenlearningandapplyingthesubsequentlin- earonextractedfeatures,kernelmethodssuchas GaussianRBForexponential ˜ 2 kernelareoftenadopted tocapturehigherorderinformationandmakelinearclas- morediscriminative.Recently,effortsincombining CNNswith2 nd orderfeatureinteractions,eitherbyreplac- inghand-craftedfeatureswithCNNfeatures[ 7 ]orjointly trainedinanend-to-endfashion,yieldedimpressiveperfor- mancegainsonawiderangeofvisualtasks.Representative examplesincludevisualrecognition[ 23 , 11 ], visualquestionanswering[ 10 ],texturerepresentationand synthesis[ 13 , 22 ],facerecognition[ 35 ]andstyletransfer [ 12 ].Notably,bothGao etal .[ 11 ]andFukui etal .[ 10 ] usedTensorSketch[ 31 ]tocompactlycompressthefullbi- linearvectorby2ordersofmagnitudewhilepreservethe sameperformance. Inthiswork,weproposeacompactanddifferentiable waytogenerateexplicitfeaturemaps.Wegeneralizethe strategyusedin[ 11 , 10 ]torepresenthigherorderfeature interactions.Forafeaturevector x ofdimension d ,we generateits i th order( i  2 )compactexplicitfeaturemap withCountSketch[ 6 ]andcircularconvolution.Inprac- tice,peopleoftenoperatecircularconvolutioninfrequency domainviaFastFourierTransform(FFT)andInverseFast FourierTransform(IFFT).Ithasbeenproven,boththeo- reticallyandpracticallyin[ 31 ],thatthismethodisableto compactlyapproximatepolynomialkernels.Asillustrated inFig. 1 ,withastackofCountSketch,element-wisemul- tiplication,FFTandIFFTunits,higherorderinformation canbecompactlypreserved.Thekernelpoolingmethodis appliedoneverysinglespatiallocationonthefeaturemap ofaCNN.Andthefeaturevectoristheresultofglobal averagepoolingacrossallspatiallocations. Denotetheproposedkernelpoolingmethodas ˚ .Then fortwofeaturevectors x and y ,theinnerproductbetween ˚ ( x ) and ˚ ( y ) canapproximateakerneluptoacertainor- der p asfollows(seeSec. 3 formoredetails): ˚ ( x ) > ˚ ( y ) ˇ p X i =0  2 i ( x > y ) i ˇK ( x ; y ) (1) Throughtheintroductionofkernelfunctionsassociated withReproducingkernelHilbertspace,linearop- erateonhigh-dimensionalEuclideanspacebecomehighly discriminative.Combinetheproposedpoolingmethodwith aCNN,asshowninFig. 2 ,themodelcanbetrainedend- to-endviaback-propagationoferrors.The compositionofthekernel,asdeterminedbycoef f  i g p i =0 ,canbeeithertoapproximateacertain kernellikeGaussianRBFuptoorder p orlearnedfromdata. Tosumup,therearetwomaincontributionsinthiswork. Firstly,weproposeageneralkernelpoolingmethodvia compactexplicitfeaturemapping.Usingthelinearclas- onthefeaturemapisapproximatelysameasapplying thekerneltrick.Secondly,theproposedkernelpoolingis differentiableandcanbecombinedwithaCNNforjoint optimization.Thecompositionofthekernelcanalsobe learnedsimultaneouslyduringthetraining. 2.RelatedWork Theproposedkernelpoolingmethodreliesontheexist- ingeffortsonlowdimensionalcompactapproximationof explicitfeaturemaps.Rahimi etal .[ 33 ]isoneofthe workonusingrandomfeaturesforGaussianandLaplacian kernels.Later,thesimilarideawasgeneralizedtootherker- nelssuchasMaji etal .[ 26 ]forthehistogramintersection kernelandVedaldi etal .[ 42 ]for ˜ 2 kernel.Onthecompact approximationofpolynomialkernels,recentproposedRan- domMaclaurinbyKar etal .[ 17 ],TensorSketchbyPham etal .[ 31 ]andSubspaceEmbeddingbyAvron etal .[ 2 ]are themostnoticeablerepresentatives.Thereisalsoalineof workthattriestolearnhigherorderinteractionsfromthe datathroughoptimization[ 24 , 34 , 3 ].Wedifferfromthese workbythecombinationofConvolutionalNeuralNetworks (CNNs)inanend-to-endfashion.Withthejointoptimiza- tion,wecanleveragethepowerfuloff-the-shelffullycon- volutionalnetworkarchitecturestolearnbetterfeaturesdi- rectlyfromdata. Sincethedimensionof p th orderpooledfeaturegrows exponentiallywith p ,theuseof p> 2 inrealworldap- plicationsisoftenlimited.Inthecaseof p =2 ,the modelisusuallyreferredasBilinearmodels,intro- ducedbyTenenbaumandFreeman[ 40 ].Bilinearmod- elsdemonstrateimpressiveperformanceonvisualtasksap- pliedonbothhand-craftedfeatures[ 5 ]andlearnedfea- tures[ 23 , 35 , 22 , 12 ].Recently,fueledbycompact2 nd or- derpolynomialkernelapproximationwithTensorSketch [ 6 , 31 ],samevisualrecognitionperformancescanbepre- servedwithmuchlowerfeaturedimension[ 11 ]andnew applicationonvisualquestionansweringisenabled[ 10 ]. Wedifferfromtheseworkbygeneralizingthecompactrep- resentationfromBilinearmodelswith 2 nd orderpolynomial kernelto p th orderTaylorserieskernelinSec. 3 . Thecompositionofthekernelcanalsobelearnedthrough theend-to-endtrainingwithaCNN(seeSec. 3.3 ). 3.KernelPooling Wetheconceptofﬁpoolingﬂastheprocessof encodingandaggregatingfeaturemapsintoaglobalfea- turevector.ThearchitectureofConvolutionalNeuralNet- works(CNNs)canberegardedasfullyconvolutionallay- ersfollowedbythesubsequentpoolinglayersandalinear .Tab. 1 summariespoolingstrategiesadoptedin commonlyusedCNNarchitectures.Typicallypeopleuse astackoffullyconnectedlayerwithLinearUnit (ReLU)asinthecaseofAlexNet[ 20 ]andVGG[ 38 ].Fully connectedlayersoftenperformwellingeneralbutintro- duceheavycomputationandlargenumberofparameters, hencemakesthenetworkslowandeasytoovThere- centlyproposedInception[ 39 ]andResidualLearning[ 15 ] onlyuseglobalaveragepoolingonthefeaturemap.This Figure3.Anillustrationoftensorproduct.The p -leveltensor product x ( p ) of x 2 R c isa c p dimensionalvector. strategyismorecomputationallyefbutitdoesnot capturehigherorderfeatureinteractions,whicharebelieved crucialinmanyvisualrecognitiontasks[ 23 , 35 , 22 ].The bilinearmodels[ 5 , 23 ]explicitlygeneratethe c 2 dimen- sionalfeaturemapfor2 nd orderpolynomialkernel,which islatercompactlyapproximatedin[ 11 , 10 ]usingTensor Sketch[ 31 ].InlightofthesuccessofBilinearmodels,we proposeanapproachtogobeyondBilinearmodelsandcap- turehigherorderfeatureinteractions.WeTayler serieskernelandshowitsexplicitfeaturemapcanbecom- pactlyapproximated.Thenwedemonstratehowtousethe compactfeatureprojectionofTaylorserieskerneltoap- proximatecommonlyusedkernelssuchasGaussianRBF. 3.1.ExplicitfeatureprojectionviaTensorproduct Supposetheoutputfeaturemapofaconvolutionlayer is X 2 R h  w  c withheight h ,width w andnumberof channels c ,wedenotethe c dimensionalfeaturevectorofa spatiallocationon X as x =[ x 1 ;x 2 ;:::;x c ] > 2 R c . Theexplicitfeatureprojection ˚ ( : ) ofakernelfunction K ( :;: ) isbydecomposingthethevalueofkernel functionappliedontwofeaturevectors x and y astheinner productbetweentheirfeaturemaps: K ( x ; y )= ˚ ( x ) > ˚ ( y ) (2) Commonlyusedkernelfunctionsincludepolynomialker- nels ( x > y ) p ,GaussianRBFkernel exp(   k x  y k 2 ) , ˜ 2 kernel P c i =1 2 x i y i x i + y i , etc .Noticethatsomeofthekernels maycorrespondtoandimensionalfeatureprojec- tion( e.g .,GaussianRBF). WeintroducetheconceptofTensorproductandthen demonstrateitcanbeusedtogettheexplicitfeatureprojec- tionofatypeofkernelcalled Taylorserieskernel . First,wethe 2 -leveltensorproduct( i.e .,outer product xx > )of x as: x (2) = x  x = 2 6 6 6 4 x 1 x 1 x 1 x 2  x 1 x c x 2 x 1 x 2 x 2  x 2 x c . . . . . . . . . . . . x c x 1 x c x 2  x c x c 3 7 7 7 5 2 R c 2 (3) AlexNet/VGG Inception/ResNet Bilinear CompactBilinear Ours Strategy ˙ ( W 2 ˙ ( W 1 X )) 1 hw P i;j X ij 1 hw P i;j X ij X > ij 1 hw P i;j TS ( X ij ) 1 hw P i;j ˚ ( X ij ) Dimension d c c 2 d d Time O ( hwcd ) O ( hwc ) O ( hwc 2 ) O ( hw ( c + d log d )) O ( hwp ( c + d log d )) Space O ( hwcd ) 0 0 2 c pc Parameters O ( hwcd ) 0 0 0 0 or p Table1.AsummaryofpoolingstrategiesadoptedincommonlyusedCNNarchitectures. X representthefeaturemapofsize h  w  c , where h , w and c istheheight,widthandnumberofchannels; d representsthefeaturedimensionforthesubsequentlinear and p istheorderweusedfortheproposedkernelpooling. ˙ ( : ) , TS ( : ) and ˚ ( : ) denotestheReLUunit,TensorSketch[ 31 ]and theproposedkernelpoolingmehtod,respectively. Similarly,the p -leveltensorproductfor p  2 isas: x ( p ) = x  | {z } ptimes x 2 R c p (4) Wealsohave x (0) =1 and x (1) = x .Fig. 3 illustratesthe originalfeaturevector x andits 2 -leveland 3 -leveltensor product x (2) and x (3) .Ithasbeenshownin[ 36 ]thatthe p - leveltensorproductistheexplicitfeatureprojectionof p th orderPolynomialkernel: ( x > y ) p =( x ( p ) ) > ( y ( p ) ) (5) WetheTaylorserieskerneloforder p asfollows: K Taylor ( x ; y )= p X i =0  2 i ( x > y ) i (6) Sincethenon-negativelinearcombinationofkernelsisstill akernel[ 36 ],theTaylorserieskernelisavalidkernelas itcanbeexpressedasnon-negativelinearcombinationsof Polynomialkernels. Itiscleartoseethattheexplicitfeatureprojectionof Taylorserieskernelisgivenby: ˚ Taylor ( x )=[  0 ( x (0) ) > ;:::; p ( x ( p ) ) > ] > (7) Composedbytheconcatenationofscaledtensorproducts f  i x ( i ) g p i =0 , ˚ ( x ) 1 isalongfeaturevectorwithdimension O ( c p ) .Eveninthecaseof c =512 and p =3 , c p isstill largerthan 10 8 .Suchahighdimensionhindersitsappli- cationsinanyrealworldproblems.Therefore,acompact approximationmethodisneeded. 3.2.Compactapproximation Thecompactapproximationmethodisdifferentiableand hasgoodtimeandspacecomplexity.Thereareseveralre- centlyproposedworkonkernelapproximationwithrandom featureprojections[ 33 , 17 , 31 , 2 ].Webuildourapproxima- tionmethodonTensorSketching[ 31 ],becauseitconsumes lesstimeandspacecomparedto[ 33 , 17 ],anditiseasierto implementcomparedto[ 2 ]. 1 Forsimplicity,unlessotherwisewewilldropthesubscript of K Taylor and ˚ Taylor intheremainderofthepaper. Algorithm1: CountSketchforTaylorserieskernel Input: x 2 R c ;p; f d i g p i =2 ; f  i g p i =0 Output: ˚ ( x ) 2 R d ,where d =1+ c + P p i =2 d i ,s.t. ˚ ( x ) > ˚ ( y ) ˇK ( x ; y )= P p i =0  2 i ( x > y ) i . 1 Initialization: ˚ ( x )   [  2 0 ; x > ] > , P  1 . 2 for t   1 to p do 3 Generate 2 independenthashfunctions h t and s t . Theoutputsof h t and s t areuniformlydrawn from f 1 ; 2 ;:::;d t g and f +1 ;  1 g ,respectively. 4 CalculatetheCountSketchof x as C t ( x )= [ c 1 ;c 2 ;:::;c d t ] > ; where c i = P i : h t ( i )= j s t ( i ) x i . 5 P P FFT ( C t ( x )) 6 if t  2 then 7 ˚ ( x )   concatenate ( ˚ ( x ) ; FFT  1 ( P )) 8 return ˚ ( x ) 3.2.1Taylorserieskernel Tocompactlyapproximatethe p -leveltensorproduct x ( p ) ,wetheCountSketch[ 6 ]of x as: C ( x )=[ c 1 ;c 2 ;:::;c d ] > ; where c i = X i : h ( i )= j s ( i ) x i (8) TheCountSketch C ( x ) isa d -dimensionalvectorcalcu- latedusing 2 hashfunctions h ( : ) and s ( : ) .Theiroutputs areuniformlydrawnfrom f 1 ; 2 ;:::;d g and f +1 ;  1 g ,re- spectively.The p -leveltensorproduct x ( p ) canthenbeap- proximatedas: ~x ( p ) = FFT  1 ( FFT ( C 1 ( x ))  FFT ( C p ( x ))) (9) where C i ( x ) istheCountSketchcalculatedfrom 2 i inde- pendenthashfunctions h 1 ;h 2 ;:::;h i and s 1 ;s 2 ;:::;s i ,  denotestheelement-wisemultiplication,FFTandFFT  1 is theFastFourierTransformanditsInverse. CombiningEqn. 7 andEqn. 9 ,thefeaturemapofaTay- lorserieskernelcanbecompactlyapproximated,asde- scribedinAlg. 1 .Inputsincludetheoriginalfeaturevec- tor x ,theorder p oftheTaylorserieskerneltobeapproxi- mated,targetfeaturedimensions d i ( i  2) wewanttouse Figure4.ApproximatingGaussianRBFkernelbyTaylorseries kernelwithvariant p .Withoutlossofgenerality,weignorethe constant  whenplotting.Theapproximationerrordependson theinnerproductvalue x > y and  .Withtheproperchoiceof  basedon x > y ,using p =4 wouldbesuftoapproximate GaussianRBF. forestimating x ( i ) anditsassociatedcoef  i .Com- paredwiththeexplicitfeaturemapinEqn. 7 ,wereduce thefeaturedimensionfromexponentialtolinear.More ,from P p i =0 c i to d =1+ c + P p i =2 d i ,where d ˝ c i ; 8 i  2 . Ithasbeenprovedthat ~x ( p ) inEqn. 9 isanunbiasedfea- turemapestimatorfor p th orderPolynomialkernel.The relativeestimationerrorcanbeboundedbyChebyshev'sin- equality(seeLemma7in[ 31 ]forthedetailedproof).Sim- ilarly,theestimationerrorofusingAlg. 1 canbebounded as: P h    ˚ ( x ) > ˚ ( y ) K ( x ; y )      K ( x ; y ) i  1 d min  2  p ) (10) where d min =min( d 2 ;:::;d p ) and  p )= ( 2( p  1) ; if C =  1 2 C 2 ( C 2 p  1) C 2  1 ; otherwise C = 1 cos  isaconstantthatequalstothereciprocalofthe cosinesimilaritybetweentwofeaturevectors x and y .In ourexperience,wehigherdimensionalfeature(large d min )givesbetterapproximation,kernelswithlarger p in- troducelargererror,andtheerrorboundalsodependsheav- ilyontheanglebetweentwofeaturevectors. Figure5.Learningkernelcompositionbyend-to-endtrainingwith aCNN.Thecoefofthekernelarejointlylearnedtogether withweightsofotherCNNlayersviaback-propagationoftheloss (denotedbyoutgoingarrowsfromﬁLossﬂ). 3.2.2GaussianRBFkernel TheTaylorexpansionofGaussianRBFkernel[ 32 ]can beexpressedas: K RBF ( x ; y )=exp    k x  y k 2  =exp    ( k x k 2 + k y k 2  2 x > y )  =  exp  2  x > y  = 1 X i =0  (2  ) i i ! ( x > y ) i (11) where  =exp    ( k x k 2 + k y k 2 )  isaconstantand  =exp(  2  ) if x and y are ` 2 -normalized.Compared withTaylorserieskernelinEqn. 6 ,itisclearthatTaylorse- rieskernelcanbeusedtoapproximateGaussianRBFterm bytermuptoorder p bysetting  2 i as  (2  ) i i ! .Otherker- nelscanalsobeapproximatediftheyhaveaTaylorexpan- sioninthesimilarform.Fig. 4 illustratestheapproxima- tionofGaussianRBFbyTaylorserieskernelwithvariant p .Theapproximationerrordependsontheinnerproduct value x > y .Ingeneral,thecloserthevalueisto0,the smallertheapproximationerror.Soweneedtochoose  carefullybasedon x > y .Withtheproperchoiceof  ,using p =4 wouldbesuftoapproximateGaussianRBF. Experimentsonkernelapproximationerrorandtheeffect of  willbediscussedextensivelyinSec. 4.2 . 3.3.Learningkernelcomposition TheproposedkernelpoolingmethodinAlg. 1 relies onsimplecomputationswithasetofedhashfunctions f h t g and f s t g ,FFTandFFT  1 ,whicharealldifferentiable. CombinedwithaCNN,thelossfromthesoftmaxlayercan gothroughtheproposedkernelpoolinglayerandbepropa- gatedbacktotheprecedingfullyconvolutionlayers. Insteadofusingedcoeftoapprox- imateacertainkernelsuchasGaussianRBF,thecompo- sitionofthekernelcanbelearnedfromdata,asillustrated inFig. 5 .Designingandchoosingagoodkernelisachal- lengingtaskbecauseitishardtoprobetheunderlyingdis- tributionofhigh-dimensionalfeatures.Therefore,aker- nelfunctionisoftenchosenempiricallyorthroughcross- validation.Byjointlylearningthekernelcompositionto- getherwithCNNweightsinanend-to-endfashion,wear- guethelearnedkernelismoreadaptiveandsuitabletothe dataweareworkingon. 4.ExperimentalEvaluations Theproposedkernelpoolingmethodisevaluatedin termsofbothkernelapproximationerrorandvisualrecog- nitionaccuracy.Sec. 4.1 introducesexperimentsetupand baselinemethods.Then,inSec. 4.2 ,werunacomprehen- sivestudyofkernelapproximationqualityonCNNfea- tures.Wealsoinvestigatethesuchasthe choiceoffeaturedimension  d ,kernelorder p and  .Sec. 4.3 isthemajorpartoftheexperiment,inwhichwepresent extensiveevaluationsonvariousvisualrecognitiontasks, includingtherecognitionofbird[ 43 ],car[ 19 ],aircraft[ 27 ] andfood[ 4 ].Theproposedkernelpoolingmethodachieves state-of-the-artresultsonalldatasets. 4.1.Experimentsetup WeevaluateallpoolingstrategieslistedinTab. 1 .For CNNarchitectures,weuseVGG-16[ 38 ]andResNet-50 [ 15 ],bothofwhichachievedstate-of-the-artperformance onImageNet[ 9 ].VGG-16has 13 convolutionwithReLU layersand 3 fullyconnectedlayersincludingthelinear layerwithsoftmaxforion.ResNet-50consistsof 49 convolutionlayersfollowedbyglobalaveragepooling andthelinearsoftmaxlayer.BothVGGandResNet reducethespatialresolutionoftheinputimagebyafactor of 2 5 =32 duringtheconvolution.InthecaseofBilinear, CompactBilinearandourmodel,wekeepthefullyconvo- lutionalpartofthenetworkandusetheoutputfeaturemap fromthelastconvolutionlayer( i.e .,thefeaturevector x in Alg. 1 correspondstotheactivationateachspatiallocation oflastlayer'sfeaturemap).Forstandardpoolingmethods, wechooseVGG-16andResNet-50[ 15 ]asrepresentatives forfullyconnectedpoolingandglobalaveragepooling,re- spectively.TheperformanceofVGG-16andResNet-50is reportedbyne-tuningtheentirenetworkfromImageNet pre-trainedweights. 4.1.1Poolingmethods Wecomparetheperformanceofkernelpoolingmethods withthefollowingbaselines: VGGwithfullyconnectedpooling( VGG ):Thisisthe originalVGG-16networkproposedin[ 38 ].Thearchitec- tureofVGG-16isageneralizationoftheground-breaking AlexNet[ 20 ].InAlexNet,onlyoneconvolutionlayerisap- pliedtotheinputimageandthefeaturemapofa spatialresolution.InVGG,however,moreconvolutionlay- ers( 2 to 3 )areappliedforeachspatialresolution,which achievedstate-of-the-artperformanceonImageNetChal- lenge2014.BothAlexNetandVGGusethesamefully connectedpoolingscheme(astackoftwofullyconnected withReLUlayers)forthesubsequentsoftmaxlayer.Due totheednumberofnodesdesignedinfullyconnected layers,VGGrequiresaedinputimagesizeof 224  224 . Foreachofthedataset,wereplacethelastlinearlayerof VGGtomatchthenumberofcategoriesandthen thewholenetworkfromImageNetpre-trainedweights. ResidualLearningwithaveragepooling( ResNet ):Al- thoughthefullyconnectedlayerworkswellinpractice,it hasseveraldrawbacksincludingtheheavycomputationand largestorageneededaswellasthetendtoovRecently proposeddeepernetworksbasedonInceptionmodule[ 39 ] andResidualmodule[ 15 ]useglobalaveragepoolingafter convolutionlayersforthesubsequentlinear.The globalaveragepoolingislightweight,capableoftakingin- putofanysizeandparameter-free.However,itfailstocap- turenonlinearinformationinfeaturemaps.Wechoosea strongbaselineofResNetascomparison. BilinearPooling( BP ):Weapplyfullbilinearpoolingon topoftheconv 5 3 featuremapfromVGG-16,whichissame asthebest-performedB-CNN[D,D]in[ 23 ].Thefeature dimensionofthebilinearvectoris d =512  512 ˇ 260 K. Wedon'tcombineResNetwithbilinearpoolingbecause ResNethas 2048 channelsinthefeaturemap.The bruteforcebilinearvectorhasthedimensionof 2048  2048 ˇ 4 : 2 M,whichistoolargetouseinpractice. CompactBilinearPooling( CBP ):WeuseTensorSketch withedhashfunctionstoapproximatebilinearvectoron thefeaturemapofVGG-16andResNet-50.Whereasthe originalpaper[ 11 ]onlyusedVGG-16.Typically,compact bilinearpoolingcanachievesameperformanceasfullbi- linearpoolingwith d  8192 ,reducingtheoriginalfeature dimensionbyordersofmagnitude.Forafaircomparison, wesetthefeaturedimensioninCBPtobethesameasour kernelpoolingmethodinallexperiments. TheproposedKernelPooling( KP ):Weevaluatethepro- posedkernelpoolingmethodinthesamecontextasBPand CBP.Fortheactivation x ateachspatiallocationonthe featuremap,weapplyAlg. 1 togetthecompactfeature map ˚ ( x ) .SameasBPandCBP,thefeaturevector isaveragepooledacrossallthespatiallocations.Thecom- positionofthekernelisevaluatedwithlearnedcoef viaback-propagation.Thechoiceofkernelorder p ,feature dimension d and  willbediscussedinSec. 4.2 . 4.1.2Implementation Ourimplementationfollowsthecommonlyusedpractice in[ 20 , 38 , 23 , 11 ].Wehavetwoimageinputsizes: 224  224 and 448  448 .Foreachimageinputsize S  S ,we subtractitwiththepixel-wiseimagemean,andweresize theoriginalimagesothatitsshortersideis S whilekeeping itsaspectratio.Thenwecropa S  S squareimagefrom theoriginalimage.Duringtraining,arandomsquareimage iscropped.Boththeoriginalcropanditshorizontal areutilizedfordataaugmentation.Duringinference,the centerimageiscropped.Wepasstheoriginalcropandits horizontaltotheCNNindependently.Theaverageof theirscoresisourscore. Wefollowthepost-processingstepsin[ 23 , 11 ]tothe featurevector y beforethelinear,becausethe experimentsshowthatitimprovesrecogni- tionperformance.Weapplyelement-wisesignedsquare root: y   sign ( y ) p j y j followedby ` 2 normalization: y   y = k y k onthecompactfeature y vector. Forthesakeoffasterconvergenceandbetterperfor- mance,weusepre-trainedweightsfortheneuralnetwork. Theintialweightsoftheconvolutionallayersarepre- trainedonImageNetdataset,andtheinitial weightsofthelinearisobtainedbytraininga logisticregressiononthecompactkernelpooling ofpre-trainedCNNfeatures.Westartthewith 10 xsmallerlearningrate( i.e . 0 : 001 forVGGand 0 : 01 for ResNet)anddivideitby 10 afterevery 30 epochs.Weuse amomentumof 0 : 9 andaweightdecayof 0 : 0005 forVGG and 0 : 0001 forResNet.Thetrainingusuallyconvergesat around 50 epochs.Themodeldivergesduetolargegradi- entssometimes.Therefore,gradientclipping[ 29 ]isapplied toensureallgradientsfallintherangebetween  1 and +1 . WeuseTw[ 1 ]toimplementandtrainallthe models.OnasingleNVIDIATeslaK40GPU,thefor- wardandbackwardtimeofbothVGG-16andResNet-50 withkernelpoolingisabout 500 msona 448  448 image and 100 msona 224  224 image.Kernelpoolingrequires around 50 mswith d =4096 and p =4 . 4.2.Kernelapproximationand Thissubsectionpresentstheexperimentsonkernelap- proximationerrorusingAlg. 1 onCNNfeatures.Using VGG-16trainedonImageNet,weextractconv 5 3 feature mapsonthetrainingsetofCUB-200-2011[ 43 ],within- putsizeof 224  224 .Foreachspatiallocationinthe featuremap,thefeatureisa c =512 dimensionalvec- tor.Withoutlossofgenerality,weusethesamefeature poolingdimension  d foreachorderinkernelpooling( i.e ., d i =  d for i  2 ).Therefore,thefeaturedimensionis d =1+ c + P p i =2  d =513+( p  1)  d .Fig. 6 showsthe relativeapproximationerrorofGaussianRBFkernelinlog scale,withvariantfeaturepoolingdimension  d ,order p and Figure6.RelativeapproximationerrorforGaussianRBFkernel appliedonCNNfeatureswithvariantkernel  .Therelativeapproximationerrorbetweentwofeature vector x and y isgivenby:  = j ˚ ( x ) > ˚ ( y ) K RBF ( x ; y ) j K RBF ( x ; y ) (12) Wecomparekernelpooingwiththefeaturedimension  d from 50 to 5000 withthestepof 50 .Eachdatapointisthe averagederroron 100 Krandomlyselectedfeaturepairs. FromFig. 6 ,wehavethefollowingobservations:higher featurepoolingdimensiongivesbetterapproximationin general;approximationerroralsogoesdownwithincreas- ingorder p ;  playsakeyroleintheapproximationerror. TheaboveverifytheinsightsfromEqn. 10 .In Fig. 4 wecanseethatwithsuffeaturedimensionand orderaswellasaproper  ,wecanachievecloseto 1% rel- ativeerror.Inlightofthis,weuse  d =4096 and p =4 forallthefollowingexperiments.Theoutputvectorhasa dimensionof d =1+512+3  4096=12801 forVGG, and 1+2048+3  4096=14337 forResNet.Thehyper- parameter  issetasthereciprocalofthemeanofinner productsbetweenfeaturevectorsinthetrainingsettoen- surethat  x > y issmallonaverageandwecangetagood kernelapproximation. 4.3.Visualrecognition Weevaluateonthefollowingvisualrecognitiontasks. Birdspeciesrecognition :WeuseCUB-200dataset[ 43 ] forthistask.Thedatasetconsistsof 11 ; 788 imagesfrom 200 birdspecies.Eachcategoryhasaround 30 imagesfor bothtrainingandtesting. Carmake,model,year :TheStanford Cardataset[ 19 ]isusedforthistask.Ithas 16 ; 185 images of 196 classeswithcarmake,modelandyear. Aircraft :Theaircraftdataset [ 27 ]wasintroducedinFGComp2013challenge,which contains 100 aircraftcategoriesandeachhas 100 images. Dataset CNN Original BP [ 23 ] CBP [ 11 ] KP Others CUB[ 43 ] VGG-16[ 38 ] 73.1* 84.1 84.3 86.2 82.084.1 ResNet-50[ 15 ] 78.4 N/A 81.6 84.7 [ 18 ][ 16 ] StanfordCar[ 19 ] VGG-16 79.8* 91.3 91.2 92.4 92.6 82.7 ResNet-50 84.7 N/A 88.6 91.1 [ 18 ][ 14 ] Aircraft[ 27 ] VGG-16 74.1* 84.1 84.1 86.9 80.7 ResNet-50 79.2 N/A 81.6 85.7 [ 14 ] Food-101[ 4 ] VGG-16 81.2 82.4 82.4 84.2 50.76 ResNet-50 82.1 N/A 83.2 85.5 [ 4 ] Table2.Performancecomparisonsamongallbaselines,whereKPistheproposedkernelpoolingmethodwithlearnedcoefFol- lowingthestandardexperimentalsetup,weusetheinputsizeof 448  448 forCUB,StanfordCarandAircraftdatasetsexcepttheoriginal VGG-16(markedbyanasterisk*),whichrequiresaedinputsizeof 224  224 .ForFood-101,weusetheinputsizeof 224  224 for allthebaselines. Figure7.Imagesweusedforvisualrecognition.Fromlefttoright, eachcolumncontainsexamplesfromCUBBird[ 43 ],StanfordCar [ 19 ],Aircraft[ 27 ]andFood-101[ 4 ]. Foodrecognition :ForthistaskweuseFood-101dataset [ 4 ],whichisbyfarthelargestpubliclyavailablefoodrecog- nitiondatasettothebestofourknowledge.Thisisalarge- scaledatasetwith 101 ; 000 imagesand 1000 imagesper eachcategory.Thisdatasetischallengingbecausethetrain- ingimagesarenoisyandthebackgroundisnotclean. SampleimagesforeachtaskareshowninFig. 7 .Perfor- mancecomparisonwithallthebaselinesandstate-of-the-art methodsispresentedinTab. 2 .TheproposedKernelPool- ingwithlearnedcoefoutperformsallotherbaselines byalargemargin(around1-3%)onallthedatasets. 4.4.Discussion Inthissubsection,wediscusstherelativeimportanceof higherorderinformationfordifferentCNNarchitectures. WeexaminedlearnedkernelcoefonCUBdataset withkernelpoolingonVGGandResNet.Wefoundthat highorderfeatureinteractions,especially2 nd and3 rd or- der,areweightedmoreinVGGcomparedwithResNet.In ResNet,thereisnoobviousdistinctionamong 3 orders. Webelievethisisduetothedifferenceoftheunderlying networkarchitectures. OnereasonmightbethatinVGG,thenon-linearfeature interactionsaremainlycapturedbyfully-connectedlayers. Soremovingthefully-connectedlayersde- gradetheoriginal1 st orderfeature.SinceResNetonlyusea globalaveragepoolinglayerandhasaverylargereceptive thefeaturesatdifferentlocationsofthefeaturemapis encouragedtorepresentsimilarinformation.Togetherwith theresidualmoduleandamuchdeeperconvolutionalarchi- tecture,theoutputconvolutionfeaturecouldimplicitlycap- turemoreinformationthanVGG.Inourexperiments,we thattheperformanceofbothVGG-16andResNet-50 canbeimprovedwhentheproposedkernelpoolingmethod isutilized.Theseexperimentsverifytheeffectivenessof usinghigh-orderfeatureinteractionsinthecontextofCNN. 5.Conclusion Inthispaper,wehaveintroducedanoveldeepker- nelpoolingmethodasahigh-orderrepresentationforvi- sualrecognition.Theproposedmethodcaptureshigh- orderandnon-linearfeatureinteractionsviacompactex- plicitfeaturemapping.Theapproximatedrepresentation isfullydifferentiable,thusthekernelcompositioncanbe learnedtogetherwithaCNNinanend-to-endmanner.Ex- tensiveexperimentsdemonstratethatdeepkernelpooling methodachievesstate-of-the-artperformanceonvarious recognitiontasks. Acknowledgements ThisworkwassupportedinpartbyGoogleFocusedRe- searchAward,AWSCloudCreditsforResearch,Microsoft ResearchAwardandaFacebookequipmentdonation. References [1] M.Abadi,A.Agarwal,P.Barham,E.Brevdo,Z.Chen, C.Citro,G.S.Corrado,A.Davis,J.Dean,M.Devin,etal. Tw:Large-scalemachinelearningonheterogeneous distributedsystems. arXivpreprintarXiv:1603.04467 ,2016. 7 [2] H.Avron,H.Nguyen,andD.Woodruff.Subspaceembed- dingsforthepolynomialkernel.In NIPS ,2014. 1 , 3 , 4 [3] M.Blondel,M.Ishihata,A.Fujino,andN.Ueda.Polyno- mialnetworksandfactorizationmachines:Newinsightsand eftrainingalgorithms.In ICML ,2016. 1 , 3 [4] L.Bossard,M.Guillaumin,andL.VanGool.Food-101Œ miningdiscriminativecomponentswithrandomforests.In ECCV ,2014. 6 , 8 [5] J.Carreira,R.Caseiro,J.Batista,andC.Sminchisescu.Se- manticsegmentationwithsecond-orderpooling.In ECCV , 2012. 3 [6] M.Charikar,K.Chen,andM.Farach-Colton.Findingfre- quentitemsindatastreams.In InternationalColloquiumon Automata,Languages,andProgramming ,2002. 1 , 2 , 3 , 4 [7] M.Cimpoi,S.Maji,andA.Vedaldi.Deepbanksfor texturerecognitionandsegmentation.In CVPR ,2015. 2 [8] N.DalalandB.Triggs.Histogramsoforientedgradientsfor humandetection.In CVPR ,2005. 1 [9] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei- Fei.Imagenet:Alarge-scalehierarchicalimagedatabase.In CVPR ,2009. 6 [10] A.Fukui,D.H.Park,D.Yang,A.Rohrbach,T.Darrell, andM.Rohrbach.Multimodalcompactbilinearpooling forvisualquestionansweringandvisualgrounding. arXiv preprintarXiv:1606.01847 ,2016. 1 , 2 , 3 [11] Y.Gao,O.Beijbom,N.Zhang,andT.Darrell.Compact bilinearpooling.In CVPR ,2016. 1 , 2 , 3 , 6 , 7 , 8 [12] L.A.Gatys,A.S.Ecker,andM.Bethge.Aneuralalgorithm ofartisticstyle. arXivpreprintarXiv:1508.06576 ,2015. 2 , 3 [13] L.A.Gatys,A.S.Ecker,andM.Bethge.Texturesynthesis andthecontrolledgenerationofnaturalstimuliusingconvo- lutionalneuralnetworks.In NIPS ,2015. 2 [14] P.-H.Gosselin,N.Murray,H.J ´ egou,andF.Perronnin.Re- visitingthevectorfor Pat- ternRecognitionLetters ,2014. 8 [15] K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearning forimagerecognition.In CVPR ,2016. 1 , 3 , 6 , 8 [16] M.Jaderberg,K.Simonyan,A.Zisserman,etal.Spatial transformernetworks.In NIPS ,2015. 8 [17] P.KarandH.Karnick.Randomfeaturemapsfordotproduct kernels.In AISTATS ,2012. 1 , 3 , 4 [18] J.Krause,H.Jin,J.Yang,andL.Fei-Fei.Fine-grained recognitionwithoutpartannotations.In Proceedingsofthe IEEEConferenceonComputerVisionandPatternRecogni- tion ,pages5546Œ5555,2015. 8 [19] J.Krause,M.Stark,J.Deng,andL.Fei-Fei.3dobjectrep- resentationsforcategorization.In ICCVWork- shop ,2013. 6 , 7 , 8 [20] A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenet withdeepconvolutionalneuralnetworks.In NIPS ,2012. 1 , 3 , 6 , 7 [21] S.Lazebnik,C.Schmid,andJ.Ponce.Beyondbagsof features:Spatialpyramidmatchingforrecognizingnatural scenecategories.In CVPR ,2006. 2 [22] T.-Y.LinandS.Maji.Visualizingandunderstandingdeep texturerepresentations.In CVPR ,2016. 2 , 3 [23] T.-Y.Lin,A.RoyChowdhury,andS.Maji.Bilinearcnnmod- elsforvisualrecognition.In ICCV ,2015. 1 , 2 , 3 , 6 , 7 , 8 [24] R.Livni,S.Shalev-Shwartz,andO.Shamir.Onthecom- putationalefyoftrainingneuralnetworks.In NIPS , 2014. 1 , 3 [25] D.G.Lowe.Distinctiveimagefeaturesfromscale-invariant keypoints. IJCV ,2004. 1 [26] S.MajiandA.C.Berg.Max-marginadditivefor detection.In ICCV ,2009. 3 [27] S.Maji,E.Rahtu,J.Kannala,M.Blaschko,andA.Vedaldi. Fine-grainedvisualofaircraft. arXivpreprint arXiv:1306.5151 ,2013. 6 , 7 , 8 [28] A.OlivaandA.Torralba.Modelingtheshapeofthescene: Aholisticrepresentationofthespatialenvelope. IJCV ,2001. 2 [29] R.Pascanu,T.Mikolov,andY.Bengio.Onthedifof trainingrecurrentneuralnetworks.In ICML ,2013. 7 [30] F.Perronnin,J.S ´ anchez,andT.Mensink.Improvingthe kernelforlarge-scaleimageIn ECCV , 2010. 2 [31] N.PhamandR.Pagh.Fastandscalablepolynomialkernels viaexplicitfeaturemaps.In KDD ,2013. 1 , 2 , 3 , 4 , 5 [32] T.PoggioandF.Girosi.Networksforapproximationand learning. ProceedingsoftheIEEE ,78(9):1481Œ1497,1990. 5 [33] A.RahimiandB.Recht.Randomfeaturesforlarge-scale kernelmachines.In NIPS ,2007. 3 , 4 [34] S.Rendle.Factorizationmachineswithlibfm. ACMTrans- actionsonIntelligentSystemsandTechnology(TIST) ,2012. 1 , 3 [35] A.RoyChowdhury,T.-Y.Lin,S.Maji,andE.Learned- Miller.Facewithbilinearcnns. arXivpreprint arXiv:1506.01342 ,2015. 2 , 3 [36] B.Sch ¨ olkopfandA.J.Smola. Learningwithkernels:sup- portvectormachines,regularization,optimization,andbe- yond .MITpress,2002. 4 [37] D.Scholkopf,F.Achlioptas,andM.Bernhard.Sampling techniquesforkernelmethods. NIPS ,2002. 1 [38] K.SimonyanandA.Zisserman.Verydeepconvolutional networksforlarge-scaleimagerecognition. arXivpreprint arXiv:1409.1556 ,2014. 1 , 3 , 6 , 7 , 8 [39] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed, D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich. Goingdeeperwithconvolutions.In CVPR ,2015. 1 , 3 , 6 [40] J.B.TenenbaumandW.T.Freeman.Separatingstyleand contentwithbilinearmodels. Neuralcomputation ,2000. 3 [41] V.Vapnik. Thenatureofstatisticallearningtheory .Springer Science&BusinessMedia,2013. 1 [42] A.VedaldiandA.Zisserman.Efadditivekernelsvia explicitfeaturemaps. PAMI ,2012. 3 [43] C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. Thecaltech-ucsdbirds-200-2011dataset. CaliforniaInsti- tuteofTechnology ,2011. 6 , 7 , 8 [44] C.WilliamsandM.Seeger.Usingthenystr ¨ ommethodto speedupkernelmachines.In NIPS ,2001. 1 [45] J.Yang,K.Yu,Y.Gong,andT.Huang.Linearspatialpyra- midmatchingusingsparsecodingforimage In CVPR ,2009. 2  
TheApolloScapeDatasetforAutonomousDriving XinyuHuang,XinjingCheng,QichuanGeng,BinbinCao, DingfuZhou,PengWang,YuanqingLin,andRuigangYang BaiduResearch,Beijing,China NationalEngineeringLaboratoryofDeepLearningTechnologyandApplication,China f huangxinyu01,chengxinjing,gengqichuan,caobinbin g @baidu.com f zhoudingfu,wangpeng54,linyuanqing,yangruigang g @baidu.com Abstract Sceneparsingaimstoassignaclass(semantic)label foreachpixelinanimage.Itisacomprehensiveanal- ysisofanimage.Giventheriseofautonomousdriving, pixel-accurateenvironmentalperceptionisexpectedtobea  keyenablingtechnicalpiece.However,providingalarge  scaledatasetforthedesignandevaluationofscenepars-  ingalgorithms,inparticularforoutdoorscenes,hasbeen  difTheper-pixellabellingprocessisprohibitivelyex- pensive,limitingthescaleofexistingones.Inthispaper, wepresentalarge-scaleopendataset,ApolloScape,that  consistsofRGBvideosandcorrespondingdense3Dpoint  clouds.Comparingwithexistingdatasets,ourdatasethas  thefollowinguniqueproperties.Thestisitsscale,our  initialreleasecontainsover140KimagesŒeachwithits per-pixelsemanticmask,upto1Misscheduled.Thesecond isitscomplexity.Capturedinvarioustrafconditions,the  numberofmovingobjectsaveragesfromtenstooverone  hundred(Figure1).Andthethirdisthe3Dattribute,each  imageistaggedwithhigh-accuracyposeinformationatcm  accuracyandthestaticbackgroundpointcloudhasmmrel- ativeaccuracy.Weareabletolabelthesemanyimagesby aninteractiveandeflabellingpipelinethatutilizes  thehigh-quality3Dpointcloud.Moreover,ourdatasetalso  containsdifferentlanemarkingsbasedonthelanecolors  andstyles.Weexpectournewdatasetcandeeplybene- variousautonomousdrivingrelatedapplicationsthatin- cludebutnotlimitedto2D/3Dsceneunderstanding,local-  ization,transferlearning,anddrivingsimulation. 1.Introduction Semanticsegmentation,orsceneparsing,ofurbanstreet viewsisoneofmajorresearchtopicsintheareaofau- tonomousdriving.Anumberofdatasetshavebeencol-  lectedinvariouscitiesinrecentyears,aimingtoincrease Figure1.Anexampleofcolorimage( top ),2Dsemanticlabel ( middle ),anddepthmapforthestaticbackground( bottom ). variabilityandcomplexityofurbanstreetviews.The  Cambridge-drivingLabeledVideodatabase(CamVid)[1]  couldbethedatasetwithsemanticannotatedvideos.  Thesizeofthedatasetisrelativelysmall,whichcontains 701manuallyannotatedimageswith32semanticclasses capturedfromadrivingvehicle.TheKITTIVisionBench-  markSuite[4]collectedandlabeledadatasetfordifferent  computervisiontaskssuchasstereo,opticalw,2D/3D  objectdetectionandtracking.Forinstance,7,481training  and7,518testimagesareannotatedby2Dand3Dbound- ingboxesforthetasksofobjectdetectionandobjectorien- tationestimation.Thisdatasetcontainsupto15carsand  30pedestriansineachimage.However,pixel-levelannota-  tionsareonlymadepartiallybythirdpartieswithoutquality 1 arXiv:1803.06184v1  [cs.CV]  16 Mar 2018controls.Asaresult,semanticsegmentationbenchmarkis notprovideddirectly.TheCityscapesDataset[2]focuses  on2Dsemanticsegmentationofstreetviewsthatcontains  30classes,5,000imageswithannotations,and20,000  imageswithcoarseannotations.Althoughvideoframesare available,onlyoneimage(20thimageineachvideosnip- pet)isannotated.TheTorontoCitybenchmark[12]collects  LIDARdataandimagesincludingstereoandpanoramas  frombothdronesandmovingvehicles.Currently,thiscould  bethelargestdataset,whichcoversthegreaterTorontoarea.  However,asmentionedbyauthors,itisnotpossibletoman- uallylabelthisscaleofdataset.Therefore,onlytwoseman- ticclasses,i.e.,buildingfootprintsandroads,areprovided  asthebenchmarktaskofthesegmentation. Inthispaper,wepresentanon-goingprojectaimedto provideanopenlarge-scalecomprehensivedatasetforur-  banstreetviews.TheeventualdatasetwillincludeRGB  videoswithmillionshighresolutionimageandperpixelan-  notation,survey-gradedense3Dpointswithsemanticseg-  mentation,stereoscopicvideowithrareevents,night-vision sensors.Ouron-goingcollectionwillfurthercoverawide rangeofenvironment,weather,andtrafconditions.Com-  paringwithexistingdatasets,ourdatasethasthefollowing  characteristics: 1. Thesubset,143,906imageframeswithpixelan- notations,hasbeenreleased.Wedivideourdataset  intoeasy,moderate,andhardsubsets.Thedif levelsaremeasuredbasedonnumberofvehiclesand pedestriansperimagethatoftenindicatesthescene  complexity.Ourgoalistocaptureandannotatearound  onemillionvideoframesandcorresponding3Dpoint  clouds. 2. Ourdatasethassurvey-gradedense3Dpointcloudfor staticobjects.Arendereddepthmapisassociatedwith  eachimage,creatingthepixel-annotatedRGB-D  videoforoutdoorscenes. 3. Inadditiontotypicalobjectannotations,ourdataset alsocontainsgrainlabellingoflanemarkings  (with28classes). 4. Aninteractiveandef2D/3Djoint-labelling pipelineisdesignedforthisdataset.Onaverageit  saves70%labelingtime.Basedonourlabelling  pipeline,allthe3Dpointcloudswillbeassignedwith  aboveannotations.Therefore,ourdatasetisthe  opendatasetofstreetviewscontaining3Dannotations. 5. Theinstance-levelannotationsareavailableforvideo frames,whichareespeciallyusefultodesignspatial-  temporalmodelsforpredication,tracking,andbehav-  ioranalysisofmovableobjects. Figure2.Acquisitionsystemconsistsoftwoleaserscanners,upto sixvideocameras,andacombinedIMU/GNSSsystem. Wehavealreadyreleasethebatchofourdataset at http://apolloscape.auto .Moredatawillbe addedperiodically. 2.Acquisition RieglVMX-1HA[9]isusedasouracquisitionsys- temthatmainlyconsistsoftwoVUX-1HAlaserscanners ( 360  FOV,rangefrom1.2mupto420mwithtargetre- vitylargerthan80%),VMX-CS6camerasystem(two  frontcamerasareusedwithresolution 3384  2710 ),and themeasuringheadwithIMU/GNSS(positionaccuracy  20 ˘ 50 mm,roll&pitchaccuracy 0 : 005  ,andheading accuracy 0 : 015  ). Thelaserscannersutilizestwolaserbeamstoscanits surroundingsverticallythataresimilartothepush-broom  cameras.Comparingwithcommon-usedVelodyneHDL- 64E[11],thescannersareabletoacquirehigherdensityof pointcloudsandobtainhighermeasuringaccuracy/preci-  sion(5mm/3mm).Thewholesystemhasbeeninternally  calibratedandsynchronized.Itismountedonthetopofa  mid-sizeSUV(Figure2)thatdrivesatthespeedof30km  perhourandthecamerasaretriggeredeveryonemeter. However,theacquiredpointcloudsofmovingobjectscould behighlydistortedorevencompletelymissing. 3.Dataset Currently,wehavereleasedthepartofthedataset thatcontains143,906videoframesandcorresponding pixel-levelannotationsforsemanticsegmentationtask.In  thereleaseddataset,89,430instance-levelannotationsfor  movableobjectsarefurtherprovided,whichcouldbepar- ticularlyusefulforinstance-levelvideoobjectsegmentation andpredication.Table2showsacomparisonofseveral  keypropertiesbetweenourdatasetandotherstreet-view  datasets. 2 Table1.TotalandaveragenumberofinstancesinKitti, Cityscapes,andourdataset(instance-level).Theletters,e,m,and  h,indicateeasy,moderate,andhardsubsetsrespectively. CountKittiCityscapesOurs(instance) total (  10 4 ) person0.62.454.3 vehicle3.04.1198.9 averageperimageemh person0.87.01.16.216.9  vehicle4.111.812.724.038.1 car--9.716.624.5  motorcycle--0.10.82.5  bicycle--0.21.12.4  rider--0.83.36.3 truck--0.80.81.4 bus--0.71.30.9  tricycle000.40.30.2 Thedatasetiscollectedfromdifferenttracesthatpresent easy,moderate,andheavyscenecomplexities.Similarto  theCityscapes,wemeasurethescenecomplexitybasedon  theamountofmovableobjects,suchaspersonandvehi-  cles.Table1comparesthescenecomplexitiesbetweenour  datasetandotheropendatasets[2,4].Inthetable,wealso showthestatisticsfortheindividualclassesofmovableob- jects.Wethatbothtotalnumberandaveragenum-  berofobjectinstancesaremuchhigherthanthoseofother  datasets.Moreimportantly,ourdatasetcontainsmorechal-  lengingenvironmentsareshowninFigure3.Forinstance,  twoextremelightingconditions(e.g.,darkandbright)ap- pearinthesameimagethatcouldbecausedbytheshadow ofanoverpass.ofmultiplenearbyvehicleson  abussurfacemayfailmanyinstance-levelsegmentational-  gorithms.Wewillcontinuereleasemoredatainnearfu-  turewithlargediversitiesoflocation,trafconditions,and  weathers. 3.1. Weannotate25differentlabelscoveredbyvegroups. Table3givesthedetailsoftheselabels.TheIDsshownin thetablearetheIDsusedfortraining.Thevalue255in- dicatestheignoringlabelsthatcurrentlyarenotevaluated  duringthetestingphase.Theoftheclasses  aresimilartothecityscapedatasetwithseveraldifferences.  Forinstance,weaddonenewﬁtricycleﬂclassthatisaquite  populartransportintheeastAsiacountries.Thisclasscov- ersallkindsofthree-wheeledvehiclesthatcouldbeboth motorizedandhuman-powered.Whiletheriderclassinthe  cityscapeisasthepersononmeansoftransport,  weconsiderthepersonandthetransportasasinglemoving objectandmergethemtogetherasasingleclass. Wealsoannotate28differentlanemarkingsthatcur- rentlyarenotavailableinexistingopendatasets.Theanno-  tationsarebasedonlaneboundaryattributesinclud-  ingcolor(e.g.,whiteandyellow)andtype(e.g.,solidand broken).Table4givesdetailedinformationoftheselane markings.Weseparateﬁvisibleoldmarkingﬂfromother  classes,whichrepresentstheﬁghostmarkingﬂthatisvis-  ibleremnantsofoldlanemarking.Thiskindofmarking  isapersistentprobleminmanycountriesthatcouldcause  confusionevenforhumandrivers. 4.LabelingProcess Inordertomakeourlabelingofvideoframesaccurate andefwedevelopalabelingpipelineasshownin Figure4.Thepipelinemainlyconsistsoftwostages,3Dla- belingand2Dlabeling,tohandlestaticbackground/objects  andmovingobjectsrespectively.Thebasicideaofour  pipelineissimilartotheonedescribedin[14],whilesome  keytechniquesusedinourpipelinearedifferent.Forin-  stance,thealgorithmstohandlemovingobjectsarediffer- ent. Thepointcloudsofmovingobjectscouldbehighlydis- tortedasmentionedintheSection2.Therefore,wetake threestepstoeliminatethispartofpointclouds:1)scan  thesameroadsegmentmultiplerounds;2)alignthesepoint  clouds;3)removethepointsbasedonthetemporalconsis-  tency.Notethatadditionalcontrolpointscouldbeaddedto furtherimprovealignmentperformanceinthestep2). Inordertospeedupthe3Dlabelingprocess,we over-segmentpointcloudsintopointclustersbasedonspa- tialdistancesandnormaldirections.Then,welabelthese  pointclustersmanually.Basedonpartoflabeleddata,we  alsore-trainthePointNet++model[7]topre-segmentthe pointcloudsthatcouldachievebettersegmentationperfor- mance.Asthesepreliminaryresultsstillcannotbeused  directlyasthegroundtruth,wetheresultsby  wrongannotationsmanually.Thewrongannotationsoften  occuraroundtheobjectboundaries.The3Dlabelingtool  isdevelopedtointegratetheabovemodulestogether.The userinterfacedesignofthetoolasshowninFigure5fur- therspeedupthelabelingprocess,whichincludes3Dro-  tation,(inverse-)selectionbypolygons,matchingbetween  pointcloudsandcameraviews,andsoon. Oncethe3Dannotationsaregenerated,theannotations ofstaticbackground/objectsforallthe2Dimageframesare  generatedautomaticallyby3D-2Dprojections.Thesplat- tingtechniquesincomputergraphicsarefurtherappliedto handleunlabeledpixelsthatareoftencausedbymissing  pointsorstrong Tospeedupthe2Dlabelingprocess,wetrainaCNN networkformovableobjects[13]andpre-segmentthe2D  images.Anotherlabelingtoolfor2Dimagesisdeveloped 3 Figure3.Someimageswithchallengingenvironments(center-croppedforvisualizationpurpose).Thelastrowcontainsenlargedregions enclosedbyyellowrectangles. Table2.Comparisonbetweenourdatasetandtheotherstreet-viewdatasets.ﬁRealdataﬂmeanswhetherthedataiscollectedfromour physicalworld.ﬁ3Dlabelsﬂmeanswhetheritcontainsa3Dmapofsceneswithsemanticlabel.ﬁ2Dvideolabelsﬂmeanswhetherithas  per-pixelsemanticlabel.ﬁ2D/3Dlanelabelsﬂmeanswhetherithas3Dsemanticlabelsandvideoper-pixellabelsforlanemarkings. DatasetRealDataCameraPose3DLabels2DVideoLabels2D/3DLaneLabels CamVid[1] p ---- Kitti[4] pp sparse-- Cityscapes[2] p --selectedframes- Toronto[12] pp building&roadselectedpixels- Synthia[10]- p - pp P.E.B.[8]- p - p - Ours pp dense pp toorthesegmentationresults.Again,thewrong annotationsoftenoccuraroundtheobjectboundariesthat  couldbecausedbymerge/splitofmultipleobjectsandharsh  lightingconditions.Our2Dlabelingtoolisdesignedsothat thecontrolpointsoftheboundariescouldbeeasilyselected andadjusted. Figure6presentsanexampleof2Dannotatedimage. Noticethatsomebackgroundclassessuchasfence,traf  light,andvegetationareabletobeannotatedindetails.In  otherdatasets,theseclassescouldbeambiguouscausedby occlusionsorlabeledasawholeregioninordertosavela-  belingefforts. 5.BenchmarkSuite Given3Dannotations,2Dpixelandinstance-levelanno- tations,backgrounddepthmaps,cameraposeinformation, anumberoftaskscouldbeIncurrentrelease,we  mainlyfocusonthe2Dimageparsingtask.Wewouldlike  toaddmoretasksinnearfuture. 4 Figure4.Our2D/3Dlabelingpipelinethathandlesstaticbackground/objectsandmovingobjectsseparately. Figure5.Theuserinterfaceofour3Dlabelingtool. Figure6.Anexampleof2Dannotationwithboundariesindetails. 5.1.ImageParsingMetric Givensetofgroundtruthlabels S = f L i g N i =1 andset ofpredictedlabels S  = f ^ L i g N  i =1 ,theintersectoverunion (IoU)metric[3]foraclass c iscomputedas, IoU ( S ; S  ;c )= P N i =1 tp ( i;c ) P N  i =1 ( tp ( i;c )+ fp ( i;c )+ tn ( i;c )) (1) tp ( i;c )= X p 1 ( L i ( p )= c  ^ L i ( p )= c ) fp ( i;c )= X p 1 ( L i ( k ) 6 = c  ^ L i ( p )= c ) tn ( i;c )= X p 1 ( L i ( k )= c  ^ L i ( p ) 6 = c ) ThentheoverallmeanIoUistheaverageofallCclasses: F ( S ; S  )= 1 C P c IoU ( S ; S  ;c ) : 5.2.Perbasedevaluation Trackinginformationbetweenconsecutiveframesisnot availableinthecurrentrelease.Therefore,weusepre-frame basedevaluation.However,ratherthanevaluatingallthe imagestogetherthatissameastheevaluationforsingleim-  age,weconsiderper-frameevaluation. 5.2.1Metricforvideosemanticsegmentation Weproposetheper-frameIoUmetricthatevaluateseach predictedframeindependently. Givenasequenceofimageswithgroundtruthlabels S = f L i g N i =1 andpredictedlabel S  = f ^ L i g N  i =1 .Letthe metricbetweentwocorrespondingimagesis m ( L ; ^ L ) : Each predictedlabel L willcontainper-pixelprediction. F ( S ; S  )= mean ( P i m ( L i ; ^ L i ) P i N i ) (2) m ( L i ; ^ L i )=[  ;IoU ( L i = j; ^ L i = j ) ;  ] T (3) N i =[  ; 1( j 2L ( L i ) orj 2L ( ^ L i ) ;  ] (4) 5 Table3.Detailsofclassesinourdataset. GroupClassIDDescription movablecar1 objectmotorcycle2 bicycle3  person4  rider5personon motorcycle, bicycleor tricycle truck6  bus7  tricycle8three-wheeled vehicles, motorized,or human-powered surfaceroad9 sidewalk10 infrastructuretrafcone11movableand cone-shaped markers bollard12edwithmany differentshapes fence13 traflight14  pole15 trafsign16 wall17  trashcan18  billboard19  building20 bridge255 tunnel255  overpass255 naturevegetation21 voidvoid255otherunlabeled objects whereIoUiscomputedbetweentwobinarymasks M 1 and M 2 . j 2L ( L i ) meanslabel j isappearedintheground truthlabel L i . 5.3.Metricforvideoobjectinstancesegmentation Wematchbetweengroundtruthandpredictedin- stancesbasedonthethresholdingofoverlappingareas.For  eachpredictedinstance,iftheoverlappingareabetweenthe predictedinstanceandtheignoringlabelsislargerthana threshold,thepredictedinstanceisremovedfromtheeval-  uation.Noticethatthegroupclasses,suchascargroupand  bicyclegroup,arealsoignoredintheevaluation.Predicted Table4.Detailsoflanemarkingsinourdataset(y:yellow, w:white). TypeColorUseID solidwdividing200 solidydividing204 doublesolidwdividing,nopass213 doublesolidydividing,nopass209  solid&brokenydividing,one-waypass207  solid&brokenwdividing,one-waypass206 brokenwguiding201  brokenyguiding203 doublebrokenyguiding208 doublebrokenwguiding211 doublebrokenwstop216  doublesolidwstop217 solidwchevron218 solidychevron219 solidwparking210 crosswalkwparallel215 crosswalkwzebra214 arrowwrightturn225  arrowwleftturn224 arrowwthru&rightturn222 arrowwthru&leftturn221  arrowwthru220  arrowwu-turn202  arrowwleft&rightturn226 symbolwrestricted212 bumpn/aspeedreduction205 visibleoldy/wn/a223  marking voidvoidotherunlabeled255 instancesthatarenotmatchedarecountedasfalsepositives. Weusetheinterpolatedaverageprecision(AP)[5]as themetricforobjectsegmentation.TheAPiscomputedfor  eachclassforalltheimageframesforeachvideoclip.The  meanAP(mAP)isthencomputedforallthevideoclipsand  alltheclasses. 6.ExperimentResultsforImageParsing WeconductedourexperimentsontheWideResNet-38 network[13]thattradesdepthforwidthcomparingwiththe originalResNetstructure[6].Thereleasedmodelis tunedusingourdatasetwithinitiallearningrate0.0001,  cropsize 512  512 ,uniformsampling,10timesdataaug- mentation,and100epochs.Thepredictionsarecomputed 6 Table5.ResultsofimageparsingbasedonResNet-38network using5Ktrainingimages. IoU GroupClassCityscapeOurs movablecar94.6787.12 objectmotorcycle70.5127.99 bicycle78.5548.65 person83.1757.12 rider65.456.58  truck62.4325.28  bus88.6148.73 mIoU 77.6343.07 surfaceroad97.9492.03 sidewalk84.0846.42 infrastructurefence61.4942.08 traflight70.9867.49  pole62.1146.02  trafsign78.9379.60 wall58.818.41 building92.6665.71 naturevegetation92.4190.53 withonesinglescale1.0andwithoutanypost-processing  steps.Tobecomparablewiththetrainingandtestingin  theResNet-38network,weselectasmallsubsetfromour  datasetthatconsistsof5,378trainingimagesand671test-  ingimages,whichareatthesameorderoflabeledim- agesintheCityscapesdataset(i.e.,around5Ktrainingim- agesand500testimages).Table5showstheparsingresults  ofclassesincommonforthesetwodatasets.Noticethat  theIoUscomputedbasedonourdatasetaremuchlower  thantheIoUsfromtheCityscapes.ThemIoUformovable  objectsinourdatasetis34.6%lowerthantheoneforthe Cityscapes(commonclassesforbothdatasets). 7.ConclusionandFutureWork Inthiswork,wepresentalarge-scalecomprehensive datasetofstreetviews.Thisdatasetcontains1)higherscene complexitiesthanexistingdatasets;2)2D/3Dannotations  andposeinformation;3)variousannotatedlanemarkings;  4)videoframeswithinstance-levelannotations. Inthefuture,wewillrstenlargeourdatasettoachieve onemillionannotatedvideoframeswithmorediv  conditionsincludingsnow,rain,andfoggyenvironments.  Second,weplantomountstereocamerasandapanoramic camerasysteminnearfuturetogeneratedepthmapsand panoramicimages.Inthecurrentrelease,thedepthinfor-  mationforthemovingobjectsisstillmissing.Wewould  liketoproducecompletedepthinformationforbothstatic backgroundandmovingobjects. References [1] G.J.Brostow,J.Fauqueur,andR.Cipolla.Semanticobject classesinvideo:Agroundtruthdatabase. PatternRecognitionLetters ,30(2):88Œ97,2009. [2] M.Cordts,M.Omran,S.Ramos,T.Rehfeld,M.Enzweiler, R.Benenson,U.Franke,S.Roth,andB.Schiele.The  cityscapesdatasetforsemanticurbansceneunderstanding.  In Proc.oftheIEEEConferenceonComputerVisionand PatternRecognition(CVPR) ,2016. [3] M.Everingham,S.A.Eslami,L.VanGool,C.K.Williams, J.Winn,andA.Zisserman.Thepascalvisualobjectclasses  challenge:Aretrospective. Internationaljournalofcom- putervision ,111(1):98Œ136,2015. [4] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun.Visionmeets robotics:Thekittidataset. InternationalJournalofRobotics Research(IJRR) ,2013. [5] B.Hariharan,P.Arbel ´ aez,R.Girshick,andJ.Malik.Simul- taneousdetectionandsegmentation.In EuropeanConfer- enceonComputerVision ,pages297Œ312.Springer,2014. [6] K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearn- ingforimagerecognition.In ProceedingsoftheIEEEcon- ferenceoncomputervisionandpatternrecognition ,pages 770Œ778,2016. [7] C.R.Qi,L.Yi,H.Su,andL.J.Guibas.Pointnet++:Deephi- erarchicalfeaturelearningonpointsetsinametricspace.In  AdvancesinNeuralInformationProcessingSystems ,pages 5105Œ5114,2017. [8] S.R.Richter,Z.Hayder,andV.Koltun.Playingforbench- marks.In InternationalConferenceonComputerVision (ICCV) ,2017. [9] RIEGL.VMX-1HA. http://www.riegl.com/ ,2018. [Online;accessed01-March-2018]. [10] G.Ros,L.Sellart,J.Materzynska,D.Vazquez,andA.M. Lopez.Thesynthiadataset:Alargecollectionofsynthetic  imagesforsemanticsegmentationofurbanscenes.In Pro- ceedingsoftheIEEEConferenceonComputerVisionand  PatternRecognition ,pages3234Œ3243,2016. [11] VelodyneLidar.HDL-64E. http://velodynelidar. com/ ,2018.[Online;accessed01-March-2018]. [12] S.Wang,M.Bai,G.Mattyus,H.Chu,W.Luo,B.Yang, J.Liang,J.Cheverie,S.Fidler,andR.Urtasun.Torontocity:  Seeingtheworldwithamillioneyes.In Proceedingsofthe IEEEConferenceonComputerVisionandPatternRecogni-  tion ,pages3009Œ3017,2017. [13] Z.Wu,C.Shen,andA.v.d.Hengel.Widerordeeper:Revis- itingtheresnetmodelforvisualrecognition. arXivpreprint arXiv:1611.10080 ,2016. [14] J.Xie,M.Kiefel,M.-T.Sun,andA.Geiger.Semanticin- stanceannotationofstreetscenesby3dto2dlabeltransfer.  In ProceedingsoftheIEEEConferenceonComputerVision andPatternRecognition ,pages3688Œ3697,2016. 7  
Proceedingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguistics andthe7thInternationalJointConferenceonNaturalLanguageProcessing ,pages1127Œ1137, Beijing,China,July26-31,2015. c  2015AssociationforComputationalLinguistics 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137  
Information-theoreticSemi-supervisedMetricLearning viaEntropyRegularization GangNiu GANG @ SG . CS . TITECH . AC . JP DepartmentofComputerScience,TokyoInstituteofTechnology,Tokyo152-8552,Japan BoDai BOHR . DAI @ GMAIL . COM DepartmentofComputerScience,PurdueUniversity,WestLafayette,IN47907,USA MakotoYamada YAMADA @ SG . CS . TITECH . AC . JP MasashiSugiyama SUGI @ CS . TITECH . AC . JP DepartmentofComputerScience,TokyoInstituteofTechnology,Tokyo152-8552,Japan Abstract Weproposeageneralinformation-theoreticap- proachcalledS ERAPH (SEmi-supervisedmetRic leArningParadigmwithHyper-sparsity) formet- riclearningthatdoesnotrelyuponthemanifold assumption.Giventheprobabilityparameterized byaMahalanobisdistance,wemaximizetheen- tropyofthatprobabilityonlabeleddataandmin- imizeitonunlabeleddatafollowing entropyreg- ularization ,whichallowsthesupervisedandun- supervisedpartstobeintegratedinanaturaland meaningfulway.Furthermore,S ERAPH isregu- larizedbyencouragingalow-rankprojectionin- ducedfromthemetric.TheoptimizationofS ER - APH issolvedefandstablybyanEM- likeschemewiththeanalyticalE-Stepandcon- vexM-Step.ExperimentsdemonstratethatS ER - APH comparesfavorablywithmanywell-known globalandlocalmetriclearningmethods. 1.Introduction Agoodmetricforinputdataisakeyfactorformanyma- chinelearningalgorithms.Classicalmetriclearningmeth- odsfallintothreetypes:(a)Supervisedtyperequiringclass labels(e.g., Sugiyama , 2007 );(b)Supervisedtyperequir- ingweaklabels,i.e.,  1 g -valuedlabelsthatindicatethe similarity/dissimilarityofdatapairs(e.g., Weinbergeretal. , 2005 ; Davisetal. , 2007 );(c)Unsupervisedtyperequiring nolabelinformation(e.g., Belkin&Niyogi , 2001 ).Types (a)and(b)haveastrictlimitationforreal-worldapplica- Appearingin Proceedingsofthe 29 th InternationalConference onMachineLearning ,Edinburgh,Scotland,UK,2012.Copyright 2012bytheauthor(s)/owner(s). tionssincetheyneedlotsoflabels.Basedonthebeliefthat preservingthegeometricstructureinanunsupervisedman- nercanbebetterthanrelyingonthelimitedlabels,semi- supervisedmetriclearninghasemerged.Tothebestofour knowledge,allsemi-supervisedextensionsemploy off-the- shelf techniquesintype(c)suchasprincipalcomponent analysis( Yangetal. , 2006 ; Sugiyamaetal. , 2010 )orman- ifoldembedding( Hoietal. , 2008 ; Baghshah&Shouraki , 2009 ; Liuetal. , 2010 ).Theycanberegardedaspropagat- inglabelsalonganassistantmetricbysomeunsupervised techniquesandlearningatargetmetricimplicitlyinasu- pervisedmanner. However,thetargetandassistantmetricsassumedifferent forms,oneMahalanobisdistanceoveraEuclidean spaceandonegeodesicdistanceoveracurvedspaceora Riemannianmanifold.Thetwometricsalsoshareslightly differentgoals:thetargetmetrictriestolearnametricso thatdatainthesameclassarecloseanddatafromdiffer- entclassesarefarapart(e.g.,Fisherdiscriminantanalysis 1 ( Fisher , 1936 )),andtheassistantonetriestoidentifyand preservetheintrinsicgeometricstructure(e.g.,Laplacian eigenmaps( Belkin&Niyogi , 2001 )).Simplyputtingthem togetherworksinpractice,buttheparadigmisconceptu- allyneithernaturalnor Inthispaper,weproposeasemi-supervisedmetriclearn- ingapproachS ERAPH (SEmi-supervisedmetRicleArning ParadigmwithHyper-sparsity)asan information-theoretic alternative tothemanifold-basedmethods.Ourideaisto optimizeametricbyoptimizingaconditionalprobability parameterizedbythatmetric.Following entropyregular- ization ( Grandvalet&Bengio , 2004 ),wemaximizetheen- tropyofthatprobabilityonlabeleddata,andminimizeit 1 Notethatlearningametricisequivalenttolearningaprojec- tioninthescenarioofdimensionalityreduction. Information-theoreticSemi-supervisedMetricLearningviaEntropyRegularization onunlabeleddata,whichcanachievethesparsityofthe posteriordistribution( Grac¸aetal. , 2009 ),i.e.,thelowun- certainty/entropyofunobservedweaklabels.Furthermore, weemploy mixed-normregularization ( Yingetal. , 2009 ) toencouragethesparsityoftheprojectionmatrix,i.e.,the lowrankoftheprojectionmatrixinducedfromthemetric. Unifyingtheposteriorsparsityandtheprojectionsparsity bringsustothe hyper-sparsity .Thankstothisproperty,the metriclearnedbyS ERAPH possesseshighdiscriminability evenunderanoisyenvironment. Ourcontributionscanbesummarizedasfollows.First,we formulatethesupervisedmetriclearningproblemasanin- stanceofthegeneralizedmaximumentropydistributiones- timation( Dud ´ &Schapire , 2006 ).Second,weproposea semi-supervisedextensionoftheaboveestimationfollow- ingentropyregularization( Grandvalet&Bengio , 2004 ). Noticethatourextensioniscompatiblewiththemanifold- basedextension,whichmeansthatS ERAPH couldadoptan additionalmanifoldregularizationterm. 2.ProposedApproach Inthissection,weformulatethemodelofS ERAPH and thendeveloptheEM-likealgorithmtosolvethemodel. 2.1.Notations Supposewehaveatrainingset X = f x i j x i 2 R m g n i =1 thatcontains n pointseachwith m features.Letthesetsof similaranddissimilardatapairsbe S = f ( x i ;x j ) j x i and x j aresimilar g ; D = f ( x i ;x j ) j x i and x j aredissimilar g : Withsomeabuseofterminology,wereferto S[D asthe labeleddataand U = f ( x i ;x j ) j i 6 = j; ( x i ;x j ) 62S[Dg astheunlabeleddata.Aweaklabel y i;j =1 isassignedto ( x i ;x j ) 2S ,or y i;j =  1 to ( x i ;x j ) 2D .Weabbrevi- ate P ( x i ;x j ) 2S[D , P ( x i ;x j ) 2U and P y 2f 1 ;  1 g as P S[D , P U and P y .ConsiderlearningaMahalanobisdistance metricfor x;x 0 2 R m oftheform d ( x;x 0 )= k x  x 0 k A = q ( x  x 0 ) > A ( x  x 0 ) ; where > isthetransposeoperatorand A 2 R m  m isasym- metricandpositivematrixtobelearned 2 .The probability p A ( y j x;x 0 ) oflabeling ( x;x 0 ) 2 R m  R m with y =  1 isparameterizedbythematrix A .Whenap- plying p A ( y j x;x 0 ) to ( x i ;x j ) ,itisabbreviatedas p A i;j ( y ) . 2 Inthispaper, A isalwaysassumedsymmetricpositivesemi- andwillnotbeexplicitlywrittenforbrevity. 2.2.Basicmodel Tobeginwith,wederiveaprobabilisticmodeltoinvesti- gatetheconditionalprobabilityof y =  1 given ( x;x 0 ) 2 R m  R m .Weresorttoaparametricformof p A ( y j x;x 0 ) , andwillfocusonitfortheout-of-sampleability. The maximumentropyprinciple ( Jaynes , 1957 )suggests thatweshouldchoosetheprobabilitydistributionwiththe maximumentropyoutofalldistributionsthatmatchthe datamoments.Let 3 H ( p A i;j )=  X y p A i;j ( y )ln p A i;j ( y ) betheentropyoftheconditionalprobability p A i;j ( y ) ,and f ( x;x 0 ;y ; A ): R m  R m f +1 ;  1 g7! R beafeaturefunctionthatisconvexwithrespectto A .The constrainedoptimizationproblemis max A;p A i;j ;˘ X S[D H ( p A i;j )  1 2  ˘ 2 s : t :    X S[D E p A i;j [ f ( x i ;x j ;y ; A )]  X S[D f ( x i ;x j ;y i;j ; A )     ˘; (1) where ˘ isaslackvariableand > 0 isaregularization parameter.ThepenaltypresumestheGaussianpriorofthe expecteddatamomentsfromtheempiricaldatamoments, whichisessentiallyconsistentinspiritwiththe generalized maximumentropyprinciple ( Dud ´ &Schapire , 2006 )(see Appendix B.1 ). Theorem1. Theprimalsolution p  A isgivenintermsof thedualsolution ( A  ;  ) by p  A ( y j x;x 0 )= exp(   f ( x;x 0 ;y ; A  )) Z ( x;x 0 ; A  ;  ) ; (2) where Z ( x;x 0 ; A; )= P y 0 exp(  ( x;x 0 ;y 0 ; A )) ,and ( A  ;  ) canbeobtainedbysolvingthedualproblem min  X S[D ln Z ( x i ;x j ; A; )  X S[D  ( x i ;x j ;y i;j ; A )+  2  2 : (3) theregularizedlog-likelihoodfunctiononlabeled data(i.e.,onobservedweaklabels)as L 1 ( A; )= X S[D ln p A i;j ( y i;j )   2  2 : Then,forsupervisedmetriclearning,theregularizedmax- imumlog-likelihoodestimationandthegeneralizedmaxi- mumentropyestimationareequivalent. 4 3 Throughoutthispaper,weadoptthat 0ln0=0 . 4 TheproofsofalltheoremsareinAppendix A . Information-theoreticSemi-supervisedMetricLearningviaEntropyRegularization Whenconsidering f ( x;x 0 ;y ; A ) thatshouldtakemoments aboutthemetricinformationintoaccount,wepropose f ( x;x 0 ;y ; A; )= y 2 ( k x  x 0 k 2 A   ) ; (4) where > 0 isahyperparameterusedasthethresholdto separatethesets S and D underthetargetmetric d ( x;x 0 ) . Nowtheprobabilisticmodel( 2 )becomes p A ( y j x;x 0 )= 1 1+exp(   ( k x  x 0 k 2 A   )) : (5) Fortheoptimalsolution ( p  A ;A  ;  ) ,wehopefor p  A ( y i;j j x i ;x j ) > 1 = 2 ;y i;j ( k x i  x j k 2 A    ) < 0 ; sotheremustbe   < 0 . AlthoughweuseEq.( 4 )asourfeaturefunction,otherop- tionsareavailable.PleaseseeAppendix C.1 fordetails. 2.3.Regularization Inthissubsection,weextend L 1 ( A; ) byentropyregular- izationtosemi-supervisedlearning.Moreover,weregular- izeourobjectivebytrace-normregularization. Ourunsupervisedpartdoesnotrelyuponthemanifoldas- sumptionandisnotintheparadigmofsmoothingthepro- jectedtrainingdata.Inordertobeintegratedwiththesu- pervisedpartmorenaturallyinphilosophy,wefollowthe minimumentropyprinciple ( Grandvalet&Bengio , 2004 ), andhence p A i;j shouldhavelowentropyoruncertaintyfor ( x i ;x j ) 2U .Roughlyspeaking,theresultantdiscrimina- tivemodelspreferpeakeddistributionsonunlabeleddata, whichcarriesoutaprobabilistic low-densityseparation . Subsequently,accordingto Grandvalet&Bengio ( 2004 ), ouroptimizationbecomes max  L 2 ( A; )= X S[D ln p A i;j ( y i;j )   2  2 +  X U X y p A i;j ( y )ln p A i;j ( y ) ; where   0 isaregularizationparameter. Inaddition,wehopeforthedimensionalityreductionabil- itybyencouragingalow-rankprojectioninducedfrom A . Thisishelpfulindealingwithcorrupteddataordatadis- tributedintrinsicallyinalow-dimensionalsubspace.Itis knownthatthetraceisaconvexrelaxationoftherankfor amatrix,sowereviseouroptimizationprobleminto max  L ( A; )= X S[D ln p A i;j ( y i;j )   2  2 +  X U X y p A i;j ( y )ln p A i;j ( y )   tr( A ) ; (6) where tr( A ) isthetraceof A ,and   0 isaregularization parameter. Optimization( 6 )isthemodelofS ERAPH ,andwesay thatitisequippedwiththehyper-sparsitywhenboth  and  arepositive.S ERAPH possessesstandardkernelandman- ifoldextensions.Formoreinformation,pleaserefertoAp- pendix C.2 and C.3 . 2.4.Algorithm Fromnowonwewillsimplifythemodel( 6 )andderivea practicalalgorithm.First,weeliminate  from( 6 ),thanks tothefactthatweuseasimplefeaturefunction( 4 )in( 1 ). Theorem2. thesimploptimizationproblemas 5 max A ^ L ( A )= X S[D ln^ p A i;j ( y i;j ) +  X U X y ^ p A i;j ( y )ln^ p A i;j ( y )  ^  tr( A ) ; (7) wheretheprobabilisticmodelis ^ p A ( y j x;x 0 )= 1 1+exp( y ( k x  x 0 k 2 A  ^  )) : (8) Let ^ A and ( A  ;  ) betheoptimalsolutionsto ( 7 ) and ( 6 ) , respectively.Then,thereexisthyperparame- ters ^  and ^  ,suchthat ^ A isequivalentto A  withrespect to d ( x;x 0 ) ,andtheresulting ^ p A ( y j x;x 0 ) parameterized by ^ A and ^  isidenticaltotheoriginal p A ( y j x;x 0 ) param- eterizedby A  ,   and  . Remark 1 . Afterthe  isdropped,  and  arebuttheregularizationparameter  remains thesame,whichmeansthatthetradeoffbetweenthesuper- visedandunsupervisedpartshasnotbeenaffected. Optimization( 7 )couldbedirectlysolvedbythegradient projectionmethod( Polyak , 1967 ),eventhoughitisnon- convex.Nevertheless,wewouldliketoposeitasanEM- likeiterativeschemetoaccessthederandomizationbythe initialsolution,thestabilityforthegradientupdate,and theinsensitivitytothestepsize,justtonameafewofthe gainedalgorithmicproperties. TheEM-likealgorithmrunsasfollows.Inthebeginning, weinitializeanonparametricprobability q ( y j x i ;x j ) ,and thentheM-StepandtheE-Stepgetexecutedrepeatedlyun- tilthestoppingconditionsare Atthe t -thE-Step,similarlyto Grac¸aetal. ( 2009 ),wehave foreachpair ( x i ;x j ) 2U that min q KL( q jj p A i;j )+  E q [  ln p A i;j ( y )] ; (9) where KL istheKullback-Leiblerdivergence,and p A i;j is parameterizedbythemetric A ( t ) foundatthelastM-Step. Optimization( 9 )canbesolvedanalytically. 5 Thenewfunctionsandparametersaredenotedby ^  within thistheoremforthesakeofclarity. Information-theoreticSemi-supervisedMetricLearningviaEntropyRegularization Theorem3. Thesolutionto ( 9 ) isgivenby q ( y j x i ;x j )= p A i;j ( y )exp(  ln p A i;j ( y )) P y 0 p A i;j ( y 0 )exp(  ln p A i;j ( y 0 )) : (10) Ontheotherhand,atthe t -thM-Step,wenewmetric A ( t ) throughtheprobability q ( y j x i ;x j ) whichisgener- atedinthelastE-Stepandonlyfor ( x i ;x j ) 2U : max A F ( A )= X S[D ln p A i;j ( y i;j ) +  X U X y q ( y j x i ;x j )ln p A i;j ( y )   tr( A ) : (11) Itcouldbesolvedbythegradientprojectionmethodwith- outworryaboutlocalmaximausingthecalculationof rF givenby rF ( A )=  X S[D y i;j  1  p A i;j ( y i;j )  x i;j   X U X y yq ( y j x i ;x j )  1  p A i;j ( y )  x i;j   m ; where x i;j =( x i  x j )( x i  x j ) > ,sincetheconvexityof thefeaturefunction f ( x;x 0 ;y ; A ) withrespectto A implies theconvexityoftheobjective F ( A ) . Aremarkablepropertyof F ( A ) isthatitsgradientisuni- formlybounded,regardlessofthescaleof A ,i.e.,themag- nitudeof tr( A ) . Theorem4. Theobjective F ( A ) isLipschitzcontinuous, andthebestLipschitzconstant Lip  F ( F ) withrespectto theFrobeniusnorm kk F  Lip  F ( F )  (# S +# D +  # U )(diam( X )) 2 +  (12) where diam( X )=max x i ;x j 2X k x i  x j k 2 isthediameter of X ,and # measuresthecardinalityofaset. Inourcurrentimplementation,theinitialsolutionis q (  1 j x i ;x j )=1 ,whichmeansthatwetreatallunlabeledpairs asdissimilarpairs.Theoverallasymptotictimecomplex- ityis O ( n 2 m + m 3 ) inwhichthestoppingcriteriaofthe M-StepandthewholeEM-likeiterationareignored.Dis- cussionsaboutthecomputationalcomplexityandthefast implementationcanbefoundinAppendix D . 3.Discussions Inthissection,wediscussthesparsityissues,namely,we canobtainthe posteriorsparsity ( Grac¸aetal. , 2009 )byen- tropyregularizationandthe projectionsparsity ( Yingetal. , 2009 )bytrace-normregularization. Bya`sparse'posteriordistribution,wemeanthattheun- certainty(i.e.,theentropyorvariance)islow.SeeFigure 1 asanexample.Recallthatsupervisedmetriclearningaims (a)Dataset (b)Euclidean (c)Non-sparse (d)Non-sparse (e)Sparse (f)Sparse Figure1. Sparsevs.non-sparseposteriordistributions.Sixweak labelsareconstructedaccordingtofourclasslabels.Theleftthree panelsshowtheoriginaldataandtheprojecteddatabymetrics learnedwith/withouttheposteriorsparsity.Therightthreepan- elsexhibitone-nearest-neighborresultsbasedonthe Euclideandistanceandtwolearnedmetrics. atametricunderwhichdatainthesameclassarecloseand datafromdifferentclassesarefarapart.Thisresultsinthe metricwhichignoresthehorizontalfeatureandfocuseson theverticalfeature.However,theverticalfeatureisimpor- tant,andtakingcareoftheposteriorsparsitywouldleadto abettermetricasillustratedin(e)and(f).Therefore,we prefertakingtheposteriorsparsityintoaccountinaddition totheaforementionedgoal,andthentheriskofov weaklylabeleddatacanbereduced. Wecanrewrite L 2 ( A; ) asasoftposteriorregularization (PR)objective( Grac¸aetal. , 2009 ).Lettheauxiliaryfeature functionbe g ( x;x 0 ;y )=  ln p A ( y j x;x 0 ) ,thenmaximiz- ing L 2 ( A; ) isequivalentto max  L 1 ( A; )   X U E p A i;j [ g ( x i ;x j ;y )] : (13) Ontheotherhand,accordingtooptimization(7)of Grac¸a etal. ( 2009 ),thesoftPRobjectiveshouldtakeaformas max  L 1 ( A; )  min q  KL( q jj p A )+  X U ˘ i;j  s : t : E q [ g ( x i ;x j ;y )]  ˘ i;j ; 8 ( x i ;x j ) 2U ; (14) Information-theoreticSemi-supervisedMetricLearningviaEntropyRegularization (a)Dataset (b)Euclidean (c)Non-sparse (d)Non-sparse (e)Sparse (f)Sparse Figure2. Sparsevs.non-sparseprojections.Thesettingsandthe layoutofpanelsaresimilartoFigure 1 . where ˘ i;j areslackvariables.Since q isunconstrained,we canoptimizeitwithrespecttoed A and  .Itiseasyto seethat q shouldbe p A (restrictedon U ),sotheKLtermis zeroandtheexpectationtermistheentropy,whichimplies theequivalenceofoptimizations( 13 )and( 14 ). Besidestheaboveposteriorsparsity,wealsohopeforthe projectionsparsity,whichmayguidethelearnedmetricto bettergeneralizationperformance.SeeFigure 2 asanex- ampleofitseffectiveness,wherethehorizontalfeatureis informativeandtheverticalfeatureisuseless. Theunderlyingtechniqueisthemixed-normregularization ( Argyriouetal. , 2006 ).Denotethe ` (2 ; 1) -normofasym- metricmatrix M as k M k (2 ; 1) = P m k =1 ( P m k 0 =1 M 2 k;k 0 ) 1 = 2 . Similarlyto Yingetal. ( 2009 ),let P 2 R m  m beapro- jection,and W = P > P bethemetricinducedfrom P .Let the i -thcolumnof P and W be P i and W i .If P i isiden- ticallyzero,the i -thcomponentof x hasnocontributionto z = Px .Sincethecolumn-wisesparsityof W and P are equivalent,wecanpenalize k W k (2 ; 1) toreachthecolumn- wisesparsityof P . Nevertheless,thisisfeatureselectionratherthandimen- sionalityreduction.Recallthatthegoalistoselectafew mostrepresentativedirectionsofinputdatawhicharenot restrictedtothecoordinateaxes.Thesolutionistopickan extratransformation V 2O m torotate x beforetheprojec- tionwhere O m isthesetoforthonormalmatricesofsize m ,andadd V totheoptimizationvariables.Consequently, wepenalize k W k (2 ; 1) ,project x to z = PVx ,andsince A =( PV ) > ( PV )= V > WV ,wearriveat max ;V L 2 ( A; )   k W k (2 ; 1) (15) s : t :A = V > WV;W = W > ;W  0 ;V 2O m : Theequivalenceofoptimizations( 6 )and( 15 )isguaranteed byLemma1of Yingetal. ( 2009 ). Moreover,thereisanotherbasedonthe infor- mationmaximizationprinciple ( Gomesetal. , 2010 ).Please seeAppendix B.2 fordetails. 4.RelatedWorks Xingetal. ( 2002 )initiatedmetriclearningbasedonpair- wisesimilarity/dissimilarityconstraintsbyglobaldistance metriclearning(G DM ).Severalexcellentmetriclearning methodshavebeendevelopedinthelastdecade,including neighborhoodcomponentanalysis(N CA ; Goldbergeretal. , 2004 ),largemarginnearestneighbor(L MNN ; Weinbergeretal. , 2005 ),andinformation-theoreticmetric learning(I TML ; Davisetal. , 2007 ). BothI TML andS ERAPH areinformation-theoretic,butthe ideasandmodelsarequitedifferent.I TML agen- erativemodel p A ( x )=exp(  1 2 k x   k 2 A ) =Z ,where  isunknownmeanvalueand Z isanormalizingconstant. ComparedwithG DM ,I TML regularizestheKL-divergence between p A 0 ( x ) and p A ( x ) ,andtransformsthistermtoa Log-Detregularization.Byspecifying A 0 = 1 n I m ,itbe- comesthemaximumentropyestimationof p A ( x ) .Thus, itprefersthemetricclosetotheEuclideandistance.S ER - APH alsofollowsthemaximumentropyprinciple,butthe probabilisticmodel p A ( y j x;x 0 ) isdiscriminative. AprobabilisticG DM wasdesignedintuitivelyasabaseline intheexperimentalpartof Yangetal. ( 2006 ).Itisaspecial caseofoursupervisedpart.Infact,S ERAPH ismuchmore general.PleaserefertoSection2.2fordetails. Subsequently,localdistancemetriclearning(L DM ; Yang etal. , 2006 )isthepioneerofsemi-supervisedmetriclearn- ing,whichassumesthattheeigenvectorsof A aretheprin- cipalcomponentsoftrainingdata. Hoietal. ( 2008 )com- binesmanifoldregularizationtothemin-maxprincipleof G DM basedon Belkin&Niyogi ( 2001 ),and Baghshah &Shouraki ( 2009 )showsthat Roweis&Saul ( 2000 )is alsousefulforsemi-supervisedmetriclearning. Liuetal. ( 2010 )bringstheelement-wisesparsityto Hoietal. ( 2008 ). ThemanifoldextensiondescribedinAppendix C.3 canbe attachedtoallmetriclearningmethods,whereasourunsu- Information-theoreticSemi-supervisedMetricLearningviaEntropyRegularization Table1. ofbenchmarkdatasets. #classes#features ( m ) #training ( n ) #test#classlabels E # S E # D # U iris 34100381015.1029.904905 wine 313100781013.9831.024905 ionosphere 2341002512097.5092.504760 balance 341004651020.3824.624905 breastcancer 2301004691023.5421.464905 diabetes 281006681023.0221.984905 USPS 1  5 ; 20 5641002500105404905 USPS 1  5 ; 40 5642002500203016019710 USPS 1  10 ; 20 10642002500201018019710 USPS 1  10 ; 40 10644002500406072079020 MNIST 1 ; 7 219610010004244944 MNIST 3 ; 5 ; 8 31961501500992711139 pervisedpartappliestoprobabilisticmethodsonly.How- ever,anyprobabilisticmethodwithanexplicitexpression oftheposteriordistributionadoptstwosemi-supervisedex- tensions,whiledeterministicmethodssuchasL MNN can- notfromentropyregularization. Duetolimitedspace,weleaveoutsparsemetriclearning androbustmetriclearning.Instead,werecommend Huang etal. ( 2009 )and Huangetal. ( 2010 )forthelatestreviews ofsparseandrobustmetriclearningrespectively. 5.Experiments 5.1.Setup WecomparedS ERAPH withtheEuclideandistance,four famoussupervisedandtworepresentativesemi-supervised metriclearningmethods 6 :globaldistancemetriclearning (G DM ; Xingetal. , 2002 ),neighborhoodcomponentanal- ysis(N CA ; Goldbergeretal. , 2004 ),largemarginnearest neighbor(L MNN ; Weinbergeretal. , 2005 ), information-theoreticmetriclearning(I TML ; Davisetal. , 2007 ),localdistancemetriclearning(L DM ; Yangetal. , 2006 ),andmanifoldFisherdiscriminantanalysis(M FDA ; Baghshah&Shouraki , 2009 ). Table 1 describestheofthedatasetsusedin ourexperiments.Thetopsixdatasets(i.e.,iris,wine,iono- sphere,balance,breastcancer,anddiabetes)comefrom the UCImachinelearningrepository 7 ,whilethe USPS and MNIST comefromthehomepageofthelateSamRoweis 8 . Gray-scaleimagesofhandwrittendigitsaredownsampled to 8  8 and 14  14 pixelresolutionresultingin64-and 196-dimensionalvectorsforUSPSandMNIST.Thesym- 6 Wedownloadedthecodesofallbaselinemethodsfromthe ofwebsitesprovidedbytheoriginalauthorsexceptM FDA . 7 http://archive.ics.uci.edu/ml/. 8 http://cs.nyu.edu/Ÿroweis/data.html. bolUSPS 1  5 ; 20 means20trainingdatafromeachofthe 5classes,USPS 1  10 ; 40 means40trainingdatafrom eachofall10classes,MNIST 1 ; 7 meansdigits1versus7, andsoforth.Notethatinthelasttwotasks,thedimension- alityofdataisgreaterthanthesizeofalltrainingdata. Inourexperiments,allmethodswererepeatedlyrunon50 randomsamplings.Foreachrandomsampling,classlabels ofthefewdatawererevealed,andthesets S and D wereconstructedaccordingtotheserevealedclasslabels. Thesizesof S , D and U wereedforallsamplingsof USPSandMNISTbutrandomforthesamplingsofUCI datasets.Wemeasuredtheperformanceoftheone-nearest- neighborbasedonthelearnedmetricsaswellas thecomputationtimeforlearningthemetrics. FoursettingsofS ERAPH wereincludedinourexperiments (exceptontwoartidatasets):S ERAPH none standsfor  =  =0 ,S ERAPH post for  = #( S[D ) # U and  =0 , S ERAPH proj for  =0 and  =1 ,andS ERAPH hyper for  = #( S[D ) # U and  =1 .Weed  =1 forsimplicity. Therewasnocross-validationforeachrandomsampling, otherwisethelearnedmetricswouldbehighlydependent uponthe,andalsobecauseofthelargevari- anceoftheperformancegiventhelimitedsu- pervisedinformation.Thehyperparametersofothermeth- ods,e.g.,thenumberofreduceddimensions,thenumberof nearestneighbors,andthepercentageofprincipalcompo- nents,wereselectedasthebestvaluebasedonanother10 randomsamplingsifdefaultvaluesorheuristicswerenot providedbytheoriginalauthors. 5.2.Results Figures 1 and 2 hadpreviouslydisplayedthevisuallycom- prehensiveresultsofthesparsityregularizationontwoar- datasetsrespectively.(c)and(d)inboth Information-theoreticSemi-supervisedMetricLearningviaEntropyRegularization Table2. Meanswithstandarderrorsofthenearest-neighborrate(in%)onUCI,USPSandMNISTbenchmarks.For eachdataset,thebestmethodandcomparableonesbasedontheunpaired t -testatthelevel5%arehighlightedinboldface. iriswineionospherebalancebreastcancerdiabetes E UCLIDEAN 9 : 58  0 : 7312 : 93  0 : 8323 : 60  0 : 8927 : 15  0 : 7514 : 11  1 : 0732 : 94  0 : 65 G DM 8 : 95  0 : 7111 : 52  0 : 77 20 : 82  0 : 82 22 : 89  1 : 0811 : 86  0 : 83 30 : 73  0 : 59 N CA 10 : 32  0 : 8315 : 03  1 : 1226 : 68  0 : 8232 : 97  1 : 3114 : 63  1 : 0932 : 95  0 : 65 L MNN 9 : 81  0 : 7914 : 83  0 : 9722 : 25  0 : 7524 : 00  1 : 3413 : 86  0 : 8432 : 02  0 : 60 I TML 5 : 57  0 : 538 : 22  0 : 6620 : 35  0 : 64 22 : 04  0 : 80 9 : 60  0 : 4931 : 21  0 : 73 L DM 7 : 27  0 : 7217 : 21  1 : 4124 : 54  0 : 92 21 : 22  0 : 93 14 : 85  0 : 9234 : 33  0 : 60 M FDA 6 : 58  0 : 5411 : 55  1 : 0323 : 66  0 : 9123 : 61  1 : 0011 : 21  0 : 8031 : 64  0 : 62 S ERAPH none 6 : 21  0 : 48 8 : 13  0 : 5819 : 70  0 : 4320 : 25  0 : 64 11 : 39  0 : 49 29 : 86  0 : 61 S ERAPH post 4 : 79  0 : 377 : 46  0 : 5119 : 64  0 : 4519 : 98  0 : 67 11 : 33  0 : 50 29 : 87  0 : 57 S ERAPH proj 5 : 79  0 : 547 : 39  0 : 5019 : 53  0 : 4620 : 94  0 : 649 : 61  0 : 4930 : 43  0 : 65 S ERAPH hyper 5 : 31  0 : 437 : 38  0 : 4919 : 33  0 : 4220 : 15  0 : 6310 : 04  0 : 5230 : 02  0 : 63 USPS 1  5 ; 20 USPS 1  5 ; 40 USPS 1  10 ; 20 USPS 1  10 ; 40 MNIST 1 ; 7 MNIST 3 ; 5 ; 8 E UCLIDEAN 36 : 63  0 : 8028 : 43  0 : 6049 : 17  0 : 5039 : 30  0 : 3910 : 42  0 : 67 37 : 30  0 : 81 G DM 37 : 62  0 : 77 ----- N CA 37 : 55  0 : 8428 : 39  0 : 6057 : 01  0 : 8249 : 21  0 : 6610 : 42  0 : 67 37 : 75  0 : 92 L MNN 36 : 43  0 : 7828 : 93  0 : 6148 : 12  0 : 5743 : 68  0 : 589 : 99  0 : 71 36 : 49  0 : 82 I TML 35 : 86  0 : 7427 : 40  0 : 6547 : 40  0 : 6039 : 44  0 : 579 : 94  0 : 6940 : 83  0 : 93 L DM 47 : 19  1 : 5132 : 52  0 : 8559 : 13  0 : 7343 : 18  0 : 5314 : 54  1 : 4145 : 53  1 : 16 M FDA 42 : 52  0 : 8228 : 82  0 : 6252 : 13  0 : 5937 : 78  0 : 509 : 35  0 : 7242 : 39  0 : 92 S ERAPH none 36 : 08  0 : 7527 : 41  0 : 6047 : 29  0 : 5838 : 36  0 : 559 : 97  0 : 71 36 : 44  0 : 84 S ERAPH post 35 : 79  0 : 7527 : 37  0 : 6047 : 12  0 : 5838 : 20  0 : 5510 : 98  0 : 79 36 : 45  0 : 84 S ERAPH proj 36 : 01  0 : 75 26 : 17  0 : 57 47 : 42  0 : 6235 : 42  0 : 549 : 28  0 : 72 36 : 55  0 : 80 S ERAPH hyper 32 : 79  0 : 7725 : 26  0 : 5644 : 89  0 : 5833 : 41  0 : 477 : 61  0 : 5735 : 71  0 : 84 wereobtainedbyG DM ,while(e)and(f)weregen- eratedbyS ERAPH with  =10  #( S[D ) # U ; =0 inFigure 1 and  =0 ; =300 inFigure 2 .WecanseefromFigures 1 and 2 thatS ERAPH improvedsupervisedglobalmetric learningdramaticallybythesparsityregularization. Theexperimentalresultsoftheone-nearest-neighborclas- arereportedinTable 2 (G DM wassometimesvery slowandexcludedfromthecomparison).S ERAPH isfairly promising,especiallywiththehyper-sparsity(  = #( S[D ) # U and  =1 ).Itwasbestortieoveralltasks,andoftenstatis- ticallybetterthanothersonUCIdatasetsex- ceptI TML .Itwasbetterthanallothermethodsstatistically onUSPS,andS ERAPH hyper outperformed bothS ERAPH post andS ERAPH proj .Moreover,itimproved theaccuracyevenontheill-posedMNISTtasks,thoughthe improvementwasonMNIST 3 ; 5 ; 8 .Inaword, S ERAPH canreducetheriskofovweaklylabeled datawiththehelpofunlabeleddata,andhenceoursparsity regularizationwouldbereasonableandpractical. InvividcontrastwithS ERAPH thatexhibitednicegeneral- izationcapability,supervisedmethodsmightlearnametric evenworsethantheEuclideandistanceduetoov problems,especiallyN CA thatoptimizedtheleave-one-out performancebasedonsuchlimitedlabelinformation.The powerfulL MNN didnotbehavesatisfyingly,sinceitwas hardlyfultoalotofneighborsbelongingtothe sameclasswithinlabeleddata.I TML wasthesecondbest methodthoughitcanonlyaccessweaklylabeleddata,but itbecamelessusefulfordiftasks.Ontheotherhand, weobservedthatL DM mightfailwhentheprincipalcom- ponentsoftrainingdatawerenotclosetotheeigenvectors ofthetargetmatrix,andM FDA mightfailiftheamountof trainingdatacannotrecovertheunderlyingmanifoldwell. Anobservationisthattheglobalmetriclearningoftenout- performedthelocalone,ifthesupervisedinformationwas insufThisphenomenonindicatesthatthelocalmet- riclearningtendstothelocalneighborhoodinformation exceedinglyandthensuffersfromovproblems. Finally,wereportinFigure 3 thecomputationtimeofeach algorithmoneachtask(excludingG DM ).Generallyspeak- ing,S ERAPH wasthesecondfastestmethod,andthefastest M FDA involvesonlysomematrixmultiplicationandasin- gleeigen-decomposition.Improvementsmaybeexpected ifweprograminMatlabwithC/C++. Information-theoreticSemi-supervisedMetricLearningviaEntropyRegularization TheTaskID 1:iris 2:wine 3:ionosphere 4:balance 5:breastcancer 6:diabetes 7:USPS 1  5 ; 20 8:USPS 1  5 ; 40 9:USPS 1  10 ; 20 10:USPS 1  10 ; 40 11:MNIST 1 ; 7 12:MNIST 3 ; 5 ; 8 Figure3. Computationtime(perrun)ofdifferentmetriclearningalgorithms. 6.Conclusions Inthispaper,weproposedaninformation-theoreticsemi- supervisedmetriclearningapproachS ERAPH asanalterna- tivetothemanifold-basedmethods.Thegeneralizedmaxi- mumentropyestimationforsupervisedmetriclearningwas ourfoundation.Thenasemi-supervisedextensionthatcan achievetheposteriorsparsitywasobtainedviaentropyreg- ularization.Moreover,weenforcedatrace-normregular- izationthatcanreachtheprojectionsparsity.Theresulting optimizationwassolvedbyanEM-likeschemewithsev- eralnicealgorithmicproperties,andthelearnedmetrichad highdiscriminabilityevenunderanoisyenvironment. ExperimentsonbenchmarkdatasetsshowedthatS ERAPH oftenoutperformedstate-of-the-artfully-/semi-supervised metriclearningmethodsgivenonlylimitedsupervisedin- formation.Anoteisthatinourexperimentsthepos- teriorandprojectionsparsityweredemonstratedtobevery helpfulforhigh-dimensionaldata ifandonlyif theywere combinedwitheachother,i.e.,integratedintothehyper- sparsity.Anin-depthstudyofthisinteractionisleftasour futurework. Acknowledgments Theauthorswouldliketothankanonymousreviewersfor helpfulcomments.GNissupportedbytheMEXTschol- arshipNo.103250,MYissupportedbytheJSTPRESTO program,andMSissupportedbytheFIRSTprogram. References Argyriou,A.,Evgeniou,T.,andPontil,M.Multi-taskfeature learning.In NIPS ,2006. Baghshah,M.andShouraki,S.Semi-supervisedmetriclearning usingpairwiseconstraints.In IJCAI ,2009. Belkin,M.andNiyogi,P.Laplacianeigenmapsandspectraltech- niquesforembeddingandclustering.In NIPS ,2001. Bellare,K.,Druck,G.,andMcCallum,A.Alternatingprojections forlearningwithexpectationconstraints.In UAI ,2009. Davis,J.,Kulis,B.,Jain,P.,Sra,S.,andDhillon,I.Information- theoreticmetriclearning.In ICML ,2007. Dud ´ M.andSchapire,R.E.Maximumentropydistribution estimationwithgeneralizedregularization.In COLT ,2006. Fisher,R.A.Theuseofmultiplemeasurementsintaxonomic problems. AnnalsofEugenics ,7(2):179Œ188,1936. Goldberger,J.,Roweis,S.,Hinton,G.,andSalakhutdinov,R. Neighbourhoodcomponentsanalysis.In NIPS ,2004. Gomes,R.,Krause,A.,andPerona,P.Discriminativeclustering byregularizedinformationmaximization.In NIPS ,2010. Grac¸a,J.,Ganchev,K.,Taskar,B.,andPereira,F.Posteriorvs. parametersparsityinlatentvariablemodels.In NIPS ,2009. Grandvalet,Y.andBengio,Y.Semi-supervisedlearningbyen- tropyminimization.In NIPS ,2004. Hoi,S.,Liu,W.,andChang,S.-F.Semi-superviseddistancemet- riclearningforcollaborativeimageretrieval.In CVPR ,2008. Huang,K.,Ying,Y.,andCampbell,C.GSML:Aframe- workforsparsemetriclearning.In ICDM ,2009. Huang,K.,Jin,R.,Xu,Z.,andLiu,C.Robustmetriclearningby smoothoptimization.In UAI ,2010. Jaynes,E.T.Informationtheoryandstatisticalmechanics. Phys- icalReview ,106(4):620Œ630,1957. Liu,W.,Ma,S.,Tao,D.,Liu,J.,andLiu,P.Semi-supervised sparsemetriclearningusingalternatinglinearizationoptimiza- tion.In KDD ,2010. Polyak,B.T.Ageneralmethodforsolvingextremalproblems(in Russian). SovietMathematicsDoklady ,174(1):33Œ36,1967. Roweis,S.andSaul,L.Nonlineardimensionalityreductionby locallylinearembedding. Science ,290:2323Œ2326,2000. Sugiyama,M.Dimensionalityreductionofmultimodallabeled databylocalFisherdiscriminantanalysis. JournalofMachine LearningResearch ,8:1027Œ1061,2007. Sugiyama,M.,Id ´ e,T.,Nakajima,S.,andSese,J.Semi-supervised localFisherdiscriminantanalysisfordimensionalityreduction. MachineLearning ,78(1-2):35Œ61,2010. Weinberger,K.,Blitzer,J.,andSaul,L.Distancemetriclearning forlargemarginnearestneighborIn NIPS ,2005. Xing,E.,Ng,A.,Jordan,M.I.,andRussell,S.Distancemetric learningwithapplicationtoclusteringwithside-information. In NIPS ,2002. Yang,L.,Jin,R.,Sukthankar,R.,andLiu,Y.Anefalgo- rithmforlocaldistancemetriclearning.In AAAI ,2006. Ying,Y.,Huang,K.,andCampbell,C.Sparsemetriclearningvia smoothoptimization.In NIPS ,2009.  
ActiveTransferLearningforCross-SystemRecommendation LiliZhao €,SinnoJialinPan à,EvanWeiXiang !,ErhengZhong €,ZhongqiLu €,QiangYang ¤€€HongKongUniversityofScienceandTechnology,HongKong àInstituteforInfocommResearch,Singapore !BaiduInc.,China ¤HuaweiNoahÕsArkLab,ScienceandTechnologyPark,Shatin,HongKong €¤{lzhaoae,ezhong,zluab,qyang }@cse.ust.hk,àjspan@i2r.a-star.edu.sg, !evan.xiang@gmail.com AbstractRecommendersystems,especiallythenewlylaunched ones,havetodealwiththedata-sparsityissue,where  littleexistingratinginformationisavailable.Recently,  transferlearninghasbeenproposedtoaddressthisprob-  lembyleveragingtheknowledgefromrelatedrecom-  mendersystemswhererichcollaborativedataareavail-  able.However,mostprevioustransferlearningmod-  elsassumethatentity-correspondencesacrossdifferent  systemsaregivenasinput,whichmeansthatforany  entity(e.g.,auseroranitem)inatargetsystem,its  correspondingentityinasourcesystemisknown.This  assumptioncanhardlybesatisÞedinreal-worldsce-  narioswhereentity-correspondencesacrosssystemsare  usuallyunknown,andthecostofidentifyingthemcan  beexpensive.Forexample,itisextremelydifÞcultto  identifywhetherauser AfromFacebookandauser BfromTwitterarethesameperson.Inthispaper,we proposeaframeworktoconstructentitycorrespondence  withlimitedbudgetbyusingactivelearningtofacili-  tateknowledgetransferacrossrecommendersystems.  SpeciÞcally,forthepurposeofmaximizingknowledge  transfer,weÞrstiterativelyselectentitiesinthetarget  systembasedonourproposedcriteriontoquerytheir  correspondencesinthesourcesystem.Wethenplug  theactivelyconstructedentity-correspondencemapping  intoageneraltransferredcollaborative-Þlteringmodel  toimproverecommendationquality.Weperformexten-  siveexperimentsonrealworlddatasetstoverifytheef-  fectivenessofourproposedframeworkforthiscross-  systemrecommendationproblem. Introduction CollaborativeÞltering(CF)technologies,especiallymatrix factorizationmethods,haveachievedsigniÞcantsuccessin  theÞeldofrecommendersystems.CFaimstogeneraterec-  ommendationsforauserbyutilizingtheobservedprefer-  encesofotheruserswhosehistoricalbehaviorsarecorre-  latedwiththatofthetargetuser.However,CFperforms  poorlywhenlittlecollaborativeinformationisavailable.  Thisisreferredtoasthedatasparsityproblem,whichisa  commonchallengingprobleminmanynewlylaunchedrec-  ommendersystems. Copyright c!2013,AssociationfortheAdvancementofArtiÞcial Intelligence(www.aaai.org).Allrightsreserved. Recently,transferlearning(PanandYang2010)hasbeen proposedtoaddressthedata-sparsityprobleminthetarget  recommendersystembyusingthedatafromsomerelated  recommendersystems.Acommonmotivationbehindtrans-  ferlearningisthatmanycommercialWebsitesoftenattract  similarusers(e.g.,Twitter,Facebook,etc.),orprovidesim-  ilarproductitems(e.g.,Amazon,eBay,etc.),thus,asource  CFmodelbuiltwithrichcollaborativedatacanbecom-  pressedasapriortoassistthetrainingofamorepreciseCF  modelforthetargetrecommendersystems(Li,Yang,and  Xue2009a).Thisapproachisalsoknownascross-system  collaborativeÞltering. Previoustransfer-learningapproachestocross-systemCF canbeclassiÞedintotwocategories:(1)CFmethods withcross-systementitycorrespondence,and(2)those withoutcross-systementity-correspondence.Intheformercategory,  MehtaandHofmann(2006)andPan etal. (2010)proposed toembedthecross-systementity-correspondencesascon-  straintstojointlylearntheCFmodelsforthesourceand  targetrecommendersystemswithanaimtoimprovethe  performanceofthetargetCFsystem.Althoughtheseap-  proacheshaveshownpromisingresults,theyrequiretheex-  istenceofentitycorrespondencemappings,suchasusercor-  respondenceoritemcorrespondence,acrossdifferentsys-  tems.ThisstrongprerequisiteisoftendifÞculttosatisfyin  mostreal-worldscenarios,assomespeciÞcusersoritemsin  onesystemmaybemissinginothersystems.Forexample,  userpopulationsofTwitterandFacebookservicesaresome-  timesoverlapping,buttheyarenotidentical,asisthecase  withAmazonandeBay.Inaddition,eventhoughtheremay  existpotentialentitycorrespondencesacrossdifferentsys-  tems,theymaybeexpensiveortime-consumingtoberec-  ognizedasusersmayusedifferentnames,oranitemmaybe  nameddifferentlyindifferentonlinecommercialsystems. Inthesecondcategorywherenoassumptionismadeon pre-existingcross-systemmappings,researchershavefo-  cusedoncapturingthegroup-levelbehaviorsofusers.For  example,Li etal. (2009a)proposedacodebook-based- transfer(CBT)methodforcross-domainCF,whereentity-  correspondencesacrosssystemsarenotrequired.Themain  assumptionofCBTisthatspeciÞcusersoritemsmaybe  differentacrosssystems,butthegroupsofthemshouldbe-  havesimilarly.Therefore,CBTaimstoÞrstgenerateaset  ofcluster-leveluser-itemratingpatternsfromthesourcedo- main,whichisreferredtoasacodebook.Thecodebookcan beusedasapriorforlearningtheCFmodelinthetarget system.Li etal. (2009b)furtherproposedaprobabilistic modelforcross-domainCFwhichsharesasimilarmoti- vationwithCBT.However,comparedtotheapproachesin theformercategory,whichmakeuseofcross-systementity-  correspondencesasabridge,theseapproachesarelesseffec- tiveforknowledgetransferacrossrecommendersystems. Inthispaper,weassumethatthecross-systementity- correspondencesareunknowningeneral,butthatthese  mappingscanbeidentiÞedwithacost.Inparticular,we proposeauniÞedframeworktoactivelyconstructentity- correspondencemappingsacrossrecommendersystems, andthenintegratethemintoatransferlearningapproach  withpartialentity-correspondencemappingsforthecross- systemCF.Theproposedframeworkconsistsoftwomajor components:¥anactivelearningalgorithmtoconstructentity- correspondencesacrosssystemswithaÞxedbudget,  and¥anextendedtransfer-learningbasedCFapproachwith  partialentity-correspondencemappingsforcross-domain CF. Notations&Preliminaries Denoteby DatargetCFtask,whichisassociatedwith anextremelysparsepreferencematrix X(d)!md"nd,wheremdisthenumberofusersand ndisthenumber ofitems.Eachentry x(d)uvofX(d)correspondstouser uÕspreferenceonitem v.If x(d)uv"=0,itmeansforuser u,thepreferenceonitem visobserved,otherwiseunobserved. LetIdbethesetofallobserved (u,v )pairsof X(d).The goalistopredictusersÕunobservedpreferencesbasedon  afewobservedpreferences.Forratingrecommendersys- tems,preferencesarerepresentedbynumericalvalues(e.g., [1,2,..., 5],onestarthroughÞvestars).Incross-systemCF, besidesD,supposewehaveasourceCFtask Swhichis associatedwitharelativelydensepreferencematrix X(s)!ms"ns,where msisthenumberofusersand nsisthe numberofitems.Similarly,let Isbethesetofallob- served (u,v )pairsof X(s).Furthermore,weassumethatthe cross-systementity-correspondencesareunknown,butcan  beidentiÞedwithcost.Ourgoalisto1)activelyconstruct entity-correspondencesacrossthesourceandtargetsystems withbudget,and2)makeuseofthemforknowledgetransfer fromthesourcetask Stothetargettask D.Inthesequel,we denoteby X!,itheithcolumnofthematrix X,andsuper- script#thetransposeofvectorormatrix,andusethewords ÒdomainÓandÒsystemÓinterchangeably. Maximum-MarginMatrixFactorization OuractivetransferlearningframeworkforCFisbasedon  Maximum-MarginMatrixFactorization(MMMF)(Srebro, Rennie,andJaakkola2005),whichaimstolearnafullyob- servedmatrix Y!m"ntoapproximateatargetprefer- encematrix X!m"nbymaximizingthepredictivemar- ginandminimizingthetracenormof Y.SpeciÞcally,the objectiveofMMMFforbinarypreferencepredictionsisto minimizeJ=!(u,v)$Ih(yuváxuv)+!#Y#!,(1)whereIisthesetofobserved (u,v )pairsof X,h(z)=(1$z)+=max(0 ,1$z)istheHingeloss, #á#!de-notesthetracenorm,and !%0isatrade-offparameter. Inbinarypreferencepredictions, yuv=+1 denotesthat userulikestheitem v,while yuv=$1denotesdislike. Theobjective(1)canbeextendedtoordinalratingpredic-  tions,andsolvedefÞciently(RennieandSrebro2005).Sup- posexuv!{1,2,...,R },onecanuse R$1thresholds"1,"2,..., "R%1torelatethereal-valued yuvtothediscrete- valued xuvbyrequiring "xuv%1+1&yuv&"xuv$1,where"0=$'and"R='.Furthermore,suppose Ycanbedecomposedas Y=U#V,where U!k"mandV!k"n.TheobjectivefunctionofMMMFforordinal ratingpredictionscanbewrittenasfollows, min U,V,"J=!(u,v)$IR%1!r=1h"Truv""ur$U#!uV!v##+!(#U#F+#V#F),(2)whereTruv=+1 forr%xuv,while Truv=$1forr<x uv,and#á#FdenotestheFrobeniusnorm.Thethresholds !={"ur}Õscanbelearnedtogetherwith UdandVdfromthe data.Notethatthethresholds {"ur}Õshereareuser-speciÞc. Alterativegradientdescentmethodscanbeappliedtosolve theoptimizationproblem(2). ActiveTransferLearningforCross-SystemCF OverallFramework Inthissection,weintroducetheoverallframeworkonac- tivetransferlearningforcross-systemCFasdescribedin Algorithm1.Tobeginwith,weapplyMMMFonthetarget collaborativedatatolearnaCFmodel.Afterthat,weitera- tivelyselect Kentitiesbasedonourproposedentityselec- tionstrategiestoquerytheircorrespondencesinthesource system.WethenapplytheextendedMMMFmethodinthe transferlearningmanneronthesourceandtargetcollabora- tivedatatolearnanupdatedCFmodel. Inthefollowingsections,wedescribetheentityselection strategiesandtheextendedMMMFfortransferlearningin  detail,respectively. MMMFwithPartialEntityCorrespondence Denoteby U(s)CandV(s)Cthefactorsub-matricesof U(s)andV(s)fortheentitieswhoseindicesarein C,respectively. Similarly,denoteby U(d)CandV(d)Cthefactorsub-matrices fortheentitieswhoseindicesarein Crespectively.Here Cdenotestheindicesofthecorrespondingentities(canbeei- therusersoritems)betweenthesourceandtargetsystems. Algorithm1 ActiveTransferLearningforCross-SystemCF Input:U(s),V(s),X(d),T,and K.Output:U(d),and V(d).Initialize:Apply(2)on X(d)togenerate !(d)0,U(d)0andV(d)0.for t=0to T$1doStep1: SetC(s)=ActiveLearn (!(d)t,U(d)t,V(d)t,K),whereC(s)isthesetoftheindicesoftheselectedenti- ties(eitherusersoritems),and |C(s)|=K.Step2: QueryC(s)inthesourcesystemtoidentifytheir correspondingindices C(d).Step3: ApplyMMMF TL(U(s),V(s),X(d),C(s),C(d))toupdate !(d)t+1,U(d)t+1,and V(d)t+1.endfor Return: U(d)(U(d)T,V(d)(V(d)T.Theproposedapproachwithpartialentity-correspondences tocross-systemCFcanbewrittenas, min U,V,"J=!(u,v)$IR%1!r=1h"Truv""ur$UT!uV!v##+!#U#F+!#V#F+!CR$U(d)C,V(d)C,U(s)C,V(s)C%,(3)wherethelasttermisaregularizationtermthataimstouse U(s)CandV(s)Caspriorstolearnmoreprecise U(d)CandV(d)C,whichcanbeexpandedtoobtainmoreprecise U(d)andV(d)respectively.Theassociated !C%0isatrade-off parametertocontroltheimpactoftheregularizationterm. Intuitively,asimplewaytodeÞnetheregularizationterm istoenforcethetargetfactorsub-matrices U(d)CandV(d)Ctobethesameasthesourcefactorsub-matrices U(s) CandV(s)Crespectivelyasfollows, R$U(d)C,V(d)C,U(s) C,V(s)C%=&&&W(s)C$W(d)C&&&F,(4)whereW(d)C=[U(d)CV(d)C]andW(s)C=[U(s)CV(s)C].This ÒidenticalÓassumptionissimilartothatofCollectiveMatrix Factorization(CMF)(SinghandGordon2008),andmaynot holdinpractice.Inthesequel,asabaseline,wedenoteby MMMFCMF theextendedMMMFmethodbyplugging(4) into(3). Alternatively,weproposetousethesimilaritiesbetween entitiesestimatedinthesourcesystemaspriorstoconstrain  thesimilaritiesbetweenentitiesinthetargetsystem.The motivationisthatiftwoentitiesinthesourcesystemare  similartoeachother,thentheircorrespondencestendtobe similartoeachotherinthetargetsystemaswell.Therefore, weproposethefollowingformoftheregularizationterm, R$U(d)C,V(d)C,U(s)C,V(s)C%=tr$W(d)CL(s) CW(d)TC%,(5)wheretr (á)denotesthetraceofamatrix, L(s) C='L(s) U00L(s) V(,and L(s)U=D(s) U$A(s) Uisknownasthe Laplacianmatrix,where A(s)U=U(s)# CU(s) Cisthesimilarity matrixoftheusersinthesourcesystem,whoseindicesarein theset C,and D(s)Uisadiagonalmatrixwithdiagonalentries D(s) Uii=)jA(s) Uij.ThedeÞnitionof L(s) Vonitemsissimilar. Notethatasimilarregularizationtermhasbeenproposed by(LiandYeung2009).However,theirworkisfocusedon utilizingrelationalinformationforsingle-domainCF,and  theLaplacianMatrixisconstructedbylinksbetweenenti- tiesinsteadofentity-similaritiesinasourcedomain.Inthe sequel,wedenotebyMMMF TLtheproposedMMMFex- tensionbyplugging(5)into(3). ActivelyConstructingEntityCorrespondences Inthissection,wedescribeamargin-basedmethodforac-  tivelyconstructingentitycorrespondences.Acommonmo- tivationbehindmargin-basedactivelearningapproachesis thatgivenamargin-basedmodel,themarginofanexam- pledenotescertaintytothepredictionontheexample.The smallerthemarginisforanexample,thelowerthecertainty  isforitsprediction. MarginsonUser-ItemPairs SupposethatMMMF(2)or MMMFTL(3)isperformedonthecollaborativedatainthe targetsystem,thengivenauser u,foreachthreshold "k,wherek!{1,...,R $1},themarginofauser-itempair (u,v )canbedeÞnedas *+,#(d)k(u,v )=U(d)T!,uV(d)!,v$"k,ify(d)u,v>k, #(d)k(u,v )="k$U(d)T!,uV(d)!,v,ify(d)u,v&k,(6)wherey(d)u,v=x(d)u,vifx(d)u,visobserved;otherwise, y(d)u,v=U(d)T!,uV(d)!,v.BasedontheabovedeÞnition,foreachuser- itempair (u,v ),wehave R$1margins.Amongthem,the marginstotheleft(lower)andright(upper)boundariesof thecorrectintervalhavethehighestimportance,whichwe denoteby #(d)L(u,v )and#(d)R(u,v ),respectively.Similarto othermargin-basedactivelearningmethods,weassumethat, fortheunobserveduser-itempairs {(u,v )}Õs,thepredictions oftheCFmodelarecorrect,andthuswecanobtaintheÒcor-  rectÓintervalsoftheunobserveduser-itempairsaswell.In- tuitively,forapair (u,v ),when #(d)L(u,v )=#(d)R(u,v ),the conÞdenceofthepredictionisthehighest.WedeÞneanor- malizedmarginofauser-itempair (u,v )asfollows, -#(d)(u,v )=1 $...#(d)L(u,v )$#(d)R(u,v )...#(d)L(u,v )+#(d)R(u,v ).(7)Notethat -#(d)(u,v )![0,1].MarginsonEntities WiththemargindeÞnitionofauser- itempairin(7),wearenowreadytodeÞnethemarginofan entity(eitherauseroranitem).Forsimplicity,intherestof thesection,weonlydescribethedeÞnitiononthemarginof auser,andproposetheuserselectionstrategiesbasedonthe deÞnition.ThemarginofanitemcanbedeÞnedsimilarly,  anditemselectionstrategiescanbedesignedaccordingly aswell.Byobservingthatgivenapreferencematrix X(d)withmdusersand nditems,auser ucanberepresented bythepairsbetweentheuserandeachitem(i.e., ndpairsintotal).Itisreasonabletoassumethatthemarginofthe userucanbedecomposedtothemarginsoftheuser-item pairs{(u,v i)}Õs.Furthermore,foreachuser u,ratingson someitemsareobserved,whoseitemindicesaredenotedby  Idu,whiletheothersareunobserved,whoseitemindicesare denotedby /Idu.Thereforeweproposethemarginofauser asfollows, #(d)u=$1|Idu|!v$Idu-#(d)(u,v )+(1 $$)1|/Idu|!v$!Idu-#(d)(u,v ),(8)wheretheÞrsttermcanbereferredtoastheaverageofthe ÒtrueÓmarginsoftheuser-itempairswithobservedratings, andthesecondtermcanbereferredtoastheaverageofthe ÒpredictiveÓmarginsoftheuser-itempairswithunobserved ratings.Thetradeoffparameter $![0,1]istobalancethe impactofthetwotermstotheoverallmarginoftheuser.In  thispaper,wesimplyset $=0.5.BasedonthemarginofauserasdeÞnedin(8),wepro- posetwouser-selectionstrategiesasfollows. ¥MGmin:ineachiteration,werankusersinascendingor- derintermsoftheircorrespondingmargin {#(d)u}Õs,and selectthetop Kuserstoconstruct Cforquery. Thisstrategycanreturnthemostuncertainusersinthe currentCFmodel.However,duetothe long-tailprobleminCF(ParkandTuzhilin2008),manyitemsorusersin thelongtailhaveonlyfewratings.Thus,themostun- certainusersinthetargetrecommendersystemtendto beinthelongtailwithhighprobabilities.Furthermore, sinceweassumethesourceandtargetrecommendersys- temsbesimilar,iftheusersareinthelongtailinthe  targetsystem,thentheircounterpartstendtobelong-tail usersinthesourcesystemaswell.Thisimpliesthatthe factorsub-matrices U(s)CandV(s)Ctobetransferredfrom thesourcesystemmaynotbeprecise,resultinginlimited knowledgetransferthrough(5).Thus,weproposeanother user-selectionstrategyasfollows. ¥MGhybrid :ineachiteration,weÞrstapplyMG mintose- lectK1users,denotedby C1,where K1<K.Afterthat fortherestusers {ui}Õs,weapplythescoringfunctionde- Þnedin(9)torankthemindescendingorder,andselect K$K1userstoconstruct C2.Finally,weset C=C1)C2."(d)(ui,Cu)=)uj$Cusim(ui,uj)#(d)ui)uj$Cusim(ui,uj),(9)wheresim (ui,uj)=|Idui&Iduj|max(|Idui|,|Iduj|)isthemeasureof correlationbetweentheusers uiandujbasedontheirrat- ingbehaviors.Themotivationbehindthescoringfunction  (9)isthatweaimtoselectuserswhoare1)informative (withlargevaluesof {#(d)ui}Õs)andthussupposedtobe ÒactiveÓinsteadofthelongtail;2)ofstrongcorrelation tothepre-selectedmostuncertainusersin C1(withlarge valuesof )uj$Cusim(ui,uj))andthussupposedtobe helpfultorecommenditemsforthembasedontheintrin-  sicassumptioninCF. ExperimentsDatasetsandExperimentalSetting Weevaluateourproposedframeworkontwodatasets:Net- ßix1andDouban 2.TheNetßixdatasetcontainsmorethan 100millionratingsgivenbymorethan 480,000userson around18,000movieswithratingsin {1,2,3,4,5}.Douban isapopularrecommendationwebsiteinChina.Itcontains threetypesofitemsincludingmovies,booksandmusicwith ratingscale [1,5].ForNetßixdataset,weÞlteroutmovieswithlessthan 5ratingsforourexperiments.Thedatasetispartitionedinto twopartswithdisjointsetsofusers,whilesharingthewhole setofmovies.Onepartconsistsofratingsgivenby 50%userswith 1.2%ratingdensity,whichservesasthesource domain.Theremainingusersareconsideredasthetarget  domainwith 0.7%ratingdensity.ForDouban,wecollecta datasetconsistingof 12,000usersand 100,000itemswith onlymoviesandbooks.Userswithlessthan10ratingsare discarded.Thereremain 270,000ratingson 3,500books,and1,400,000ratingson 8,000movies,givenby 11,000users.Thedensityoftheratingsonbooksandmoviesare 0.6%and1.5%respectively.Weconsidermovieratingsas thesourcedomainandbookratingsasthetargetdomain.In  thistask,allusersaresharedbutitemsaredisjoint.Further-  more,sincethereareabout 6,000moviessharedbyNetßix andDouban,weextractratingsonthesharedmoviesfrom  NetßixandDoubanrespectively,andobtain 490,000ratingsgivenby 120,000usersfromDoubanwithratingdensity 0.7%,and 1,600,000ratingsgivenby 10,000usersfrom Netßixwithdensity 2.6%.WeconsiderratingsonNetßix asthesourcedomainandthoseonDoubanasthetargetdo-  main.Intotal,weconstructthreecross-systemCFtasks,and denoteby Netßix*Netßix,DoubanMovie *DoubanBookandNetßix*DoubanMovie ,respectively. Intheexperiments,wespliteachtargetdomaindatainto atrainingsetof 80%preferenceentriesandatestsetof 20%preferenceentries,andreporttheaverageresultsof 10ran-domtimes.Theparametersofthemodel,i.e.,thenumber oflatentfactors kandthenumberofiterations Taretuned onsomehand-outdataof Netßix*Netßix,andÞxedtoall experiments. 3Here,T=10 ,and k=20 .Forevaluationcri- terion,weuseRootMeanSquareError(RMSE)deÞnedas, RMSE=0112!(u,v)$I(xuv$öxuv)2|I| ,wherexuvandöxuvarethetrueandpredictedratingsrespec- tively,and |I| isthenumberoftestratings.Thesmalleris thevalue,thebetteristheperformance. OverallComparisonResults IntheÞrstexperiment,wequalitativelyshowtheeffective- nessofourproposedactivetransferlearningframeworkfor 1http://www.netßix.com 2http://www.douban.com 3Supposetotalbudgetis !whichisthetotalnumberofcor- respondencestobeconstructed,wesetthenumberofcorrespon- dencesactivelyconstructedineachiterationas K=!/T.Table1:OverallcomparisonresultsonthethreedatasetsintermsofRMSE. Tasks MethodsNoTransf(w/ocorr.) NoTransf(0.1%corr.) CBTMMMFTLMFMMMFMFMMMF(w/ocorr.) (0.1%corr.) (100%corr.) Netßix"Netßix0.89000.8800.91120.91030.88460.86920.8527(±0.0004)(±0.0001)(±0.0002)(±0.0004)(±0.0002)(±0.0003)(±0.0002)Movie "Book(Douban) 0.88040.87840.88760.88370.86560.82920.8126(±0.0017)(±0.0002)(±0.0003)(±0.0001)(±0.0002)(±0.0003)(±0.0002)Netßix"DoubanMovie 0.85200.85780.86430.85890.82460.77400.7576(±0.0003)(±0.0002)(±0.0001)(±0.0002)(±0.0002)(±0.0001)(±0.0001)cross-domainCFascomparedwiththefollowingbaselines: ¥NoTransfwithoutcorrespondences:toapplystate-of-the- artCFmodelsonthetargetdomaincollaborativedatadi- rectlywithouteitheractivelearningortransferlearning. Inthispaper,forstate-of-the-artCFmodels,weuselow-  rankMatrixFactorization(MF)(Koren,Bell,andVolin- sky2009)andMMMF. ¥NoTransfwithactivelyconstructedcorrespondences:to Þrstapplyactivelearningstrategytoconstructcross- domainentity-correspondences,andthenalignthesource  andtargetdomaindatatogenerateauniÞeditem-userma- trix.Finally,weapplystate-of-the-artCFmodelsonthe uniÞedmatrixforrecommendations. ¥CBT:toapplythecodebook-based-transfer(CBT)  methodonthesourceandtargetdomaindataforrecom-  mendations.AsintroducedintheÞrstsection,CBTdoes notrequireanyentity-correspondencestobeconstructed. ¥MMMFTLwithfullcorrespondences:toapplythepro- posedMMMF TLonthesourceandtargetdomain datawithfullentity-correspondencesforrecommenda-  tions.Notethatthismethodwhichassumesallentity- correspondencesbeavailablecanbeconsideredasanup- perboundoftheactivetransferlearningmethod. Theoverallcomparisonresultsonthethreecross-domain tasksareshowninTable1.Fortheactivelearningstrate- gies,weuseMG hybrid asproposedin(9).Ascanbeob- servedfromtheÞrstgroupofcolumnsinthetable,apply- ingstate-of-the-artCFmodelsontheextremelysparsetarget domaindatadirectlyisnotabletoobtainpreciserecommen- dationresultsintermsofRMSE.Theresultsofthesecond groupofcolumnsinthetablesuggestthataligningallthe sourceandtargetdatatoauniÞeditem-usermatrixandthen performingstate-of-the-artCFmodelsonitcannothelpto boosttherecommendationperformance,butmayevenhurt theperformancecomparedtothatofapplyingCFmodels onthetargetdomaindataonly.Thisisbecausethealign-  mentmakesthematrixtobefactorizedlargerbutstillvery sparse,resultinginamoredifÞcultlearningtask.Fromthe tablewecanalsoobservethatthetransferlearningmethod CBTperformsbetterthantheNoTransfmethods.However, ourproposedactivetransferlearningmethodMMMF TLwithonly 0.1%entity-correspondencesachievesmuchbet- terperformancethanCBTintermsofRMSE.Thisveri- Þestheconclusionthatmakinguseofcross-systementity- correspondencesasabridgeisusefulforknowledgetrans- feracrossrecommendersystems.Finally,byconsideringthe performanceofMMMF TLwithfullentity-correspondences astheknowledge-transferupperbound,andtheperformance  ofMMMFasthebaseline,ourproposedactivetransfer learningmethodcanachievearound 70%knowledgetrans- ferratioonaverageonthethreetaskswhileonlyrequires  0.1%entity-correspondencestobelabeled. ExperimentsonDiff.ActiveLearningStrategies Inthesecondexperiment,weaimtoverifytheperformance  ofourproposedactivetransferlearningframeworkplug- gingwithdifferententityselectionstrategies.Here,weuse MMMFTLasthebasetransferlearningapproachtocross- domainCF.Forentityselectionstrategies,besidesthetwo strategies,MG minandMG hybrid ,presentedinthemodelin- troductionsection,wealsoconductcomparisonexperiments  onthefollowingstrategies. ¥Rand:toselectentitiesrandomlyinthetargetdomainto querytheircorrespondencesinthesourcedomain. ¥Many:toselecttheentitieswithmosthistoricalratingsin  thetargetdomaintoquerytheircorrespondencesinthe  sourcedomain. ¥Few:toselecttheentitieswithfewesthistoricalratingsin thetargetdomaintoquerytheircorrespondencesinthe sourcedomain. ¥MGmax:toselecttheentitieswithlargest {#(d)u}Õsasde- Þnedin(8)inthetargetdomaintoquerytheircorrespon- dencesinthesourcedomain. Figure1showstheresultsofMMMF TLwithdifferenten- tityselectionstrategiesundervaryingproportionsofentity-  correspondencestobelabeled.FromtheÞgure,wecan observethatthemargin-basedapproaches(i.e.,MG min,MGmax,andMG hybrid )performmuchbetterthanotherap- proachesbasedondifferentcriteria.Inaddition,compared withMG minandMG max,ourproposedMG hybrid notonly selectsuncertainentitiesbutalsoselectsinformativeentities  whichhavestrongcorrelationstothemostuncertainentities, thusperformsslightlybetter. ExperimentsonDiff.Cross-domainRegularizers Asmentionedinthemodelintroductionsection,theregu- larizationterm R(U(d)C,V(d)C,U(s)C,V(s)C)in(3)forcross- systemknowledgetransfercanbesubstitutedbydiffer- entforms,e.g.,(4)or(5),resultingindifferenttransfer learningapproaches,MMMF CMF orMMMF TLaccord-ingly.Therefore,inthethirdexperiment,weuseMG hybrid 00.111020304050607080901000.850.8550.860.8650.870.8750.880.885Proportion of labeled correspondences (unit: %)RMSE  MGhybridMGminMGmaxRandManyFew(a)Netßix "Netßix00.111020304050607080901000.810.820.830.840.850.860.870.880.89Proportion of labeled correspondences (unit: %)RMSE  MGhybridMGminMGmaxRandManyFew(b)Movie "Book(Douban) 00.111020304050607080901000.750.760.770.780.790.80.810.820.83Proportion of labeled correspondences (unit: %)RMSE  MGhybridMGminMGmaxRandManyFew(c)Netßix "DoubanMovie Figure1:Resultsondifferententityselectionstrategiesundervaryingproportionsofentity-respondencestobelabeled. 0.111020304050607080901000.850.8550.860.8650.870.8750.88Proportion of labeled correspondences (unit: %)RMSE  MMMFcmfMMMFTL(a)Netßix "Netßix0.111020304050607080901000.810.820.830.840.850.860.87Proportion of labeled correspondences (uint: %)RMSE  MMMFcmfMMMFTL(b)Movie "Book(Douban) 0.111020304050607080901000.750.7550.760.7650.770.7750.780.7850.790.7950.8Proportion of labeled correspondences (unit: %)RMSE  MMMFcmfMMMFTL(c)Netßix "DoubanMovie Figure2:Resultsondifferentcross-domainregularizersundervaryingproportionsofentity-respondencestobelabeled. astheentityselectionstrategy,andcomparetheperfor- manceofMMMF CMF andMMMF TLintermsofRMSE. AscanbeseenfromFigure2,theproposedMMMF TLoutperformsMMMF CMF consistentlyonthethreecross- systemtasksundervaryingproportionsofthelabeledentity- correspondences.Thisimpliesthatusingsimilaritiesbe- tweenentitiesfromthesourcedomaindataaspriorsismore safeandusefulforknowledgetransferacrossrecommender systemsthanusingthefactormatricesfactorizedfromthe sourcedomaindatadirectly. RelatedWork BesidestheworksintroducedintheÞrstsection,thereare severalotherrelatedworksonapplyingtransferlearning forCF.Pan etal. (2012)developedanapproachknownas TIF(TransferbyIntegrativeFactorization)tointegratethe  auxiliaryuncertainratingsasconstraintsintothetargetma- trixfactorizationproblem.Cao etal. (2010)andZhang etal.(2010)extendedtheCMFmethodtosolvemulti-domain CFproblemsinamulti-tasklearningmannerrespectively. Ourworkisalsorelatedtopreviousworksonactivelearn- ingtoCF(Shi,Zhao,andTang2012;Mello,Aufaure,and Zimbrao2010;RishandTesauro2008;JinandSi2004;  Boutilier,Zemel,andMarlin2003),whichassumedthatthe usersareabletoprovideratingstoeveryitemofthesys- tem.However,thisassumptionmaynotholdinmanyreal- worldscenariosbecauseusersmaynotbefamiliarwithall itemsofthesystem,thustheymayfailtoprovideratingson  them.Alternatively,weproposetoactivelyconstructentity- correspondencemappingsacrosssystems. AnotherrelatedresearchtopicisdevelopingauniÞed frameworkforactivelearningandtransferlearning.Most previousworksonthistopicarefocusedonstandardclas- siÞcationtasks(Sahaetal.2011;Raietal.2010;Shi,Fan, andRen2008;ChanandNg2007).Inthispaper,ourstudy  onactivetransferlearningisfocusedonaddressingthedata- sparsityprobleminCF,whichisdifferentfromtheprevi- oustasksonclassiÞcationorregression.Theexistingframe- worksoncombiningactivelearningandtransferlearning cannotbedirectlyappliedtoourproblem. ConclusionsandFutureWork Inthispaper,wepresentanovelframeworkonactivetrans- ferlearningforcross-systemrecommendations.Inthepro- posedframework,we1)extendprevioustransferlearning approachestoCFinapartialentity-correspondingman- ner,and2)proposeseveralentityselectionstrategiesto activelyconstructentity-correspondencesacrossdifferent recommendersystems.Ourexperimentalresultsshowthat comparedwiththetransferlearningmethodwhichrequires fullentity-correspondences,ourproposedframeworkcan achieve 70%knowledge-transferratio,whileonlyrequires 0.1%oftheentitiestohavecorrespondence.Forfuture work,weareplanningtoapplytheproposedframeworkto  otherapplications,suchascross-systemlinkpredictionin socialnetworks. Acknowledgement WethankthesupportofHongKongRGCgrants621812and  621211.References Boutilier,C.;Zemel,R.S.;andMarlin,B.2003.Active collaborativeÞltering.In UAI ,98Ð106. Cao,B.;Liu,N.N.;andYang,Q.2010.Transferlearningfor  collectivelinkpredictioninmultipleheterogenousdomains. InICML,159Ð166. Chan,Y.S.,andNg,H.T.2007.Domainadaptationwith  activelearningforwordsensedisambiguation.In ACL .Jin,R.,andSi,L.2004.Abayesianapproachtowardactive learningforcollaborativeÞltering.In UAI ,278Ð285. Koren,Y.;Bell,R.;andVolinsky,C.2009.Matrixfac-  torizationtechniquesforrecommendersystems. Computer42(8):30Ð37.Li,W.-J.,andYeung,D.-Y.2009.Relationregularizedma- trixfactorization.In IJCAI,1126Ð1131. Li,B.;Yang,Q.;andXue,X.2009a.Canmoviesandbooks collaborate?:cross-domaincollaborativeÞlteringforspar- sityreduction.In IJCAI,2052Ð2057. Li,B.;Yang,Q.;andXue,X.2009b.Transferlearningfor  collaborativeÞlteringviaarating-matrixgenerativemodel. InICML,617Ð624. Mehta,B.,andHofmann,T.2006.Crosssystemper- sonalizationandcollaborativeÞlteringbylearningmanifold alignments.In KI.244Ð259. Mello,C.E.;Aufaure,M.-A.;andZimbrao,G.2010.Active  learningdrivenbyratingimpactanalysis.In RecSys,341Ð 344.Pan,S.J.,andYang,Q.2010.Asurveyontransferlearn- ing.IEEETransactionsonKnowledgeandDataEngineer- ing22(10):1345Ð1359.Pan,W.;Xiang,E.W.;Liu,N.N.;andYang,Q.2010.Trans- ferlearningincollaborativeÞlteringforsparsityreduction. InAAAI.Pan,W.;Xiang,E.;andYang,Q.2012.Transferlearning incollaborativeÞlteringwithuncertainratings.In Twenty- SixthAAAIConferenceonArtiÞcialIntelligence .Park,Y.-J.,andTuzhilin,A.2008.Thelongtailofrecom- mendersystemsandhowtoleverageit.In RecSys,11Ð18. Rai,P.;Saha,A.;Daum «e,III,H.;andVenkatasubramanian, S.2010.Domainadaptationmeetsactivelearning.In  NAACLHLT2010WorkshoponActiveLearningforNatural LanguageProcessing ,27Ð32. Rennie,J.D.M.,andSrebro,N.2005.Fastmaximum marginmatrixfactorizationforcollaborativeprediction.In ICML,713Ð719. Rish,I.,andTesauro,G.2008.Activecollaborativepredic- tionwithmaximummarginmatrixfactorization.In ISAIM.Saha,A.;Rai,P.;III,H.D.;Venkatasubramanian,S.;and  DuVall,S.L.2011.Activesuperviseddomainadaptation. InECML/PKDD(3) ,97Ð112. Shi,X.;Fan,W.;andRen,J.2008.Activelytransferdomain knowledge.In ECML/PKDD(2) ,342Ð357. Shi,L.;Zhao,Y.;andTang,J.2012.Batchmodeactive learningfornetworkeddata. ACMTransactiononIntelligent SystemsandTechnology 3(2):33:1Ð33:25.Singh,A.P.,andGordon,G.J.2008.Relationallearning viacollectivematrixfactorization.In KDD,650Ð658. Srebro,N.;Rennie,J.D.M.;andJaakkola,T.S.2005.  Maximum-marginmatrixfactorization.In NIPS17 ,1329Ð 1336.Tang,J.;Yan,J.;Ji,L.;Zhang,M.;Guo,S.;Liu,N.;Wang, X.;andChen,Z.2011.CollaborativeusersÕbrandpref-  erenceminingacrossmultipledomainsfromimplicitfeed- backs.In AAAI.Zhang,Y.;Cao,B.;andYeung,D.-Y.2010.Multi-domain collaborativeÞltering.In UAI ,725Ð732.  
ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4018ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4019ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4020ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4021 ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4022ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4023ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4024ProceedingsoftheTwenty-SixthInternationalJointConferenceonArIntelligence(IJCAI-17) 4025 
CNN-RNN:AFrameworkforMulti-labelImage JiangWang 1YiYang 1JunhuaMao 2ZhihengHuang 3 ChangHuang 4 WeiXu 11BaiduResearch 2UniversityofCaliforniaatLosAngles 3FacebookSpeech 4HorizonRobotics AbstractWhiledeepconvolutionalneuralnetworks(CNNs)have shownagreatsuccessinsingle-labelimage  itisimportanttonotethatrealworldimagesgenerallycon-  tainmultiplelabels,whichcouldcorrespondtodifferent  objects,scenes,actionsandattributesinanimage.Tradi-  tionalapproachestomulti-labelimagelearn  independentsforeachcategoryandemployrank-  ingorthresholdingontheresults.Thesetech-  niques,althoughworkingwell,failtoexplicitlyexploitthe  labeldependenciesinanimage.Inthispaper,weutilize  recurrentneuralnetworks(RNNs)toaddressthisproblem.  CombinedwithCNNs,theproposedCNN-RNNframework  learnsajointimage-labelembeddingtocharacterizethe  semanticlabeldependencyaswellastheimage-labelrel-  evance,anditcanbetrainedend-to-endfromscratchto  integratebothinformationinaframework.Exper-  imentalresultsonpublicbenchmarkdatasetsdemonstrate  thattheproposedarchitectureachievesbetterperformance  thanthestate-of-the-artmulti-labelmodels.  1.Introduction Everyreal-worldimagecanbeannotatedwithmultiple labels,becauseanimagenormallyaboundswithrichse-  manticinformation,suchasobjects,parts,scenes,actions,  andtheirinteractionsorattributes.Modelingtherichse-  manticinformationandtheirdependenciesisessentialfor  imageunderstanding.Asaresult, multi-labeltaskisreceivingincreasingattention[ 12,9,24,36].In- spiredbythegreatsuccessfromdeepconvolutionalneural  networksin single-labelimageinthepastfew years[ 17,29,32],whichdemonstratestheeffectivenessof end-to-endframeworks,weexploretolearnaframe-  workfor multi-labelimage AcommonapproachthatextendsCNNstomulti-label istotransformitintomultiplesingle-label  problems,whichcanbetrainedwiththerank-  ingloss[ 9]orthecross-entropyloss[ 12].However,when treatinglabelsindependently,thesemethodsfailtomodel  ThisworkwasdonewhentheauthorsareatBaiduResearch. AirplaneGreatPyreneesArchery Sky,Grass,RunwayDog,Person,RoomPerson,Hat,Nike Figure1.WeshowthreeimagesrandomlyselectedfromImageNet  2012dataset.Thesecondrowshowstheircorre-  spondinglabelannotations.Foreachimage,thereisonlyonela-  bel( i.e. Airplane,GreatPyrenees,Archery)annotatedintheIm- ageNetdataset.However,everyimageactuallycontains multiplelabels,assuggestedinthethirdrow. thedependencybetweenmultiplelabels.Previousworks  haveshownthatmulti-labelproblemsexhibit  stronglabelco-occurrencedependencies[ 39].Forinstance, sky andcloudusuallyappeartogether,while waterandcars almostneverco-occur. Tomodellabeldependency,mostexistingworksare basedongraphicalmodels[ 39],amongwhichacommon approachistomodeltheco-occurrencedependencieswith  pairwisecompatibilityprobabilitiesorco-occurrenceprob-  abilitiesanduseMarkovrandom[ 13]toinferthe naljointlabelprobability.However,whendealingwitha  largesetoflabels,theparametersofthesepairwiseproba-  bilitiescanbeprohibitivelylargewhilelotsoftheparam-  etersareredundantifthelabelshavehighlyoverlapping  meanings.Moreover,mostofthesemethodseithercannot  modelhigher-ordercorrelations[ 39],orcompu- tationalcomplexitytomodelmorecomplicatedlabelrela-  tionships[ 20].Inthispaper,weexplicitlymodelthela- beldependencieswithrecurrentneuralnetworks(RNNs)to  capturehigher-orderlabelrelationshipswhilekeepingthe  computationalcomplexitytractable.WethatRNNsig-  improvesaccuracy. FortheCNNpart,toavoidproblemslikeov previousmethodsnormallyassumeallsharethe  sameimagefeatures[ 36].However,whenusingthesame imagefeaturestopredictmultiplelabels,objectsthatare  smallintheimagesareeasilygetignoredorhardtorec-  ognizeindependently.Inthiswork,wedesigntheRNNs 12285 frameworktoadapttheimagefeaturesbasedonthepre-  viouspredictionresults,byencodingtheattentionmodels  implicitlyintheCNN-RNNstructure.Theideabehindit  istoimplicitlyadapttheattentionalareainimagessothe  CNNscanfocusitsattentionondifferentregionsoftheim-  ageswhenpredictingdifferentlabels.Forexample,when  predictingmultiplelabelsforimagesinFigure 1,ourmodel willshiftitsattentiontosmallerones( i.e. Runway,Person, Hat)afterrecognizingthedominantobject( i.e. Airplane,GreatPyrenees,Archery).Thesesmallobjectsarehardto  recognizebyitself,butcanbeeasilyinferredgivenenough  contexts. Finally,manyimagelabelshaveoverlappingmeanings. Forexample, catandkittenhavealmostthesamemeanings andareofteninterchangeable.Notonlydoesexploiting  thesemanticredundanciesreducethecomputationalcost,it  alsoimprovesthegeneralizationabilitybecausethelabels  withduplicatesemanticscangetmoretrainingdata. Thelabelsemanticredundancycanbeexploitedbyjoint image/labelembedding,whichcanbelearnedviacanoni-  calcorrelationanalysis[ 10],metriclearning[ 19],orlearn- ingtorankmethods[ 37].Thejointimage/labelembedding mapseachlabelorimagetoanembeddingvectorinajoint  low-dimensionalEuclideanspacesuchthattheembeddings  ofsemanticallysimilarlabelsareclosetoeachother,and  theembeddingofeachimageshouldbeclosetothatof  itsassociatedlabelsinthesamespace.Thejointembed-  dingmodelcanexploitlabelsemanticredundancybecause  itessentiallysharesparametersforsemanti-  callysimilarlabels.However,thelabelco-occurrencede-  pendencyislargelyignoredinmostofthesemodels. Inthispaper,weproposeaCNN-RNNframe- workformulti-labelimagewhicheffectively  learnsboththesemanticredundancyandtheco-occurrence  dependencyinanend-to-endway.Theframeworkofthe  proposedmodelisshowninFigure 2.Themulti-labelRNN modellearnsajointlow-dimensionalimage-labelembed-  dingtomodelthesemanticrelevancebetweenimagesand  labels.Theimageembeddingvectorsaregeneratedbya  deepCNNwhileeachlabelhasitsownlabelembedding  vector.Thehigh-orderlabelco-occurrencedependencyin  thislow-dimensionalspaceismodeledwiththelongshort  termmemoryrecurrentneurons,whichmaintainsthein-  formationoflabelcontextintheirinternalmemorystates.  TheRNNframeworkcomputestheprobabilityofamulti-  labelpredictionsequentiallyasanorderedpredictionpath,  wheretheaprioriprobabilityofalabelateachtimestepcan  becomputedbasedontheimageembeddingandtheout-  putoftherecurrentneurons.Duringprediction,themulti-  labelpredictionwiththehighestprobabilitycanbeapprox-  imatelyfoundwithbeamsearchalgorithm.Theproposed  CNN-RNNframeworkisaframeworkwhichcom-  binestheadvantagesofthejointimage/labelembedding VGG ConvNet Recurrent   Neurons Joint  Embedding Space shipseaENDFigure2.AnillustrationoftheCNN-RNNframeworkformulti-  labelimageTheframeworklearnsajointembed-  dingspacetocharacterizetheimage-labelrelationshipaswellas  labeldependency.Theredandbluedotsarethelabelandim-  ageembeddings,respectively,andtheblackdotsarethesumof  theimageandrecurrentneuronoutputembeddings.Therecurrent  neuronsmodelthelabelco-occurrencedependenciesinthejoint  embeddingspacebysequentiallylinkingthelabelembeddingsin  thejointembeddingspace.Ateachtimestep,theprobabilityofa  labeliscomputedbasedontheimageembeddingandtheoutput  oftherecurrentneurons.(bestviewedincolor)  andlabelco-occurrencemodels,anditcanbetrainedinan  end-to-endway. Comparedwithstate-of-the-artmulti-labelimageclassi- methods,theproposedRNNframeworkhasseveral  advantages: Theframeworkemploysanend-to-endmodeltouti-  lizethesemanticredundancyandtheco-occurrence  dependency,bothofwhichisindispensableforeffec-  tivemulti-label Therecurrentneuronsismorecompactandmorepow-  erfulmodelofhigh-orderlabelco-occurrencedepen-  dencythanotherlabelco-occurrencemodelsinthis  task.Theimplicitattentionmechanismintherecurrentneu-  ronsadapttheimagefeaturestobetterpredictsmall  objectsthatneedmorecontexts. WeevaluatetheproposedCNN-RNNframeworkwith exhaustiveexperimentsonpublicmulti-labelbenchmark  datasetsinlcudingNUS-WIDE,MicrosoftCOCO,and  PASCALVOC2007.Experimentalresultsdemonstratethat  theproposedmethodachievesbetterperfor-  mancecomparedtothecurrentstate-of-the-artmulti-label  methods.Wealsovisualizetheattentionalre-  gionsoftheRNNframeworkwiththeDeconvolutionalnet- 2286 works[ 40].Interestingly,thevisualizationshowsthatthe RNNframeworkcanfocusonthecorrespondingimagere-  gionswhenpredictingdifferentlabels,whichisverysimilar  tohumans'multi-labelprocess.  2.RelatedWork Theprogressofimageispartlyduetothe creationoflarge-scalehand-labeleddatasetssuchasIma-  geNet[ 5],andthedevelopmentofdeepconvolutionalneu- ralnetworks[ 17].Recentworkthatextendsdeepconvolu- tionalneuralnetworkstomulti-labelachieves  goodresults.Deepconvolutionalranking[ 9]optimizesa top-k rankingobjective,whichassignssmallerweightsto thelossifthepositivelabel.Hypotheses-CNN-Pooling  [36]employsmaxpoolingtoaggregatethepredictions frommultiplehypothesisregionproposals.Thesemethods  largelytreateachlabelindependentlyandignorethecorre-  lationsbetweenlabels. Multi-labelcanalsobeachievedbylearn- ingajointimage/labelembedding.MultiviewCanonical  CorrelationAnalysis[ 10]isathree-waycanonicalanaly- sisthatmapstheimage,label,andthesemanticsintothe  samelatentspace.WASABI[ 37]andDEVISE[ 7]learn thejointembeddingusingthelearningtorankframework  withWARPloss.Metriclearning[ 19]learnsadiscrimi- nativemetrictomeasuretheimage/labelsimilarity.Matrix  completion[ 1]andbloom[ 3]canalsobeemployed aslabelencodings.Thesemethodseffectivelyexploitthe  labelsemanticredundancy,buttheyfallshortonmodeling  thelabelco-occurrencedependency. Variousapproacheshavebeenproposedtoexploitthela- belco-occurrencedependencyformulti-labelimageclassi-  [ 28]learnsachainofbinarywhereeach predictswhetherthecurrentlabelexistsgiventhe  inputfeatureandthealreadypredictedlabels.Thelabel  co-occurrencedependencycanalsobemodeledbygraphi-  calmodels,suchasConditionalRandomField[ 8],Depen- dencyNetwork[ 13],andco-occurrencematrix[ 39].Label augmentmodel[ 20]augmentsthelabelsetwithcommon labelcombinations.Mostofthesemodelsonlycapture  pairwiselabelcorrelationsandhavehighcomputationcost  whenthenumberoflabelsislarge.Thelow-dimensional  recurrentneuronsintheproposedRNNmodelaremore  computationallyefrepresentationsforhigh-orderla-  belcorrelation. RNNwithLSTMcaneffectivelymodelthelong-term temporaldependencyinasequence.Ithasbeensuccess-  fullyappliedinimagecaptioning[ 25,35],machinetransla- tion[ 31],speechrecognition[ 11],languagemodeling[ 30],andwordembeddinglearning[ 18].Wedemonstratethat RNNwithLSTMisalsoaneffectivemodelforlabelde-  pendency. input  gate forget   gate output gate update   term xtitftotr(t)o(t)Figure3.AschematicillustrationofaLSTMneuron.EachLSTM  neuronhasaninputgate,aforgetgate,andanoutputgate.  3.Method Sinceweaimtocharacterizethehigh-orderlabelcorre- lation,weemploylongshorttermmemory(LSTM)neu-  rons[ 15]asourrecurrentneurons,whichhasbeendemon- stratedtobeapowerfulmodeloflong-termdependency.  3.1.LongShortTermMemoryNetworks(LSTM) RNN[ 15]isaclassofneuralnetworkthatmaintains internalhiddenstatestomodelthedynamictemporalbe-  haviourofsequenceswitharbitrarylengthsthroughdirected  cyclicconnectionsbetweenitsunits.Itcanbeconsidered  asahiddenMarkovmodelextensionthatemploysnonlin-  eartransitionfunctionandiscapableofmodelinglongterm  temporaldependencies.LSTMextendsRNNbyadding  threegatestoanRNNneuron:aforgetgate f tocontrol whethertoforgetthecurrentstate;aninputgate i toindi- cateifitshouldreadtheinput;anoutputgate o tocontrol whethertooutputthestate.ThesegatesenableLSTMto  learnlong-termdependencyinasequence,andmakeitis  easiertooptimize,becausethesegateshelptheinputsig-  naltoeffectivelypropagatethroughtherecurrenthidden  statesr ( t ) withoutaffectingtheoutput.LSTMalsoef- fectivelydealswiththegradientvanishing/explodingissues  thatcommonlyappearduringRNNtraining[ 26].x t =  ( U r :r ( t  1)+ U w w k ( t )) i t =  ( U i r r ( t  1)+ U i ww k ( t )) f t =  ( U f r r ( t  1)+ U f ww k ( t )) o t =  ( U o r r ( t  1)+ U o ww k ( t )) r ( t )= f t  r ( t  1)+ i t  x t o ( t )= r ( t )  o ( t ) (1)where ( : ) isanactivationfunction,  istheproductwith gatevalue,andvarious W matricesarelearnedparame- ters.Inourimplementation,weemploylinearunits  (ReLU)astheactivationfunction[ 4].2287 3.2.Model WeproposeanovelCNN-RNNframeworkformulti- labelproblem.TheillustrationoftheCNN-  RNNframeworkisshowninFig. 4.Itcontainstwoparts: TheCNNpartextractssemanticrepresentationsfromim-  ages;theRNNpartmodelsimage/labelrelationshipandla-  beldependency. Wedecomposeamulti-labelpredictionasanordered predictionpath.Forexample,labelsﬁzebraﬂandﬁelephantﬂ  canbedecomposedaseither(ﬁzebraﬂ,ﬁelephantﬂ)or(ﬁele-  phantﬂ,ﬁzebraﬂ).Theprobabilityofapredictionpathcan  becomputedbytheRNNnetwork.Theimage,label,and  recurrentrepresentationsareprojectedtothesamelow-  dimensionalspacetomodeltheimage-textrelationshipas  wellasthelabelredundancy.TheRNNmodelisemployed  asacompactyetpowerfulrepresentationofthelabelco-  occurrencedependencyinthisspace.Ittakestheembed-  dingofthepredictedlabelateachtimestepandmaintainsa  hiddenstatetomodelthelabelco-occurrenceinformation.  Theaprioriprobabilityofalabelgiventhepreviouslypre-  dictedlabelscanbecomputedaccordingtotheirdotprod-  uctswiththesumoftheimageandrecurrentembeddings.  Theprobabilityofapredictionpathcanbeobtainedasthe  productofthea-priorprobabilityofeachlabelgiventhe  previouslabelsinthepredictionpath. Alabel k isrepresentedasaone-hotvector e k = [0 ;::: 0; 1; 0;:::; 0] ,whichis1atthe k -thlocation,and0 elsewhere.Thelabelembeddingcanbeobtainedbymulti-  plyingtheone-hotvectorwitha labelembeddingmatrix U l .Thek -throwof U l isthelabelembeddingofthelabel k .w k = U l :e k : (2)Thedimensionof w k isusuallymuchsmallerthanthenum- beroflabels. Therecurrentlayertakesthelabelembeddingofthepre- viouslypredictedlabel,andmodelstheco-occurrencede-  pendenciesinitshidden recurrentstates bylearningnon- linearfunctions: o ( t )= h o ( r ( t  1) ;w k ( t )) ;r ( t )= h r ( r ( t  1) ;w k ( t )) (3)wherer ( t ) ando ( t ) arethehiddenstatesandoutputsofthe recurrentlayeratthetimestept,respectively, w k ( t ) isthe labelembeddingofthe t -thlabelinthepredictionpath,and h o ( : ) ;h r ( : ) arethenon-linearRNNfunctions,whichwill bedescribedindetailsinSec. 3.1.Theoutputoftherecurrentlayerandtheimagerepresen- tationareprojectedintothesamelow-dimensionalspaceas  thelabelembedding. x t = h ( U x o o ( t )+ U x I I ) ; (4)whereU x o andU x I aretheprojectionmatricesforrecurrent layeroutputandimagerepresentation,respectively.The Current  LabelLabel Embeddingek(t)Recurrent   Layer wk(t)Projection  Layer o(t)Image r(t)ConvNet IPrediction  Layer Predicted Label probability x(t)UlUlTFigure4.ThearchitectureoftheproposedRNNmodelformulti-  labelTheconvolutionalneuralnetworkisemployed  astheimagerepresentation,andtherecurrentlayercapturesthe  informationofthepreviouslypredictedlabels.Theoutputlabel  probabilityiscomputedaccordingtotheimagerepresentationand  theoutputoftherecurrentlayer.  numberofcolumnsof U x o andU x I arethesameasthela- belembeddingmatrix U l .I istheconvolutionalneuralnet- workimagerepresentation.WewillshowinSec 4.5thatthe learnedjointembeddingeffectivelycharacterizestherele-  vanceofimagesandlabels. Finally,thelabelscorescanbecomputedbymultiplying thetransposeof U l andx t tocomputethedistancesbetween x t andeachlabelembedding. s ( t )= U T l x t : (5)Thepredictedlabelprobabilitycanbecomputedusingsoft-  maxnormalizationonthescores.  3.3.Inference Apredictionpath isasequenceoflabels ( l 1 ;l 2 ;l 3 ;  ;l N ) ,wheretheprobabilityofeachla- bell t canbecomputedwiththeinformationoftheimage I andthepreviouslypredictedlabels l 1 ;  ;l t  1 .TheRNN modelpredictsmultiplelabelsbytheprediction  paththatmaximizestheaprioriprobability. l 1 ;  ;l k =argmax l 1 ;  ;l kP( l 1 ;  ;l k j I ) =argmax l 1 ;  ;l kP( l 1 j I )  P( l 2 j I;l 1 )  P( l k j I;l 1 ;  ;l k  1 ) : (6)Sincetheprobability P( l k j I;l 1 ;  ;l k  1 ) doesnothave Markovproperty,thereisnooptimalpolynomialalgo-  rithmtotheoptimalpredictionpath.Wecanem-  ploythegreedyapproximation,whichpredictslabel ^l t = argmax l t P( l t j I;l 1 ;  ;l t  1 ) attimestep t andthela- belprediction ^l t atlaterpredictions.However,thegreedy algorithmisproblematicbecauseifthepredictedlabel  iswrong,itisverylikelythatthewholesequencecannot 2288 Cat Dog Person ENDCar Cat Dog Person ENDCar Cat Dog Person ENDCar Cat Dog Person ENDCar Cat Dog Person ENDCar Cat Dog Person ENDCar Figure5.Anexampleofthebeamsearchalgorithmwithbeam  sizeN =2 .Thebeamsearchalgorithmthebest N pathswiththehighestprobability,bykeepingasetofintermediatepaths  ateachtimestepanditerativelyaddinglabelstheseintermediate  paths. becorrectlypredicted.Thus,weemploythebeamsearch  algorithmtothetop-rankedpredictionpath. Anexampleofthebeamsearchalgorithmcanbefound inFigure 5.Insteadofgreedilypredictingthemostproba- blelabel,thebeamsearchalgorithmthetop- N mostprobablepredictionpathsas intermediatepaths S ( t ) ateach timestep t .S ( t )= fP1 ( t ) ;P 2 ( t ) ;  ;P N ( t ) g(7)Attimestep t +1 ,weadd N mostprobablelabelstoeach intermediatepath Pi ( t ) togetatotalof N  N paths.The N predictionpathswithhighestprobabilityamongthese pathsconstitutethe intermediatepaths fortimestep t +1 .ThepredictionpathsendingwiththeENDsignareaddedto  thecandidatepath setC .Theterminationconditionofthe beamsearchisthattheprobabilityofthecurrentintermedi-  atepathsissmallerthanthatofallthecandidatepaths.It  indicatesthatwecannotanymorecandidatepathswith  greaterprobability.  3.4.Training LearningCNN-RNNmodelscanbeachievedbyus- ingthecross-entropylossonthesoftmaxnormalization  ofscoresoftmax ( s ( t )) andemployingback-propagation throughtimealgorithm.Inordertoavoidthegradientvan-  ishing/explodingissues,weapplythermspropoptimization  algorithm[ 33].Althoughitispossibletothecon- volutionalneuralnetworkinourarchitecture,wekeepthe  convolutionalneuralnetworkunchangedinourimplemen-  tationforsimplicity. Oneimportantissueoftrainingmulti-labelCNN-RNN modelsistodeterminetheordersofthelabels.Intheex-  perimentsofthispaper,thelabelordersduringtrainingare determinedaccordingtotheiroccurrencefrequenciesinthe  trainingdata.Morefrequentlabelsappearearlierthanthe  lessfrequentones,whichcorrespondstotheintuitionthat  easierobjectsshouldbepredictedtohelppredictmore  difobjects.Weexploredlearninglabelordersbyiter-  ativelytheeasiestpredictionorderingandorderen-  semblesasproposedin[ 28]orsimplyusingedrandom order,buttheydonothavenotableeffectsontheperfor-  mance.Wealsoattemptedtorandomlypermutethelabel  ordersineachmini-batch,butitmakesthetrainingvery  diftoconverge.  4.Experiments Inourexperiments,theCNNmoduleusesthe16lay- ersVGGnetwork[ 29]pretrainedonImageNet2012clas- challengedataset[ 5]usingCaffedeeplearning framework[ 16].Thedimensionsofthelabelembedding andofLSTMRNNlayerare64and512,respectively.We  employweightdecayrate0.0001,momentumrate0.9,and  dropout[ 4]rate0.5foralltheprojectionlayers. Weevaluatetheproposedmethodonthreebenchmark multi-labeldatasets:NUS-WIDE,Microsoft  COCO,andVOCPASCAL2007datasets.Theevaluation  demonstratesthattheproposedmethodachievessuperior  performancetostate-of-the-artmethods.Wealsoquali-  tativelyshowthattheproposedmethodlearnsajointla-  bel/imageembeddinganditfocusesitsattentionindifferent  imageregionsduringthesequentialprediction.  4.1.EvaluationMetric Theprecisionandrecallofthegeneratedlabelsareem- ployedasevaluationmetrics.Foreachimage,wegenerate  k 1highestrankedlabelsandcomparethegeneratedlabels tothegroundtruthlabels.Theprecisionisthenumberof  correctlyannotatedlabelsdividedbythenumberofgener-  atedlabels;therecallisthenumberofcorrectlyannotated  labelsdividedbythenumberofground-truthlabels. Wealsocomputetheper-classandoverallprecision(C- PandO-P)andrecallscores(C-RandO-R),wheretheav-  erageistakenoverallclassesandalltestingexamples,re-  spectively.TheF1(C-F1andO-F1)scoreisthegeometrical  averageoftheprecisionandrecallscores.Wealsocompute  themeanaverageprecision(MAP)@Nmeasure[ 34].4.2. NUS-WIDEdataset[ 2]isawebimagedatasetthatcon- tains269,648imagesand5018tagsfromFlickr.Therearea  totalof1000tagsafterremovingnoisyandraretags.These  imagesarefurthermanuallyannotatedinto81conceptsby 1ForRNNmodel,wesettheminimumpredictionlengthduringbeam searchtoensurethatatleast k labelsarepredicted. 2289 Figure6.OneexampleimagefromNUS-WIDE(left)andMS-  COCO(right)datasets,theground-truthannotationsandour  model'spredictions. MethodC-PP-RC-F1 O-PO-RO-F1 MAP@10MetricLearning[ 19]-----21.3-Multi-edgegraph[ 23]--- 35.037.036.0 -KNN[ 2]32.619.324.342.953.447.6-Softmax31.731.231.4 47.859.553.0 -WARP[ 9]31.735.633.548.660.553.9-JointEmbedding[ 38]--- --- 40.3CNN-RNN40.530.434.749.961.755.256.1Table1.ComparisonsonNUS-WIDEDataseton81conceptsfor  k =3 .humanannotators.Anexampleoftheannotationsandpre-  dictionsforbothoflabelsetisshownintheleftsideof  Fig.6.Thequalityof81-tagannotationsisrelativelyhigh, whilethe1000-tagannotationsareverynoisy.InFig. 6,wecansometagswithduplicatesemantics,suchas  ﬁcloudﬂandﬁcloudsﬂ,somecompletelywrongtags,suchas  ﬁphotographerﬂ,andsometagsthataretoogeneraltohave  meanings,suchasﬁbeautifulﬂ. Weevaluatetheproposedmethodonlessnoisy 81conceptslabels.Wecomparetheproposedmethod  withstate-of-the-artmethodsincludingKnearestneighbor  search[ 2],softmaxprediction,WARPmethod[ 9],metric learning[ 19],andjointembedding[ 38]inTable 1.Since thereislessnoisein81conceptslabels,allmethodsachieve  fairlygoodperformance.Althoughwedonot  ourconvolutionalneuralnetworkimagerepresentation,the  proposedRNNframeworkoutperformsthestate-of-the-art  methods.Inparticular,wetheCNN-RNNframework  achieves8%higherprecision,becauseitiscapableofex-  ploitingthelabelcorrelationtooutthelabelsthatcan  notpossiblyexisttogether. WealsocompareRNNmodelwithsoftmax,DSLR[ 22],andWARPmodelsonthemorechallenging1000-taglabel  setinTable 2.Thepredictionaccuracyofallthemeth- odsareverylow,becausethelabelsonthisdatasetare  verynoisy,buttheproposedmethodstilloutperformsall  thebaselinemethods.Wetheproposedmethodcannot MethodC-PP-RC-F1 O-PO-RO-F1 MAP@10Softmax14.218.616.117.128.821.524.3DLSR[ 22]--- 20.025.022.4 -WARP 14.515.915.218.330.822.924.8CNN-RNN19.215.317.118.531.223.3 26.6Table2.ComparisonsonNUS-WIDEDataseton1000tagsfor  k =10 .MethodC-PP-RC-F1 O-PO-RO-F1 MAP@10Softmax59.057.058.060.262.161.147.4WARP 59.352.555.7 59.861.460.7 49.2Binarycross-entropy 59.358.658.961.765.063.3-NoRNN 65.354.559.3 68.561.365.7 57.2CNN-RNN66.055.660.469.266.467.861.2Table3.ComparisonsonMS-COCODatasetfor k =3 .distinguishgender-relatedlabelssuchasﬁactorﬂandﬁac-  tressﬂ,becauseourconvolutionalneuralnetworkistrained  onImageNet,whichdoesnothavetheannotationforthis  task.Moremulti-labelpredictionexamplescanbefoundin  thesupplementalmaterials.  4.3.MicrosoftCOCO MicrosoftCOCO(MS-COCO)dataset[ 21]isanimage recognition,segmentation,andcaptioningdataset.Itcon-  tains123thousandimagesof80objectstypeswithper-  instancesegmentationlabels.Amongthoseimages,82783  imagesareutilizedastrainingdata,and40504imagesare  employedastestingdata.Weutilizetheobjectannotations  asthelabels.Anexampleoftheannotationsandpredictions  oftheMS-COCOdatasetisshownintherightofFig. 6.An interestingpropertyoftheMS-COCOdatasetisthatmost  imagesinthisdatasetcontainmultipleobjects,andthese  objectsusuallyhavestrongco-occurrencedependencies.  Forexample,ﬁbaseballgloveﬂandﬁsportballﬂhavehigh  co-occurrenceprobability,whileﬁzebraﬂandﬁcatﬂnever  appeartogether. Wecomparethesoftmax,multi-labelbinarycrossen- tropy,andWARP[ 9]modelswiththeCNN-RNNmodelin Table 3.Sincethenumberoftheobjectsperimagevaries considerablyinthisdataset,wedonotsettheminimum  lengthofthepredictionpathduringbeamsearch.Itcan  beobservedthattheproposedmethodachievesmuchbetter  performancebothintermsofoverallprecisionandrecall.It  hasaslightlylowerper-classrecallbecauseitmayoutput  lessthan k labelsforanimageanditusuallychoosesnotto predictthesmallobjectsthathavelittleco-occurrencede-  pendencieswithotherlargerobjects.Wealsoreplacethere-  currentlayerwithalinearembeddinglayerintheproposed  architectureandevaluatetheperformance.Wethatre-  movingtherecurrentlayeraffectstherecallof  themulti-label Theper-classprecisionandrecalloftheproposedframe- 2290 Figure7.Theper-classprecisionandrecalloftheRNNmodelonMS-COCOdataset. planebikebirdboatbottlebuscarcatchaircowtabledoghorsemotorpersonplantsheepsofatraintv mAPINRIA[ 14]77.269.356.266.645.568.183.453.658.351.162.245.278.469.786.152.454.454.375.862.163.5CNN-SVM[ 27]88.581.083.582.042.072.585.381.659.958.566.577.881.878.890.254.871.162.687.471.8 73.9I-FT[ 36]91.484.787.581.840.273.086.484.851.863.967.982.784.076.990.451.579.954.189.565.874.4HCP-1000C[ 36]95.190.192.889.951.580.0 91.791.657.777.8 70.989.389.3 85.293.064.085.762.794.478.3 81.5CNN-RNN96.783.194.292.861.282.189.194.264.283.670.092.491.784.293.759.893.275.399.778.684.0Table4.results(APin%)comparisononPASCALVOC2007dataset. workisshowninFig. 7.Wetheproposedframework performsverywellonlargeobjects,suchasﬁpersonﬂ,ﬁze-  braﬂ,andﬁstopsignﬂ,andtheobjectswithhighdependen-  cieswithotherobjectsorthescene,suchasﬁsportsbarﬂ  andﬁbaseballgloveﬂ.Itperformsverypoorlyonsmallob-  jectswithlittledependencieswithotherobjects,suchas  ﬁtoasterﬂandﬁhairdrierﬂ,becausetheglobalconvolutional  neuralnetworkimagefeatureshavelimiteddiscriminative  abilitytorecognizesmallobjects.Thereiszeroprediction  forﬁtoasterﬂandﬁhairdrierﬂ,resultinginzeroprecisionand  recallscoresforthesetwolabels. Therelationshipbetweenrecallandboundingboxarea onMicrosoftCOCOdatasetisshowninFig. 8.Wecan observethattherecallisthegenerallyhigherastheobject  islarger,unlesstheobjectissolargethatitalmostthe  wholeimage,wheresomeimportantinformationmightlose  duringtheimagecroppingprocessinCNN.  4.4.PASCALVOC2007 PASCALVisualObjectClassesChallenge(VOC) datasets[ 6]arewidelyusedasthebenchmarkformultil- abelVOC2007datasetcontains9963im-  agesdividedinto train ,valandtestsubsets.Weconduct ourexperimentson trainval/test splits(5011/4952images). TheevaluationisAveragePrecision(AP)andmeanofAP Figure8.Therelationshipbetweenrecallandboundingboxarea  onMicrosoftCOCOdataset.  (mAP).Thecomparisontostate-of-the-artmethodsisshownin Table 4.INRIA[ 14]isbasedonthetransitionalfeature extraction-coding-poolingpipeline.CNN-SVM[ 27]di- rectlyappliesSVMontheOverFeatfeaturespre-trained  onImageNet.I-FT[ 36]employsthesquaredlossfunc- tiononthesharedCNNfeaturesonPASCALVOCfor  multi-labelHCP-1000C[ 36]employsregion 2291 LabelNearestNeighbors glacierarctic,norway,volcano,tundra,lakes sky nature,blue,clouds,landscape,bravo sunsetsun,landscape,light,bravo,yellow railrailway,track,locomotive,tracks,steam catdog,bear,bird,hairdrier,toaster cow horse,sheep,bear,zebra,elephant Table5.Nearestneighborsforlabelembeddingsof1klabelsof  NUS-WIDEandMS-COCOdatasets hawk, eagle, fauna,   wind, bird glacier, volcano, cli˜,  arctic, lakes portraits, costume,   female, asian, hat portraits,people, street,   hospital, woman landscape, mountain,   nature, mountains,  bravo bird, hwak, nature,  bravo, birds Images KNNClassi- ˚cation Figure9.Nearestneighborlabelsandtoppredictions  bysoftmaxmodelforthreequeryimagesof1000labelseton  NUS-WIDEdataset.  proposalinformationtounetheCNNfeaturespre-  trainedonImageNet1000dataset,andachivesbetterper-  formancethanthemethodsthatdonotexploitregioninfor-  mation.TheproposedCNN-RNNmethodoutperformsthe  I-FTmethodbyalargemargin,anditalsoperformsbetter  thanHCP-1000Cmethod,althoughtheRNNmethoddoes  nottaketheregionproposalinformationintoaccount.  4.5.Labelembedding Inadditiontobeingabletogeneratemultiplelabels,the CNN-RNNmodelalsoeffectivelylearnsajointlabel/image  embedding.Thenearestneighborsofthelabelsintheem-  beddingspaceforNUS-WIDEandMS-COCO,areshown  inTable 5.Wecanseethatalabelishighlysemantically relatedtoitsnearest-neighborlabels. Fig.9showsthenearestneighborlabelsforimageson NUS-WIDE1000-tagdatasetcomputedaccordingtolabel  embeddingw k andimageembedding U x I I .Inthejointem- beddingspace,animageanditsnearestneighborlabelsare  semanticallyrelevant.Moreover,wethatcomparedto  thetop-rankedlabelspredictedbymodel,the  nearestneighborlabelsareusuallymoreFor  example,thenearestneighborlabelsﬁhawkﬂandﬁglacierﬂ  aremorethanﬁbirdﬂandﬁlandscapeﬂ. OriginalImageInitialAttentionAttentionafterword Figure10.TheattentionalvisualizationfortheRNNmulti-label  framework.Thisimagehastwoground-truthlabels:ﬁelephantﬂ  andﬁzebraﬂ.Thebottom-leftimageshowstheframework'satten-  tioninthebeginning,andthebottom-rightimageshowsitsatten-  tionafterpredictingﬁelephantﬂ.  4.6.AttentionVisualization ItisinterestingtoinvestigatehowtheCNN-RNNframe- work'sattentionchangeswhenpredictingdifferentlabels.  WevisualizetheattentionoftheRNNmulti-labelmodel  withDeconvolutionalnetworks[ 40]inFig. 10.Givenan inputimage,theattentionoftheRNNmultilabelmodelat  eachtimestepistheaverageofthesynthesizedimageof  allthelabelnodesatthesoftmaxlayerusingDeconvolu-  tionalnetwork.Thegroundtruthlabelsofthisimageare  ﬁelephantﬂandﬁzebraﬂ.(Noticethatthevisualizationof  attentiondoesnotutilizethegroundtruthlabels)Atthebe-  ginning,theattentionvisualizationshowsthatthemodel  looksoverthewholeimageandpredictsﬁelephantﬂ.Af-  terpredictingﬁelephantﬂ,themodelshiftsitsattentionto  theregionsofzebraandpredictsﬁzebraﬂ.Thevisualization  showsthatalthoughtheRNNframeworkdoesnotlearnan  explicitattentionalmodel,itmanagestosteeritsattentionto  differentimageregionswhenclassifyingdifferentobjects.  5.ConclusionandFutureWork WeproposeaCNN-RNNframeworkformulti- labelimageTheproposedframeworkcom-  binestheadvantagesofthejointimage/labelembedding  andlabelco-occurrencemodelsbyemployingCNNand  RNNtomodelthelabelco-occurrencedependencyina  jointimage/labelembeddingspace.Experimentalresultson  severalbenchmarkdatasetsdemonstratethattheproposed  approachachievessuperiorperformancetothestate-of-the-  artmethods. Theattentionvisualizationshowsthattheproposed modelcansteeritsattentiontodifferentimageregionswhen  predictingdifferentlabels.However,predictingsmallob-  jectsisstillchallengingduetothelimiteddiscriminative-  nessoftheglobalvisualfeatures.Itisaninterestingdi-  rectiontonotonlypredictthelabels,butalsopredictthe  segmentationoftheobjectsbyconstructinganexplicitat-  tentionmodel.Wewillinvestigatethatinourfuturework. 2292 References [1]R.S.Cabral,F.Torre,J.P.Costeira,andA.Bernardino. Matrixcompletionformulti-labelimageIn  AdvancesinNeuralInformationProcessingSystems ,pages 190Œ198,2011. 3[2]T.-S.Chua,J.Tang,R.Hong,H.Li,Z.Luo,andY.Zheng. Nus-wide:areal-worldwebimagedatabasefromnational  universityofsingapore.In ProceedingsoftheACMinter- nationalconferenceonimageandvideoretrieval ,page48. ACM,2009. 5,6[3]M.M.Cisse,N.Usunier,T.Artieres,andP.Gallinari.Ro- bustbloomtersforlargemultilabeltasks.In  AdvancesinNeuralInformationProcessingSystems ,pages 1851Œ1859,2013. 3[4]G.E.Dahl,T.N.Sainath,andG.E.Hinton.Improving deepneuralnetworksforlvcsrusinglinearunits  anddropout.In Acoustics,SpeechandSignalProcessing (ICASSP),2013IEEEInternationalConferenceon ,pages 8609Œ8613.IEEE,2013. 3,5[5]J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei- Fei.Imagenet:Alarge-scalehierarchicalimagedatabase.  InComputerVisionandPatternRecognition,2009.CVPR 2009.IEEEConferenceon ,pages248Œ255.IEEE,2009. 3,5[6]M.Everingham,L.VanGool,C.K.Williams,J.Winn,and A.Zisserman.Thepascalvisualobjectclasses(voc)chal-  lenge.Internationaljournalofcomputervision ,88(2):303Œ 338,2010. 7[7]A.Frome,G.S.Corrado,J.Shlens,S.Bengio,J.Dean, T.Mikolov,etal.Devise:Adeepvisual-semanticembed-  dingmodel.In AdvancesinNeuralInformationProcessing Systems,pages2121Œ2129,2013. 3[8]N.GhamrawiandA.McCallum.Collectivemulti-labelclas- In Proceedingsofthe14thACMinternationalcon- ferenceonInformationandknowledgemanagement ,pages 195Œ200.ACM,2005. 3[9]Y.Gong,Y.Jia,T.Leung,A.Toshev,andS.Ioffe.Deep convolutionalrankingformultilabelimageannotation. arXivpreprintarXiv:1312.4894 ,2013. 1,3,6[10]Y.Gong,Q.Ke,M.Isard,andS.Lazebnik.Amulti-view embeddingspaceformodelinginternetimages,tags,and  theirsemantics. Internationaljournalofcomputervision ,106(2):210Œ233,2014. 2,3[11]A.Graves,A.-R.Mohamed,andG.Hinton.Speechrecog- nitionwithdeeprecurrentneuralnetworks.In Acoustics,SpeechandSignalProcessing(ICASSP),2013IEEEInter-  nationalConferenceon ,pages6645Œ6649.IEEE,2013. 3[12]M.Guillaumin,T.Mensink,J.Verbeek,andC.Schmid. Tagprop:Discriminativemetriclearninginnearestneigh-  bormodelsforimageauto-annotation.In ComputerVision, 2009IEEE12thInternationalConferenceon ,pages309Œ 316.IEEE,2009. 1[13]Y.GuoandS.Gu.Multi-labelusingcon- ditionaldependencynetworks.In IJCAIProceedings- InternationalJointConferenceonIntelligence ,vol- ume22,page1300,2011. 1,3[14]H.Harzallah,F.Jurie,andC.Schmid.Combiningef objectlocalizationandimageIn ComputerVision,2009IEEE12thInternationalConferenceon ,pages 237Œ244.IEEE,2009. 7[15]S.HochreiterandJ.Schmidhuber.Longshort-termmemory. Neuralcomputation ,9(8):1735Œ1780,1997. 3[16]Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Gir- shick,S.Guadarrama,andT.Darrell.Caffe:Convolu-  tionalarchitectureforfastfeatureembedding. arXivpreprint arXiv:1408.5093,2014. 5[17]A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenet withdeepconvolutionalneuralnetworks.In  Advancesinneuralinformationprocessingsystems ,pages 1097Œ1105,2012. 1,3[18]P.LeandW.Zuidema.Compositionaldistributionalse- manticswithlongshorttermmemory. arXivpreprint arXiv:1503.02510,2015. 3[19]J.Li,X.Lin,X.Rui,Y.Rui,andD.Tao.Adistributedap- proachtowarddiscriminativedistancemetriclearning. Neu-ralNetworksandLearningSystems,IEEETransactionson ,2014.2,3,6[20]X.Li,F.Zhao,andY.Guo.Multi-labelimagec withaprobabilisticlabelenhancementmodel.UAI,2014. 1,3[21]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra- manan,P.Doll ´ar,andC.L.Zitnick.Microsoftcoco:Com- monobjectsincontext.In ComputerVisionŒECCV2014 ,pages740Œ755.Springer,2014. 6[22]Z.Lin,G.Ding,M.Hu,Y.Lin,andS.S.Ge.Imagetagcom- pletionviadual-viewlinearsparsereconstructions. Com-puterVisionandImageUnderstanding ,124:42Œ60,2014. 6[23]D.Liu,S.Yan,Y.Rui,andH.-J.Zhang.taganalysis withmulti-edgegraph.In Proceedingsoftheinternational conferenceonMultimedia ,pages25Œ34.ACM,2010. 6[24]A.Makadia,V.Pavlovic,andS.Kumar.Anewbaselinefor imageannotation.In ComputerVisionŒECCV2008 ,pages 316Œ329.Springer,2008. 1[25]J.Mao,W.Xu,Y.Yang,J.Wang,andA.Yuille.Deepcap- tioningwithmultimodalrecurrentneuralnetworks(m-rnn).  InICLR,2015. 3[26]R.Pascanu,T.Mikolov,andY.Bengio.Onthedif cultyoftrainingrecurrentneuralnetworks. arXivpreprint arXiv:1211.5063,2012. 3[27]A.S.Razavian,H.Azizpour,J.Sullivan,andS.Carlsson. Cnnfeaturesoff-the-shelf:anastoundingbaselineforrecog-  nition.In ComputerVisionandPatternRecognitionWork- shops(CVPRW),2014IEEEConferenceon ,pages512Œ519. IEEE,2014. 7[28]J.Read,B.Pfahringer,G.Holmes,andE.Frank.Classi- chainsformulti-label Machinelearning ,85(3):333Œ359,2011. 3,5[29]K.SimonyanandA.Zisserman.Verydeepconvolutional networksforlarge-scaleimagerecognition. arXivpreprint arXiv:1409.1556,2014. 1,5[30]M.Sundermeyer,R.Schl ¨uter,andH.Ney.Lstmneuralnet- worksforlanguagemodeling.In INTERSPEECH,2012. 32293 [31]I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequence learningwithneuralnetworks.In AdvancesinNeuralInfor- mationProcessingSystems ,pages3104Œ3112,2014. 3[32]C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed, D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabi-  novich.Goingdeeperwithconvolutions. arXivpreprint arXiv:1409.4842,2014. 1[33]T.TielemanandG.Hinton.Lecture6.5-rmsprop:Divide thegradientbyarunningaverageofitsrecentmagnitude.  COURSERA:NeuralNetworksforMachineLearning ,4, 2012.5[34]A.TurpinandF.Scholer.Userperformanceversuspreci- sionmeasuresforsimplesearchtasks.In Proceedingsof the29thannualinternationalACMSIGIRconferenceonRe-  searchanddevelopmentininformationretrieval ,pages11Œ 18.ACM,2006. 5[35]O.Vinyals,A.Toshev,S.Bengio,andD.Erhan.Show andtell:Aneuralimagecaptiongenerator. arXivpreprint arXiv:1411.4555,2014. 3[36]Y.Wei,W.Xia,J.Huang,B.Ni,J.Dong,Y.Zhao,and S.Yan.Cnn:Single-labeltomulti-label. arXivpreprint arXiv:1406.5726,2014. 1,3,7[37]J.Weston,S.Bengio,andN.Usunier.Wsabie:Scalingup tolargevocabularyimageannotation.In IJCAI,volume11, pages2764Œ2770,2011. 2,3[38]F.Wu,X.Jiang,X.Li,S.Tang,W.Lu,Z.Zhang,and Y.Zhuang.Cross-modallearningtorankvialatentjoint  representation.ImageProcessing,IEEETransactionson ,24(5):1497Œ1509,2015. 6[39]X.Xue,W.Zhang,J.Zhang,B.Wu,J.Fan,andY.Lu. Correlativemulti-labelmulti-instanceimageannotation.In  ComputerVision(ICCV),2011IEEEInternationalConfer-  enceon ,pages651Œ658.IEEE,2011. 1,3[40]M.D.Zeiler,D.Krishnan,G.W.Taylor,andR.Fergus. Deconvolutionalnetworks.In ComputerVisionandPat- ternRecognition(CVPR),2010IEEEConferenceon ,pages 2528Œ2535.IEEE,2010. 3,82294  
StackGAN:TexttoPhoto-realisticImageSynthesis withStackedGenerativeAdversarialNetworks HanZhang 1,TaoXu 2,HongshengLi 3,ShaotingZhang 4,XiaogangWang 3,XiaoleiHuang 2,DimitrisMetaxas 11RutgersUniversity 2LehighUniversity 3TheChineseUniversityofHongKong 4BaiduResearch {han.zhang,dnm }@cs.rutgers.edu,{tax313,xih206 }@lehigh.edu{hsli,xgwang }@ee.cuhk.edu.hk,zhangshaoting@baidu.com AbstractSynthesizinghigh-qualityimagesfromtextdescriptions isachallengingproblemincomputervisionandhasmany  practicalapplications.Samplesgeneratedbyexistingtext-  to-imageapproachescanroughlyreßectthemeaningofthe  givendescriptions,buttheyfailtocontainnecessarydetails  andvividobjectparts.Inthispaper,weproposeStacked  GenerativeAdversarialNetworks(StackGAN)togenerate  256!256photo-realisticimagesconditionedontextde- scriptions.Wedecomposethehardproblemintomoreman-  ageablesub-problemsthroughasketch-reÞnementprocess.  TheStage-IGANsketchestheprimitiveshapeandcolors  oftheobjectbasedonthegiventextdescription,yield-  ingStage-Ilow-resolutionimages.TheStage-IIGANtakes  Stage-Iresultsandtextdescriptionsasinputs,andgener-  ateshigh-resolutionimageswithphoto-realisticdetails.It  isabletorectifydefectsinStage-Iresultsandaddcom-  pellingdetailswiththereÞnementprocess.Toimprovethe  diversityofthesynthesizedimagesandstabilizethetraining  oftheconditional-GAN,weintroduceanovelConditioning  Augmentationtechniquethatencouragessmoothnessinthe  latentconditioningmanifold.Extensiveexperimentsand  comparisonswithstate-of-the-artsonbenchmarkdatasets  demonstratethattheproposedmethodachievessigniÞcant  improvementsongeneratingphoto-realisticimagescondi-  tionedontextdescriptions.  1.Introduction Generatingphoto-realisticimagesfromtextisanim- portantproblemandhastremendousapplications,includ-  ingphoto-editing,computer-aideddesign, etc.Recently, GenerativeAdversarialNetworks(GAN)[ 8,5,23]have shownpromisingresultsinsynthesizingreal-worldim-  ages.Conditionedongiventextdescriptions,conditional- !"#$%&#'(%#$%)"#*+%  )#*"%$,-+%&./01%,2%  #*$%"+/(%/2(%)#23$4%  /2(%"/$%/%.,23%  ,'/23+%&+/1% !"#$%&#'(%"/$%/%  5+..,)%&+..5%/2(%  */'$6$4%3'+5%&/014%  )#23$4%/2(%&',)2%  *"',/*4%2/7+%)#*"%  /%&./01%8/0+% !"#$%8.,)+'%"/$%  ,9+'./77#23%7#21%  7,#2*+(%7+*/.$%  $6'',62(#23%/%'#23%  ,8%$",'*%5+..,)%  8#./-+2*$% :/;%<*/01=>?%  %%%%%%<*/3+@A%%%%%%%  %%%%%%BCDBC%%%%%%  %%%%%%#-/3+$%  % :&;%<*/01=>?%  %%%%%%<*/3+@AA%%  %%%%%%EFBDEFB%  %%%%%%#-/3+$%  :0;%G/2#../%=>?%%  %%%%%EFBDEFB%  %%%%%#-/3+$% Figure1.ComparisonoftheproposedStackGANandavanilla  one-stageGANforgenerating256 !256images.(a)Giventext descriptions,Stage-IofStackGANsketchesroughshapesandba-  siccolorsofobjects,yieldinglow-resolutionimages.(b)Stage-II  ofStackGANtakesStage-Iresultsandtextdescriptionsasinputs,  andgenerateshigh-resolutionimageswithphoto-realisticdetails.  (c)Resultsbyavanilla256 !256GANwhichsimplyaddsmore upsamplinglayerstostate-of-the-artGAN-INT-CLS[ 26].Itisun- abletogenerateanyplausibleimagesof256 !256resolution. GANs[ 26,24]areabletogenerateimagesthatarehighly relatedtothetextmeanings. However,itisverydifÞculttotrainGANtogenerate high-resolutionphoto-realisticimagesfromtextdescrip-  tions.Simplyaddingmoreupsamplinglayersinstate-of-  the-artGANmodelsforgeneratinghigh-resolution( e.g .,256!256)imagesgenerallyresultsintraininginstability 15907 andproducesnonsensicaloutputs(seeFigure 1(c)).The maindifÞcultyforgeneratinghigh-resolutionimagesby  GANsisthatsupportsofnaturalimagedistributionandim-  pliedmodeldistributionmaynotoverlapinhighdimen-  sionalpixelspace[ 31,1].Thisproblemismoresevere astheimageresolutionincreases.Reed etal .onlysuc- ceededingeneratingplausible64 !64imagesconditioned ontextdescriptions[ 26],whichusuallylackdetailsand vividobjectparts, e.g .,beaksandeyesofbirds.More- over,theywereunabletosynthesizehigherresolution( e.g .,128!128)imageswithoutprovidingadditionalannotations ofobjects[ 24].Inanalogytohowhumanpaintersdraw,wedecompose theproblemoftexttophoto-realisticimagesynthesisinto  twomoretractablesub-problemswithStackedGenerative  AdversarialNetworks(StackGAN).Low-resolutionimages  areÞrstgeneratedbyourStage-IGAN(seeFigure 1(a)).On thetopofourStage-IGAN,westackStage-IIGANtogen-  eraterealistichigh-resolution( e.g .,256 !256)imagescon- ditionedonStage-Iresultsandtextdescriptions(seeFig-  ure1(b)).ByconditioningontheStage-Iresultandthe textagain,Stage-IIGANlearnstocapturethetextinfor-  mationthatisomittedbyStage-IGANanddrawsmorede-  tailsfortheobject.Thesupportofmodeldistributiongener-  atedfromaroughlyalignedlow-resolutionimagehasbetter  probabilityofintersectingwiththesupportofimagedistri-  bution.ThisistheunderlyingreasonwhyStage-IIGANis  abletogeneratebetterhigh-resolutionimages. Inaddition,forthetext-to-imagegenerationtask,the limitednumberoftrainingtext-imagepairsoftenresultsin  sparsityinthetextconditioningmanifoldandsuchspar-  sitymakesitdifÞculttotrainGAN.Thus,weproposea  novelConditioningAugmentationtechniquetoencourage  smoothnessinthelatentconditioningmanifold.Itallows  smallrandomperturbationsintheconditioningmanifold  andincreasesthediversityofsynthesizedimages. Thecontributionoftheproposedmethodisthreefold: (1)WeproposeanovelStackedGenerativeAdversar-  ialNetworksforsynthesizing photo-realistic imagesfrom textdescriptions.ItdecomposesthedifÞcultproblem  ofgeneratinghigh-resolutionimagesintomoremanage-  ablesubproblemsandsigniÞcantlyimprovethestateof  theart.TheStackGANfortheÞrsttimegeneratesim-  agesof256 !256resolutionwithphoto-realisticdetails fromtextdescriptions.(2)AnewConditioningAugmen-  tationtechniqueisproposedtostabilizetheconditional  GANtrainingandalsoimprovesthediversityofthegen-  eratedsamples.(3)Extensivequalitativeandquantitative  experimentsdemonstratetheeffectivenessoftheoverall  modeldesignaswellastheeffectsofindividualcompo-  nents,whichprovideusefulinformationfordesigningfu-  tureconditionalGANmodels.Ourcodeisavailableat https://github.com/hanzhanggit/StackGAN. 2.RelatedWork Generativeimagemodelingisafundamentalproblemin computervision.Therehasbeenremarkableprogressin  thisdirectionwiththeemergenceofdeeplearningtech-  niques.VariationalAutoencoders(VAE)[ 13,28]for- mulatedtheproblemwithprobabilisticgraphicalmodels  whosegoalwastomaximizethelowerboundofdatalike-  lihood.Autoregressivemodels( e.g .,PixelRNN)[ 33]that utilizedneuralnetworkstomodeltheconditionaldistri-  butionofthepixelspacehavealsogeneratedappealing  syntheticimages.Recently,GenerativeAdversarialNet-  works(GAN)[ 8]haveshownpromisingperformancefor generatingsharperimages.Buttraininginstabilitymakes  ithardforGANmodelstogeneratehigh-resolution( e.g .,256!256)images.Severaltechniques[ 23,29,18,1,3]havebeenproposedtostabilizethetrainingprocessand  generatecompellingresults.Anenergy-basedGAN[ 38]hasalsobeenproposedformorestabletrainingbehavior. Builtuponthesegenerativemodels,conditionalimage generationhasalsobeenstudied.Mostmethodsutilized  simpleconditioningvariablessuchasattributesorclassla-  bels[ 37,34,4,22].Thereisalsoworkconditionedonim- agestogenerateimages,includingphotoediting[ 2,39],do- maintransfer[ 32,12]andsuper-resolution[ 31,15].How- ever,super-resolutionmethods[ 31,15]canonlyaddlimited detailstolow-resolutionimagesandcannotcorrectlarge  defectsasourproposedStackGANdoes.Recently,several  methodshavebeendevelopedtogenerateimagesfromun-  structuredtext.Mansimov etal .[17]builtanAlignDRAW modelbylearningtoestimatealignmentbetweentextand  thegeneratingcanvas.Reed etal .[27]usedconditionalPix- elCNNtogenerateimagesusingthetextdescriptionsand  objectlocationconstraints.Nguyen etal .[20]usedanap- proximateLangevinsamplingapproachtogenerateimages  conditionedontext.However,theirsamplingapproachre-  quiresaninefÞcientiterativeoptimizationprocess.With  conditionalGAN,Reed etal .[26]successfullygenerated plausible64 !64imagesforbirdsandßowersbasedontext descriptions.Theirfollow-upwork[ 24]wasabletogener- ate128 !128imagesbyutilizingadditionalannotationson objectpartlocations. BesidesusingasingleGANforgeneratingimages,there isalsowork[ 36,5,10]thatutilizedaseriesofGANsforim- agegeneration.Wang etal .[36]factorizedtheindoorscene generationprocessintostructuregenerationandstylegen-  erationwiththeproposed S2-GAN.Incontrast,thesecond stageofourStackGANaimstocompleteobjectdetailsand  correctdefectsofStage-Iresultsbasedontextdescriptions.  Dentonetal .[5]builtaseriesofGANswithinaLapla- cianpyramidframework.Ateachlevelofthepyramid,a  residualimagewasgeneratedconditionedontheimageof  thepreviousstageandthenaddedbacktotheinputimage  toproducetheinputforthenextstage.Concurrenttoour 5908 work,Huang etal .[10]alsoshowedthattheycangenerate betterimagesbystackingseveralGANstoreconstructthe  multi-levelrepresentationsofapre-traineddiscriminative  model.However,theyonlysucceededingenerating32 !32images,whileourmethodutilizesasimplerarchitectureto  generate256 !256imageswithphoto-realisticdetailsand sixty-fourtimesmorepixels.  3.StackedGenerativeAdversarialNetworks Togeneratehigh-resolutionimageswithphoto-realistic details,weproposeasimpleyeteffectiveStackedGenera-  tiveAdversarialNetworks.Itdecomposesthetext-to-image  generativeprocessintotwostages(seeFigure 2).-Stage-IGAN: itsketchestheprimitiveshapeandba- siccolorsoftheobjectconditionedonthegiventext  description,anddrawsthebackgroundlayoutfroma  randomnoisevector,yieldingalow-resolutionimage. -Stage-IIGAN: itcorrectsdefectsinthelow-resolution imagefromStage-Iandcompletesdetailsoftheobject  byreadingthetextdescriptionagain,producingahigh-  resolutionphoto-realisticimage. 3.1.Preliminaries GenerativeAdversarialNetworks(GAN)[ 8]arecom- posedoftwomodelsthatarealternativelytrainedtocom-  petewitheachother.Thegenerator Gisoptimizedtore- producethetruedatadistribution pdatabygeneratingim- agesthataredifÞcultforthediscriminator Dtodifferentiate fromrealimages.Meanwhile, Disoptimizedtodistinguish realimagesandsyntheticimagesgeneratedby G.Overall, thetrainingprocedureissimilartoatwo-playermin-max  gamewiththefollowingobjectivefunction, min Gmax DV(D,G )=Ex!pdata[log D(x)]+ Ez!pz[log(1 "D(G(z)))],(1)wherexisarealimagefromthetruedatadistribution pdata,andzisanoisevectorsampledfromdistribution pz(e.g .,uniformorGaussiandistribution). ConditionalGAN[ 7,19]isanextensionofGANwhere boththegeneratoranddiscriminatorreceiveadditionalcon-  ditioningvariables c,yielding G(z,c )andD(x,c ).This formulationallows Gtogenerateimagesconditionedon variables c.3.2.ConditioningAugmentation AsshowninFigure 2,thetextdescription tisÞrsten- codedbyanencoder,yieldingatextembedding !t.In previousworks[ 26,24],thetextembeddingisnonlinearly transformedtogenerateconditioninglatentvariablesasthe  inputofthegenerator.However,latentspaceforthetext  embeddingisusuallyhighdimensional( >100dimen-sions).Withlimitedamountofdata,itusuallycausesdis-  continuityinthelatentdatamanifold,whichisnotdesirable forlearningthegenerator.Tomitigatethisproblem,we  introduceaConditioningAugmentationtechniquetopro-  duceadditionalconditioningvariables öc.Incontrasttothe Þxedconditioningtextvariable cin[ 26,24],werandomly samplethelatentvariables öcfromanindependentGaussian distribution N(µ(!t),!(!t)),wherethemean µ(!t)anddiagonalcovariancematrix !(!t)arefunctionsofthetext embedding!t.TheproposedConditioningAugmentation yieldsmoretrainingpairsgivenasmallnumberofimage-  textpairs,andthusencouragesrobustnesstosmallpertur-  bationsalongtheconditioningmanifold.Tofurtherenforce  thesmoothnessovertheconditioningmanifoldandavoid  overÞtting[ 6,14],weaddthefollowingregularizationterm totheobjectiveofthegeneratorduringtraining, DKL(N(µ(!t),!(!t))||N (0,I)),(2)whichistheKullback-Leiblerdivergence(KLdivergence)  betweenthestandardGaussiandistributionandthecondi-  tioningGaussiandistribution.Therandomnessintroduced  intheConditioningAugmentationisbeneÞcialformodel-  ingtexttoimagetranslationasthesamesentenceusually  correspondstoobjectswithvariousposesandappearances.  3.3.Stage-IGAN Insteadofdirectlygeneratingahigh-resolutionimage conditionedonthetextdescription,wesimplifythetaskto  Þrstgeneratealow-resolutionimagewithourStage-IGAN,  whichfocusesondrawingonlyroughshapeandcorrectcol-  orsfortheobject. Let!tbethetextembeddingofthegivendescription, whichisgeneratedbyapre-trainedencoder[ 25]inthispa- per.TheGaussianconditioningvariables öc0fortextembed- dingaresampledfrom N(µ0(!t),!0(!t))tocapturethe meaningof !twithvariations.Conditionedon öc0andran- domvariable z,Stage-IGANtrainsthediscriminator D0andthegenerator G0byalternativelymaximizing LD0inEq.( 3)andminimizing LG0inEq.( 4),LD0=E(I0,t)!pdata[log D0(I0,!t)]+ Ez!pz,t!pdata[log(1 "D0(G0(z,öc0),!t))],(3)LG0=Ez!pz,t!pdata[log(1 "D0(G0(z,öc0),!t))]+ "DKL(N(µ0(!t),!0(!t))||N (0,I)),(4)wheretherealimage I0andthetextdescription tarefrom thetruedatadistribution pdata.zisanoisevectorrandomly sampledfromagivendistribution pz(Gaussiandistribution inthispaper). "isaregularizationparameterthatbalances thetwotermsinEq.( 4).Weset "=1forallourex- periments.Usingthereparameterizationtrickintroduced  in[ 13],both µ0(!t)and!0(!t)arelearnedjointlywiththe restofthenetwork. ModelArchitecture. Forthegenerator G0,toobtain textconditioningvariable öc0,thetextembedding !tisÞrst 5909 Figure2.ThearchitectureoftheproposedStackGAN.TheStage-Igeneratordrawsalow-resolutionimagebysketchingroughshapeand  basiccolorsoftheobjectfromthegiventextandpaintingthebackgroundfromarandomnoisevector.ConditionedonStage-Iresults,the  Stage-IIgeneratorcorrectsdefectsandaddscompellingdetailsintoStage-Iresults,yieldingamorerealistichigh-resolutionimage.  fedintoafullyconnectedlayertogenerate µ0and#0(#0arethevaluesinthediagonalof !0)fortheGaussiandistri- bution N(µ0(!t),!0(!t)).öc0arethensampledfromthe Gaussiandistribution.Our Ngdimensionalconditioning vector öc0iscomputedby öc0=µ0+#0#$(where#istheelement-wisemultiplication, $$N(0,I)).Then, öc0isconcatenatedwitha Nzdimensionalnoisevectortogener- atea W0!H0imagebyaseriesofup-samplingblocks. Forthediscriminator D0,thetextembedding !tisÞrst compressedto Nddimensionsusingafully-connectedlayer andthenspatiallyreplicatedtoforma Md!Md!Ndtensor.Meanwhile,theimageisfedthroughaseriesof  down-samplingblocksuntilithas Md!Mdspatialdimen- sion.Then,theimageÞltermapisconcatenatedalongthe  channeldimensionwiththetexttensor.Theresultingten-  sorisfurtherfedtoa1 !1convolutionallayertojointly learnfeaturesacrosstheimageandthetext.Finally,afully-  connectedlayerwithonenodeisusedtoproducethedeci-  sionscore.  3.4.Stage-IIGAN Low-resolutionimagesgeneratedbyStage-IGANusu- allylackvividobjectpartsandmightcontainshapedistor-  tions.Somedetailsinthetextmightalsobeomittedinthe  Þrststage,whichisvitalforgeneratingphoto-realisticim-  ages.OurStage-IIGANisbuiltuponStage-IGANresults  togeneratehigh-resolutionimages.Itisconditionedon  low-resolutionimagesandalsothetextembeddingagainto  correctdefectsinStage-Iresults.TheStage-IIGANcom-  pletespreviouslyignoredtextinformationtogeneratemore  photo-realisticdetails. Conditioningonthelow-resolutionresult s0=G0(z,öc0)andGaussianlatentvariables öc,thediscriminator Dandgenerator GinStage-IIGANaretrainedbyalter- nativelymaximizing LDinEq.( 5)andminimizing LGinEq.( 6),LD=E(I,t )!pdata[log D(I,! t)]+ Es0!pG0,t!pdata[log(1 "D(G(s0,öc),!t))],(5)LG=Es0!pG0,t!pdata[log(1 "D(G(s0,öc),!t))]+ "DKL(N(µ(!t),!(!t))||N (0,I)),(6)DifferentfromtheoriginalGANformulation,therandom  noisezisnotusedinthisstagewiththeassumptionthat therandomnesshasalreadybeenpreservedby s0.Gaus- sianconditioningvariables öcusedinthisstageand öc0usedinStage-IGANsharethesamepre-trainedtextencoder,  generatingthesametextembedding !t.However,Stage- IandStage-IIConditioningAugmentationhavedifferent  fullyconnectedlayersforgeneratingdifferentmeansand  standarddeviations.Inthisway,Stage-IIGANlearnsto  captureusefulinformationinthetextembeddingthatis  omittedbyStage-IGAN. ModelArchitecture. WedesignStage-IIgeneratoras anencoder-decodernetworkwithresidualblocks[ 9].Sim- ilartothepreviousstage,thetextembedding !tisused togeneratethe Ngdimensionaltextconditioningvector öc,whichisspatiallyreplicatedtoforma Mg!Mg!Ngtensor. Meanwhile,theStage-Iresult s0generatedbyStage-IGAN isfedintoseveraldown-samplingblocks( i.e.,encoder)un- tilithasaspatialsizeof Mg!Mg.Theimagefeatures andthetextfeaturesareconcatenatedalongthechanneldi-  mension.Theencodedimagefeaturescoupledwithtext  featuresarefedintoseveralresidualblocks,whicharede-  signedtolearnmulti-modalrepresentationsacrossimage  andtextfeatures.Finally,aseriesofup-samplinglayers 5910 (i.e.,decoder)areusedtogeneratea W!Hhigh-resolutionimage.Suchageneratorisabletohelprectifydefectsinthe  inputimagewhileaddmoredetailstogeneratetherealistic  high-resolutionimage. Forthediscriminator,itsstructureissimilartothatof Stage-Idiscriminatorwithonlyextradown-samplingblocks  sincetheimagesizeislargerinthisstage.Toexplicitlyen-  forceGANtolearnbetteralignmentbetweentheimageand  theconditioningtext,ratherthanusingthevanilladiscrimi-  nator,weadoptthematching-awarediscriminatorproposed  byReed etal .[26]forbothstages.Duringtraining,the discriminatortakesrealimagesandtheircorrespondingtext  descriptionsaspositivesamplepairs,whereasnegativesam-  plepairsconsistoftwogroups.TheÞrstisrealimageswith  mismatchedtextembeddings,whilethesecondissynthetic  imageswiththeircorrespondingtextembeddings.  3.5.Implementationdetails Theup-samplingblocksconsistofthenearest-neighbor upsamplingfollowedbya3 !3stride1convolution.Batch normalization[ 11]andReLUactivationareappliedafter everyconvolutionexceptthelastone.Theresidualblocks  consistof3 !3stride1convolutions,Batchnormalization andReLU.Tworesidualblocksareusedin128 !128Stack- GANmodelswhilefourareusedin256 !256models.The down-samplingblocksconsistof4 !4stride2convolutions, BatchnormalizationandLeakyReLU,exceptthattheÞrst  onedoesnothaveBatchnormalization. Bydefault, Ng=128 ,Nz=100 ,Mg=16 ,Md=4,Nd=128 ,W0=H0=64 andW=H=256 .Fortrain- ing,weÞrstiterativelytrain D0andG0ofStage-IGAN for600epochsbyÞxingStage-IIGAN.Thenweiteratively  trainDandGofStage-IIGANforanother600epochsby ÞxingStage-IGAN.AllnetworksaretrainedusingADAM  solverwithbatchsize64andaninitiallearningrateof  0.0002.Thelearningrateisdecayedto 1/2ofitsprevious valueevery100epochs.  4.Experiments Tovalidateourmethod,weconductextensivequantita- tiveandqualitativeevaluations.Twostate-of-the-artmeth-  odsontext-to-imagesynthesis,GAN-INT-CLS[ 26]and GAWWN[ 24],arecompared.Resultsbythetwocompared methodsaregeneratedusingthecodereleasedbytheirau-  thors.Inaddition,wedesignseveralbaselinemodelsto  investigatetheoveralldesignandimportantcomponentsof  ourproposedStackGAN.FortheÞrstbaseline,wedirectly  trainStage-IGANforgenerating64 !64and256 !256im- agestoinvestigatewhethertheproposedstackedstructure  andConditioningAugmentationarebeneÞcial.Thenwe  modifyourStackGANtogenerate128 !128and256 !256imagestoinvestigatewhetherlargerimagesbyourmethod  resultinhigherimagequality.Wealsoinvestigatewhether  inputtingtextatbothstagesofStackGANisuseful. 4.1.Datasetsandevaluationmetrics CUB[ 35]contains200birdspecieswith11,788images. Since80%ofbirdsinthisdatasethaveobject-imagesize  ratiosoflessthan0.5[ 35],asapre-processingstep,we cropallimagestoensurethatboundingboxesofbirdshave  greater-than-0.75object-imagesizeratios.Oxford-102[ 21]contains8,189imagesofßowersfrom102differentcat-  egories.Toshowthegeneralizationcapabilityofourap-  proach,amorechallengingdataset,MSCOCO[ 16]isalso utilizedforevaluation.DifferentfromCUBandOxford-  102,theMSCOCOdatasetcontainsimageswithmultiple  objectsandvariousbackgrounds.Ithasatrainingsetwith  80kimagesandavalidationsetwith40kimages.Each  imageinCOCOhas5descriptions,while10descriptions  areprovidedby[ 25]foreveryimageinCUBandOxford- 102datasets.Followingtheexperimentalsetupin[ 26],wedirectlyusethetrainingandvalidationsetsprovided  byCOCO,meanwhilewesplitCUBandOxford-102into  class-disjointtrainingandtestsets. Evaluationmetrics. ItisdifÞculttoevaluatetheper- formanceofgenerativemodels( e.g .,GAN).Wechoosea recentlyproposednumericalassessmentapproachÒincep-  tionscoreÓ[ 29]forquantitativeevaluation, I=exp( ExDKL(p(y|x)||p(y))),(7)wherexdenotesonegeneratedsample,and yisthelabel predictedbytheInceptionmodel[ 30].Theintuitionbehind thismetricisthatgoodmodelsshouldgeneratediversebut  meaningfulimages.Therefore,theKLdivergencebetween  themarginaldistribution p(y)andtheconditionaldistribu- tionp(y|x)shouldbelarge.Inourexperiments,wedirectly usethepre-trainedInceptionmodelforCOCOdataset.For  Þne-graineddatasets,CUBandOxford-102,weÞne-tune  anInceptionmodelforeachofthem.Assuggestedin[ 29],weevaluatethismetriconalargenumberofsamples( i.e.,30krandomlyselectedsamples)foreachmodel. Althoughtheinceptionscorehasshowntowellcorrelate withhumanperceptiononvisualqualityofsamples[ 29],it cannotreßectwhetherthegeneratedimagesarewellcon-  ditionedonthegiventextdescriptions.Therefore,wealso  conducthumanevaluation.Werandomlyselect50textde-  scriptionsforeachclassofCUBandOxford-102testsets.  ForCOCOdataset,4ktextdescriptionsarerandomlyse-  lectedfromitsvalidationset.Foreachsentence,5im-  agesaregeneratedbyeachmodel.Giventhesametextde-  scriptions,10users(notincludinganyoftheauthors)are  askedtoranktheresultsbydifferentmethods.Theaverage  ranksbyhumanusersarecalculatedtoevaluateallcom-  paredmethods.  4.2.Quantitativeandqualitativeresults Wecompareourresultswiththestate-of-the-arttext-to- imagemethods[ 24,26]onCUB,Oxford-102andCOCO 5911 !"#$!"#% &'(()% "*+$"*+% ,-./0&')% 12$-% 324/567-689% +:$+:% &');<)1;=>,% 1?64%4@.AA%B653%  ?.4%.%C?6-2%  B52.4-D%A6E?-%  E52F%?2.3D%.93%  BA./0%C69E4%  .93%-.6A% '%B653%C6-?%.%  @236G@%85.9E2%  B6AA%C?6-2%B83F%  E5.F%C69E4%.93%  C2BB23%H22-%% '%4@.AA%F2AA8C%  B653%C6-?%.%  BA./0%/58C9%  .93%.%4?85-%  BA./0%7869-23%  B2.0% '%4@.AA%B653%  C6-?%I.5F69E%  4?.324%8H%  B58C9%C6-?%  C?6-2%G9325%-?2%  2F24% 1?2%B653%64%  4?85-%.93%  4-GBBF%C6-?%  F2AA8C%89%6-4%  B83F% 1?64%B653%64%523%  .93%B58C9%69%  /8A85D%C6-?%.%  4-GBBF%B2.0% 1?64%4@.AA%  BA./0%B653%?.4%  .%4?85-D%4A6E?-AF%  /G5I23%B6AA%.93%  A89E%A2E4% Figure3.ExampleresultsbyourStackGAN,GAWWN[ 24],andGAN-INT-CLS[ 26]conditionedontextdescriptionsfromCUBtestset. !"#$!"#% &'()*+,-% ./$'% 0/1)234'356% #7$#7% +,-89-.8:;&% .<31%=>5?/2%<(1%  (%>5'%5=%1@(>>%  4A24>/%4/'(>1%36%  (%05@/8>3*/%  )56=3BA2('356% .<31%=>5?/2%31%  436*C%?<3'/C%  (60%D/>>5?%36%  )5>52C%(60%<(1%  4/'(>1%'<('%(2/%  1'234/0%% .<31%=>5?/2%31%  ?<3'/%(60%  D/>>5?%36%)5>52C%  ?3'<%4/'(>1%'<('%  (2/%?(ED%(60%  1@55'<% .<31%=>5?/2%<(1%  4/'(>1%'<('%(2/%  0(2*%436*%?3'<%  ?<3'/%/0B/1%  (60%436*%  1'(@/6%% FBB1%=2A3'%  )(60D%6A'1%  (60%@/('%  1/2E/0%56%  ?<3'/%031<% ,%1'2//'%13B6%  56%(%1'54>3B<'%  45>/%36%'</%  @300>/%5=%(%  0(D% ,%B25A4%5=%  4/54>/%56%1*31%  1'(60%36%'</%  165?% ,%43)'A2/%5=%(%  E/2D%)>/(6%  >3E36B%255@% Figure4.ExampleresultsbyourStackGANandGAN-INT-CLS[ 26]conditionedontextdescriptionsfromOxford-102testset(leftmost fourcolumns)andCOCOvalidationset(rightmostfourcolumns). MetricDatasetGAN-INT-CLS GAWWN OurStackGAN Inception scoreCUB2.88±.043.62±.073.70±.04Oxford2.66±.03/3.20±.01COCO7.88±.07/8.45±.03Human rankCUB2.81±.031.99±.041.37±.02Oxford1.87±.03/1.13±.03COCO1.89±.04/1.11±.03Table1.InceptionscoresandaveragehumanranksofourStack-  GAN,GAWWN[ 24],andGAN-INT-CLS[ 26]onCUB,Oxford- 102,andMS-COCOdatasets.  datasets.Theinceptionscoresandaveragehumanranks  forourproposedStackGANandcomparedmethodsarere-  portedinTable 1.Representativeexamplesarecomparedin Figure3andFigure 4.OurStackGANachievesthebestinceptionscoreandav- eragehumanrankonallthreedatasets.Comparedwith  GAN-INT-CLS[ 26],StackGANachieves28.47%improve- mentintermsofinceptionscoreonCUBdataset(from2.88  to3.70),and20.30%improvementonOxford-102(from  2.66to3.20).ThebetteraveragehumanrankofourStack-  GANalsoindicatesourproposedmethodisabletogenerate  morerealisticsamplesconditionedontextdescriptions. AsshowninFigure 3,the64 !64samplesgeneratedby GAN-INT-CLS[ 26]canonlyreßectthegeneralshapeand colorofthebirds.Theirresultslackvividparts( e.g .,beak andlegs)andconvincingdetailsinmostcases,whichmake  themneitherrealisticenoughnorhavesufÞcientlyhighres-  olution.Byusingadditionalconditioningvariablesonloca- 5912 !"#$%&'( )*#$%+( !"#$%&''( )*#$%+( ,%-"( .%+/0)1")23( ,4)+(5)0.()+(  567%(8)"4(84)"%(  #3.(4#+(#(9%0:(  +420"(5%#;( ,4)+(5)0.(4#+(  8)3$+("4#"(#0%(  50283(#3.(4#+(  #(:%6628(5%66:( ,4)+(5)0.()+(  84)"%<(56#/;<(  #3.(50283()3(  /2620<(8)"4(#(  50283(5%#;( =(84)"%(5)0.(  8)"4(#(56#/;(  /0283(#3.(  :%6628(5%#;( ,4)+()+(#(+*#66<(  56#/;(5)0.(8)"4(  #(84)"%(50%#+"(  #3.(84)"%(23(  "4%(8)3$5#0+>( ,4%(5)0.(4#+(  +*#66(5%#;<(  8)"4(0%..)+4(  50283(/0283(  #3.($0#:(5%66:( ,4)+(5)0.()+(  84)"%(56#/;(#3.(  :%6628()3(/2620<(  8)"4(#(+420"(  56#/;(5%#;( Figure5.SamplesgeneratedbyourStackGANfromunseentextsinCUBtestset.Eachcolumnliststhetextdescription,imagesgenerated  fromthetextbyStage-IandStage-IIofStackGAN. !"#$%&$'($)*%&$"+,-.()%/(.0%*('"&"&+%)$*)% 10'+$)% +$&$('*$2%/(.0% *$3*%"&%*$)*%)$*) Figure6.Forgeneratedimages(column1),retrievingtheirnearest  trainingimages(columns2-6)byutilizingStage-IIdiscriminator  Dtoextractvisualfeatures.The L2distancesbetweenfeatures arecalculatedfornearest-neighborretrieval.  tionconstraints,GAWWN[ 24]obtainsabetterinception scoreonCUBdataset,whichisstillslightlylowerthan  ours.Itgenerateshigherresolutionimageswithmorede-  tailsthanGAN-INT-CLS,asshowninFigure 3.However, asmentionedbyitsauthors,GAWWNfailstogenerateany  plausibleimageswhenitisonlyconditionedontextde-  scriptions[ 24].Incomparison,ourStackGANcangener- ate256 !256photo-realisticimagesfromonlytextdescrip- tions.Figure5illustratessomeexamplesoftheStage-Iand Stage-IIimagesgeneratedbyourStackGAN.Asshown  intheÞrstrowofFigure 5,inmostcases,Stage-IGAN isabletodrawroughshapesandcolorsofobjectsgiven  textdescriptions.However,Stage-Iimagesareusually  blurrywithvariousdefectsandmissingdetails,especially  forforegroundobjects.Asshowninthesecondrow,Stage-  IIGANgenerates4 !higherresolutionimageswithmore convincingdetailstobetterreßectcorrespondingtextde-  scriptions.ForcaseswhereStage-IGANhasgenerated  plausibleshapesandcolors,Stage-IIGANcompletesthe  details.Forinstance,inthe 1stcolumnofFigure 5,witha satisfactoryStage-Iresult,Stage-IIGANfocusesondraw- ingtheshortbeakandwhitecolordescribedinthetextas  wellasdetailsforthetailandlegs.Inallotherexamples,  differentdegreesofdetailsareaddedtoStage-IIimages.In  manyothercases,Stage-IIGANisabletocorrectthede-  fectsofStage-Iresultsbyprocessingthetextdescription  again.Forexample,whiletheStage-Iimageinthe 5thcol- umnhasabluecrownratherthanthereddishbrowncrown  describedinthetext,thedefectiscorrectedbyStage-II  GAN.Insomeextremecases( e.g .,the 7thcolumnofFig- ure5),evenwhenStage-IGANfailstodrawaplausible shape,Stage-IIGANisabletogeneratereasonableobjects.  WealsoobservethatStackGANhastheabilitytotransfer  backgroundfromStage-IimagesandÞne-tunethemtobe  morerealisticwithhigherresolutionatStage-II. Importantly,theStackGANdoesnotachievegoodre- sultsbysimplymemorizingtrainingsamplesbutbycap-  turingthecomplexunderlyinglanguage-imagerelations.  Weextractvisualfeaturesfromourgeneratedimagesand  alltrainingimagesbytheStage-IIdiscriminator Dofour StackGAN.Foreachgeneratedimage,itsnearestneighbors  fromthetrainingsetcanberetrieved.Byvisuallyinspect-  ingtheretrievedimages(seeFigure 6),wecanconclude thatthegeneratedimageshavesomesimilarcharacteristics  withthetrainingsamplesbutareessentiallydifferent.  4.3.Componentanalysis Inthissubsection,weanalyzedifferentcomponentsof StackGANonCUBdatasetwithourbaselinemodels.The  inceptionscoresforthosebaselinesarereportedinTable 2.ThedesignofStackGAN. AsshownintheÞrstfour rowsofTable 2,ifStage-IGANisdirectlyusedtogenerate images,theinceptionscoresdecreasesigniÞcantly.Such  performancedropcanbewellillustratedbyresultsinFig-  ure7.AsshownintheÞrstrowofFigure 7,Stage-IGAN failstogenerateanyplausible256 !256sampleswithout 5913 !"#$%&&"'()*"+(,-"%"'&%./"-0%*"%1*" +(12#"%1*"30%,4)0#"2)05"+(12#" 6789678":,%20;<"=!>" +(,-?4,"@!" 6789678":,%20;<"=!>" +(,-"@!" 6789678":,%./=!>" +(,-"@!A" B09,",+(.0" B-(#"'()*"(#".?$C&0,0&5")0*"+(,-"'&%./" +(12#"%1*"C?(1,5"'0%/" Figure7.ConditioningAugmentation(CA)helpsstabilizethe  trainingofconditionalGANandimprovesthediversityofthegen-  eratedsamples.(Row1)withoutCA,Stage-IGANfailstogen-  erateplausible256 !256samples.Althoughdifferentnoisevector zisusedforeachcolumn,thegeneratedsamplescollapsetobe thesameforeachinputtextdescription.(Row2-3)withCAbut  Þxingthenoisevectors z,methodsarestillabletogeneratebirds withdifferentposesandviewpoints. MethodCATexttwice Inceptionscore 64"64Stage-IGAN no/2.66±.03yes/2.95±.02256"256Stage-IGAN no/2.48±.00yes/3.02±.01128"128StackGAN yesno3.13±.03noyes3.20±.03yesyes3.35±.02256"256StackGAN yesno3.45±.02noyes3.31±.03yesyes3.70±.04Table2.Inceptionscorescalculatedwith30,000samplesgener-  atedbydifferentbaselinemodelsofourStackGAN.  usingConditioningAugmentation(CA).AlthoughStage-I  GANwithCAisabletogeneratemorediverse256 !256samples,thosesamplesarenotasrealisticassamplesgen-  eratedbyStackGAN.Itdemonstratesthenecessityofthe  proposedstackedstructure.Inaddition,bydecreasingthe  outputresolutionfrom256 !256to128 !128,theinception scoredecreasesfrom3.70to3.35.Notethatallimagesare  scaledto299 !299beforecalculatingtheinceptionscore. Thus,ifourStackGANjustincreasestheimagesizewithout  addingmoreinformation,theinceptionscorewouldremain  thesameforsamplesofdifferentresolutions.Therefore,the  decreaseininceptionscoreby128 !128StackGANdemon- stratesthatour256 !256StackGANdoesaddmoredetails intothelargerimages.Forthe256 !256StackGAN,ifthe textisonlyinputtoStage-I(denotedasÒnoTexttwiceÓ),the  inceptionscoredecreasesfrom3.70to3.45.Itindicatesthat  processingtextdescriptionsagainatStage-IIhelpsreÞne  Stage-Iresults.Thesameconclusioncanbedrawnfrom  theresultsof128 !128StackGANmodels. ConditioningAugmentation. Wealsoinvestigatethe efÞcacyoftheproposedConditioningAugmentation(CA).  ByremovingitfromStackGAN256 !256(denotedasÒno CAÓinTable 2),theinceptionscoredecreasesfrom3.70to 3.31.Figure 7alsoshowsthat256 !256Stage-IGAN(and StackGAN)withCAcangeneratebirdswithdifferentposes !"#$%&'($&)$*+,-.#/#.0$'#($ $!"#$%&'($&)$*+,-.#/#.0$0#..+1 !!"&)$%&'($&)$*+,-.#/#.0$'#($1&/"$%.2*3$1&45)$24($-+&4/0$%#23$ $!"#$%!%&'((!)(*+!)$,-!#'%!'!%#.,"!/.$0"1!)+'2!'0-!),.30!.0!$"%!3$04%! Figure8.(Lefttoright)Imagesgeneratedbyinterpolatingtwosen-  tenceembeddings.GradualappearancechangesfromtheÞrstsen-  tenceÕsmeaningtothatofthesecondsentencecanbeobserved.  Thenoisevector zisÞxedtobezerosforeachrow. andviewpointsfromthesametextembedding.Incontrast,  withoutusingCA,samplesgeneratedby256 !256Stage- IGANcollapsetononsensicalimagesduetotheunstable  trainingdynamicsofGANs.Consequently,theproposed  ConditioningAugmentationhelpsstabilizetheconditional  GANtrainingandimprovesthediversityofthegenerated  samplesbecauseofitsabilitytoencouragerobustnessto  smallperturbationsalongthelatentmanifold. Sentenceembeddinginterpolation. Tofurtherdemon- stratethatourStackGANlearnsasmoothlatentdataman-  ifold,weuseittogenerateimagesfromlinearlyinterpo-  latedsentenceembeddings,asshowninFigure 8.WeÞxthe noisevector z,sothegeneratedimageisinferredfromthe giventextdescriptiononly.ImagesintheÞrstrowaregen-  eratedbysimplesentencesmadeupbyus.Thosesentences  containonlysimplecolordescriptions.Theresultsshow  thatthegeneratedimagesfrominterpolatedembeddings  canaccuratelyreßectcolorchangesandgenerateplausible  birdshapes.Thesecondrowillustratessamplesgenerated  frommorecomplexsentences,whichcontainmoredetails  onbirdappearances.Thegeneratedimageschangetheir  primarycolorfromredtoblue,andchangethewingcolor  fromblacktobrown.  5.Conclusions Inthispaper,weproposeStackedGenerativeAdversar- ialNetworks(StackGAN)withConditioningAugmenta-  tionforsynthesizing photo-realistic images.Theproposed methoddecomposesthetext-to-imagesynthesistoanovel  sketch-reÞnementprocess.Stage-IGANsketchestheob-  jectfollowingbasiccolorandshapeconstraintsfromgiven  textdescriptions.Stage-IIGANcorrectsthedefectsin  Stage-Iresultsandaddsmoredetails,yieldinghigherreso-  lutionimageswithbetterimagequality.Extensivequantita-  tiveandqualitativeresultsdemonstratetheeffectivenessof  ourproposedmethod.Comparedtoexistingtext-to-image  generativemodels,ourmethodgenerateshigherresolution  images( e.g .,256 !256)withmorephoto-realisticdetails anddiversity. 5914 References [1]M.ArjovskyandL.Bottou.Towardsprincipledmethodsfor traininggenerativeadversarialnetworks.In ICLR,2017. 2[2]A.Brock,T.Lim,J.M.Ritchie,andN.Weston.Neuralphoto editingwithintrospectiveadversarialnetworks.In ICLR,2017.2[3]T.Che,Y.Li,A.P.Jacob,Y.Bengio,andW.Li.Mode regularizedgenerativeadversarialnetworks.In ICLR,2017. 2[4]X.Chen,Y.Duan,R.Houthooft,J.Schulman,I.Sutskever, andP.Abbeel.Infogan:Interpretablerepresentationlearning  byinformationmaximizinggenerativeadversarialnets.In  NIPS,2016. 2[5]E.L.Denton,S.Chintala,A.Szlam,andR.Fergus.Deep generativeimagemodelsusingalaplacianpyramidofadver-  sarialnetworks.In NIPS,2015. 1,2[6]C.Doersch.Tutorialonvariationalautoencoders. arXiv:1606.05908,2016. 3[7]J.Gauthier.Conditionalgenerativeadversarialnetworksfor convolutionalfacegeneration. Technicalreport ,2015. 3[8]I.J.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu, D.Warde-Farley,S.Ozair,A.C.Courville,andY.Bengio.  Generativeadversarialnets.In NIPS,2014. 1,2,3[9]K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearning forimagerecognition.In CVPR,2016. 4[10]X.Huang,Y.Li,O.Poursaeed,J.Hopcroft,andS.Belongie. Stackedgenerativeadversarialnetworks.In CVPR,2017. 2,3[11]S.IoffeandC.Szegedy.Batchnormalization:Accelerating deepnetworktrainingbyreducinginternalcovariateshift.In  ICML,2015. 5[12]P.Isola,J.-Y.Zhu,T.Zhou,andA.A.Efros.Image-to-image translationwithconditionaladversarialnetworks.In CVPR,2017.2[13]D.P.KingmaandM.Welling.Auto-encodingvariational bayes.In ICLR,2014. 2,3[14]A.B.L.Larsen,S.K.S¿nderby,H.Larochelle,and O.Winther.Autoencodingbeyondpixelsusingalearned  similaritymetric.In ICML,2016. 3[15]C.Ledig,L.Theis,F.Huszar,J.Caballero,A.Aitken,A.Te- jani,J.Totz,Z.Wang,andW.Shi.Photo-realisticsingleim-  agesuper-resolutionusingagenerativeadversarialnetwork.  InCVPR,2017. 2[16]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra- manan,P.Dollr,andC.L.Zitnick.Microsoftcoco:Common  objectsincontext.In ECCV,2014. 5[17]E.Mansimov,E.Parisotto,L.J.Ba,andR.Salakhutdinov. Generatingimagesfromcaptionswithattention.In ICLR,2016.2[18]L.Metz,B.Poole,D.Pfau,andJ.Sohl-Dickstein.Unrolled generativeadversarialnetworks.In ICLR,2017. 2[19]M.MirzaandS.Osindero.Conditionalgenerativeadversar- ialnets. arXiv:1411.1784,2014. 3[20]A.Nguyen,J.Yosinski,Y.Bengio,A.Dosovitskiy,and J.Clune.Plug&playgenerativenetworks:Conditionaliter-  ativegenerationofimagesinlatentspace.In CVPR,2017. 2[21]M.-E.NilsbackandA.Zisserman.Automatedßowerclassi- Þcationoveralargenumberofclasses.In ICCVGIP,2008. 5[22]A.Odena,C.Olah,andJ.Shlens.Conditionalimagesynthe- siswithauxiliaryclassiÞergans.In ICML,2017. 2[23]A.Radford,L.Metz,andS.Chintala.Unsupervisedrepre- sentationlearningwithdeepconvolutionalgenerativeadver-  sarialnetworks.In ICLR,2016. 1,2[24]S.Reed,Z.Akata,S.Mohan,S.Tenka,B.Schiele,and H.Lee.Learningwhatandwheretodraw.In NIPS,2016. 1,2,3,5,6,7[25]S.Reed,Z.Akata,B.Schiele,andH.Lee.Learningdeep representationsofÞne-grainedvisualdescriptions.In CVPR,2016.3,5[26]S.Reed,Z.Akata,X.Yan,L.Logeswaran,B.Schiele,and H.Lee.Generativeadversarialtext-to-imagesynthesis.In  ICML,2016. 1,2,3,5,6[27]S.Reed,A.vandenOord,N.Kalchbrenner,V.Bapst, M.Botvinick,andN.deFreitas.Generatinginterpretable  imageswithcontrollablestructure. Technicalreport ,2016. 2[28]D.J.Rezende,S.Mohamed,andD.Wierstra.Stochastic backpropagationandapproximateinferenceindeepgenera-  tivemodels.In ICML,2014. 2[29]T.Salimans,I.J.Goodfellow,W.Zaremba,V.Cheung, A.Radford,andX.Chen.Improvedtechniquesfortraining  gans.In NIPS,2016. 2,5[30]C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna. Rethinkingtheinceptionarchitectureforcomputervision.In  CVPR,2016. 5[31]C.K.Snderby,J.Caballero,L.Theis,W.Shi,andF.Huszar. Amortisedmapinferenceforimagesuper-resolution.In  ICLR,2017. 2[32]Y.Taigman,A.Polyak,andL.Wolf.Unsupervisedcross- domainimagegeneration.In ICLR,2017. 2[33]A.vandenOord,N.Kalchbrenner,andK.Kavukcuoglu. Pixelrecurrentneuralnetworks.In ICML,2016. 2[34]A.vandenOord,N.Kalchbrenner,O.Vinyals,L.Espeholt, A.Graves,andK.Kavukcuoglu.Conditionalimagegenera-  tionwithpixelcnndecoders.In NIPS,2016. 2[35]C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. TheCaltech-UCSDBirds-200-2011Dataset.TechnicalRe-  portCNS-TR-2011-001,CaliforniaInstituteofTechnology,  2011.5[36]X.WangandA.Gupta.Generativeimagemodelingusing styleandstructureadversarialnetworks.In ECCV,2016. 2[37]X.Yan,J.Yang,K.Sohn,andH.Lee.Attribute2image:Con- ditionalimagegenerationfromvisualattributes.In ECCV,2016.2[38]J.Zhao,M.Mathieu,andY.LeCun.Energy-basedgenerative adversarialnetwork.In ICLR,2017. 2[39]J.Zhu,P.Kr ¬ahenb¬uhl,E.Shechtman,andA.A.Efros.Gen- erativevisualmanipulationonthenaturalimagemanifold.  InECCV,2016. 25915  
Multi-ReferenceTrainingwithPseudo-References forNeuralTranslationandTextGeneration RenjieZheng 1 MingboMa 1 ; 2 LiangHuang 1 ; 2 1 SchoolofEECS,OregonStateUniversity,Corvallis,OR 2 BaiduResearch,Sunnyvale,CA zheng@renj.me f cosmmb,liang.huang.sh g @gmail.com Abstract Neuraltextgeneration,includingneuralma- chinetranslation,imagecaptioning,andsum- marization,hasbeenquitesuccessfulrecently. However,duringtrainingtime,typicallyonly onereferenceisconsideredforeachexam- ple,eventhoughthereareoftenmultiplerefer- encesavailable,e.g.,4referencesinNISTMT evaluations,and5referencesinimagecap- tioningdata.Weinvestigateseveraldif- ferentwaysofutilizingmultiplehumanref- erencesduringtraining.Butmoreimpor- tantly,wethenproposeanalgorithmtogener- ateexponentiallymanypseudo-referencesby compressingexistinghumanreferences intolatticesandthentraversingthemtogener- atenewpseudo-references.Theseapproaches leadtosubstantialimprovementsoverstrong baselinesinbothmachinetranslation(+1.5 BLEU)andimagecaptioning(+3.1BLEU/ +11.7CIDEr). 1Introduction Neuraltextgenerationhasattractedmuchatten- tioninrecentyearsthankstoitsimpressivegener- ationaccuracyandwideapplicability.Inaddition todemonstratingcompellingresultsformachine translation(MT)( Sutskeveretal. , 2014 ; Bahdanau etal. , 2014 ),bysimpleadaptation,practicallyvery sameorsimilarmodelshavealsoproventobesuc- cessfulforsummarization( Rushetal. , 2015 ; Nal- lapatietal. , 2016 )andimageorvideocaptioning ( Venugopalanetal. , 2015 ; Xuetal. , 2015a ). Themostcommonneuraltextgeneration modelisbasedontheencoder-decoderframe- work( Sutskeveretal. , 2014 )whichgeneratesa variable-lengthoutputsequenceusinganRNN- baseddecoderwithattentionmechanisms( Bah- danauetal. , 2014 ; Xuetal. , 2015b ).Thereare manyrecenteffortsinimprovingthegeneration accuracy,e.g.,ConvS2S( Gehringetal. , 2017 )and Transformer( Vaswanietal. , 2017 ).However,all theseeffortsarelimitedtotrainingwithasingle referenceevenwhenmultiplereferencesareavail- able. Multiplereferencesareessentialforevaluation duetothenon-uniquenessoftranslationandgen- erationunliketasks.InMT,even thoughthetrainingsetsareusuallywithsin- glereference(bitext),theevaluationsetsoften comewithmultiplereferences.Forexample,the NISTChinese-to-EnglishandArabic-to-English MTevaluationdatasets(2003Œ2008)haveintotal around10,000Chinesesentencesand10,000Ara- bicsentenceseachwith4differentEnglishtrans- lations.Ontheotherhand,forimagecaption- ingdatasets,multiplereferencesaremorecom- monnotonlyforevaluation,butalsofortrain- ing,e.g.,theMSCOCO( Linetal. , 2014 )dataset provides5referencesperimageandPASCAL- 50SandABSTRACT-50S( Vedantametal. , 2015 ) evenprovide50referencesperimage.Canweuse theextrareferencesduringtraining?Howmuch canwefromtrainingwithmultiplerefer- ences? Wethereforeinvestigateseveraldifferent waysofutilizingexistinghuman-annotatedrefer- ences,whichincludeSampleOne( Karpathyand Fei-Fei , 2015 ),Uniform,andShufmethods(ex- plainedinSec. 2 ).AlthoughSampleOnehasbeen exploredinimagecaptioning,tothebestofour knowledge,thisisthetimethatanMTsystem istrainedwithmultiplereferences. Actually,fourorvereferencesstillcoveronly atinyfractionoftheexponentiallylargespace ofpotentialreferences( DreyerandMarcu , 2012 ). Moreimportantly,encouragedbythesuccessof trainingwithmultiplehumanreferences,wefur- therproposeaframeworktogeneratemanymore pseudo-referencesautomatically.Inparticular,we designaneuralmultiple-sequencealignmentalgo- rithmtocompressallexistinghumanreferences intoalatticebymergingsimilarwordsacrossdif- ferentreferences(seeexamplesinFig. 1 );this canbeviewedasamodern,neuralversionof paraphrasingwithmultiple-sequencealignment ( BarzilayandLee , 2003 , 2002 ).Wecanthengen- eratetheoreticallyexponentiallymorereferences fromthelattice. Wemakethefollowingmaincontributions:  Firstly,weinvestigatethreedifferentmethods formulti-referencetrainingonbothMTand imagecaptioningtasks(Section 2 ).  Secondly,weproposeanovelneural network-basedmultiplesequencealignment modeltocompresstheexistingreferences intolattices.Bytraversingtheselattices, wegenerateexponentiallymanynewpseudo- references(Section 3 ).  Wereportsubstantialimprovementsover strongbaselinesinbothMT(+1.5BLEU) andimagecaptioning(+3.1BLEU/+11.7 CIDEr)bytrainingonthenewlygenerated pseudo-references(Section 4 ). 2UsingMultipleReferences Inordertomakethemultiplereferencetraining easytoadapttoanyframeworks,wedonotchange anythingfromtheexistingmodelsitself.Ourmul- tiplereferencetrainingisachievedbyconverting amultiplereferencedatasettoasinglereference datasetwithoutlosinganyinformation. Consideringamultiplereferencedataset D , wherethe i th trainingexample, ( x i ;Y i ) ,includes onesourceinput x i ,whichisasourcesentencein MTorimagevectorinimagecaptioning,andaref- erenceset Y i = f y 1 i ; y 2 i ;::: y K i g of K references. Wehavethefollowingmethodstoconvertthemul- tiplereferencedatasettoasinglereferencedataset D 0 (notethatthefollowing D 0 sampleone , D 0 uniform and D 0 shuf areorderedsets): SampleOne :Themoststraightforwardwayisto useadifferentreferenceindifferentepochsdur- ingtrainingtoexplorethevariancesbetweenrefer- ences.Foreachexample,werandomlypickoneof the K referencesineachtrainingepoch(notethat therandomfunctionwillbeusedineachepoch). Thismethodiscommonlyusedinexistingimage captioningliteratures,suchas( KarpathyandFei- Fei , 2015 ),butneverusedinMT.Thisapproach canbeformalizedas: D 0 sampleone = j D j [ i =1 f ( x i ; y k i i ) g ;k i = rand (1 ;:::;K ) Uniform :Althoughallreferencesareaccessible byusingSampleOne,itisnotguaranteedthatall referencesareusedduringtraining.Soweintro- duce Uniform whichbasicallycopies x i training example K timesandeachtimewithadifferent reference.Thisapproachcanbeformalizedas: D 0 uniform = j D j [ i =1 K [ k =1 f ( x i ; y k i ) g  isbasedonUniform,butshufallthe sourceandreferencepairsinrandomorderbefore eachepoch.So,formallyitis: D 0 shuf = Shuf ( D 0 uniform ) SampleOneissupervisedbydifferenttraining signalsindifferentepochswhilebothUniformand Shufincludeallthereferencesatonetime.Note thatweusemini-batchduringtraining.Whenwe setthebatchsizeequaltotheentiretrainingset sizeinbothUniformandShuftheybecome equivalent. 3Pseudo-ReferencesGeneration Intextgenerationtasks,thegivenmultiplerefer- encesareonlyasmallportioninthewholespace ofpotentialreferences.Tocoveralargernumber ofreferencesduringtraining,wewanttogenerate morepseudo-referenceswhichissimilartoexist- ingones. Ourbasicideaistocompressdifferentrefer- ences y 0 ; y 1 ;:::; y K intoalattice.Weachieve thisbymergingsimilarwordsinthereferences. Finally,wegeneratemorepseudo-referencesby simplytraversingthecompressedlatticeandse- lectthosewithhighqualityaccordingtoitsBLEU score. Takethefollowingthreereferencesfrom theNISTChinese-to-Englishmachinetranslation datasetasanexample: 1. Indonesiareiterateditsopposition toforeignmilitarypresence 2. Indonesiarepeatsitsopposition againststationofforeigntroopsin Indonesia 3. Indonesiareiteratesoppositionto garrisoningforeignarmies Figure1:Latticeconstructionwithwordalignment.(b-c)ishardwordalignmentand33pseudo-referencescanbe generated.(d-e)issoftwordalignment,213pseudo-referencescanbegenerated. 3.1NaiveIdea:HardWordAlignment Thesimplestwaytocompressdifferentreferences intoalatticeistodopairwisereferencecompres- sioniteratively.Ateachtime,weselecttworefer- encesandmergethesamewordsinthem. Consideringthepreviousexample,wecande- riveaninitiallatticefromthethreereferencesas showninFig. 1(a) .Assumethatwedoa pairwisereferencecompressionontworef- erences,wecanmergeatfoursharingwords: Indonesia , its , opposition and foreign ,and thelatticewillturntoFig. 1(b) .Ifwefurthercom- presstheandthirdreferences,wecanmergeat Indonesia , opposition , to and foreign ,which givesthelatticeFig. 1(c) .Bysimplytraversing thelattice,33newpseudo-referencescanbe generated.Forexample: 1. Indonesiareiterateditsopposition togarrisoningforeignarmies 2. Indonesiarepeatsitsoppositionto foreignmilitarypresence 3. Indonesiareiteratesoppositionto foreigntroopsinIndonesia 4. ... However,thissimplehardalignmentmethod (onlyidenticalwordscanbealigned)suffersfrom twoproblems: 1. Differentwordsmayhavesimilarmeanings andneedtobemergedtogether.Forexam- ple,inthepreviousexample, reiterated , repeats and reiterates shouldbemerged together.Similarly, military , troops and armies alsohavesimilarmeanings.Ifthe Figure2:Mistakesfromhardwordalignmentbymergingatﬁ to ﬂ. latticecanalignthesewords,wecangener- atethelatticeshowninFig. 1(e) whichcan generate213pseudo-references. 2. Identicalwordsmayhavedifferentmean- ingindifferentcontextsandshouldnotbe merged.Consideringthefollowingtworef- erencesfromtheCOCOimagecaptioning dataset(correspondingpictureisshownin Fig. 2 ): 1. Twoelephantsinanenclosurenext toabrickbuilding 2. Twoelephantstrytofitthrougha smallentry Followingthepreviouslydescribedalgorithm, wecanmergethetworeferencesatﬁ two elephants ﬂ,atﬁ to ﬂandatﬁ a ﬂ.However,ﬁ to ﬂin thetworeferencesareverydifferent(itisaprepo- sitioninthereferenceandanveinthe second)andshouldnotbemerged.Thus,thelat- ticeinFig. 2 (b)willgeneratethefollowingwrong pseudo-references: 1. Twoelephantstrytoasmallentry 2. Twoelephantsinanenclosurenext tofitthroughabrickbuilding Therefore,weneedtoinvestigateabetter methodtocompressthelattice. 3.2MeasuringWordSimilarityinContext Totackletheabovelistedtwoproblemsofhard alignment,weneedtoidentifysynonymsand wordswithsimilarmeanings. BarzilayandLee ( 2002 )utilizeanexternalsynonymsdictionaryto getthesimilarityscorebetweenwords.How- ever,thismethodignoresthegivencontextofeach word.Forexample,inFig. 1(a) ,therearetwo Indonesia 'sinthesecondpathofreference.If weuseasynonymsdictionary,both Indonesia to- kenswillbealignedtothe Indonesia inthe orthirdsentencewiththesamescore.Thisincor- rectalignmentwouldleadtomeaninglesslattice. Thus,weintroducethesemanticsubstitution matrixwhichmeasuresthesemanticsimilarityof eachwordpairsincontext.Formally,givenasen- tencepair y i and y j ,webuildasemanticsubsti- tutionmatrix M = R j y i  y j j ,whosecell M u;v representsthesimilarityscorebetweenword y i;u andword y j;v . Weproposeanewneuralnetwork-basedmul- tiplesequencealignmentalgorithmtotakecon- textintoconsideration.Webuildalanguage model(LM)toobtainthesemanticrepresentation ofeachword,thenthesewordrepresentationsare usedtoconstructthesemanticsubstitutionmatrix betweensentences. Fig. 3 showsthearchitectureofthebidirectional LM( MousaandSchuller , 2017 ).Theoptimiza- tiongoalofourLMistominimizethe i th word's predictionerrorgiventhesurroundingword'shid- denstate: p ( w i j ! h i  1    h i +1 ) (1) Foranynewgivensentences,weconcatenate bothforwardandbackwardhiddenstatestorep- resenteachword y i;u inasentence y i .Wethen calculatethenormalized cosine similarityscoreof word y i;u and y j;v as: M u;v = cosine ( ! h u    h u ; ! h v    h v ) (2) Figure3:BidirectionalLanguageModel Figure4:SemanticSubstitutionMatrix Fig. 4 showsanexampleofthesemanticsub- stitutionmatrixoftwosentencesinexample referencesofFig. 1(a) . 3.3IterativePairwiseWordAlignmentusing DynamicProgramming Withthehelpofsemanticsubstitutionmatrix M u;v whichmeasurespairwisewordsimilarity,weneed totheoptimalwordalignmenttocompress referencesintoalattice. Unfortunately,thiscomputationisexponential inthenumberofsequences.Thus,weuseiter- ativepairwisealignmentwhichgreedilymerges sentencepairs( Durbinetal. , 1998 ). Basedonpairwisesubstitutionmatrixwecan anoptimalpairwisesequencealignmentas anoptimalpathfrom M 0 ; 0 to M j y i j ; j y j j .This isadynamicprogrammingproblemwiththe statetransitionfunctiondescribedinEquation( 3 ). Fig. 5 showstheoptimalpathaccordingtothese- manticsubstitutionmatrixinFig. 4 .Thereisagap ifthecontinuousstepgoesverticalorhorizontal, andanalignmentifitgoesdiagonal. opt ( u;v )= 8 > < > : opt ( u  1 ;v  1)+ M u;v opt ( u  1 ;v ) opt ( u;v  1) (3) Whatordershouldwefollowtodotheiter- ativepairwisewordalignment?Intuitively,we needtocompressthemostsimilarreferencepair sincethiscompressionwillleadtomore alignedwords.Followingthisintuition,weorder referencepairsbythemaximumalignmentscore opt ( j y i j ; j y j j ) (i.e.thescoreofbottom-rightcell inFig. 5 )whichisthesumofallalignedwords. Figure5:DynamicProgrammingonSemanticSubsti- tutionMatrix Usingthisorder,wecaniterativelymergeeach sentencepairindescendingorder,unlessboththe sentenceshavealreadybeenmerged(thiswillpre- ventgeneratingacycliclattice). Sincethesemanticsubstitutionmatrix M u;v , asanormalizedcosinesimilarity,scales in (0 ; 1) ,it'sverylikelyfortheDPalgorithmto alignunrelatedwords.Totacklethisproblem,we deductaglobalpenalty p fromeachcellof M u;v . Withtheglobalpenalty p ,theDPalgorithmwill notalignawordpair ( y i;u ; y i;v ) unless M u;v  p . Afterthepairwisereferencesalignment,we mergethosealignedwords.Forexample,inFig. 1 , afterwegenerateaninitiallatticeasshownin Fig. 1(a) ,wethencalculatethemaximumalign- mentscoreofallsentencepairs.Afterthat,the latticeturnsintoFig. 1(d) bymergingthetwo references(assumingtheyhavethehighestscore) accordingtopairwisealignmentshowninFig. 5 . Thenwepickthesentencepairwithnexthighest alignmentscore(assumingit'sthelasttwosen- tences).Similartothepreviousstep,wealign- mentsaccordingtothedynamicprogrammingand mergetothelattice(seeFig. 1(e) ). 3.4TraverseLatticeandPseudo-References SelectionbyBLEU Wegeneratepseudo-referencesbysimplytravers- ingthegeneratedlattice.Forexample,ifwetra- versethelatticeshowninFig. 1(e) ,wecan generate213pseudo-refrencesintotal. Then,wecanputthosegeneratedpseudo- referencestoexpandthetrainingdataset.Tobal- ancethenumberofgeneratedpseudo-references foreachexample,weforcethetotalnumber ofpseudo-referencesfromeachexampletobe (a)MachineTranslationDataset (b)ImageCaptioningDataset Figure6:Analysisofgeneratedreferences K 0 .Forthoseexamplesgenerating k pseudo- referencesand k>K 0 ,wecalculateallpseudo- references'BLEUscoresbasedongoldrefer- ences,andonlykeeptop K 0  k pseudo-references withhighestBLEUscore. 4Experiments Toinvestigatetheempiricalperformancesofour proposedalgorithm,weconductexperimentson machinetranslationandimagecaptioning. 4.1MachineTranslation WeevaluateourapproachonNISTChinese-to- Englishtranslationdatasetwhichconsistsof1M pairsofsinglereferencedataand5974pairsof 4referencedata(NIST2002,2003,2004,2005, 2006,2008).Table 1 showsthestatisticsofthis dataset.Wepre-trainourmodelona1Mpairs singlereferencedatasetandthentrainontheNIST 2002,2003,2004,2005.WeusetheNIST2006 datasetasvalidationsetandNIST2008astest sets. Fig. 6(a) analyzesthenumberandquality ofgeneratedreferencesusingourproposedap- proach.Wesettheglobalpenaltyas0.9andonly calculatethetop50generatedreferencesforthe averageBLEUanalysis.Fromthewecan seethatwhenthesentencelengthgrows,thenum- berofgeneratedreferencesgrowsexponentially. Togenerateenoughreferencesforthefollowing experiments,wesetaninitialglobalpenaltyas0.9 andgraduallydecreaseitby0.05untilwecollect nolessthan100references.Wetrainabidirec- tionallanguagemodelonthepre-trainingdataset andtrainingdatasetwithGlove( Penningtonetal. , 2014 )wordembeddingsizeof300dimension,for 20epochstominimizetheperplexity Weemploybyte-pairencoding(BPE)( Sennrich etal. , 2015 )whichreducesthesourceandtarget languagevocabularysizesto18kand10k.We adoptlengthreward( Huangetal. , 2017 )to optimalsentencelength.Weuseatwolayerbidi- rectionalLSTMastheencoderandatwolayer LSTMasthedecoder.Weperformpre-training for20epochstominimizeperplexityonthe1M dataset,withabatchsizeof64,wordembedding sizeof500,beamsizeof15,learningrateof0.1, learningratedecayof0.5anddropoutrateof0.3. Wethentrainthemodelin30epochsandusethe bestbatchsizeamong100,200,400foreachup- datemethod.Thesebatchsizesaremultipleofthe numberofreferencesusedinexperiments,soitis guaranteedthatallthereferencesofonesingleex- ampleareinonebatchfortheUniformmethod. Thelearningrateissetas0.01andlearningrate decayas0.75.Wedoeachexperimentthreetimes andreporttheaverageresult. Table 2 showsthetranslationqualityonthedev- setofmachinetranslationtask.Besidestheorig- inal4referencesinthetrainingset,wegener- ateanotherfourdatasetwith10,20,50and100 referencesincludingpseudo-referencesusinghard wordalignmentandsoftwordalignment.We comparethethreeupdatemethods(SampleOne, Uniform,Shufe)withalwaysusingtheref- erence(First).Allresultsofsoftwordalignment arebetterthancorrespondinghardwordalignment resultsandthebestresultisachievedwith50ref- erencesusingUniformandsoftwordalignment. AccordingtoTable 3 ,Shufwithoriginal4refer- enceshas+0.7BLEUimprovementandUniform Task Pre-training Training Validation Testing MachineTranslation #ofexamples 1,000,000 4,667 616 691 #ofrefsperexample 1 4 4 4 ImageCaptioning #ofexamples - 113,287 5,000 5,000 #ofrefsperexample - 5 5 5 Table1:Statisticsofdatasetsusedinfollowingexperiments. #ofRefs Method BLEU 0 Pre-train 37.44 1 First  38.64 4 SampleOne 38.81 Uniform 38.78 Shuf 38.87 IncludesPseudo-Refs HardAlign SoftAlign 10 SampleOne 37.48 39.41 Uniform 39.20 39.35 Shuf 39.13 39.53 20 SampleOne 37.27 38.70 Uniform 39.14 39.46 Shuf 39.12 39.42 50 SampleOne 37.42 37.62 Uniform 39.30 39.65 Shuf 38.98 39.08 100 SampleOne 37.54 37.63 Uniform 39.23 39.46 Shuf 38.88 39.03 Table2:BLEUontheMTvalidationset.  Baseline #ofRefs Method BLEU 0 Pre-train 33.58 1 First  34.49 4 Shuf 35.20(+0.7) y 50 Uniform 35.98 (+1.5) Table3:BLEUontheMTtestset. y Includespseudo- referencesgeneratedbysoftwordalignmentalgorithm.  Baseline. with50referenceshas+1.5BLEUimprovement. FromFig. 7(b) ,wecanseethatusingtheSam- pleOnemethod,thetranslationqualitydropsdra- maticallywithmorethan10references.Thismay beduetothehighervarianceofusedreferencein eachepoch. 4.2ImageCaptioning Fortheimagecaptioningtask,weusethewidely- usedMSCOCOimagecaptioningdataset.Follow- ingpriorwork,weusetheKapathysplit( Karpathy andFei-Fei , 2015 ).Table 1 showsthestatisticsof thisdataset.WeuseResnet( Heetal. , 2016 )toex- tractimagefeatureof2048featuresizeandsimple fullyconnectedlayerofsize512toanLSTMde- #ofRefs Method BLEU CIDEr 1 First 26.27 79.05 5 SampleOne  29.03 85.39 Uniform 30.05 89.76 Shuf 30.41 91.21 IncludesPseudo-Refs HardAlign SoftAlign BLEUCIDEr BLEUCIDEr 10 SampleOne 30.6391.76 30.9892.02 Uniform 30.4091.48 30.7791.89 Shuf 30.6892.01 30.9192.22 20 SampleOne 30.6992.25 30.9192.32 Uniform 30.7391.69 31.0392.61 Shuf 31.5694.99 31.9295.59 50 SampleOne 30.7691.81 31.0792.17 Uniform 30.6692.30 30.9992.61 Shuf 30.8393.26 31.0694.19 Table4:BLEU/CIDErontheimagecaptioningvalida- tionset.  Baseline. #ofRefs Method BLEUCIDEr 1 First 26.7080.70 5 SampleOne  28.6785.41 5 Shuf 30.94(+2.3)94.10(+8.7) y 20 Shuf 31.79 (+3.1) 97.10 (+11.7) Table5:BLEU/CIDErontheimagecaptioningtestset withsoft. y Includespseudo-referencesgeneratedby softwordalignmentalgorithm.  Baseline. coder.Wetraineverymodelfor100epochsand calculatetheBLEUscoreonvalidationsetandse- lectthebestmodel.Foreveryupdatemethod,we theoptimalbatchsizeamong50,250,500, 1000andweuseabeamsizeof5. Fig. 6(b) analyzesthecorrelationbetweenaver- agereferenceslengthwiththenumberandquality ofgeneratedreferences.Wesetglobalpenaltyas 0.6(whichisalsoadoptedforthegeneratedref- erencesinthefollowingexperiments)andcalcu- latethetop50generatedreferencesfortheaver- ageBLEUanalysis.Sincethelengthoforiginal referencesismuchshorterthanthepreviousma- chinetranslationdataset,ithasworsequalityand fewergeneratedreferences. Table 4 showsthatthebestresultisachieved with20referencesusingShufThisresultis Image OriginalReferences agraytabbycatiscurledinaredbowlthatsitsonatablenearawindow abrownandblackcatissleepinginabowlonatable agreytigercatsleepinginabrownbowlonatable animageofacatsittinginsideofabowlonthekitchentable acatasleepinafruitbowlonadiningroomtable GeneratedLatticeusingSoftAlignment ID Pseudo-references BLEU 1 agreytigercatsleepinginabrownbowlonatablenearawindow 100.0 2 agreytigercatsleepinginabrownbowlonadiningroomtable 100.0 3 abrownandblackcatissleepinginabowlonthekitchentable 100.0 ... ... ... 48 agreytigercatsleepinginafruitbowlonatable 97.1 49 acatasleepinaredbowlthatsitsonatable 97.1 50 agraytabbycatissleepinginabowlonatable 97.1 ... ... ... 73723 agreyandtabbycatinsideofaredbowlonthediningroomtable 0.0 73724 agreyandtabbycatinsideofaredbowlonakitchentable 0.0 Table6:Trainingexamplethatgeneratesmaximumnumberofpseudo-references(73724).Theselected8pseudo- referencesaresortedaccordingtotheirBLEUscore. (a)Learningcurveofdifferentmethods with50References (b)MTwithdifferentnumberofreferences Figure7:Translationqualityofmachinetranslation taskondev-setwithsoftalignment differentfromtheresultofmachinetranslation taskwhereUniformmethodisthebest.This maybebecausethereferencesinimagecaption- ingdatasetaremuchmorediversethanthosein machinetranslationdataset.Differentcaptions ofoneimagecouldeventalkaboutdifferentas- pects.WhenusingtheUniformmethod,thehigh varianceofreferencesinonebatchmayharmthe modelandleadtoworsetextgenerationquality. Table 5 showsthatitoutperformsSampleOne with4originalreferences,whichisadoptedin previouswork( KarpathyandFei-Fei , 2015 ),+3.1 BLEUscoreand+11.7CIDEr. 4.3CaseStudy Fig. 6 showsatrainingexampleintheCOCO datasetanditscorrespondinggeneratedlatticeand pseudo-referenceswhichissortedaccordingtoits BLEUscore.Ourproposedalgorithmgenerates 73724pseudo-referencesintotal.Allthetop50 pseudo-references'BLEUscoresareabove97.1 andthetopthreeevenachieve100.0BLEUscore thoughtheyarenotidenticaltoanyoriginalrefer- ences.AlthoughtheBLEUoflasttwosentences is0.0,theyarestillvalidtodescribethispicture. (a)Learningcurveofdifferentmethodswith 20References (b)Imagecaptioningwithdifferentnumberof references Figure8:Textgenerationqualityofimagecaptioning taskonvalidationsetwithsoftalignment 5Conclusions Weintroduceseveralmultiple-referencetraining methodsandaneural-basedlatticecompression framework,whichcangeneratemoretrainingref- erencesbasedonexistingones.Ourproposed frameworkoutperformsthebaselinemodelson bothMTandimagecaptioningtasks. Acknowledgments ThisworkwassupportedinpartbyDARPA grantN66001-17-2-4030,andNSFgrantsIIS- 1817231andIIS-1656051.Wethanktheanony- mousreviewersforsuggestionsandJunekiHong forproofreading. References DzmitryBahdanau,KyunghyunCho,andYoshuaBen- gio.2014.Neuralmachinetranslationbyjointly learningtoalignandtranslate. CoRR . ReginaBarzilayandLillianLee.2002.Bootstrapping lexicalchoiceviamultiple-sequencealignment.In ProceedingsoftheACL-02ConferenceonEmpirical MethodsinNaturalLanguageProcessing-Volume 10 . ReginaBarzilayandLillianLee.2003.Learning toparaphrase:Anunsupervisedapproachusing multiple-sequencealignment.In Proceedingsofthe 2003ConferenceoftheNorthAmericanChapter oftheAssociationforComputationalLinguisticson HumanLanguageTechnology-Volume1 . MarkusDreyerandDanielMarcu.2012.Hyter: Meaning-equivalentsemanticsfortranslationeval- uation.In Proceedingsofthe2012Conferenceof theNorthAmericanChapteroftheAssociationfor ComputationalLinguistics:HumanLanguageTech- nologies ,pages162Œ171.AssociationforComputa- tionalLinguistics. RichardDurbin,SeanEddy,AndersKrogh,and GraemeMitchison.1998.Biologicalsequenceanal- ysis:probabilisticmodelsofproteinsandnucleic acids. JonasGehring,MichaelAuli,DavidGrangier,Denis Yarats,andYannDauphin.2017.Convolutionalse- quencetosequencelearning.In ICML . KaimingHe,XiangyuZhang,ShaoqingRen,andJian Sun.2016.Deepresiduallearningforimagerecog- nition. ConferenceonComputerVisionandPattern RecognitionCVPR . LiangHuang,KaiZhao,andMingboMa.2017.When tooptimalbeamsearchforneuraltextgener- ation(modulobeamsize).In EMNLP2017 . AndrejKarpathyandLiFei-Fei.2015.Deepvisual- semanticalignmentsforgeneratingimagedescrip- tions.In ProceedingsoftheIEEEconference oncomputervisionandpatternrecognition ,pages 3128Œ3137. Tsung-YiLin,MichaelMaire,SergeJ.Belongie, LubomirD.Bourdev,RossB.Girshick,JamesHays, PietroPerona,DevaRamanan,PiotrDoll ´ ar,and C.LawrenceZitnick.2014.MicrosoftCOCO:com- monobjectsincontext. AmrMousaandBj ¨ ornSchuller.2017.Contextualbidi- rectionallongshort-termmemoryrecurrentneural networklanguagemodels:Agenerativeapproachto sentimentanalysis.In Proceedingsofthe15thCon- ferenceoftheEuropeanChapteroftheAssociation forComputationalLinguistics:Volume1,LongPa- pers . RameshNallapati,BowenZhou,andMingboMa. 2016.Classifyorselect:Neuralarchitecturesfor extractivedocumentsummarization. CoRR . JeffreyPennington,RichardSocher,andChristo- pherD.Manning.2014.Glove:Globalvectorsfor wordrepresentation.In EmpiricalMethodsinNat- uralLanguageProcessing(EMNLP) ,pages1532Œ 1543. AlexanderM.Rush,SumitChopra,andJasonWeston. 2015.Aneuralattentionmodelforabstractivesen- tencesummarization. RicoSennrich,BarryHaddow,andAlexandraBirch. 2015.Neuralmachinetranslationofrarewordswith subwordunits. arXivpreprintarXiv:1508.07909 . IlyaSutskever,OriolVinyals,andQuocV.Le.2014. Sequencetosequencelearningwithneuralnet- works. Proceedingsofthe27thInternationalCon- ferenceonNeuralInformationProcessingSystems . AshishVaswani,NoamShazeer,NikiParmar,Jakob Uszkoreit,LlionJones,AidanN.Gomez,Lukasz Kaiser,andIlliaPolosukhin.2017.Attentionisall youneed.In NIPS . RamakrishnaVedantam,C.LawrenceZitnick,and DeviParikh.2015.Cider:Consensus-basedimage descriptionevaluation.In CVPR . SubhashiniVenugopalan,MarcusRohrbach,Jeffrey Donahue,RaymondJ.Mooney,TrevorDarrell,and KateSaenko.2015.Sequencetosequence-videoto text.In ICCV . KelvinXu,JimmyBa,RyanKiros,KyunghyunCho, AaronCourville,RuslanSalakhudinov,RichZemel, andYoshuaBengio.2015a.Show,attendandtell: Neuralimagecaptiongenerationwithvisualatten- tion. Proceedingsofthe32ndInternationalConfer- enceonMachineLearning(ICML-15) . KelvinXu,JimmyBa,RyanKiros,Kyunghyun Cho,AaronC.Courville,RuslanSalakhutdinov, RichardS.Zemel,andYoshuaBengio.2015b. Show,attendandtell:Neuralimagecaptiongener- ationwithvisualattention.In ProceedingsofMa- chineLearningResearch .  
ChinesePoetryGenerationwithPlanningbasedNeuralNetwork ZheWang y ,WeiHe z ,HuaWu z ,HaiyangWu z ,WeiLi z ,HaifengWang z ,EnhongChen y y UniversityofScienceandTechnologyofChina,Hefei,China z BaiduInc.,Beijing,China xiaose@mail.ustc.edu.cn,cheneh@ustc.edu.cn f hewei06,wu hua,wuhaiyang,liwei08,wanghaifeng g @baidu.com Abstract Chinesepoetrygenerationisaverychallengingtaskinnaturallanguageprocessing.Inthispaper, weproposeanoveltwo-stagepoetrygeneratingmethodwhichplansthesub-topicsofthe poemaccordingtotheuser'swritingintent,andthengenerateseachlineofthepoemsequentially, usingarecurrentneuralnetworkencoder-decoderframework.Theproposedplanning- basedmethodcanensurethatthegeneratedpoemiscoherentandsemanticallyconsistentwiththe user'sintent.Acomprehensiveevaluationwithhumanjudgmentsdemonstratesthatourproposed approachoutperformsthestate-of-the-artpoetrygeneratingmethodsandthepoemqualityis somehowcomparabletohumanpoets. 1Introduction TheclassicalChinesepoetryisagreatandimportantheritageofChineseculture.Duringthehistory ofmorethantwothousandyears,millionsofbeautifulpoemsarewrittentopraiseheroiccharacters, beautifulscenery,love,friendship,etc.TherearedifferentkindsofChineseclassicalpoetry,suchas TangpoetryandSongiambics.Eachtypeofpoetryhastofollowsomestructural,rhythmical andtonalpatterns.Table1showsanexampleofquatrainwhichwasoneofthemostpopulargenresof poetryinChina.Theprinciplesofaquatraininclude:Thepoemconsistsoffourlinesandeachlinehas veorsevencharacters;everycharacterhasaparticulartone,Ping(theleveltone)orZe(thedownward tone);thelastcharacterofthesecondandlastlineinaquatrainmustbelongtothesamerhymecategory (Wang,2002).Withsuchstrictrestrictions,thewell-writtenquatrainisfullofrhythmicbeauty. Inrecentyears,theresearchofautomaticpoetrygenerationhasreceivedgreatattention.Mostap- proachesemployrulesortemplates(Tosaetal.,2008;Wuetal.,2009;Netzeretal.,2009;Oliveira, 2009;Oliveira,2012),geneticalgorithms(Manurung,2004;Zhouetal.,2010;Manurungetal.,2012), summarizationmethods(Yanetal.,2013)andstatisticalmachinetranslationmethods(JiangandZhou, 2008;Heetal.,2012)togeneratepoems.Morerecently,deeplearningmethodshaveemergedasa promisingdiscipline,whichconsidersthepoetrygenerationasasequence-to-sequencegenerationprob- lem(ZhangandLapata,2014;Wangetal.,2016;Yietal.,2016).Thesemethodsusuallygeneratethe linebyselectingonelinefromthedatasetofpoemsaccordingtotheuser'swritingintents(usually asetofkeywords),andtheotherthreelinesaregeneratedbasedonthelineandthepreviouslines. Theuser'swritingintentcanonlyaffecttheline,andtherestthreelinesmayhavenoassociation withthemaintopicofthepoem,whichmayleadtosemanticinconsistencywhengeneratingpoems.In addition,topicsofpoemsareusuallyrepresentedbythewordsfromthecollectedpoemsinthetraining corpus.Butasweknow,thewordsusedinpoems,especiallypoemswritteninancienttime,aredifferent frommodernlanguages.Asaconsequence,theexistingmethodsmayfailtogeneratemeaningfulpoems ifauserwantstowriteapoemforamodernterm(e.g.,BarackObama). Inthispaper,weproposeanovelpoetrygeneratingmethodwhichgeneratespoemsinatwo-stage procedure:thecontentsofpoems(ﬁwhattosayﬂ)areexplicitlyplanned,andthensurfacerealization (ﬁhowtosayﬂ)isconducted.Givenauser'swritingintentwhichcanbeasetofkeywords,asentenceor ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense.Licensedetails: http:// creativecommons.org/licenses/by/4.0/ arXiv:1610.09889v2  [cs.CL]  7 Dec 2016Y ˝ ˛ ThoughtsinaStillNight − M   III  (PPZZP)Theluminousmoonshinebeforemybed, ‚ / 0  ˝˝˝  (*ZZPP)Isthoughttobethefrostfallenontheground. > 4 ˙    (*ZPPZ)Iliftmyheadtogazeatthecliffmoon, N 4 ˛ E aaa  (PPZZP)Andthenbowdowntomuseonmydistanthome. Table1:AnexampleofTangpoetry.Thetoneisshownattheendofeachline.Prepresentsthe level-tone,andZrepresentsthedownward-tone;*indicatesthatthetonecanbeeither.Therhyming charactersareinboldface. evenadocumentdescribedbynaturallanguage,thestepistodetermineasequenceofsub-topicsfor thepoemusingapoemplanningmodel,witheachlinerepresentedbyasub-topic.Thepoemplanning modeldecomposestheuser'swritingintentintoaseriesofsub-topics,andeachsub-topicisrelatedto themaintopicandrepresentsanaspectofthewritingintent.Thenthepoemisgeneratedlinebyline, andeachlineisgeneratedaccordingtothecorrespondingsub-topicandtheprecedinggeneratedlines, usingarecurrentneuralnetworkbasedencoder-decodermodel(RNNenc-dec).WemodifytheRNN enc-decframeworktosupportencodingofbothsub-topicsandtheprecedinglines.Theplanningbased mechanismhastwoadvantagescomparedtothepreviousmethods.First,everylineofthegenerated poemhasacloserconnectiontouser'swritingintent.Second,thepoemplanningmodelcanlearnfrom extraknowledgesourcebesidesthepoemdata,suchaslarge-scalewebdataorknowledgeextractedfrom encyclopedias.Asaconsequence,itcanbridgethemodernconceptsandthesetofwordscoveredby ancientpoems.TakethetermﬁBarackObamaﬂastheexample:usingtheknowledgefromencyclopedias, thepoemplanningmodelcanextendtheuser'squery,BarackObama,toaseriesofsub-topicssuchas outstanding,power,etc.,thereforeensuringsemanticconsistencyinthegeneratedpoems. Thecontributionofthispaperistwo-fold.First,weproposeaplanning-basedpoetrygenerating framework,whichexplicitlyplansthesub-topicofeachline.Second,weuseaRNNencoder- decoderframework,whichsupportsencodingofbothsub-topicsandtheprecedinglines,togeneratethe poemlinebyline. Therestofthispaperisorganizedasfollows.Section2describessomepreviousworkonpoetry generationandcomparesourworkwithpreviousmethods.Section3describesourplanningbased poetrygenerationframework.WeintroducethedatasetsandexperimentalresultsinSection4.Section5 concludesthepaper. 2RelatedWork PoetrygenerationisachallengingtaskinNLP.Oliveiraetal.(2009;2012;2014)proposedapoem generationmethodbasedonsemanticandgrammartemplates.Netzeretal.(2009)employedamethod basedonwordassociationmeasures.Tosaetal.(2008)andWuetal.(2009)usedaphrasesearch approachforJapanesepoemgeneration.Greeneetal.(2010)appliedstatisticalmethodstoanalyze, generateandtranslaterhythmicpoetry.Coltonetal.(2012)describedacorpus-basedpoetrygeneration systemthatusestemplatestoconstructpoemsaccordingtothegivenconstrains.Yanetal.(2013) consideredthepoetrygenerationasanoptimizationproblembasedonasummarizationframeworkwith severalconstraints.Manurung(2004;2012)andZhouetal.(2010)usedgeneticalgorithmsforgenerating poems.Animportantapproachtopoemgenerationisbasedonstatisticalmachinetranslation(SMT). JiangandZhou(2008)usedanSMT-basedmodelingeneratingChinesecoupletswhichcanberegarded asregulatedverseswithonlytwolines.Thelineisregardedasthesourcelanguageand translatedintothesecondline.Heetal.(2012)extendedthismethodtogeneratequatrainsbytranslating thepreviouslinetothenextlinesequentially. Recently,deeplearningmethodsachievegreatsuccessinpoemgeneration.ZhangandLapata(2014) proposedaquatraingenerationmodelbasedonrecurrentneuralnetwork(RNN).Theapproachgener- atesthelinefromthegivenkeywordswitharecurrentneuralnetworklanguagemodel(RNNLM) (Mikolovetal.,2010)andthenthesubsequentlinesaregeneratedsequentiallybyaccumulatingthesta- tusofthelinesthathavebeengeneratedsofar.Wangetal.(2016)generatedtheChineseSongiambics Figure1:Illustrationoftheplanningbasedpoetrygenerationframework. usinganend-to-endneuralmachinetranslationmodel.Theiambicisgeneratedbytranslatingthepre- viouslineintothenextlinesequentially.ThisprocedureissimilartoSMT,butthesemanticrelevance betweensentencesisbetter.Wangetal.(2016)didnotconsiderthegenerationoftheline.There- fore,thelineisprovidedbyusersandmustbeawell-writtensentenceofthepoem.Yietal.(2016) extendedthisapproachtogenerateChinesequatrains.Theproblemofgeneratingthelineisresolved byaseparateneuralmachinetranslation(NMT)modelwhichtakesonekeywordasinputandtranslates itintotheline.MarjanGhazvininejadandKnight(2016)proposedapoetrygenerationalgorithm thatgeneratestherhymewordsrelatedtothegivenkeywordandthengeneratedthewholepoem accordingtotherhymewordswithanencoder-decodermodel(Sutskeveretal.,2014). Ourworkdiffersfromthepreviousmethodsasfollows.First,wedon'tconstraintheuser'sinput.It canbesomekeywords,phrases,sentencesorevendocuments.Thepreviousmethodscanonlysupport somekeywordsormustprovidetheline.Second,weuseplanning-basedmethodtodeterminethe topicofthepoemaccordingtotheuser'sinput,witheachlinehavingonesub-topic,which guaranteesthatthegeneratedpoemiscoherentandwellorganized,thereforeavoidingtheproblemof thepreviousmethodthatonlythelineisguaranteedtoberelatedtotheuser'sintentwhilethenext linesmaybeirrelevantwiththeintentionduetothecoherentdecayproblem(Heetal.,2012;Zhangand Lapata,2014;Wangetal.,2016;Yietal.,2016).Third,therhythmortonein(Zhouetal.,2010;Yanet al.,2013;ZhangandLapata,2014;Yietal.,2016;MarjanGhazvininejadandKnight,2016)iscontrolled byrulesorextrastructures,whileourmodelcanautomaticallylearnconstrainsfromthetrainingcorpus. Finally,ourpoemgenerationmodelhasasimplerstructurecomparedwiththosein(ZhangandLapata, 2014;Yietal.,2016). 3Approaches 3.1Overview Inspiredbytheobservationthatahumanpoetshallmakeanoutlinebeforewritingapoem,we proposeaplanning-basedpoetrygenerationapproach(PPG)thatgeneratesanoutlineaccordingto theuser'swritingintentandthengeneratesthepoem.OurPPGsystemtakesuser'swritingintentas inputwhichcanbeaword,asentenceoradocument,andthengeneratesapoemintwostages:Poem PlanningandPoemGeneration.Thetwo-stageprocedureofPPGisillustratedinFigure1. Supposewearewritingapoemthatconsistsof N lineswith l i representingthe i -thlineofthepoem. InthePoemPlanningstage,theinputqueryistransformedinto N keywords ( k 1 ;k 2 ;:::;k N ) ,where k i isthe i -thkeywordthatrepresentsthesub-topicforthe i -thline.InthePoemGenerationstage, l i isgeneratedbytaking k i and l 1: i  1 asinput,where l 1: i  1 isasequenceconcatenatedbyallthelines generatedpreviously,from l 1 to l i  1 .Thenthepoemcanbegeneratedsequentially,andeachlineis generatedaccordingtoonesub-topicandalltheprecedinglines. 3.2PoemPlanning 3.2.1KeywordExtraction Theuser'sinputwritingintentcanberepresentedasasequenceofwords.Thereisanassumptioninthe PoemPlanningstagethatthenumberofkeywordsextractedfromtheinputquery Q mustbeequaltothe numberoflines N inthepoem,whichcanensureeachlinetakesjustonekeywordasthesub-topic.If theuser'sinputquery Q istoolong,weneedtoextractthemostimportant N wordsandkeeptheoriginal orderasthekeywordssequencetosatisfytherequirement. WeuseTextRankalgorithm(MihalceaandTarau,2004)toevaluatetheimportanceofwords.Itis agraph-basedrankingalgorithmbasedonPageRank(BrinandPage,1998).Eachcandidatewordis representedbyavertexinthegraphandedgesareaddedbetweentwowordsaccordingtotheirco- occurrence;theedgeweightissetaccordingtothetotalcountofco-occurrencestrengthofthetwo words.TheTextRankscore S ( V i ) isinitializedtoadefaultvalue(e.g.1.0)andcomputediteratively untilconvergenceaccordingtothefollowingequation: S ( V i )=(1  d )+ d X V j 2 E ( V i ) w ji P V k 2 E ( V j ) w jk S ( V j ) ; (1) where w ij istheweightoftheedgebetweennode V j and V i , E ( V i ) isthesetofverticesconnected with V i ,and d isadampingfactorthatusuallysetto0.85(BrinandPage,1998),andtheinitialscoreof S ( V i ) issetto1.0. 3.2.2KeywordExpansion Iftheuser'sinputquery Q istooshorttoextractenoughkeywords,weneedtoexpandsomenewkey- wordsuntiltherequirementofkeywordsnumberisWeusetwodifferentmethodsforkeywords expansion. RNNLM-basedmethod. WeuseaRecurrentNeuralNetworkLanguageModel(RNNLM)(Mikolov etal.,2010)topredictthesubsequentkeywordsaccordingtotheprecedingsequenceofkeywords: k i = argmax k P ( k j k 1: i  1 ) ,where k i isthe i -thkeywordand k 1: i  1 istheprecedingkeywordssequence. ThetrainingofRNNLMneedsatrainingsetconsistingofkeywordsequencesextractedfrompoems, withonekeywordrepresentingthesub-topicofoneline.Weautomaticallygeneratethetrainingcorpus fromthecollectedpoems.ally,givenapoemconsistingof N lines,werankthewords ineachlineaccordingtotheTextRankscorescomputedonthepoemcorpus.Thenthewordwiththe highestTextRankscoreisselectedasthekeywordfortheline.Inthisway,wecanextractakeyword sequenceforeverypoem,andgenerateatrainingcorpusfortheRNNLMbasedkeywordspredicting model. Knowledge-basedmethod. TheaboveRNNLM-basedmethodisonlysuitableforgeneratingsub- topicsforthosecoveringbythecollectedpoems.Thismethoddoesnotworkwhentheuser'squery containsout-of-domainkeywords,forexample,anamedentitynotcoveredbythetrainingcorpus. Tosolvethisproblem,weproposeaknowledge-basedmethodthatemploysextrasourcesofknowl- edgetogeneratesub-topics.Theextraknowledgesourcescanbeusedincludeencyclopedias,sugges- tionsofsearchengines,lexicaldatabases(e.g.WordNet),etc.Givenakeyword k i ,thekeyideaofthe methodistosomewordsthatcanbestdescribeorinterpret k i .Inthispaper,weusetheencyclopedia entriesasthesourceofknowledgetoexpandnewkeywordsfrom k i .Weretrievethosesatisfyingallthe followingconditionsascandidatekeywords:(1)thewordisinthewindowof [ - 5 ; 5] around k i ;(2)the part-of-speechofthewordisadjectiveornoun;(3)thewordiscoveredbythevocabularyofthepoem corpus.ThenthecandidatewordswiththehighestTextRankscoreareselectedasthekeywords. 3.3PoemGeneration InthePoemGenerationstage,thepoemisgeneratedlinebyline.Eachlineisgeneratedbytakingthe keywordbythePoemPlanningmodelandalltheprecedingtextasinput.Thisprocedurecanbe Figure2:Anillustrationofpoemgenerationmodel. consideredasasequence-to-sequencemappingproblemwithaslightdifferencethattheinputconsistsof twodifferentkindsofsequences:thekeywordbythePoemPlanningmodelandthepreviously generatedtextofthepoem.WemodifytheframeworkofanattentionbasedRNNencoder-decoder(RNN enc-dec)(Bahdanauetal.,2014)tosupportmultiplesequencesasinput. Givenakeyword k whichhas T k characters,i.e. k = f a 1 ;a 2 ;:::;a T k g ,andtheprecedingtext x which has T x characters,i.e. x = f x 1 ;x 2 ;:::;x T x g ,weencode k intoasequenceofhiddenstates [ r 1 : r T k ] , and x into [ h 1 : h T x ] ,withbi-directionalGatedRecurrentUnit(GRU)(Choetal.,2014)models.Then weintegrate [ r 1 : r T k ] intoavector r c byconcatenatingthelastforwardstateandthebackwardstate of [ r 1 : r T k ] ,where r c =  ! r T k   r 1  : (2) Weset h 0 = r c ,thenthesequenceofvectors h =[ h 0 : h T x ] representsthesemanticsofboth k and x ,asillustratedinFigure2.Noticethatwhenwearegeneratingtheline,thelengthofthepreceding textiszero,i.e. T x =0 ,thenthevectorsequence h onlycontainsonevector,i.e. h =[ h 0 ] ,therefore, thelineisactuallygeneratedfromthekeyword. Forthedecoder,weuseanotherGRUwhichmaintainsaninternalstatusvector s t ,andforeach generationstep t ,themostprobableoutput y t isgeneratedbasedon s t ,contextvector c t andprevious generatedoutput y t  1 .Thiscanbeformulatedasfollows: y t =argmax y P ( y j s t ;c t ;y t  1 ) : (3) Aftereachprediction, s t isupdatedby s t = f ( s t  1 ;c t  1 ;y t  1 ) : (4) f (  ) isanactivationfunctionofGRUand c t isrecomputedateachstepbythealignmentmodel: c t = T h  1 X j =0 a tj h j : (5) h j isthe j -thhiddenstateintheencoder'soutput.Theweight a tj iscomputedby a tj = exp( e tj ) P T h  1 k =0 exp( e tk ) ; (6) where e tj = v T a tanh( W a s t  1 + U a h j ) : (7) e tj istheattentionscoreon h j attimestept.Theprobabilityofthenextword y t canbeas: P ( y t j y 1 ;:::;y t  1 ; x ; k )= g ( s t ;y t  1 ;c t ) ; (8) where g (  ) isanonlinearfunctionthatoutputstheprobabilityof y t . Theparametersofthepoemgenerationmodelaretrainedtomaximizethelog-likelihoodofthetraining corpus: argmax N X n =1 logP ( y n j x n ; k n ) : (9) 4Experiments 4.1Dataset Inthispaper,wefocusonthegenerationofChinesequatrainwhichhas4linesandeachlinehasthe samelengthof5or7characters.Wecollected76,859quatrainsfromtheInternetandrandomlychose 2,000poemsforvalidation,2,000poemsfortesting,andtherestfortraining. AllthepoemsinthetrainingsetaresegmentedintowordsusingaCRFbasedwordsegmentation system.ThenwecalculatetheTextRankscoreforeveryword.ThewordwiththehighestTextRank scoreisselectedasthekeywordfortheline.Inthisway,wecanextractasequenceof4keywordsfor everyquatrain.Fromthetrainingcorpusofpoems,weextracted72,859keywordsequences,whichis usedtotraintheRNNlanguagemodelforkeywordexpansion(seesection3.2.2).Forknowledge-based expansion,weuseBaiduBaike 1 andWikipediaastheextrasourcesofknowledge. Afterextractingfourkeywordsfromthelinesofaquatrain,wegeneratefourtriplescomposedof(the keyword,theprecedingtext,thecurrentline),foreverypoem.TakethepoeminTable1asexample, thegeneratedtriplesareshowninTable2.AllthetriplesareusedfortrainingtheRNNenc-decmodel proposedinsection3.3. Keyword ThePrecedingText CurrentLine −  − M   I ˝ − M   I ‚ / 0  ˝   − M   I ; ‚ / 0  ˝ > 4 ˙   E a − M   I ; ‚ / 0  ˝ ; > 4 ˙   N 4 ˛ E a Table2:TrainingtriplesextractedfromthequatraininTable1. 4.2Training FortheproposedattentionbasedRNNenc-decmodel,wechosethe6,000mostfrequentlyusedchar- actersasthevocabularyforbothsourceandtargetsides.Thewordembeddingdimensionalityis512 andinitializedbyword2vec(Mikolovetal.,2013).Therecurrenthiddenlayersofthedecoderandtwo encoderscontained512hiddenunits.Parametersofourmodelwererandomlyinitializedoverauni- formdistributionwithsupport[-0.08,0.08].ThemodelwastrainedwiththeAdaDeltaalgorithm(Zeiler, 2012),wheretheminibatchwassettobe128.Themodelisselectedaccordingtotheperplexityon thevalidationset. 4.3Evaluation 4.3.1EvaluationMetrics Itiswellknownthataccurateevaluationoftextgenerationsystemisdifsuchasthepoetrygen- erationanddialogresponsegeneration(ZhangandLapata,2014;Schatzmannetal.,2005;Mouetal., 2016).Therearethousandsofwaystogenerateanappropriateandrelativepoemordialogresponsegiven atopic,thelimitedreferencesareimpossibletocoverallthecorrectresults.Liuetal.(2016)has recentlyshownthattheoverlap-basedautomaticevaluationmetricsadaptedfordialogresponses,such 1 AcollaborativeonlineencyclopediaprovidedbyChinesesearchengineBaidu: http://baike.baidu.com . Poeticness Doesthepoemfollowtherhymeandtonerequirements? Fluency Doesthepoemreadsmoothlyand Coherence Isthepoemcoherentacrosslines? Meaning Doesthepoemhaveacertainmeaningandartisticconception? Table3:Evaluationstandardsinhumanjudgement. Models Poeticness Fluency Coherence Meaning Average 5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char SMT 3.25 3.22 2.81 2.48 3.01 3.16 2.78 2.45 2.96 2.83 RNNLM 2.67 2.55 3.13 3.42 3.21 3.44 2.90 3.08 2.98 3.12 RNNPG 3.85 3.52 3.61 3.02 3.43 3.25 3.22 2.68 3.53 3.12 ANMT 4.34 4.04 4.61 4.45 4.05 4.01 4 : 09 4.04 4.27 4.14 PPG 4.11 4.15 4.58 4.56 * 4.29 * 4.49 ** 4.46 ** 4.51 ** 4.36 ** 4.43 ** Table4:Humanevaluationresultsofallthesystems.Diacritics  (p < 0.01)and  (p < 0.05)indicate thatourmodel(PPG)isbetterthanallothersystems. asBLEUandMETEOR,havelittlecorrelationwithhumanevaluation.Therefore,wecarryoutahuman studytoevaluatethepoemgenerationmodels.Following(Heetal.,2012;Yanetal.,2013;Zhangand Lapata,2014),weusefourevaluationstandardsforhumanevaluatorstojudgethepoems:ﬁPoeticnessﬂ, ﬁFluencyﬂ,ﬁCoherenceﬂ,ﬁMeaningﬂ.ThedetailedillustrationcanbeseeninTable3.Thescoreofeach aspectrangesfrom1to5withthehigherscorethebetter.Eachsystemgeneratestwenty5-character quatrainsandtwenty7-characterquatrains.Allthegeneratedpoemsareevaluatedby5expertsandthe ratingscoresareaveragedasthescore. 4.3.2Baselines Weimplementedseveralpoetrygenerationmethodsasbaselinesandemployedthesamepre-processing methodforallthemethods. SMT .AChinesepoetrygenerationmethodbasedonStatisticalMachineTranslation(Heetal.,2012). Apoemisgeneratediterativelybyﬁtranslatingﬂthepreviouslineintothenextline. RNNLM .Amethodforgeneratingtextualsequences(Graves,2013),whichisproposedbyMikolov etal.(2010).Thelinesofapoemareconcatenatedtogetherasacharactersequencewhichisusedto traintheRNNLM. RNNPG .IntheapproachofRNN-basedPoemGenerator(ZhangandLapata,2014),thelineis generatedbyastandardRNNLMandthenalltheotherlinesaregeneratediterativelybasedonacontext vectorencodedfromthepreviouslines. ANMT .TheAttentionbasedNeuralMachineTranslationmethod.Itconsiderstheproblemasa machinetranslationtask,whichissimilartothetraditionalSMTapproach.Themaindifferenceisthatin ANMT,themachinetranslationsystemisastandardattentionbasedRNNenc-decframework(Bahdanau etal.,2014). 4.3.3Results TheresultsofthehumanevaluationareshowninTable4.Wecanseethatourproposedmethod,Planning basedPoetryGeneration(PPG),outperformsallbaselinemodelsinaveragescores.Theresultsare consistentwithbothsettingsof5-characterand7-characterpoemgenerations. ThepoemsgeneratedbySMTarebetterinPoeticnessthanRNNLM,whichdemonstratesthatthe translationbasedmethodcanbettercapturethemappingrelationbetweentwoadjacentlines.ANMTis astrongbaselinewhichperformsbetterthanSMT,RNNLMandRNNPG,butlowerthanourapproach. BothANMTandPPGusetheattentionbasedenc-decframework.Themaindifferenceisthatourmethod thesub-topicsforeachlinebeforegeneratingthepoem.TheANMTmethodjusttranslatesthe precedingtextintothenextline.Withouttheguideofsub-topics,thesystemtendstogeneratemore generalbutlessmeaningfulresults.Incontrast,ourapproachexplicitlyconsidersthekeywords,which WronglyMPasHP CannotDistinguish SuccessfullyHPasHP NormalGroup 38.6% 11.3% 50.1% ExpertGroup 6.3% 10.0% 83.7% Table5:BlindtesttodistinguishHuman-writtenPoems(HP)fromMachine-generatedPoems(MP). Ë  V  Ë  V  ByaLakeatAutumnSunset ByaLakeatAutumnSunset  ˝ Ë É è  c  { ± Î Ì B ± n  Acoldautumnrainwettedmyclotheslastnight, Thewindblowsreedswithosmanthus  Š ì P ù  V  h ù ˜ ‚ à 2 A  AndIsitalonebythewindowandenjoythesunset. Andthebamboosundercloudsaresogreenasiftowdown. V â a > C q r   Â J V ° \ b  Withmountainscenerymirroredontheripplinglake, Themistyrainripplesthesmoothsurfaceoflake, q ˜ Ÿ−  A ®  Þ e ß è ® )   Asilenceprevailsoverallexceptthehoveringbirds. AndIfeelblueatsunset. Table6:Apairofpoemsselectedfromtheblindtest.Theleftoneisamachine-generatedpoem,and therightoneiswrittenbyShaotiGe,apoetlivedintheSongDynasty. hasbettercontrolsofthesub-topicforeveryline.Fromtheresultsofthehumanevaluation,itcanbe seenthattheproposedmethodobtainedverycloseperformancesinPoeticnessandFluencycompared withANMTbutmuchhigherCoherenceandMeaningscores,whichvtheeffectivenessofthe sub-topicpredictionmodel. 4.4AutomaticGenerationvs.HumanPoet Weconductedaninterestingevaluationthatdirectlycomparesourautomaticpoemgenerationsystem withhumanpoets,whichissimilartotheTuringTest(Turing,1950).Werandomlyselectedtwenty poemsfromthetestset,whicharewrittenbyancientChinesepoets.Weusedthetitlesofthesepoemsas theinputandgenerated20poemsbyourautomaticgenerationsystem.Therefore,themachine-generated poemswereunderthesamesubjectwithhuman-writtenpoems.Thenweaskedsomehumanevaluators todistinguishthehuman-writtenpoemsfrommachine-generatedones.Wehad40evaluatorsintotal. Allofthemwerewell-educatedandhadBachelororhigherdegree.Fourofthemwereprofessionalin ChineseliteratureandwereassignedtotheExpertGroup.Theotherthirty-sixevaluatorswereassigned totheNormalGroup.Intheblindtest,weshowedapairofpoemsandtheirtitletotheevaluatorateach time,andtheevaluatorwasaskedtochoosefromthreeoptions:(1)poemAiswrittenbythehuman;(2) poemBiswrittenbythehuman;(3)cannotdistinguishwhichoneiswrittenbythehuman. TheevaluationresultsareshowninTable5.Wecanseethat49.9%ofthemachine-generatedpoems arewronglyasthehuman-writtenpoemsorcannotbedistinguishedbythenormalevaluators. Butforexpertevaluators,thisnumberdropsto16.3%.Wecandrawtwoconclusionsfromtheresult: (1)underthestandardofnormalusers,thequalityofourmachine-generatedpoemsisverycloseto humanpoets;(2)butfromtheviewofprofessionalexperts,themachine-generatedpoemsstillhave someobviousshortagescomparingtohuman-writtenpoems.Table6givesanexampleforapairof poemsselectedfromourblindtest. 4.5GenerationExamples BesidestheancientpoemsinTable6,ourmethodcangeneratepoemsbasedanymodernterms.Table 7showssomeexamples.ThetitleoftheleftpoeminTable7is d R (beer) ,thekeywordsgivenbyour poemplanningmodelare d R (beer) , Ž ⁄ (aroma) ,  = (cool) and › (drunk) .Thetitleoftheright oneisanamedentity ° Ã (XinBing) ,whowasafamouswriter.Thepoemplanningsystemgenerates threekeywordsbesides ° Ã (XinBing) : % 4 (springriver) , A ˜ (stars) and • ‰ (thepast) ,whichare allrelatedtothewriter'sworks. d R ° Ã Beer XinBing Ê µ d R  8   G ° Ã     Idrinkglassesofbeertonight, Iopenupmypurehearttothemoon, o Ł Ž ⁄ % À I  C q % 4 q n ˜  Withthebottomoftheglassfullofaromaandamberlight. Withthespringriverwingpastmountains.  = Ñ Î É  ¨  A ˜ ê Á ) ¯ ï  Feelingcoldastheautumnwindblows, Althoughmyfutureisilluminatedbystars, ›      Š  • ‰ &  ¦ Ì L  Igetdrunkandenjoythemooninsightbythewestwindow. Thepaststilllingersinmydream. Table7:Examplesofpoemsgeneratedfromtitlesofmodernconcepts. 5ConclusionandFutureWork Inthispaper,weproposedanoveltwo-stagepoetrygenerationmethodwhichexplicitlydecomposes theuser'swritingintentintoaseriesofsub-topics,andthengeneratesapoemiterativelyusinga attentionbasedRNNencoder-decoderframework.TheRNNenc-decmodelhastwoencoders thatcanencodeboththesub-topicandtheprecedingtext.Theevaluationbyhumanexpertsshows thatourapproachoutperformsallthebaselinemodelsandthepoemqualityissomehowcomparableto humanpoets.Wehavealsodemonstratedthatusingencyclopediasasanextrasourceofknowledge,our approachcanexpandusers'inputintoappropriatesub-topicsforpoemgeneration.Inthefuture,wewill investigatemoremethodsfortopicplanning,suchasPLSA,LDAorword2vec.Wewillalsoapplyour approachtootherformsofliterarygenrese.g.Songiambics,YuanQuetc.,orpoemsinotherlanguages. 6Acknowledgments ThisresearchwassupportedbytheNationalBasicResearchProgramofChina(973program No.2014CB340505),theNationalKeyResearchandDevelopmentProgramofChina(GrantNo. 2016YFB1000904),theNationalScienceFoundationforDistinguishedYoungScholarsofChina(Grant No.61325010)andtheFundamentalResearchFundsfortheCentralUniversitiesofChina(GrantNo. WK2350000001).WewouldliketothankXuanLiu,QiLiu,TongXu,LinliXu,BiaoChangandthe anonymousreviewersfortheirinsightfulcommentsandsuggestions. References [Bahdanauetal.2014] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.2014.Neuralmachinetranslation byjointlylearningtoalignandtranslate. arXivpreprintarXiv:1409.0473 . [BrinandPage1998] SergeyBrinandLawrencePage.1998.Theanatomyofalarge-scalehypertextualwebsearch engine. ComputerNetworks ,30:107Œ117. [Choetal.2014] KyunghyunCho,BartVanMerri ¨ enboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares, HolgerSchwenk,andYoshuaBengio.2014.Learningphraserepresentationsusingrnnencoder-decoderfor statisticalmachinetranslation. arXivpreprintarXiv:1406.1078 . [Coltonetal.2012] SimonColton,JacobGoodwin,andTonyVeale.2012.Full-facepoetrygeneration.In ICCC . [Graves2013] AlexGraves.2013.Generatingsequenceswithrecurrentneuralnetworks. arXivpreprint arXiv:1308.0850 . [Greeneetal.2010] EricaGreene,TugbaBodrumlu,andKevinKnight.2010.Automaticanalysisofrhythmic poetrywithapplicationstogenerationandtranslation.In EMNLP . [Heetal.2012] JingHe,MingZhou,andLongJiang.2012.Generatingchineseclassicalpoemswithstatistical machinetranslationmodels.In Twenty-SixthAAAIConferenceonIntelligence . [JiangandZhou2008] LongJiangandMingZhou.2008.Generatingchinesecoupletsusingastatisticalmtap- proach.In Proceedingsofthe22ndInternationalConferenceonComputationalLinguistics-Volume1 ,pages 377Œ384.AssociationforComputationalLinguistics. [Liuetal.2016] Chia-WeiLiu,RyanLowe,IulianVSerban,MichaelNoseworthy,LaurentCharlin,andJoelle Pineau.2016.Hownottoevaluateyourdialoguesystem:Anempiricalstudyofunsupervisedevaluation metricsfordialogueresponsegeneration. arXivpreprintarXiv:1603.08023 . [Manurungetal.2012] RuliManurung,GraemeRitchie,andHenryThompson.2012.Usinggeneticalgorithmsto createmeaningfulpoetictext. JournalofExperimental&TheoreticalIntelligence ,24(1):43Œ64. [Manurung2004] HisarManurung.2004.Anevolutionaryalgorithmapproachtopoetrygeneration. [MarjanGhazvininejadandKnight2016] YejinChoiMarjanGhazvininejad,XingShiandKevinKnight.2016. Generatingtopicalpoetry.In EMNLP . [MihalceaandTarau2004] RadaMihalceaandPaulTarau.2004.Textrank:Bringingorderintotext.In EMNLP . [Mikolovetal.2010] TomasMikolov,Martin ´ at,LukasBurget,JanCernock ˚ y,andSanjeevKhudanpur.2010. Recurrentneuralnetworkbasedlanguagemodel.In INTERSPEECH ,volume2,page3. [Mikolovetal.2013] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.2013.Efestimationof wordrepresentationsinvectorspace. arXivpreprintarXiv:1301.3781 . [Mouetal.2016] LiliMou,YipingSong,RuiYan,GeLi,LuZhang,andZhiJin.2016.Sequencetobackwardand forwardsequences:Acontent-introducingapproachtogenerativeshort-textconversation.In Proceedingsthe 26thInternationalConferenceonComputationalLinguistics . [Netzeretal.2009] YaelNetzer,DavidGabay,YoavGoldberg,andMichaelElhadad.2009.Gaiku:Generating haikuwithwordassociationsnorms.In ProceedingsoftheWorkshoponComputationalApproachestoLin- guisticCreativity ,pages32Œ39.AssociationforComputationalLinguistics. [Oliveiraetal.2014] HugoGonc¸aloOliveira,RaquelHerv ´ as,AlbertoD ´ andPabloGerv ´ as.2014.Adaptinga genericplatformforpoetrygenerationtoproducespanishpoems.In ICCC . [Oliveira2009] HugoGonc¸aloOliveira.2009.Automaticgenerationofpoetry:anoverview. Universidadede Coimbra . [Oliveira2012] HugoGonc¸aloOliveira.2012.Poetryme:aversatileplatformforpoetrygeneration. Computa- tionalCreativity,ConceptInvention,andGeneralIntelligence ,1:21. [Schatzmannetal.2005] JostSchatzmann,KallirroiGeorgila,andSteveYoung.2005.Quantitativeevaluation ofusersimulationtechniquesforspokendialoguesystems.In 6thSIGdialWorkshoponDISCOURSEand DIALOGUE . [Sutskeveretal.2014] IlyaSutskever,OriolVinyals,andQuocVLe.2014.Sequencetosequencelearningwith neuralnetworks.In Advancesinneuralinformationprocessingsystems ,pages3104Œ3112. [Tosaetal.2008] NaokoTosa,HidetoObara,andMichihikoMinoh.2008.Hitchhaiku:Aninteractivesupporting systemforcomposinghaikupoem.In EntertainmentComputing-ICEC2008 ,pages209Œ216.Springer. [Turing1950] AlanMTuring.1950.Computingmachineryandintelligence. Mind ,59(236):433Œ460. [Wangetal.2016] QixinWang,TianyiLuo,DongWang,andChaoXing.2016.Chinesesongiambicsgeneration withneuralattention-basedmodel. CoRR ,abs/1604.06274. [Wang2002] LiWang.2002.Asummaryofrhymingconstraintsofchinesepoems. [Wuetal.2009] XiaofengWu,NaokoTosa,andRyoheiNakatsu.2009.Newhitchhaiku:Aninteractiverenku poemcompositionsupportingtoolappliedforsightseeingnavigationsystem.In EntertainmentComputingŒ ICEC2009 ,pages191Œ196.Springer. [Yanetal.2013] RuiYan,HanJiang,MirellaLapata,Shou-DeLin,XueqiangLv,andXiaomingLi.2013.i, poet:Automaticchinesepoetrycompositionthroughagenerativesummarizationframeworkunderconstrained optimization.In IJCAI . [Yietal.2016] XiaoyuanYi,RuoyuLi,andMaosongSun.2016.Generatingchineseclassicalpoemswithrnn encoder-decoder. CoRR ,abs/1604.01537. [Zeiler2012] MatthewD.Zeiler.2012.Adadelta:Anadaptivelearningratemethod. CoRR ,abs/1212.5701. [ZhangandLapata2014] XingxingZhangandMirellaLapata.2014.Chinesepoetrygenerationwithrecurrent neuralnetworks.In Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing (EMNLP) ,pages670Œ680,Doha,Qatar,October.AssociationforComputationalLinguistics. [Zhouetal.2010] Cheng-LeZhou,WeiYou,andXiaojunDing.2010.Geneticalgorithmanditsimplementation ofautomaticgenerationofchinesesongci. JournalofSoftware ,21(3):427Œ437.  
 A Simple PDF File  This is a small demonstration .pdf file -  just for use in the Virtual Mechanics tutorials. More text. And more  text. And more text. And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. And more text. Boring, zzzzz. And more text. And more text. And  more text. And more text. And more text. And more text. And more text.  And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. And more text. And more text. Even more. Continued on page 2 ... Simple PDF File 2  ...continued from page 1. Yet more text. And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. Oh, how boring typing this stuff. But not as boring as watching  paint dry. And more text. And more text. And more text. And more text.  Boring.  More, a little more text. The end, and just as well.  
JournalofMachineLearningResearch18(2018)1-42Submitted6/16;Revised5/17;Published4/18 ATightBoundofHardThresholding JieShen JS 2007@ RUTGERS . EDU DepartmentofComputerScience RutgersUniversity Piscataway,NJ08854,USA PingLi PINGLI 98@ GMAIL . COM BaiduResearch Bellevue,WA98004,USA Editor: SujaySanghavi Abstract Thispaperisconcernedwiththehardthresholdingoperatorwhichsetsallbutthe k largestabsolute elementsofavectortozero.Weestablisha tight boundtoquantitativelycharacterizethedeviation ofthethresholdedsolutionfromagivensignal.Ourtheoreticalresultisuniversalinthesensethat itholdsforallchoicesofparameters,andtheunderlyinganalysisdependsonlyonfundamental argumentsinmathematicaloptimization.Wediscusstheimplicationsfortwodomains: CompressedSensing. Onaccountofthecrucialestimate,webridgetheconnectionbetweenthe restrictedisometryproperty(RIP)andthesparsityparameterforavastvolumeofhardthresholding basedalgorithms,whichrendersanimprovementontheRIPconditionespeciallywhenthetrue sparsityisunknown.Thissuggeststhatinessence,manymorekindsofsensingmatricesorfewer measurementsareadmissibleforthedataacquisitionprocedure. MachineLearning. Intermsoflarge-scalemachinelearning,ayetchallengingprob- lemislearningaccuratesparsemodelsinanefmanner.Instarkcontrasttopriorwork thatattemptedthe ` 1 -relaxationforpromotingsparsity,wepresentanovelstochasticalgorithm whichperformshardthresholdingineachiteration,henceensuringsuchparsimonioussolutions. Equippedwiththedevelopedbound,weprovethe globallinearconvergence foranumberofpreva- lentstatisticalmodelsundermildassumptions,eventhoughtheproblemturnsouttobenon-convex. Keywords: sparsity,hardthresholding,compressedsensing,stochasticoptimization 1.Introduction Overthelasttwodecades,pursuingsparserepresentationshasemergedasafundamentaltechnique throughoutbioinformatics(OlshausenandField,1997),statistics(Tibshirani,1996;Efronetal., 2004),signalprocessing(Chenetal.,1998;Donohoetal.,2006;Donoho,2006;Cand ˚ esandWakin, 2008)andmathematicalscience(Chandrasekaranetal.,2012),tonamejustafew.Inordertoobtain asparsesolution,aplethoraofpracticalalgorithmshavebeenpresented,amongwhichtwopromi- nentexamplesaregreedypursuitandconvexrelaxation(TroppandWright,2010).Forinstance,as oneoftheearliestgreedyalgorithms,orthogonalmatchingpursuit(OMP)(Patietal.,1993)repeat- edlypicksacoordinateasthepotentialsupportofasolution.WhileOMPmayfailforsomedeter- ministicsensingmatrices,Tropp(2004);TroppandGilbert(2007)showedthatitrecoversthetrue signalwithhighprobabilitywhenusingrandommatricessuchasGaussian.Inspiredbythesuccess ofOMP,thetwoconcurrentworkofcompressivesamplingmatchingpursuit(CoSaMP)(Needell c  2018JieShenandPingLi. License:CC-BY4.0,see https://creativecommons.org/licenses/by/4.0/ .Attributionrequirementsareprovidedat http://jmlr.org/papers/v18/16-299.html . S HENAND L I andTropp,2009)andsubspacepursuit(SP)(DaiandMilenkovic,2009)madeimprovementbyse- lectingmultiplecoordinatesfollowedbyapruningstepineachiteration,andtherecoverycondition wasframedundertherestrictedisometryproperty(RIP)(Cand ˚ esandTao,2005).Interestingly,the morecarefulselectionstrategyofCoSaMPandSPleadstoanoptimalsamplecomplexity.The iterativehardthresholding(IHT)algorithm(Daubechiesetal.,2004;BlumensathandDavies,2008, 2009)graduallytheiteratesbygradientdescentalongwithtruncation.Foucart(2011)then developedaconcisealgorithmtermedhardthresholdingpursuit(HTP),whichcombinedtheidea ofCoSaMPandIHT,andshowedthatHTPissuperiortobothintermsoftheRIPcondition.Jain etal.(2011)proposedaninterestingvariantoftheHTPalgorithmandobtainedasharperRIPresult. Recently,Bahmanietal.(2013)andYuanetal.(2018)respectivelyextendedCoSaMPandHTPto generalobjectivefunctions,forwhichaglobalconvergencewasestablished. Sincethesparsityconstraintcountsthenumberofnon-zerocomponentswhichrendersthe problemnon-convex,the ` 1 -normwassuggestedasaconvexrelaxationdatingbacktobasispur- suit(Chenetal.,1998;DonohoandTsaig,2008)andLasso(Tibshirani,1996).Thedifferenceis thatLassolooksforan ` 1 -normconstrainedsolutionthatminimizestheresidualwhiletheprinciple ofbasispursuitistoasignalwithminimal ` 1 -normthattheobservationdata.Cand ˚ esand Tao(2005)carriedoutadetailedanalysisontherecoveryperformanceofbasispursuit.Another popularestimatorinthehigh-dimensionalstatisticsistheDantzigselector(Cand ˚ esandTao,2007) which,insteadofconstrainingtheresidualofthelinearmodel,penalizesthemaximummagnitude ofthegradient.Fromacomputationalperspective,bothbasispursuitandDantzigselectorcanbe solvedbylinearprogramming,whileLassoisformulatedasaquadraticproblem.Interestingly, undertheRIPconditionortheuniformuncertaintyassumption(Cand ˚ esetal.,2006),aseriesof workshowedthatexactrecoverybyconvexprogramsispossibleassoonastheobservationnoise vanishes(Cand ˚ esandTao,2005;Cand ˚ es,2008;Wainwright,2009;Caietal.,2010;Foucart,2012). Inthispaper,weareinterestedinthehardthresholding(HT)operatorunderlyingalargebodyof thedevelopedalgorithmsincompressedsensing(e.g.,IHT,CoSaMP,SP),machinelearning(Yuan andZhang,2013),andstatistics(Ma,2013).Ourmotivationistwo-fold.Fromahighlevel,com- paredtotheconvexprograms,theseHT-basedalgorithmsarealwaysordersofmagnitudecompu- tationallymoreefhencemorepracticalforlarge-scaleproblems(TroppandWright,2010). Nevertheless,theyusuallyrequireamorestringentconditiontoguaranteethesuccess.Thisnatu- rallyraisesaninterestingquestionofwhetherwecanderivemilderconditionsforHT-basedalgo- rithmstoachievethebestofthetwoworlds.Forpractitioners,toaddressthehugevolumeofdata, apopularstrategyinmachinelearningistoappealtostochasticalgorithmsthatsequentiallyupdate thesolution.However,asmanyresearchersobserved(Langfordetal.,2009;DuchiandSinger, 2009;Xiao,2010),itishardforthe ` 1 -basedstochasticalgorithmstopreservethesparsestructure ofthesolutionasthebatchsolversdo.Thisimmediatelyposesthequestionofwhetherweareable toapplytheprincipalideaofhardthresholdingtostochasticalgorithmswhilestillensuringafast convergence. Toelaboratetheproblemmoreprecisely,letusturntosomebasicpropertiesofhardthresh- oldingalongwithsimpleyetillustrativecases.Forageneralvector b 2 R d ,thehardthresholded signal H k ( b ) isformedbysettingallbutthelargest(inmagnitude) k elementsof b tozero.Ties arebrokenlexicographically.Hence,thehardthresholdedsignal H k ( b ) isalways k -sparse,i.e.,the numberofnon-zerocomponentsdoesnotexceed k .Moreover,theresultantsignal H k ( b ) isabest 2 AT IGHT B OUNDOF H ARD T HRESHOLDING k -sparseapproximationto b intermsofany ` p norm( p  1 ).Thatis,forany k -sparsevector x kH k ( b )  b k p k x  b k p : Inviewoftheaboveinequality,abroadlyusedboundintheliteratureforthedeviationofthe thresholdedsignalisasfollows: kH k ( b )  x k 2  2 k b  x k 2 : (1.1) Togainintuitionontheutilityof(1.1)andtospellouttheimportanceofofferingatightboundfor it,letusconsiderthecompressedsensingproblemasanexampleforwhichweaimtorecoverthe truesparsesignal x fromitslinearmeasurements.Here, b isagoodbutdenseapproximationto x obtainedby,e.g.,fullgradientdescent.Then(1.1)thatinordertoobtainastructured (i.e.,sparse)approximationbyhardthresholding,thedistanceoftheiteratetothetruesignal x is upperboundedbyamultipleof 2 totheonebefore.Forcomparison,itisworthmentioningthat ` 1 -basedconvexalgorithmsusuallyutilizethesoftthresholdingoperatorwhichenjoysthenon- expansivenessproperty(Defazioetal.,2014),i.e.,theiteratebecomesclosertotheoptimumafter projection.Thissalientfeaturemightpartiallyattributetothewiderangeofapplicationsofthe ` 1 -regularizedformulations.Hence,toderivecomparableperformanceguarantee,tighteningthe bound(1.1)iscrucialinthatitcontrolshowmuchdeviationthehardthresholdingoperatorinduces. Thisturnsouttobemoredemandingforstochasticgradientmethods,wheretheproxy b itself isaffectedbytherandomnessofsamplerealization.Inotherwords,since b doesnotminimize theobjectivefunction(itonlyoptimizestheobjectiveinexpectation),thedeviation(1.1)makes itmorechallengingtoanalyzetheconvergencebehavior.Asanexample,Nguyenetal.(2014) proposedastochasticsolverforgeneralsparsity-constrainedprogramsbutsufferedanon-vanishing optimizationerrorduetorandomness.Thisindicatesthattomitigatetherandomnessbarrier,we havetoseekabetterboundtocontroltheprecisionofthethresholdedsolutionandthevariance. 1.1SummaryofContributions Inthiswork,wemakethreecontributions: 1. Weexaminethetightnessof(1.1)thathasbeenusedforadecadeintheliteratureandshow thattheequalitythereinwillneverbeattained.Wethenimprovethisboundandquantitatively characterizethatthedeviationisinverselyproportionaltothevalueof p k .Ourboundistight, inthesensethattheequalitywebuildcanbeattainedforsignals,hencecannotbe improvedifnoadditionalinformationisavailable.Ourboundisuniversalinthesensethatit holdsforallchoicesof k -sparsesignals x andforgeneralsignals b . 2. Owingtothetightestimate,wedemonstratehowtheRIP(orRIP-like)conditionassumed byawiderangeofhardthresholdingbasedalgorithmscanberelaxed.Inthecontextof compressedsensing,itmeansthatinessence,manymorekindsofsensingmatricesorfewer measurementscanbeutilizedfordataacquisition.Formachinelearning,itsuggeststhat existingalgorithmsarecapableofhandlingmoredifstatisticalmodels. 3. Finally,wepresentancomputationallyefalgorithmthatapplieshardthresholdingin large-scalesettingandweproveitslinearconvergencetoaglobaloptimumuptothestatistical precisionoftheproblem.Wealsoprovethatwithsufsamples,ouralgorithm 3 S HENAND L I thetrueparameterforprevalentstatisticalmodels.Returningto(1.1),ouranalysisshows thatonlywhenthedeviationiscontrolledbelowthemultipleof 1 : 15 cansuchanalgorithm succeed.Thisimmediatelyimpliesthattheconventionalbound(1.1)isnotapplicableinthe challengingscenario. 1.2Notation Beforedeliveringthealgorithmandmaintheoreticalresults,letusinstateseveralpiecesofnotation thatareinvolvedthroughoutthepaper.Weuseboldlowercaseletters,e.g., v ,todenoteavector (eithercolumnorrow)andits i thelementisdenotedby v i .The ` 2 -normofavector v isdenotedby k v k 2 .Thesupportsetof v ,i.e.,indicesofnon-zeros,isdenotedby supp( v ) whosecardinalityis writtenas j supp( v ) j or k v k 0 .Wewriteboldcapitalletterssuchas M formatricesandits ( i;j ) -th entryisdenotedby m ij .Thecapitaluprightletter C anditssubscriptvariants(e.g., C 0 ; C 1 )are reservedforabsoluteconstantswhosevaluesmaychangefromappearancetoappearance. Foraninteger d> 0 ,supposethat  isasubsetof f 1 ; 2 ;:::;d g .Thenforageneralvector v 2 R d ,we P  (  ) astheorthogonalprojectionontothesupportset  whichretainselements containedin  andsetsotherstozero.Thatis, ( P  ( v )) i = ( v i ; if i 2  ; 0 ; otherwise : Inparticular,let  bethesupportsetindexingthe k largestabsolutecomponentsof v .Inthisway, thehardthresholdingoperatorisgivenby H k ( v )= P  ( v ) : Wewillalsousetheorthogonalprojectionofavector v ontoan ` 2 -ballwithradius ! .Thatis,  ! ( v )= v max f 1 ; k v k 2 =! g : 1.3Roadmap WepresentthekeytightboundforhardthresholdinginSection2,alongwithawhy theconventionalbound(1.1)isnottight.Wethendiscusstheimplicationsofthedevelopedtight boundtocompressedsensingandmachinelearninginSection3,whichshowsthattheRIPorRIP- likeconditioncanbeimprovedforanumberofpopularalgorithms.Thankstoournewestimation, Section4developsanovelstochasticalgorithmwhichapplieshardthresholdingtolarge-scaleprob- lemsandestablishesthegloballinearconvergence.Acomprehensiveempiricalstudyonthetasks ofsparserecoveryandbinaryclassiiscarriedoutinSection5.Finally,Weconcludethe paperinSection6andalltheproofsaredeferredtotheappendix. 2.TheKeyBound Wearguethattheconventionalbound(1.1)isnottight,inthesensethattheequalitythereincan hardlybeattained.Toseethis,recallhowtheboundwasderivedfora k -sparsesignal x anda generalone b : kH k ( b )  x k 2 = kH k ( b )  b + b  x k 2 ˘ kH k ( b )  b k 2 + k b  x k 2  2 k b  x k 2 ; 4 AT IGHT B OUNDOF H ARD T HRESHOLDING wherethelastinequalityholdsbecause H k ( b ) isabest k -sparseapproximationto b .Themajor issueoccursin ˘ .Thoughitisthewell-knowntriangleinequalityandtheequalitycouldbeattained ifthereisnorestrictiononthesignals x and b ,weremindherethatthesignal x doeshavea structureŒitis k -sparse.Notethatinordertotheequalityin ˘ ,wemusthave H k ( b )  b =  ( b  x ) forsome   0 ,thatis, H k ( b )=(  +1) b   x : (2.1) Onemayverifythattheaboveequalityholds ifandonlyif x = b = H k ( b ) : (2.2) Toseethis,let  bethesupportsetof H k ( b ) and  bethecomplement.Let b 1 = P  ( b )= H k ( b ) and b 2 = P  ( b ) .Likewise,we x 1 and x 2 asthecomponentsof x supportedon  and  respectively.Hence,(2.1)indicates x 1 = b 1 and x 2 =(1+   1 ) b 2 whereweassume > 0 since  =0 immediatelyimplies H k ( b )= b andhencetheequalityof(1.1)doesnothold.If k b 1 k 0 <k , thenwehave x 2 = b 2 = 0 since b 1 containsthe k largestabsoluteelementsof b .Otherwise,the factthat k x k 0  k and x 1 = b 1 implies x 2 = 0 ,andhence b 2 .Therefore,weobtain(2.2). When(2.2)happens,however,weinrealityhave kH k ( b )  x k 2 = k b  x k 2 =0 .Inother words,thefactorof 2 in(1.1)canessentiallybereplacedwithan arbitraryconstant !Inthissense, weconcludethatthebound(1.1)isnottight.Ournewestimateforhardthresholdingisasfollows: Theorem1(TightBoundforHardThresholding) Let b 2 R d beanarbitraryvectorand x 2 R d beany K -sparsesignal.Forany k  K ,wehavethefollowingbound: kH k ( b )  x k 2  p  k b  x k 2 ; =1+ ˆ + p (4+ ˆ ) ˆ 2 ;ˆ = min f K;d  k g k  K +min f K;d  k g : Inparticular,ourboundistightinthesensethatthereexistvectorsof b and x suchthatthe equalityholds. Remark2(Maximumof  ) Incontrasttotheconstantbound (1.1) ,ourresultassertsthatthe deviationresultingfromhardthresholdingisinverselyproportionalto p k (when K  d  k )in auniversalmanner.When k tendsto d , ˆ isgivenby ( d  k ) = ( d  K ) whichisstilldecreasing withrespectto k .Thus,themaximumvalueof ˆ equalsone.Eveninthiscase,wethat p  max = q 1+ p 5+1 2 = p 5+1 2 ˇ 1 : 618 . Remark3 ThoughforsomebatchalgorithmssuchasIHTandCoSaMP,theconstantbound (1.1) suftoestablishtheconvergenceduetoconditions,weshowinSection4thatitcannot ensuretheglobalconvergenceforstochasticalgorithms. Remark4 When x isnotexactly K -sparse,westillcanboundtheerrorby kH k ( b )  x k 2  kH k ( b ) H k ( x ) k 2 + kH k ( x )  x k 2 .Thus,withoutlossofgenerality,weassumedthatthesignal x is K -sparse. Proof (Sketch)Ourboundfollowsfromfullyexploringthesparsitypatternofthesignalsandfrom fundamentalargumentsinoptimization.Denote w := H k ( b ) : 5 S HENAND L I Let  bethesupportsetof w andlet  beitscomplement.Weimmediatelyhave P  ( b )= w .Let  0 bethesupportsetof x . b 1 = P  n  0 ( b ) ; b 2 = P  \  0 ( b ) ; b 3 = P  n  0 ( b ) ; b 4 = P  \  0 ( b ) : Likewise,we x i and w i for 1  i  4 .Duetotheconstruction,wehave w 1 = b 1 ; w 2 = b 2 ; w 3 = w 4 = x 1 = x 3 = 0 .Ourgoalistoestimatethemaximumvalueof k w  x k 2 2 = k b  x k 2 2 . Itiseasytoshowthatwhenattainingthemaximum, k b 3 k 2 mustbezero.Denote  := k w  x k 2 2 k b  x k 2 2 = k b 1 k 2 2 + k b 2  x 2 k 2 2 + k x 4 k 2 2 k b 1 k 2 2 + k b 2  x 2 k 2 2 + k b 4  x 4 k 2 2 : (2.3) Notethatthevariableshereonlyinvolve x and b .Arrangingtheequationweobtain (   1) k b 2  x 2 k 2 2 +  k b 4  x 4 k 2 2 k x 4 k 2 2 +(   1) k b 1 k 2 2 =0 : (2.4) Itisevidentthatforchoicesof b and x ,wehave  =1 .Sinceweareinterestedinthe maximumof  ,weassume > 1 below.Fixing b ,wecanviewtheleft-handsideoftheabove equationasafunctionof x .OnemayverifythatthefunctionhasapositiveHessianmatrix andthusitattainstheminimumatstationarypointgivenby x  2 = b 2 ; x  4 =    1 b 4 : (2.5) Ontheotherhand,(2.4)impliesthattheminimumfunctionvalueshouldnotbegreaterthanzero. Pluggingthestationarypointbackgives k b 1 k 2 2  2  (2 k b 1 k 2 2 + k b 4 k 2 2 )  + k b 1 k 2 2  0 : Solvingtheaboveinequalitywithrespectto  ,weobtain   1+  2 k b 1 k 2 2   1   k b 4 k 2 2 + r  4 k b 1 k 2 2 + k b 4 k 2 2  k b 4 k 2 2 ! : (2.6) Toderiveanupperboundthatisuniformoverthechoiceof b ,werecallthat b 1 containsthelargest absoluteelementsof b while b 4 hassmallervalues.Inparticular,theaveragein b 1 islargerthan thatin b 4 ,whichgives k b 4 k 2 2 = k b 4 k 0 k b 1 k 2 2 = k b 1 k 0 : Notethat k b 1 k 0 = k k b 2 k 0 = k  ( K k b 4 k 0 ) .Hence,combiningwiththefactthat 0 k b 4 k 0  min f K;d  k g andoptimizingover k b 4 k 0 intheaboveinequalitygives k b 4 k 2 2  min f K;d  k g k  K +min f K;d  k g k b 1 k 2 2 : (2.7) Finally,wearriveatauniformupperbound   1+ ˆ + p (4+ ˆ ) ˆ 2 ;ˆ = min f K;d  k g k  K +min f K;d  k g : SeeAppendixBforthefullproof. 6 AT IGHT B OUNDOF H ARD T HRESHOLDING Remark5(Tightness) Weconstructpropervectors b and x toestablishthetightnessofourbound byabackwardinduction.Notethat  equals  ifandonlyif k b 4 k 2 2 = ˆ k b 1 k 2 2 .Hence,wepick k b 4 k 2 2 = ˆ k b 1 k 2 2 ; x 2 = b 2 ; x 4 =    1 b 4 ; (2.8) where x 2 and x 4 areactuallychosenasthestationarypointasin (2.5) .Wenotethatthequantityof  onlydependson d , k and K ,notonthecomponentsof b or x .Pluggingtheabovebackto (2.3)   =  . Itremainstoshowthatourchoicesin (2.8) donotviolatetheof b i 's,i.e.,weneedto ensurethattheelementsin b 1 or b 2 areequaltoorgreaterthanthosein b 3 or b 4 .Notethatthereis nosuchconstraintforthe K -sparsevector x .Letusconsiderthecase K<d  k and k b 4 k 0 = K , sothat k b 1 k 0 = k and ˆ = K=k .Thus,thestequalityof (2.8) holdsassoonasalltheentriesof b havesamemagnitude.Thefact k b 4 k 0 = K alsoimplies  0 isasubsetof  duetothe of b 4 andthesparsityof x ,hencewehave x 2 = 0 = b 2 .Finally,picking x 4 aswedidin (2.8) completesthereasoningsinceitdoesnotviolatethesparsityconstrainton x . AswepointedoutandjustvtheboundgivenbyTheorem1istight.However,ifthereis additionalinformationforthesignals,abetterboundcanbeestablished.Forinstance,letusfurther assumethatthesignal b is r -sparse.If r  k ,then b 4 isazerovectorand(2.6)readsas   1 . Otherwise,wehave k b 4 k 0  min f K;r  k g and(2.7)isimprovedto k b 4 k 2 2  min f K;r  k g k  K +min f K;r  k g k b 1 k 2 2 : Henceforth,wecanshowthattheparameter ˆ isgivenby ˆ = min f K;r  k g k  K +min f K;r  k g : Notethatthefact r  d impliesthattheaboveisatighterboundthantheoneinTheorem1. WewouldalsoliketomentionthatinLemma1ofJainetal.(2014),acloselyrelatedbound wasestablished: kH k ( b )  b k 2  r d  k d  K k b  x k 2 : (2.9) Onemayusethisniceresulttoshowthat kH k ( b )  x k 2 kH k ( b )  b k 2 + k b  x k 2    1+ r d  k d  K ! k b  x k 2 ; (2.10) whichalsoimproveson(1.1)provided k>K .However,oneshortcomingof(2.10)isthatthe factordependsonthedimension.Forcomparison,werecallthatintheregime K  d  k ,our boundisfreeofthedimension.Thisturnsouttobeasalientfeaturetointegratehardthresholding intostochasticmethods,andwewillcommentonitmoreinSection4. 7 S HENAND L I 3.ImplicationstoCompressedSensing Inthissection,weinvestigatetheimplicationsofTheorem1forcompressedsensingandsignal processing.SincemostoftheHT-basedalgorithmsutilizethedeviationbound(1.1)toderivethe convergencecondition,theycanbeimprovedbyournewbound.Weexemplifythepowerofour theoremontwopopularalgorithms:IHT(BlumensathandDavies,2009)andCoSaMP(Needell andTropp,2009).WenotethatouranalysisalsoappliestotheirextensionssuchasBahmanietal. (2013).Tobeclear,thepurposeofthissectionisnotdedicatedtoimprovingthebestRIPcondition forwhichrecoveryispossiblebyanymethods(eitherconvexornon-convex).Rather,wefocuson twobroadlyusedgreedyalgorithmsandillustratehowourboundimprovesonpreviousresults. Weproceedwithabriefreviewoftheproblemsettingincompressedsensing.Compressed sensingalgorithmsaimtorecoverthetrue K -sparsesignal x  2 R d fromasetofits(perhaps noisy)measurements y = Ax  + " ; (3.1) where " 2 R d issomeobservationnoiseand A isaknown n  d sensingmatrixwith n ˝ d , hencethenamecompressivesampling.Ingeneral,themodelisnotsinceitisanunder- determinedsystem.Yet,thepriorknowledgethat x  issparseradicallychangesthepremise.That is,ifthegeometryofthesparsesignalispreservedundertheactionofthesamplingmatrix A fora restrictedsetofdirections,thenitispossibletoinvertthesamplingprocess.Suchanovelideawas asthe k threstrictedisometrypropertyof A byCand ˚ esandTao(2005),whichrequires thatthereexistsaconstant   0 ,suchthatforall k -sparsesignals x (1   ) k x k 2 2 k Ax k 2 2  (1+  ) k x k 2 2 : (3.2) The k threstrictedisometryconstant(RIC)  k isthenasthesmallestonethatthe aboveinequalities.Notethat  2 k < 1 istheminimumrequirementfordistinguishingall k -sparse signalsfromthemeasurements.Thisisbecausefortwoarbitrary k -sparsevectors x 1 and x 2 and theirrespectivemeasurements y 1 and y 2 ,theRIPconditionreadsas (1   2 k ) k x 1  x 2 k 2 2 k y 1  y 2 k 2 2  (1+  2 k ) k x 1  x 2 k 2 2 ; forwhich  2 k < 1 guaranteesthat x 1 6 = x 2 implies y 1 6 = y 2 .Todate,therearethreequintessen- tialexamplesknowntoexhibitaprofoundrestrictedisometrybehavioraslongasthenumberof measurementsislargeenough:Gaussianmatrices(optimalRIP,i.e.,verysmall  k ),partialFourier matrices(fastcomputation)andBernoulliensembles(lowmemoryfootprint).Notably,itwasshown inrecentworkthatrandommatriceswithaheavy-taileddistributionalsosatisfytheRIPwithover- whelmingprobability(Adamczaketal.,2011;Lietal.,2014). EquippedwiththestandardRIPcondition,manyefalgorithmshavebeendeveloped. Apartiallistincludes ` 1 -normbasedconvexprograms,IHT,CoSaMP,SPandregularizedOMP (NeedellandVershynin,2010),alongwithmuchinterestingworkdevotedtoimprovingorsharp- eningtheRIPcondition(WangandShim,2012;MoandShen,2012;CaiandZhang,2013;Mo, 2015).ToseewhyrelaxingRIPisofcentralinterest,notethatthestandardresult(Baraniuketal., 2008)assertsthattheRIPcondition  k   holdswithhighprobabilityoverthedrawof A provided n  C 0   2 k log( d=k ) : (3.3) Hence,aslightrelaxationofthecondition  k   maydramaticallydecreasethenumberofmea- surements.Thatbeingsaid,sincetheconstant C 0 aboveisunknown,ingeneralonecannottell 8 AT IGHT B OUNDOF H ARD T HRESHOLDING theprecisesamplesizeforgreedyalgorithms.Estimatingtheconstantisactuallythethemeof phasetransition(DonohoandTanner,2010;Donohoetal.,2013).Whileprecisephasetransition for ` 1 -basedconvexprogramshasbeenwellunderstood(Wainwright,2009),ananalogousresult forgreedyalgorithmsremainsanopenproblem.Notably,inBlanchardandTanner(2015),phase transitionforIHT/CoSaMPwasderivedusingtheconstantbound(1.1).Webelievethatourtight boundshallsharpentheseresultsandweleaveitasourfuturework.Inthepresentpaper,wefocus ontheubiquitousRIPcondition.InthelanguageofRIP,weestablishimprovedresults. 3.1IterativeHardThresholding TheIHTalgorithmrecoverstheunderlying K -sparsesignal x  byiterativelyperformingafull gradientdescentontheleast-squareslossfollowedbyahardthresholdingstep.Thatis,IHTstarts withanarbitrarypoint x 0 andatthe t -thiteration,itupdatesthenewsolutionasfollows: x t = H k  x t  1 + A > ( y  Ax t  1 )  : (3.4) NotethatBlumensathandDavies(2009)usedtheparameter k = K .However,inpracticeonemay onlyknowtoanupperboundonthetruesparsity K .Thus,weconsidertheprojectionsparsity k as aparameterthatdependson K .Toestablishtheglobalconvergencewithageometricrateof 0 : 5 , BlumensathandDavies(2009)appliedthebound(1.1)andassumedtheRIPcondition  2 k + K  0 : 18 : (3.5) Aswehaveshown,(1.1)isactuallynottightandhence,theirresults,especiallytheRIPcondition canbeimprovedbyTheorem1. Theorem6 Considerthemodel (3.1) andtheIHTalgorithm (3.4) .Pick k  K andlet f x t g t  1 be theiteratesproducedbyIHT.Then,undertheRIPcondition  2 k + K  1 = p 8  ,forall t  1   x t  x    2  0 : 5 t   x 0  x    2 +C k " k 2 ; where  isgivenbyTheorem1. Letusstudythevanillacase k = K .BlumensathandDavies(2009)required  3 K  0 : 18 whereasouranalysisshows  3 K  0 : 22 sufNotethatevenalittlerelaxationonRIP ischallengingandmayrequireseveralpagesofmathematicalinduction(Cand ˚ es,2008;Caietal., 2010;Foucart,2012).Incontrast,ourimprovementcomesfromadirectapplicationofTheorem1 whichonlyseverallinesoftheoriginalproofinBlumensathandDavies(2009).See AppendixCfordetails.Inviewof(3.3),wethatthenecessarynumberofmeasurements forIHTisdramaticallyreducedwithafactorof 0 : 67 byournewtheoreminthattheminimum requirementof n isinverselyproportionaltothesquareof  2 k + K . AnotherimportantconsequenceofthetheoremisacharacterizationontheRIPconditionand thesparsityparameter,which,tothebestofourknowledge,hasnotbeenstudiedintheliterature.In BlumensathandDavies(2009),whengraduallytuning k largerthan K ,italwaysrequires  2 k + K  0 : 18 .NotethatduetothemonotonicityofRIC,i.e.,  r   r 0 if r  r 0 ,theconditionturnsout tobemoreandmorestringent.Comparedtotheirresult,since  isinverselyproportionalto p k , Theorem6ispowerfulespeciallywhen k becomeslarger.Forexample,suppose k =20 K .Inthis 9 S HENAND L I case,Theorem6thatIHTadmitsthelinearconvergenceassoonas  41 K  0 : 32 whereas BlumensathandDavies(2009)requires  41 K  0 : 18 .Suchapropertyisappealinginpractice,in thatamongvariousreal-worldapplications,thetruesparsityisindeedunknownandwewouldlike toestimateaconservativeupperboundonit. Ontheotherhand,foragivensensingmatrix,theredoesexistafundamentallimitforthemax- imumchoiceof k .Tobemoreprecise,theconditioninTheorem6togetherwiththeprobabilistic argument(3.3)require 1 = p 8    2 k + K ; C 1  (2 k + K )log( d= (2 k + K ))  n: Althoughitcouldbeveryinterestingtoderiveaquantitativecharacterizationforthemaximum valueof k ,wearguethatitisperhapsintractableowingtotwoaspects:First,itisknownthat onehastoenumerateallthecombinationsofthe 2 k + K columnsof A tocomputetherestricted isometryconstant  2 k + K (BahandTanner,2010,2014).ThissuggeststhatitisNP-hardtoestimate thelargestadmissiblevalueof k .Also,thereisnoanalyticsolutionofthestationarypointforthe left-handsideofthesecondinequality. 3.2CompressiveSamplingMatchingPursuit TheCoSaMPalgorithmproposedbyNeedellandTropp(2009)isoneofthemostefalgo- rithmsforsparserecovery.Let F ( x )= k y  Ax k 2 2 .CoSaMPstartsfromanarbitraryinitialpoint x 0 andproceedsasfollows:  t =supp  r F ( x t  1 ) ;k  [ supp  x t  1  ; b t =argmin x F ( x ) ; s : t : supp( x ) ˆ  t ; x t = H k  b t  : ComparedtoIHTwhichperformshardthresholdingaftergradientupdate,CoSaMPprunesthe gradientatthebeginningofeachiteration,followedbysolvingaleast-squaresprogramrestricted onasmallsupportset.Inparticular,inthelaststep,CoSaMPapplieshardthresholdingtoforma k -sparseiterateforfutureupdates.TheanalysisofCoSaMPconsistsofboundingtheestimation errorineachstep.OwingtoTheorem1,weadvancethetheoreticalresultofCoSaMPbyimproving theerrorboundforitslaststep,andhencetheRIPcondition. Theorem7 Considerthemodel (3.1) andtheCoSaMPalgorithm.Pick k  K andlet f x t g t  1 be theiteratesproducedbyCoSaMP.Then,undertheRIPcondition  3 k + K   p 32  +49  9  1 = 2 4 p   1 ; itholdsthatforall t  1   x t  x    2  0 : 5 t   x 0  x    2 +C k " k 2 ; where  isgivenbyTheorem1. 10 AT IGHT B OUNDOF H ARD T HRESHOLDING Roughlyspeaking,theboundisstillinverselyproportionalto p  .Hence,itismonotonically increasingwithrespectto k ,indicatingourtheoremismoreeffectiveforalargequantityof k .In fact,fortheCoSaMPalgorithm,ourboundaboveissuperiortothebestknownresultevenwhen k = K .Toseethis,wehavetheRIPcondition  4 K  0 : 31 .Incomparison,NeedellandTropp (2009)derivedabound  4 K  0 : 1 andFoucartandRauhut(2013,Theorem6.27)improveditto  4 K < 0 : 29 forageometricrateof 0 : 5 .Wenoticethatforbinarysparsevectors,Jainetal.(2014) presentedadifferentprooftechniqueandobtainedtheRIPcondition  4 K  0 : 35 forCoSaMP. 4.HardThresholdinginLarge-ScaleOptimization Nowwemoveontothemachinelearningsettingwhereourfocusispursuinganoptimalsparse solutionthatminimizesagivenobjectivefunctionbasedonasetoftrainingsamples Z n 1 := f Z i g n i =1 . Differentfromcompressedsensing,weusuallyhavesufsampleswhichmeans n canbevery large.Therefore,thecomputationalcomplexityisofprimaryinterest.Formally,weareinterested inoptimizingthefollowingprogram: min x 2 R d F ( x ; Z n 1 )= 1 n n X i =1 f ( x ; Z i ) ; s : t : k x k 0  K; k x k 2  !: (4.1) Theglobaloptimumoftheaboveproblemisdenotedby x opt .Wenotethattheobjectivefunctionis presumedtobedecomposablewithrespecttothesamples.Thisisquiteamildconditionandmost ofthepopularmachinelearningmodelsit.Typicalexamplesinclude(butnotlimitedto)the sparselinearregressionandsparselogisticregression:  SparseLinearRegression:Forall 1  i  n ,wehave Z i =( a i ;y i ) 2 R d  R and thelossfunction F ( x ; Z n 1 )= 1 2 n k Ax  y k 2 2 istheleast-squaresandcanbeexplainedby f ( x ; Z i )= 1 2 k a i  x  y i k 2 2 .  SparseLogisticRegression:Forall 1  i  n ,wehave Z i =( a i ;y i ) 2 R d f +1 ;  1 g and thenegativelog-likelihoodispenalized,i.e., F ( x ; Z n 1 )= 1 n P n i =1 log(1+exp(  y i a i  x )) forwhich f ( x ; Z i )=log(1+exp(  y i a i  x )) . Toeasenotation,wewilloftenwrite F ( x ; Z n 1 ) as F ( x ) and f ( x ; Z i ) as f i ( x ) for i =1 ; 2 ;  ;n . Itisworthmentioningthattheobjectivefunction F ( x ) isallowedtobenon-convex.Hence,in ordertoensuretheexistenceofaglobaloptimum,anaturaloptionistoimposean ` p -norm( p  1 ) constraint(LohandWainwright,2012,2015).Herewechoosethe ` 2 -normconstraintowingtoits fastprojection.Previouswork,e.g.,Agarwaletal.(2012)prefersthecomputationallylessef ` 1 -normtopromotesparsityandtoguaranteetheexistenceofoptimum.Inourproblem,yet,we alreadyhaveimposedthehardsparsityconstraintsothe ` 2 -normconstraintisabetter Themajorcontributionofthissectionisacomputationallyefalgorithmtermedhard thresholdedstochasticvariancereducedgradientmethod(HT-SVRG)tooptimize(4.1),tackling oneofthemostimportantproblemsinlarge-scalemachinelearning:producingsparsesolutionsby stochasticmethods.Weemphasizethattheformulation(4.1)isinstarkcontrasttothe ` 1 -regularized programsconsideredbypreviousstochasticsolverssuchasProx-SVRG(XiaoandZhang,2014) andSAGA(Defazioetal.,2014).Wetargethereastochasticalgorithmforthe non-convex problem thatislessexploitedintheliterature.Fromatheoreticalperspective,(4.1)ismorediftoana- lyzebutitalwaysproducessparsesolutions,whereasperformanceguaranteesforconvexprograms 11 S HENAND L I arefruitfulbutonecannotcharacterizethesparsityoftheobtainedsolution(usuallythesolutionis notsparse).Whenweappealtostochasticalgorithmstosolvetheconvexprograms,the ` 1 -norm formulationbecomesmuchlesseffectiveintermsofnaturallyowingtotherandom- ness.SeeLangfordetal.(2009);Xiao(2010);DuchiandSinger(2009)formoredetaileddiscussion ontheissue.WealsoremarkthatexistingworksuchasYuanetal.(2018);Bahmanietal.(2013); Jainetal.(2014)investigatedthesparsity-constrainedproblem(4.1)inabatchscenario,whichis notpracticalforlarge-scalelearningproblems.Theperhapsmostrelatedworktoournewalgo- rithmisNguyenetal.(2014).Nonetheless,theoptimizationerrorthereindoesnotvanishfornoisy statisticalmodels. Ourmainresultshowsthatforprevalentstatisticalmodels,ouralgorithmisabletorecover thetrueparameterwithalinearrate.Readersshoulddistinguishtheoptimalsolution x opt and thetrueparameter.Forinstance,considerthemodel(3.1).Minimizing(4.1)doesnotamountto recovering x  ifthereisobservationnoise.Infact,theconvergenceto x opt isonlyguaranteedtoan accuracybythe statisticalprecision oftheproblem,i.e., k x   x opt k 2 ,whichisthebest onecanhopeforanystatisticalmodel(Agarwaletal.,2012).Wethattheglobalconvergenceis attributedtoboththetightboundandthevariancereductiontechniquetobeintroducedbelow,and examiningthenecessityofthemisaninterestingfuturework. Algorithm1 HardThresholdedStochasticVarianceReducedGradientMethod(HT-SVRG) Require: Trainingsamples f Z i g n i =1 ,maximumstagecount S ,sparsityparameter k ,updatefre- quency m ,learningrate  ,radius ! ,initialsolution e x 0 . Ensure: Optimalsolution e x S . 1: for s =1 to S do 2: Set e x = e x s  1 , e  = 1 n P n i =1 r f i ( e x ) , x 0 = e x . 3: for t =1 to m do 4: Uniformlypick i t 2f 1 ; 2 ;  ;n g andupdatethesolution b t = x t  1    r f i t ( x t  1 ) r f i t ( e x )+ e   ; r t = H k  b t  ; x t = ! ( r t ) : 5: endfor 6: Uniformlychoose j s 2f 0 ; 1 ;  ;m  1 g andset e x s = x j s . 7: endfor 4.1Algorithm Ouralgorithm(Algorithm1)appliestheframeworkofJohnsonandZhang(2013),wherethepri- maryideaistoleveragepastgradientsforthecurrentupdateforthesakeofvariancereductionŒa techniquethathasalonghistoryinstatistics(OwenandZhou,2000).Toguaranteethateachiterate is k -sparse,ittheninvokesthehardthresholdingoperation.Notethattheorthogonalprojection for r t willnotchangethesupportset,andhence x t isstill k -sparse.Alsonotethatoursparsity constraintin(4.1)readsas k x k 0  K .Whatwewillshowbelowisthatwhentheparameter k is properlychosen(whichdependson K ),weobtainagloballyconvergentsequenceofiterates. 12 AT IGHT B OUNDOF H ARD T HRESHOLDING Themostchallengingpartonestablishingtheglobalconvergencecomesfromthehardthresh- oldingoperation H k  r t  .Notethatitis b t thatreducestheobjectivevalueinexpectation.If b t is not k -sparse(usuallyitisdense), x t isnotequalto b t soitdoesnotdecreasetheobjectivefunction. Inaddition,comparedwiththeconvexproximaloperator(Defazioetal.,2014)whichenjoysthe non-expansivenessofthedistancetotheoptimum,thehardthresholdingstepcanenlargethedis- tanceuptoamultipleof 2 ifusingthebound(1.1).Whatmakesitamoreseriousissueisthatthese inaccurateiterates x t willbeusedforfutureupdates,andhencetheerrormightbeprogressively propagatedatanexponentialrate. Ourkeyideaistoboundthecurvatureofthefunctionfrombelowandabovetoestablish RIP-likecondition,which,combinedwithTheorem1,downscalesthedeviationresultingfromhard thresholding.Notethat  isalwaysgreaterthanone(seeTheorem1),hencethecurvatureboundis necessary.Duetovariancereduction,weshowthattheoptimizationerrorvanisheswhenrestricted onasmallsetofdirectionsassoonaswehavesufsamples.Moreover,withhardthresholding weareabletocontroltheerrorperiterationandtoobtainnear-optimalsamplecomplexity. 4.2DeterministicAnalysis WewillestablishageneraltheoremthatcharacterizestheprogressofHT-SVRGforapprox- imatinganarbitrary K -sparsesignal b x .Thenwewilldiscusshowtoproperlychoosethehyper- parametersofthealgorithm.Finallywemoveontospecify b x todevelopconvergenceresultsfora globaloptimumof(4.1)andforatrueparameter(e.g., x  ofthecompressedsensingproblem). 4.2.1A SSUMPTION Ouranalysisdependsontwopropertiesofthecurvatureoftheobjectivefunctionthathavebeen standardintheliterature.ReadersmayrefertoBickeletal.(2009);Negahbanetal.(2009);Jain etal.(2014)foradetaileddescription. 8(RestrictedStrongConvexity) Adifferentiablefunction g : R d ! R issaidtosat- isfythepropertyofrestrictedstrongconvexity(RSC)withparameter  r > 0 ,ifforallvectors x , x 0 2 R d with k x  x 0 k 0  r ,itholdsthat g ( x 0 )  g ( x )   r g ( x ) ; x 0  x    r 2   x 0  x   2 2 : 9(RestrictedSmoothness) Adifferentiablefunction g : R d ! R issaidtosatisfythe propertyofrestrictedsmoothness(RSS)withparameter L r > 0 ,ifforallvectors x , x 0 2 R d with k x  x 0 k 0  r ,itholdsthat   r g ( x 0 ) r g ( x )   2  L r   x 0  x   2 : Withtheseweassumethefollowing: ( A 1) F ( x ) theRSCconditionwithparameter  k + K . ( A 2) Forall 1  i  n , f i ( x ) theRSSconditionwithparameter L 3 k + K . Here,werecallthat K wasintroducedin(4.1)andtheparameter k wasusedinouralgorithm. ComparedtotheconvexalgorithmssuchasSAG(Rouxetal.,2012),SVRG(JohnsonandZhang, 13 S HENAND L I 2013)andSAGA(Defazioetal.,2014)thatassumestrongconvexityandsmoothnesseverywhere, weonlyassumetheseinarestrictedsense.Thisismorepracticalespeciallyinthehighdimensional regimewheretheHessianmatrixcouldbedegenerate(Agarwaletal.,2012).Wealsostressthatthe RSSconditionisimposedoneach f i ( x ) ,whereaspriorworkrequiresitfor F ( x ) whichismilder thanours(Negahbanetal.,2009). 4.2.2U PPER B OUNDOF P ROGRESS Forbrevity,letusdenote L := L 3 k + K ; :=  k + K ;c := ; wherewecallthequantity c astheconditionnumberoftheproblem.Itisalsocrucialtomeasure the ` 2 -normofthegradientrestrictedonsparsedirections,andwewrite kr 3 k + K F ( x ) k 2 :=max   kP  ( r F ( x )) k 2 : j  j 3 k + K  : Notethatforconvexprograms,theaboveevaluatedataglobaloptimumiszero.Aswillbeclear, kr 3 k + K F ( x ) k 2 howclosetheiteratesreturnedbyHT-SVRGcanbetothepoint x .For prevalentstatisticalmodels,itvanisheswhentherearesufsamples.Relatedtothisquantity, ouranalysisalsoinvolves Q ( x ):=  16  2 L!m + 2 !   kr 3 k + K F ( x ) k 2 +4  2 m kr 3 k + K F ( x ) k 2 2 ; wherewerecallthat  istheexpansivenessfactorgivenbyTheorem1,  and m areusedinthe algorithmand ! isauniversalconstantthatupperboundsthe ` 2 -normofthesignalwehopeto estimate.Virtually,withanappropriateparametersetting, Q ( x ) scalesas kr 3 k + K F ( x ) k 2 which willbeForaparticularstage s ,wedenote I s := f i 1 ;i 2 ;  ;i m g ,i.e.,thesamples randomlychosenforupdatingthesolution. Theorem10 ConsiderAlgorithm1anda K -sparsesignal b x ofinterest.Assume ( A 1) and ( A 2) . Pickthestepsize 0 << 1 = (4 L ) .If < 4 L= (4 L   ) ,thenitholdsthat E  F ( e x s )  F ( b x )    s  F ( e x 0 )  F ( b x )  + ˝ ( b x ) ; wheretheexpectationistakenover fI 1 ;j 1 ; I 2 ;j 2 ;  ; I s ;j s g and 0 << 1 providedthat m is largeenough.Inparticular,for 1 = (1   ) << 4 L= (4 L   ) ,wehave  =  1 := 1 (2   2  2 L   +1) m + 2  2 L 2   2  2 L   +1 ; ˝ ( b x )= ˝ 1 ( b x ):= Q ( b x ) 2(2   2  2 L   +1)(1   1 ) m : For   1 = (1   ) ,wehave  =  2 := 1  (1  2 L ) m + 2 L 1  2 L ;˝ ( b x )= ˝ 2 ( b x ):= Q ( b x ) 2  (1  2 L )(1   2 ) m : 14 AT IGHT B OUNDOF H ARD T HRESHOLDING TheproofcanbefoundinAppendixD.1. Remark11 Forthetheoremtohold, p < p 4 L= (4 L   )  p 4 = 3 ˇ 1 : 15 dueto L   . Hence,theconventionalbound (1.1) isnotapplicable.Incontrast,Theorem1assertsthatthis conditioncanbebytuning k slightlylargerthan K . Remark12 Withtheconditionson  and  ,thecoef  isalwayslessthanoneprovidedthat m issuflarge. Remark13 Thetheoremdoes not assertconvergencetoanarbitrarysparsevector b x .Thisis because F ( e x s )  F ( b x ) mightbelessthanzero.However,specifying b x doesgiveconvergence results,astobeelaboratedlater. 4.2.3H YPER -P ARAMETER S ETTING Beforemovingontotheconvergenceguarantee,letusdiscusstheminimumrequirementonthe hyper-parameters k , m and  ,anddeterminehowtochoosethemtosimplifyTheorem10. ForthesakeofsuccessofHT-SVRG,werequire < 4 c= (4 c  1) ,whichimplies ˆ< 1 = (16 c 2  4 c ) .Recallthat ˆ isgiveninTheorem1.Ingeneral,weareinterestedintheregime K  k ˝ d . Hence,wehave ˆ = K=k andtheminimumrequirementforthesparsityparameteris k> (16 c 2  4 c ) K: (4.2) Toourknowledge,theideaofrelaxedsparsitywasintroducedinZhang(2011)forOMPand inJainetal.(2014)forprojectedgradientdescent.However,therelaxedsparsityhereemergesina differentwayinthatHT-SVRGisastochasticalgorithm,andtheirprooftechniquecannotbeused. Wealsocontrastourtightboundtotheinequality(2.10)thatisobtainedbycombiningthe triangleinequalityandLemma1ofJainetal.(2014).Followingourproofpipeline,(2.10)gives k   1   p 4 c (4 c  1)  1  1  2  d +  p 4 c (4 c  1)  1  1  2 K whichgrowswiththedimension d ,whereasusingTheorem1thesparsityparameter k dependsonly onthedesiredsparsity K .Inthisregard,weconcludethatforthestochasticcase,ourboundisvital. Anothercomponentofthealgorithmistheupdatefrequency m .Intuitively,HT-SVRGperforms m numberofstochasticgradientupdatefollowedbyafullgradientevaluation,inordertomitigate thevariance.Inthislight, m shouldnotbetoosmall.Otherwise,thealgorithmreducestothe fullgradientmethodwhichisnotcomputationallyefOntheotherspectrum,alarge m leadstoaslowconvergencethatisintheconvergencecoef  .Toquantitatively analyzehow m shouldbeselected,letusconsiderthecase   1 = (1   ) forexample.Thecase 1 = (1   ) << 4 L= (4 L   ) followsinasimilarway.Inordertoensure  2 < 1 ,wemusthave m> 1 = (  (1  4 L )) .Inparticular,picking  =  0 L ; 0 2 (0 ; 1 = 4) ; (4.3) wethattheupdatefrequency m hastosatisfy m> c  0 (1   0 ) ; (4.4) 15 S HENAND L I whichisofthesameorderasintheconvexcase(JohnsonandZhang,2013)when  0 = . Notethatthewaywechoosethelearningrate  =  0 =L isalsoacommonpracticeinconvex optimization(Nesterov,2004). With(4.2),(4.3)and(4.4)inmind,weprovidedetailedchoicesofthehyper-parameters.Due to 0 << 1 = (4 L ) ,  1 ismonotonicallyincreasingwithrespectto  .ByTheorem1,weknowthat  isdecreasingwithrespectto k .Thus,alargerquantityof k resultsinasmallervalueof  1 ,and henceafasterrate.Interestingly,for  2 wediscoverthatthesmallerthe k is,thefasterthealgorithm concentrates.Hence,wehavethefollowingconsequence: Proposition14 Fix  and m .Thentheoptimalchoiceof  inTheorem10is  =1 = (1   ) inthe sensethattheconvergencecoef  attainstheminimum. Inlightoftheproposition,inthesectionstofollow,wewillonlyconsiderthesetting  =1 = (1   ) .Butweemphasizethatouranalysisandresultsessentiallyapplytoany   4 L= (4 L   ) . Nowlet  = 1 8 L ;m =4(8 c  1) ;k =8 c (8 c  1) K: (4.5) Thisgives  = 2 3 ;˝ ( b x )= 5 !  kr 3 k + K F ( b x ) k 2 + 1 L kr 3 k + K F ( b x ) k 2 2 : (4.6) 4.2.4G LOBAL L INEAR C ONVERGENCE Weareinthepositiontostatethegloballinearconvergencetoanoptimumofthesparsity-constrained optimizationprogram(4.1). Corollary15 Assume ( A 1) and ( A 2) .ConsidertheHT-SVRGalgorithmwithhyper-parameters givenin (4.5) .Thenthesequence f e x s g s  1 convergeslinearlytoaglobaloptimum x opt of (4.1) E  F ( e x s )  F ( x opt )    2 3  s  F ( e x 0 )  F ( x opt )  + 5 !  kr 3 k + K F ( x opt ) k 2 + 1 L kr 3 k + K F ( x opt ) k 2 2 : Proof ThisisadirectconsequenceofTheorem10. Whenever r 3 k + K F ( x opt )= 0 ,thecorollaryreadsas E  F ( e x s )  F ( x opt )    2 3  s  F ( e x 0 )  F ( x opt )  : Itimpliesthatifoneissolvingaconvexproblemwithoutthesparsityconstraintbuttheoptimal solutionhappenstobesparse,itissafetoperformhardthresholdingwithoutlossofoptimality.We exemplifysuchbehaviorwithanotheralgorithmSAGA(Defazioetal.,2014)inAppendixE.In thenoiselesscompressedsensingsettingwhere y = Ax  ,thecorollaryguaranteesthatHT-SVRG exactlyrecoverstheunderlyingtruesignal x  when F ( x ) ischosenastheleast-squareslossinthat x opt = x  and r F ( x  )= A > ( Ax   y )= 0 . Ontheotherside,theRSCpropertyimpliesthat k e x s  b x k 2  r 2max f F ( e x s )  F ( b x ) ; 0 g  + 2 kr k + K F ( b x ) k 2  : 16 AT IGHT B OUNDOF H ARD T HRESHOLDING TheproofisstraightforwardandcanbefoundinLemma14ofShenandLi(2017a).Nowwespecify b x asthetrueparameterofsomestatisticalmodel,forinstance, x  in(3.1).Itishencepossibleto establishrecoveryguaranteeof x  ,whichisknownastheproblemofparameterestimation. Corollary16 Assume ( A 1) and ( A 2) .Let L 0 betheRSSparameterof F ( x ) atthesparsitylevel 3 k + K .ConsidertheHT-SVRGalgorithmwithhyper-parametersgivenin (4.5) .Thenthesequence f e x s g s  1 recoversa K -sparsesignal x  withageometricrate E  k e x s  x  k 2   r 2 L 0    2 3  s 2    e x 0  x     2 + r 10 !  2 kr 3 k + K F ( x  ) k 2 +   r 2  3 + 3  ! kr 3 k + K F ( x  ) k 2 : TheproofcanbefoundinAppendixD.2. Remark17 TheRSSparameter L 0 of F ( x ) alwaysrangesin [ ;L ] ,whichissimplyby 4.2.5C OMPUTATIONAL C OMPLEXITY WecomparethecomputationalcomplexityofHT-SVRGtothatofprojectedgradientdescent(PGD) studiedinJainetal.(2014),whichisabatchcounterparttoHT-SVRG.First,weremarkthatthe analysisofPGDisbasedonthesmoothnessparameter L 0 of F ( x ) atsparsitylevel 2 k + K .We write c 0 = L 0  .Toachieveagivenaccuracy > 0 ,PGDrequires O ( c 0 log(1  )) iterations.Hence thetotalcomputationalcomplexityis O ( nc 0 d log(1  )) .ForHT-SVRG,inviewofCorollary15, theconvergencecoefisaconstant.Hence,HT-SVRGneeds O (log(1  )) iterationswhere wenotethattheerrorterm kr 3 k + K F ( x  ) k 2 canbemadeassmallas  withsufsamples(to beinthesequel).Ineachstage,HT-SVRGcomputesafullgradient e  followedby m times stochasticupdates.Therefore,thetotalcomplexityofHT-SVRGisgivenby O (( n + c ) d log(1  )) bynotingthefact m = O ( c ) .Inthescenario c<n ( c 0  1) ,HT-SVRGsimproveson PGDintermsoftimecost. 4.3StatisticalResults Thelastingredientofourtheoremistheterm ˝ ( b x ) whichmeasureshowclosetheiteratescouldbeto agivensparsesignal b x .Withappropriatehyper-parametersettings,thequantityreliesexclusively on kr 3 k + K F ( b x ) k 2 ,assuggestedby(4.6).Thereby,thissectionisdedicatedtocharacterizing kr 3 k + K F ( b x ) k 2 .WewillalsogiveexamplesforwhichHT-SVRGiscomputationallymoreef thanPGD.Forthepurposeofaconcreteresult,westudytwoproblems:sparselinearregression andsparselogisticregression.Thesearetwoofthemostpopularstatisticalmodelsintheliterature andhavefoundavarietyofapplicationsinmachinelearningandstatistics(Raskuttietal.,2011). Notably,itisknownthatsimilarstatisticalresultscanbebuiltforlow-rankmatrixregression,sparse precisionmatrixestimation,assuggestedinNegahbanetal.(2009);Agarwaletal.(2012). 4.3.1S PARSE L INEAR R EGRESSION Forsparselinearregression,theobservationmodelisgivenby y = Ax  + " ; k x  k 0  K; k x  k 2  !; (4.7) 17 S HENAND L I where A 2 R n  d isthedesignmatrix, y 2 R n istheresponse, " 2 R n issomenoise,and x  isthe K -sparsetrueparameterwehopetoestimatefromtheknowledgeof A and y .Notethatwhenwe havetheadditionalconstraint n ˝ d ,themodelaboveisexactlythatofcompressedsensing(3.1). Inorderto(approximately)estimatetheparameter,anaturalapproachistooptimizethefollow- ingnon-convexprogram: min x F ( x ):= 1 2 n n X i =1 k y i  a i  x k 2 2 ; s : t : k x k 0  K; k x k 2  !: (4.8) Forouranalysis,weassumethefollowingonthedesignmatrixandthenoise: ( A 3) a 1 ; a 2 ;:::; a n areindependentandidenticallydistributed(i.i.d.)Gaussianrandomvectors N ( 0 ;  ) .Allthediagonalelementsof  satisfy  jj  1 .Thenoise " isindependentof A anditsentriesarei.i.d.Gaussianrandomvariables N (0 ;˙ 2 ) . Proposition18 Considerthesparselinearregressionmodel (4.7) andtheprogram (4.8) .As- sume ( A 3) .Thenforasparsitylevel r ,  withprobabilityatleast 1  exp(  C 0 n ) ,  r =  min (  )  C 1 r log d n ;L 0 r =  max (  )+C 2 r log d n ;  withprobabilityatleast 1  C 3 r=d L r =C 4 r log d ;  andwithprobabilityatleast 1  C 5 =d kr r F ( x  ) k 2  C 6 ˙ r r log d n ; kr r F ( x opt ) k 2  L 0 r k x opt  x  k 2 +C 6 ˙ r r log d n : Above,  min (  ) and  max (  ) aretheminimumandmaximumsingularvaluesof  respectively. Werecallthat  r and L r areinvolvedinourassumptions ( A 1) and ( A 2) ,and L 0 r istheRSSpa- rameterof F ( x ) .Theestimationfor  r , L 0 r and kr r F ( x  ) k 2 followsfromstandardresultsinthe literature(Raskuttietal.,2011),whilethatfor L r followsfromPropositionE.1inBellecetal. (2016)bynotingthefactthatbounding L r amountstoestimating max i kH r ( a i ) k 2 2 .Inorderto estimate kr r F ( x opt ) k 2 ,noticethat kr r F ( x opt ) k 2 kr r F ( x opt ) r r F ( x  ) k 2 + kr r F ( x  ) k 2 kr F ( x opt ) r F ( x  ) k 2 + kr r F ( x  ) k 2  L 0 r k x opt  x  k 2 + kr r F ( x  ) k 2 ; whereweusetheofRSSinthelastinequality. Nowwelet r =3 k + K = const  c 2 K andget  =  min (  )  C 1 c 2 K log d n , L =C 4 c 2 K log d . Supposethat  min (  )=2C 4 ( K log d ) 2 and n = q  C 1 C 4 K log d with q  1 .Thenourassump- tions ( A 1) and ( A 2) aremetwithhighprobabilitywith  =C 4 ( K log d ) 2 ;L =C 4 ( K log d ) 3 ; and c = K log d: 18 AT IGHT B OUNDOF H ARD T HRESHOLDING ForCorollary15,asfaras s  C 7 log   F ( e x 0 )  F ( x opt )  ! ;n =C 7 ( !˙ ) 2   2 K log d; wehave E  F ( e x s )  F ( x opt )    +  max (  )  min (  ) k x opt  x  k 2 +   max (  )  min (  ) k x opt  x  k 2  2 forsomeaccuracyparameter > 0 .ThissuggeststhatitispossibleforHT-SVRGtoapproximate aglobaloptimumof(4.1)upto k x opt  x  k 2 ,namelythestatisticalprecisionoftheproblem. ReturningtoCorollary16,toguaranteethat E  k e x s  x  k 2    itsuftopick s  C 8 log( ! p c 0  ) ;n =C 8 ( !˙ ) 2   4 K log d: Finally,wecomparethecomputationalcosttoPGD.Itisnothardtoseethatunderthesame situation  min (  )=2C 4 ( K log d ) 2 and n = C 1 C 4 K log d , L 0 =C 4 ( K log d ) 3 ;c 0 = K log d; providedthat  max (  )=C 4 ( K log d ) 3  C 2 C 4 C 1 ( K log d ) 2 : Thus c<n ( c 0  1) ,i.e.,HT-SVRGismoreefthanPGD.Itisalsopossibletoconsiderother regimesofthecovariancematrixandthesamplesize,thoughwedonotpursueithere. 4.3.2S PARSE L OGISTIC R EGRESSION Forsparselogisticregression,theobservationmodelisgivenby Pr( y i j a i ; x  )= 1 1+exp(  y i a i  x  ) ; k x  k 0  K; k x k 2  !; 8 1  i  n; (4.9) where y i iseither 0 or 1 .Itthenlearnstheparameterbyminimizingthenegativelog-likelihood: min x F ( x ):= 1 n n X i =1 log(1+exp(  y i a i  x )) ; s : t : k x k 0  K; k x k 2  !: (4.10) Thereisalargebodyofworkshowingthatthestatisticalpropertyisratheranalogoustothatof linearregression.See,forexample,Negahbanetal.(2009).Infact,thestatisticalresultsapplyto generalizedlinearmodelsaswell. 4.4AConcurrentWork AfterwepostedtheversionShenandLi(2016)onarXiv,Lietal.(2016)madetheirworkpublic whereasimilaralgorithmtoHT-SVRGwaspresented.Theirtheoreticalanalysisappliestoconvex objectivefunctionswhileweallowthefunction F ( x ) tobenon-convex.Wealsofullycharacterize theconvergencebehaviorofthealgorithmbyshowingthetrade-offbetweenthesparsityparameter k andtheconvergencecoef  (Proposition14). 19 S HENAND L I 5.Experiments Inthissection,wepresentacomprehensiveempiricalstudyfortheproposedHT-SVRGalgorithm ontwotasks:sparserecovery(compressedsensing)andimageTheexperimentson sparserecoveryisdedicatedtoverifyingthetheoreticalresultswepresented,andwevisualizethe modelslearnedbyHT-SVRGtodemonstratethepracticalefy. 5.1SparseRecovery Tounderstandthepracticalbehaviorofouralgorithmaswellastojustifythetheoreticalanalysis, weperformexperimentsonsyntheticdata.Theexperimentalsettingsareasfollows:  DataGeneration. Thedatadimension d isedas 256 andwegeneratean n  d Gaussian randomsensingmatrix A whoseentriesarei.i.d.withzeromeanandvariance 1 =n .Then 1000 K -sparsesignals x  areindependentlygenerated,wherethesupportofeachsignal isuniformlychosen.Thatis,werunouralgorithmandthebaselinesfor 1000 trials.The measurements y foreachsignal x  isobtainedby y = Ax  whichisnoisefree.Inthisway, weareabletostudytheconvergenceratebyplottingthelogarithmoftheobjectivevalue sincetheoptimalobjectivevalueisknowntobezero.  Baselines. Wemainlycomparewithtwocloselyrelatedalgorithms:IHTandPGD.Bothof themcomputethefullgradientoftheleast-squareslossfollowedbyhardthresholding.Yet, PGDismoregeneral,inthesensethatitallowsthesparsityparameter k tobelargerthanthe truesparsity K ( k = K forIHT)andalsoconsidersaxiblestepsize  (  =1 forIHT). Hence,PGDcanbeviewedasabatchcounterparttoourmethodHT-SVRG.  EvaluationMetric. Wesayasignal x  issuccessfullyrecoveredbyasolution x if k x  x  k 2 k x  k 2 < 10  3 : Inthisway,wecancomputethepercentageofsuccessoverthe 1000 trialsforeachalgorithm.  Hyper-Parameters. Ifnotweuse m =3 n , k =9 K ,and S =10000 forHT- SVRG.Wealsousetheheuristicstepsize  =2 = svds( AA > ) forHT-SVRGandPGD, where svds( AA > ) returnsthelargestsingularvalueofthematrix AA > .Sinceforeach stage,HT-SVRGcomputesthefullgradientfor (2 m=n +1) times,weruntheIHTandPGD for (2 m=n +1) S iterationsforfaircomparison,i.e.,allofthealgorithmshavethesame numberoffullgradientevaluations. 5.1.1P HASE T RANSITION Oursimulationaimsatofferingabigpictureontherecoveryperformance.Tothisend,we varythenumberofmeasurements n from 1 to 256 ,roughlywithastepsize 8 .Wealsostudythe performancewithrespecttothetruesparsityparameter K ,whichrangesfrom 1 to 26 ,roughlywith stepsize 2 .TheresultsareillustratedinFigure1,whereabrighterblockmeansahigherpercentage ofsuccessandthebrightestonesindicateexactsparserecovery.ItisapparentthatPGDandHT- SVRGrequirefewermeasurementsforanaccuraterecoverythanIHT,possiblyduetothexibility inchoosingthesparsityparameterandthestepsize.Wealsoobservethatasastochasticalgorithm, 20 AT IGHT B OUNDOF H ARD T HRESHOLDING HT-SVRGperformscomparablytoPGD.ThissuggeststhatHT-SVRGisanappealingsolutionto large-scalesparselearningproblemsinthatHT-SVRGiscomputationallymoreef Figure1: Percentageofsuccessfulrecoveryundervarioussparsityandsamplesize. Thevalues rangefrom 0 to 100 ,whereabrightercolormeansahigherpercentageofsuccess(the brightestblockscorrespondtothevalueof 100 ).PGDadmitsahigherpercentageof recoverycomparedtoIHTbecauseitxiblychoosesthestepsizeandsparsityparameter. Asastochasticvariant,HT-SVRGperformscomparablytothebatchcounterpartPGD. InFigure2,weexemplifysomeoftheresultsobtainedfromHT-SVRGbyplottingtwokindsof curves:thesuccessofpercentageagainstthesamplesize n andthatagainstthesignalsparsity K . Inthisway,onecanexaminethedetailedvaluesandcandeterminetheminimumsamplesizefora particularsparsity.Forinstance,theleftpaneltellsthattoensurethat 80% percentsofthe 16 -sparse signalsarerecovered,wehavetocollect 175 measurements.Wecanalsolearnfromtherightpanel thatusing 232 measurements,anysignalwhosesparsityis 22 orlesscanbereliablyrecovered. Figure2: PercentageofsuccessofHT-SVRGagainstthenumberofmeasurements(left)and thesparsity(right). BasedontheresultsinFigure1andFigure2,wehaveanapproximateestimationonthemin- imumrequirementofthesamplesizewhichensuresaccurate(orexact)recovery.Nowweareto investigatehowmanymeasurementsareneededtoguaranteeasuccesspercentageof 95% and 99% . Tothisend,foreachsignalsparsity K ,welookforthenumberofmeasurements n 0 fromFigure1 where 90 percentsofsuccessareachieved.Thenwecarefullyenlarge n 0 withstepsize 1 andrun thealgorithms.TheempiricalresultsarerecordedinFigure3,wherethecirclemarkersrepresent theempiricalresultswithdifferentcolorsindicatingdifferentalgorithms,e.g.,redcircleforempir- icalobservationofHT-SVRG.Thenwetheseempiricalresultsbylinearregression,whichare 21 S HENAND L I plottedassolidordashedlines.Forexample,thegreenlineisamodelforIHT.Wethat n isalmostlinearwith K .Especially,thecurveofHT-SVRGisnearlyontopofthatofPGD,which againvHT-SVRGisanattractivealternativetothebatchmethod. Figure3: Minimumnumberofmeasurementstoachieve 95% and 99% percentageofsuccess. RedequationindicatesthelinearregressionofHT-SVRG.Themarkersandcurvesfor HT-SVRGarealmostontopofPGD,whichagainthatHT-SVRGisanappealing stochasticalternativetothebatchmethodPGD. 5.1.2I NFLUENCEOF H YPER -P ARAMETERS Next,weturntoinvestigatetheofthehyper-parameters,i.e.,thesparsityparameter k , updatefrequency m andstepsize  ontheconvergencebehaviorofHT-SVRG.Wesetthetrue sparsity K =4 andcollect 100 measurementsforeachgroundtruthsignal,i.e., n =100 .Notethat thestandardsettingweemployedis k =9 K =36 , m =3 n =300 and  =2 = svds( AA > ) ˇ 0 : 3 . Eachtimewevaryoneoftheseparameterswhiletheothertwo,andtheresultsareplotted inFigure4.Wepointoutthatalthoughtheconvergenceresult(Theorem10)isdeterministic,the vanishingoptimizationerror(Proposition18)isguaranteedunderaprobabilisticargument.Hence, itispossiblethatforaofparameters, 97% ofthesignalsareexactlyrecovered butHT-SVRGfailsontheremaining,aswehaveobservedin,e.g.,Figure2.Clearly,wearenot supposedtoaveragealltheresultstoexaminetheconvergencerate.Forourpurpose,weseta threshold 95% ,thatis,weaverageoverthesuccesstrialsifmorethan 95% percentsofthesignals areexactlyrecovered.Otherwise,wesaythatthesetofparameterscannotensureconvergenceand weaverageoverthesefailuresignalswhichwillgiveanillustrationofdivergence. TheleftpanelofFigure4vestheconditionthat k hastobelargerthan K ,whilethesecond panelshowstheupdatefrequency m canbereasonablysmallinthepriceofaslowconvergence rate.Finally,theempiricalstudydemonstratesthatourheuristicchoice  =0 : 3 workswell,and when > 3 ,theobjectivevalueexceeds 10 120 within3stages(whichcannotbedepictedinthe Forverysmallstepsizes,weplottheconvergencecurvebygraduallyenlargingtheupdate frequency m inFigure5.TheempiricalresultsagreewithTheorem10thatforany 0 << 1 = (4 L ) , HT-SVRGconvergesassoonas m islargeenough. 22 AT IGHT B OUNDOF H ARD T HRESHOLDING Figure4: ConvergenceofHT-SVRGwithdifferentparameters. Wehave 100 measurementsfor the 256 -dimensionalsignalwhereonly 4 elementsarenon-zero.Thestandardsettingis k =36 , m =300 and  =0 : 3 . Left: Ifthesparsityparameter k isnotlargeenough,HT- SVRGwillnotrecoverthesignal. Middle: Asmall m leadstoafrequentfullgradient evaluationandhenceslowconvergence. Right: Weobservedivergencewhen   3 . Figure5: Convergencebehaviorundersmallstepsize. Weobservethataslongaswepicka suflargevaluefor m ,HT-SVRGalwaysconverges.Thisisnotsurprisingsince ourtheoremguaranteesforany < 1 = (4 L ) ,HT-SVRGwillconvergeif m islarge enough.Alsonotethatthegeometricconvergencerateisobservedaftercertainiterations, e.g.,for  =3  10  5 ,thelog(error)decreaseslinearlyafter20thousandsiterations. 5.2 Inadditiontotheapplicationofsparserecovery,weillustratedthatHT-SVRGcandealwithbinary byminimizingthesparselogisticregressionproblem(4.10).Here,westudytheper- formanceonarealisticimagedatasetMNIST 1 ,consistingof60thousandstrainingsamplesand10 thousandssamplesfortesting.Thereisonedigitoneachimageofsize28-by-28,hencetotally10 classes.SomeoftheimagesareshowninFigure6. Theupdatefrequency m isedas m =3 n .Wecomputetheheuristicstepsize  asinthe previoussection,i.e.,  =2 = svds( AA > ) ˇ 10  3 .Sinceforthereal-worlddataset,thetruesparsity isactuallyunknown,wetunethesparsityparameter k andstudytheperformanceofthealgorithm. First,wevisualizevepair-wisemodelslearnedbyHT-SVRGinFigure7,whereeachrowis associatedwithabinarytaskindicatedbythetwodigitsattheleadingoftherow, andthesubsequentred-blueareusedtoillustratethelearnedmodelsunderdifferentspar- 1. http://yann.lecun.com/exdb/mnist/ 23 S HENAND L I Figure6: SampleimagesintheMNISTdatabase. sityparameter.Forexample,thethirdcolorfuldepictedonthesecondrowcorrespondsto recognizingadigitisﬁ1ﬂorﬁ7ﬂwiththesparsity k =30 .Inparticular,foreachpair,welabelthe smalldigitaspositiveandthelargeoneasnegative,andtheblueandredpixelsaretheweightswith positiveandnegativevaluesrespectively.Apparently,themodelswelearnedarediscriminative. Figure7: Visualizationofthemodels. Wevisualize5modelslearnedbyHT-SVRGunderdiffer- entchoicesofsparsityshownonthetopofeachcolumn.Notethatthefeaturedimension is784.Fromthetoprowtothebottomrow,weillustratethemodelsofﬁ0vs9ﬂ,ﬁ1vs7ﬂ, ﬁ2vs3ﬂ,ﬁ4vs5ﬂandﬁ6vs8ﬂ,whereforeachpair,welabelthesmalldigitaspositive andthelargeoneasnegative.Theredcolorrepresentsnegativeweightswhiletheblue pixelscorrespondwithpositiveweights. WealsoquantitativelyshowtheconvergenceandpredictionaccuracycurvesinFigure8.Note thathere,the y -axisistheobjectivevalue F ( e x s ) ratherthan log( F ( e x s )  F ( x opt )) ,duetothefact thatcomputingtheexactoptimumof(4.10)isNP-hard.Generallyspeaking,HT-SVRGconverges quitefastandusuallyattainstheminimumofobjectivevaluewithin20stages.Itisnotsurprisingto seethatchoosingalargequantityforthesparsityleadstoabetter(lower)objectivevalue.However, 24 AT IGHT B OUNDOF H ARD T HRESHOLDING inpracticeasmallassignmentforthesparsity,e.g., k =70 facilitatesanefcomputationwhile stillsuftoensurefastconvergenceandaccurateprediction. Figure8: Quantitativeresultsonconvergenceandaccuracy. The5demonstratethe convergencebehaviorofHT-SVRGforeachbinarytiontask,wherecurveswith differentcolorsrepresenttheobjectivevalueagainstnumberofstagesunderdifferent sparsity k .Generallyspeaking,HT-SVRGconvergeswithin20stageswhichisavery fastrate.Thelasttheaccuracyagainstthesparsityforall5 tasks,wherewethatforamoderatechoice,e.g., k =70 ,italready guaranteesanaccurateprediction(werecallthedimensionis784). 6.ConclusionandOpenProblems Inthispaper,wehaveprovidedatightboundonthedeviationresultingfromthehardthresholding operator,whichunderliesavastvolumeofalgorithmsdevelopedforsparsity-constrainedproblems. Ourderivedboundisuniversaloverallchoicesofparametersandwehaveprovedthatitcannot beimprovedwithoutfurtherinformationonthesignals.Wehavediscussedtheimplicationsof ourresulttothecommunityofcompressedsensingandmachinelearning,andhavedemonstrated thatthetheoreticalresultsofanumberofpopularalgorithmsintheliteraturecanbeadvanced.In addition,wehavedevisedanovelalgorithmwhichtacklestheproblemofsparselearninginlarge- scalesetting.Wehaveelaboratedthatouralgorithmisguaranteedtoproduceglobaloptimalsolution forprevalentstatisticalmodelsonlywhenitisequippedwiththetightbound,hencejustifyingthat theconventionalboundisnotapplicableinthechallengingscenario. Thereareseveralinterestingopenproblems.Thequestiontoaskiswhetheronecanes- tablishsharpRIPconditionorsharpphasetransitionforhardthresholdingbasedalgorithmssuch asIHTandCoSaMPwiththetightbound.Moreover,comparedtothehardthresholdedSGD method(Nguyenetal.,2014),HT-SVRGadmitsavanishingoptimizationerror.Thisposesa 25 S HENAND L I questionofwhetherweareabletoprovablyshowthenecessityofvariancereductionforsucha sparsity-constrainedproblem. Acknowledgments WewouldliketothankJingWangforinsightfuldiscussionsincetheearlystageofthework.We alsothankMartinSlawskiforhelpfuldiscussiononthestatisticalprecisionoftheproblem,and thankJianWangforbringingthepaperNguyenetal.(2014)intoourattention.Weappreciate HuanXu'shighlevelcommentsonthework.Finally,wethanktheanonymousreviewersfora carefulcheckonourproofandfortheencouragingcomments.Theworkwaspartiallyfundedby NSF-Bigdata-1419210andNSF-III-1360971. AppendixA.TechnicalLemmas Wepresentsomeusefullemmasthatwillbeinvokedbysubsequentanalysis.Thefollowingisa characterizationoftheco-coercivityoftheobjectivefunction F ( x ) .Asimilarresultwasobtained inNguyenetal.(2014)butwepresentaanalysiswhichisessentialforourpurpose. Lemma19 Foragivensupportset  ,assumethatthecontinuousfunction F ( x ) is L j  j -RSSandis  K -RSCforsomesparsitylevel K .Then,forallvectors x and x 0 with j supp( x  x 0 ) [  j K ,   r  F ( x 0 ) r  F ( x )   2 2  2 L j  j  F ( x 0 )  F ( x )   r F ( x ) ; x 0  x  : Proof Weanauxiliaryfunction G ( w ):= F ( w ) hr F ( x ) ; w i : Forallvectors w and w 0 ,wehave   r G ( w ) r G ( w 0 )   2 =   r F ( w ) r F ( w 0 )   2  L j supp( w  w 0 ) j   w  w 0   2 ; whichisequivalentto G ( w )  G ( w 0 )   r G ( w 0 ) ; w  w 0   L r 2   w  w 0   2 2 ; (A.1) where r := j supp( w  w 0 ) j .Ontheotherhand,duetotheRSCpropertyof F ( x ) ,weobtain G ( w )  G ( x )= F ( w )  F ( x ) hr F ( x ) ; w  x i  j supp( w  x ) j 2 k w  x k 2 2  0 ; providedthat j supp( w  x ) j K .Forthegivensupportset  ,wepick w = x 0  1 L j  j r  G ( x 0 ) . Clearly,forsuchachoiceof w ,wehave supp( w  x )=supp( x  x 0 ) [  .Hence,byassuming that j supp( x  x 0 ) [  j isnotlargerthan K ,weget G ( x )  G  x 0  1 L j  j r  G ( x 0 )   G ( x 0 )+ ˝ r G ( x 0 ) ;  1 L j  j r  G ( x 0 ) ˛ + 1 2 L j  j   r  G ( x 0 )   2 2 = G ( x 0 )  1 2 L j  j   r  G ( x 0 )   2 2 ; 26 AT IGHT B OUNDOF H ARD T HRESHOLDING wherethesecondinequalityfollowsfrom(A.1).Nowexpanding r  G ( x 0 ) andrearrangingthe termsgivesthedesiredresult. Lemma20 ConsidertheHT-SVRGalgorithmforastage s .Let b x bethetargetsparsevector. Let  beasupportsetsuchthat supp  x t  1  [ supp( e x ) [ supp( b x )   .Put r = j  j .Assume ( A 2) .Forall 1  t  m ,denote v t = r f i t ( x t  1 ) r f i t ( e x )+ e  .Thenwehavethefollowing: E i t j x t  1 h   P   v t    2 2 i  4 L r  F ( x t  1 )  F ( b x )  +4 L r [ F ( e x )  F ( b x )]  4 L r  r F ( b x ) ; x t  1 + e x  2 b x  +4 kP  ( r F ( b x )) k 2 2 : Proof Wehave   P   v t    2 2 =   P   r f i t ( x t  1 ) r f i t ( e x )+ e     2 2  2   P   r f i t ( x t  1 ) r f i t ( b x )    2 2 +2 kP  ( r f i t ( e x ) r f i t ( b x )  e  ) k 2 2 =2   P   r f i t ( x t  1 ) r f i t ( b x )    2 2 +2 kP  ( r f i t ( e x ) r f i t ( b x )) k 2 2 +2 kP  ( e  ) k 2 2  4 hP  ( r f i t ( e x ) r f i t ( b x )) ; P  ( e  ) i ˘ 1 =2   P   r f i t ( x t  1 ) r f i t ( b x )    2 2 +2 kP  ( r f i t ( e x ) r f i t ( b x )) k 2 2 +2 kP  ( e  ) k 2 2  4 hr f i t ( e x ) r f i t ( b x ) ; P  ( e  ) i ˘ 2  4 L r  f i t ( x t  1 )  f i t ( b x )   r f i t ( b x ) ; x t  1  b x  +4 L r [ f i t ( e x )  f i t ( b x ) hr f i t ( b x ) ; e x  b x i ] +2 kP  ( e  ) k 2 2  4 hr f i t ( e x ) r f i t ( b x ) ; P  ( e  ) i ; where ˘ 1 isbyalgebra, ˘ 2 appliesLemma19andthefactthat j  j = r . Takingtheconditionalexpectation,weobtainthefollowing: E i t j x t  1 h   P   v t    2 2 i  4 L r  F ( x t  1 )  F ( b x )  +4 L r [ F ( e x )  F ( b x )]  4 L r  r F ( b x ) ; x t  1 + e x  2 b x  +2 h 2 P  ( r F ( b x )) P  ( e  ) ; P  ( e  ) i =4 L r  F ( x t  1 )  F ( b x )  +4 L r [ F ( e x )  F ( b x )]  4 L r  r F ( b x ) ; x t  1 + e x  2 b x  + k 2 P  ( r F ( b x )) k 2 2 k 2 P  ( r F ( b x )) P  ( e  ) k 2 2 kP  ( e  ) k 2 2  4 L r  F ( x t  1 )  F ( b x )  +4 L r [ F ( e x )  F ( b x )]  4 L r  r F ( b x ) ; x t  1 + e x  2 b x  +4 kP  ( r F ( b x )) k 2 2 : Theproofiscomplete. Corollary21 AssumethesameconditionsasinLemma20.If r F ( b x )=0 ,wehave E i t j x t  1 h   P   v t    2 2 i  4 L r  F ( x t  1 )+ F ( e x )  2 F ( b x )  : 27 S HENAND L I AppendixB.ProofsforSection2 B.1ProofofTheorem1 Proof Theresultistrueforthetrivialcasethat b isazerovector.Inthefollowing,weassumethat b isnotazerovector.Denote w := H k ( b ) : Let  bethesupportsetof w andlet  beitscomplement.Weimmediatelyhave P  ( b )= w . Let  0 bethesupportsetof x .Forthesakeofsimplicity,letussplitthevector b asfollows: b 1 = P  n  0 ( b ) ; b 2 = P  \  0 ( b ) ; b 3 = P  n  0 ( b ) ; b 4 = P  \  0 ( b ) : Likewise,wedenote w 1 = P  n  0 ( w ) ; w 2 = P  \  0 ( w ) ; w 3 = P  n  0 ( w )= 0 ; w 4 = P  \  0 ( w )= 0 ; x 1 = P  n  0 ( x )= 0 ; x 2 = P  \  0 ( x ) ; x 3 = P  n  0 ( x )= 0 ; x 4 = P  \  0 ( x ) : Duetothehardthresholding,wehave w 1 = b 1 ; w 2 = b 2 : Inthisway,bysimplealgebrawehave k w  x k 2 2 = k b 1 k 2 2 + k b 2  x 2 k 2 2 + k x 4 k 2 2 ; k b  x k 2 2 = k b 1 k 2 2 + k b 2  x 2 k 2 2 + k b 3 k 2 2 + k b 4  x 4 k 2 2 : Ourgoalistoestimatethemaximumof k w  x k 2 2 = k b  x k 2 2 .Itiseasytoshowthatwhen attainingthemaximumvalue, k b 3 k 2 mustbezerosinceotherwiseonemaydecreasethistermto maketheobjectivelarger.Hence,maximizing k w  x k 2 2 = k b  x k 2 2 amountstoestimatingthe upperboundofthefollowingoverallchoicesof x and b :  := k b 1 k 2 2 + k b 2  x 2 k 2 2 + k x 4 k 2 2 k b 1 k 2 2 + k b 2  x 2 k 2 2 + k b 4  x 4 k 2 2 : (B.1) Firstly,weconsiderthecaseof k b 1 k 2 =0 ,whichmeans = 0 implying  =1 .Inthe following,weconsider k b 1 k 2 6 =0 .Inparticular,weconsider > 1 sinceweareinterestedinthe maximumvalueof  . Arranging(B.1)weobtain (   1) k b 2  x 2 k 2 2 +  k b 4  x 4 k 2 2 k x 4 k 2 2 +(   1) k b 1 k 2 2 =0 : (B.2) Letus b andthefunction G ( x 2 ; x 4 )=(   1) k b 2  x 2 k 2 2 +  k b 4  x 4 k 2 2 k x 4 k 2 2 +(   1) k b 1 k 2 2 : 28 AT IGHT B OUNDOF H ARD T HRESHOLDING Thus,(B.2)indicatesthat G ( x 2 ; x 4 ) canattaintheobjectivevalueofzero.Notethat G ( x 2 ; x 4 ) isa quadraticfunctionanditsgradientandHessianmatrixcanbecomputedasfollows: @ @ x 2 G ( x 2 ; x 4 )=2(   1)( x 2  b 2 ) ; @ @ x 4 G ( x 2 ; x 4 )=2  ( x 4  b 4 )  2 x 4 ; r 2 G ( x 2 ; x 4 )=2(   1) I ; where I istheidentitymatrix.SincetheHessianmatrixispositive G ( x 2 ; x 4 ) attainsthe globalminimumatthestationarypoint,whichisgivenby x  2 = b 2 ; x  4 =    1 b 4 ; resultingintheminimumobjectivevalue G ( x  2 ; x  4 )=  1   k b 4 k 2 2 +(   1) k b 1 k 2 2 : Inordertoguaranteethefeasiblesetof(B.2)isnon-empty,werequirethat G ( x  2 ; x  4 )  0 ; implying k b 1 k 2 2  2  (2 k b 1 k 2 2 + k b 4 k 2 2 )  + k b 1 k 2 2  0 : Solvingtheaboveinequalitywithrespectto  ,weobtain   1+ k b 4 k 2 2 + r  4 k b 1 k 2 2 + k b 4 k 2 2  k b 4 k 2 2 2 k b 1 k 2 2 : (B.3) Toderiveanupperboundthatisuniformoverthechoiceof b ,werecallthat b 1 containsthelargest absoluteelementsof b while b 4 hassmallervalues.Inparticular,theaveragedvalueof b 4 isno greaterthanthatof b 1 inmagnitude,i.e., k b 4 k 2 2 k b 4 k 0  k b 1 k 2 2 k b 1 k 0 : Notethat k b 1 k 0 = k k b 2 k 0 = k  ( K k b 4 k 0 ) .Hence,combiningwiththefactthat 0 k b 4 k 0  min f K;d  k g andoptimizingover k b 4 k 0 gives k b 4 k 2 2  min f K;d  k g k  K +min f K;d  k g k b 1 k 2 2 : Pluggingbackto(B.3),weobtain   1+ ˆ + p (4+ ˆ ) ˆ 2 ;ˆ = min f K;d  k g k  K +min f K;d  k g : Theproofiscomplete. 29 S HENAND L I AppendixC.ProofsforSection3 C.1ProofofTheorem6 WefollowtheproofpipelineofBlumensathandDavies(2009)andonlyremarkthedifferenceof ourproofandtheirs,i.e.,whereTheorem1applies.Incaseofpossibleconfusionduetonotation, wefollowthesymbolsinBlumensathandDavies.Onemayrefertothatarticleforacomplete proof. ThedifferenceoccursinEq.(22)ofBlumensathandDavies(2009),wheretheyreached (Old)    x s  x [ n +1]    2  2    x s B n +1  a [ n +1] B n +1    2 ; whileTheorem1gives (New)    x s  x [ n +1]    2  p     x s B n +1  a [ n +1] B n +1    2 : CombiningthisnewinequalityandEq.(23)therein,weobtain    x s  x [ n +1]    2  p     ( I   > B n +1  B n +1 ) r [ n ] B n +1    2 + p     (  > B n +1  B n +1 n B n +1 ) r [ n ] B n +1 n B n +1    2 : Bynotingthefactthat   B n [ B n +1    2 s + s  where s  denotesthesparsityoftheglobaloptimum andfollowingtheirreasoningofEq.(24)and(25),wehaveanewboundforEq.(26): (New)    r [ n +1]    2  p 2  2 s + s     r [ n ]    2 + p (1+  s + s  )  k e k 2 : Nowourresultfollowsbysettingthecoefof   r [ n ]   2 to 0 : 5 .Notethatspecifying  =4 gives theresultofBlumensathandDavies(2009). C.2ProofofTheorem7 WefollowtheprooftechniqueofTheorem6.27inFoucartandRauhut(2013)whichgivesthebest knownRIPconditionfortheCoSaMPalgorithmtodate.Sincemostofthereasoningissimilar, weonlypointoutthedifferenceofourproofandtheirs,i.e.,whereTheorem1applies.Incaseof confusionbynotation,wefollowthesymbolsusedinFoucartandRauhut(2013).Thereadermay refertothatbookforacompleteproof. ThedifferenceisinEq.(6.49)ofFoucartandRauhut(2013).Notethattoderivethis inequality,FoucartandRauhutinvokedtheconventionalbound(1.1),whichgives (Old)   x S  x n +1   2 2    ( x S  u n +1 ) U n +1   2 2 +4   ( x S  u n +1 ) U n +1   2 2 ; whileutilizingTheorem1gives (New)   x S  x n +1   2 2    ( x S  u n +1 ) U n +1   2 2 +    ( x S  u n +1 ) U n +1   2 2 : 30 AT IGHT B OUNDOF H ARD T HRESHOLDING CombiningthisnewinequalitywithEq.(6.50)andEq.(6.51)therein,weobtain   x S  x n +1   2  p 2  3 s + s  s 1+(   1)  2 3 s + s  1   2 3 s + s  k x n  x S k 2 + p 2  3 s + s  s 1+(   1)  2 3 s + s  1   2 3 s + s    ( A  e 0 ) ( S [ S n  T n +1   2 + 2 1   3 s + s    ( A  e 0 ) U n +1   2 ; where s  denotesthesparsityoftheoptimum.Ournewboundfollowsbysettingthecoefof k x n  x S k 2 to 0 : 5 andsolvingtheresultantequation.Notethatsetting  =4 givestheoldbound ofFoucartandRauhut. AppendixD.ProofsforSection4 D.1ProofofTheorem10 Proof Fixastage s .Letusdenote v t = r f i t ( x t  1 ) r f i t ( e x )+ e  ; sothat b t = x t  1   v t : Byspecifying =supp  x t  1  [ supp  x t  [ supp( e x ) [ supp( b x ) ,itfollowsthat r t = H k  b t  = H k  P   b t  : Thus,theEuclideandistanceof x t and b x canbeboundedasfollows:   x t  b x   2 2    r t  b x   2 2 =   H k  P   b t   b x   2 2     P   b t   b x   2 2 ; (D.1) wheretheinequalityholdsbecause x t = ! ( r t ) and k b x k 2  ! .Wealsohave   P   b t   b x   2 2 =   x t  1  b x   P   v t    2 2 =   x t  1  b x   2 2 +  2   P   v t    2 2  2   x t  1  b x ; v t  ; wherethesecondequalityusesthefactthat  x t  1  b x ; P   v t  =  x t  1  b x ; v t  .The termwillbepreservedformathematicalinduction.Thethirdtermiseasytomanipulatethanksto theunbiasednessof v t .Forthesecondterm,weuseLemma20toboundit.Putthemtogether, 31 S HENAND L I conditioningon x t  1 andtakingtheexpectationover i t for(D.1),wehave E i t j x t  1 h   x t  b x   2 2 i ˘ 1     x t  1  b x   2 2 +4  2 L  F ( x t  1 )  F ( b x )+ F ( e x )  F ( b x )   2   x t  1  b x ; r F ( x t  1 )   4  2 L  r F ( b x ) ; x t  1 + e x  2 b x  +4  2 kP  ( r F ( b x )) k 2 2 ˘ 2   (1   )   x t  1  b x   2 2  2  (1  2 L )  F ( x t  1 )  F ( b x )  +4  2 L [ F ( e x )  F ( b x )] +4  2 L kP  ( r F ( b x )) k 2    x t  1 + e x  2 b x   2 +4  2 kP  ( r F ( b x )) k 2 2   (1   )   x t  1  b x   2 2  2  (1  2 L )  F ( x t  1 )  F ( b x )  +4  2 L [ F ( e x )  F ( b x )]+4  2 Q 0 (4 L! + Q 0 ) where ˘ 1 appliesLemma20, ˘ 2 appliesAssumption ( A 1) andwewrite Q 0 := kr 3 k + K F ( b x ) k 2 for brevity. Nowsummingovertheinequalitiesover t =1 ; 2 ;  ;m ,conditioningon e x andtakingthe expectationwithrespectto I s = f i 1 ;i 2 ;  ;i m g ,wehave E I s j e x h k x m  b x k 2 2 i  [  (1   )  1] E I s j e x m X t =1   x t  1  b x   2 2 +   x 0  b x   2 2 +4  2 Q 0 (4 L! + Q 0 ) m  2  (1  2 L ) E I s j e x m X t =1  F ( x t  1 )  F ( b x )  +4  2 Lm [ F ( e x )  F ( b x )] =[  (1   )  1] m E I s ;j s j e x k e x s  b x k 2 2 + k e x  b x k 2 2 +4  2 Q 0 (4 L! + Q 0 ) m  2  (1  2 L ) m E I s ;j s j e x [ F ( e x s )  F ( b x )]+4  2 Lm [ F ( e x )  F ( b x )]  [  (1   )  1] m E I s ;j s j e x k e x s  b x k 2 2 +  2  +4  2 Lm  [ F ( e x )  F ( b x )]  2  (1  2 L ) m E I s ;j s j e x [ F ( e x s )  F ( b x )]+4  2 Q 0 (4 L! + Q 0 ) m +2 Q 0 !; (D.2) wherewerecallthat j s istherandomlychosenindexusedtodetermine e x s (seeAlgorithm1).The lastinequalityholdsduetotheRSCconditionand   x t   2  ! .Forbrevity,wewrite Q :=4  2 Q 0 (4 L! + Q 0 ) m +2 Q 0 !;Q 0 = kr 3 k + K F ( b x ) k 2 : Basedon(D.2),wediscusstwocasestoexaminetheconvergenceofthealgorithm. Case1.  (1   )  1 .Thisimmediatelyresultsin E I s j e x h k x m  b x k 2 2 i   2  +4  2 Lm  [ F ( e x )  F ( b x )]  2  (1  2 L ) m E I s ;j s j e x [ F ( e x s )  F ( b x )]+ Q; 32 AT IGHT B OUNDOF H ARD T HRESHOLDING whichimplies  (1  2 L ) m E I s ;j s j e x [ F ( e x s )  F ( b x )]   1  +2  2 Lm  [ F ( e x )  F ( b x )]+ Q 2 : Pick  suchthat 1  2 L> 0 ; (D.3) weobtain E I s ;j s j e x [ F ( e x s )  F ( b x )]   1  (1  2 L ) m + 2 L 1  2 L  [ F ( e x )  F ( b x )]+ Q 2  (1  2 L ) m : Toguaranteetheconvergence,wemustimpose 2 L 1  2 L < 1 : (D.4) Putting(D.3),(D.4)and  (1   )  1 togethergives < 1 4 L ;  1 1   : (D.5) Theconvergencecoefhereis  = 1  (1  2 L ) m + 2 L 1  2 L : (D.6) Thus,wehave E [ F ( e x s )  F ( b x )]   s h F ( e x 0 )  F ( b x ) i + Q 2  (1  2 L )(1   ) m ; wheretheexpectationistakenover fI 1 ;j 1 ; I 2 ;j 2 ;  ; I s ;j s g . Case2.  (1   ) > 1 .Inthiscase,(D.2)implies E I s j e x h k x m  b x k 2 2 i   2  +4  2 Lm  [ F ( e x )  F ( b x )]+ Q +  2  [  (1   )  1] m  2  (1  2 L ) m  E I s ;j s j e x [ F ( e x s )  F ( b x )] : Rearrangingthetermsgives  2   2  2 L   +1  m E I s ;j s j e x [ F ( e x s )  F ( b x )]   1+2  2 Lm  [ F ( e x )  F ( b x )]+ Q 2 : Toensuretheconvergence,theminimumrequirementsare 2   2  2 L   +1 > 0 ; 2   2  2 L   +1 > 2  2 L: 33 S HENAND L I Thatis, 4  2  2  +   1 < 0 : Weneedtoguaranteethefeasiblesetoftheaboveinequalityisnon-emptyforthepositivevariable  .Thus,werequire 4  2  2  4  4 L (   1) > 0 ; whichisequivalentto < 4 L 4 L   : Combiningitwith  (1   ) > 1 gives 1 1   << 4 L 4 L   : Toensuretheabovefeasiblesetisnon-empty,weimpose 1 1   < 4 L 4 L   ; sothat 0 << 1 4 L ; 1 1   << 4 L 4 L   : (D.7) Theconvergencecoefforthiscaseis  = 1 (2   2  2 L   +1) m + 2  2 L 2   2  2 L   +1 : (D.8) Thus, E [ F ( e x s )  F ( b x )]   s h F ( e x 0 )  F ( b x ) i + Q 2(2   2  2 L   +1)(1   ) m : Bycombining(D.5)and(D.7),theminimumrequirementfor  and  is 0 << 1 4 L ;< 4 L 4 L   : Theproofiscomplete. D.2ProofofCorollary16 Proof Bynotingtheconcavityofthesquarerootfunction,wehave E h p max f F ( e x s )  F ( b x ) ; 0 g i  q E  max f F ( e x s )  F ( b x ) ; 0 g   q (2 = 3) s max f F ( e x 0 )  F ( b x ) ; 0 g + ˝ ( b x ) : 34 AT IGHT B OUNDOF H ARD T HRESHOLDING Supposethat F ( x ) RSSwithparameter L 0 2 [ ;L ] .Itfollowsthat F ( e x 0 )  F ( b x )  D r F ( b x ) ; e x 0  b x E + L 0 2    e x 0  b x    2 2  1 2 L 0 kr k + K F ( b x ) k 2 2 + L 0    e x 0  b x    2 2 : Recallthat ˝ ( b x )= 5 !  kr 3 k + K F ( b x ) k 2 + 1 L kr 3 k + K F ( b x ) k 2 2 : Henceusing p a + b + c + d  p a + p b + p c + p d gives E h p max f F ( e x s )  F ( b x ) ; 0 g i  p L 0  2 3  s 2    e x 0  b x    2 + r 5 !  kr 3 k + K F ( b x ) k 2 +   1  + r 1 2  ! kr 3 k + K F ( b x ) k 2 : Finally,theRSCpropertyimmediatelysuggeststhat(see,e.g.,Lemma20inShenandLi(2017b)) E  k e x s  b x k 2   r 2  E h p max f F ( e x s )  F ( b x ) ; 0 g i + 2 kr k + K F ( b x ) k 2   r 2 L 0    2 3  s 2    e x 0  b x    2 + r 10 !  2 kr 3 k + K F ( b x ) k 2 +   r 2  3 + 3  ! kr 3 k + K F ( b x ) k 2 : Theproofiscomplete. AppendixE.HT-SAGA WedemonstratethatthehardthresholdingstepcanbeintegratedintoSAGA(Defazioetal.,2014) asshowninAlgorithm2.NotethattheonlydifferenceofAlgorithm2andtheoneproposed inDefazioetal.(2014)isthatweperformhardthresholdingratherthanproximaloperator.Hence, ouralgorithmguarantees k -sparsesolution. Theorem22 AssumethesameconditionsasinDefazioetal.(2014).Furtherassumetheoptimum of (4.1) withoutthesparsityconstrainthappenstobe k -sparse.Then,thesequenceofthesolutions producedbyAlgorithm2convergestotheoptimumwithgeometricrateforsomeproperlychosen sparsityparameter k . Proof theLyapunovfunction Z asfollows: Z t := Z ( x t ; f ˚ t i g )= 1 n n X i =1 f i ( ˚ t i )  F ( b x )  1 n n X i =1  r f i ( b x ) ; ˚ t i  b x  + c   x t  b x   2 2 : 35 S HENAND L I Algorithm2 SAGAwithHardThresholding(HT-SAGA) Require: Thecurrentiterate x t andofeach r f i ( ˚ t i ) attheendofiteration t ,thestepsize  . Ensure: Thenewiterate. 1: Pick j 2f 1 ; 2 ;  ;n g uniformlyatrandom. 2: Take ˚ t +1 j = x t andstore r f j ( ˚ t +1 j ) inthetable.Allotherentriesinthetableremainun- changed. 3: Updatethenewiterate x t +1 asfollows: b t +1 = x t   " r f j ( ˚ t +1 j ) r f j ( ˚ t j )+ 1 n n X i =1 r f i ( ˚ t i ) # ; x t +1 = H k  b t +1  : Weexamine Z t +1 .Wehave E " 1 n X i f i ( ˚ t +1 i ) # = 1 n F ( x t )+  1  1 n  1 n X i f i ( ˚ t i ) ; E "  1 n X i  r f i ( b x ) ; ˚ t +1 i  b x  # =  1 n  r F ( b x ) ; x t  b x    1  1 n  1 n X i  r f i ( b x ) ; ˚ t i  b x  : Also, c   x t +1  b x   2 2     b t +1  b x   2 2 =    b t +1  b x +  r F ( b x )   2 2 : Fortheterm,wehave  E   b t +1  b x +  r F ( b x )   2 2   (1   )   x t  b x   2 2 +   (1+  )  2   L  E   r f j ( x t ) r f j ( b x )   2 2  2  ( L   ) L  F ( x t )  F ( b x )   r F ( b x ) ; x t  b x    2    r F ( x t ) r F ( b x )   2 2 +2  (1+   1 )  2 L " 1 n X i f i ( ˚ t i )  F ( b x )  1 n X i  r f i ( b x ) ; ˚ t i  b x  # : Therefore, E [ Z t +1 ]  Z t  1  Z t +  1 n  2  ( L   ) L  2  2    F ( x t )  F ( b x )   r F ( b x ) ; x t  b x  +  1  +2  (1+   1 )  2 L  1 n  " 1 n X i f i ( ˚ t i )  F ( b x )  1 n X i  r f i ( b x ) ; ˚ t i  b x  # +  c       x t  b x   2 2 +  (1+  )   1 L   E   r f j ( x t ) r f j ( b x )   2 2 : 36 AT IGHT B OUNDOF H ARD T HRESHOLDING Inordertoguaranteetheconvergence,wechoosepropervaluesfor  , c ,  ,  and  suchthatthe termsinroundbracketsarenon-positive.Thatis,werequire c     0 ; (1+  )   1 L  0 ; 1 n  2  ( L   ) L  2  2   0 ; 1  +2  (1+   1 )  2 L  1 n  0 : Pick  = 1 2( n + L ) ;  = 2 n + L L ;  = 1  ; wethetwoinequalities.Pick c = 1 2  (1   ) n : Thenbythelasttwoequalities,werequire 1      (1   ) L  (1   ) Ln +1 : Ontheotherhand,byTheorem1,wehave > 1 : Thus,werequire 1 <  (1   ) L  (1   ) Ln +1 ; Byalgebra,theaboveinequalitieshasnon-emptyfeasiblesetprovidedthat (6  2  8  2 L ) n 2 +(14 L    16 L 2 ) n +8 L 2 (1  L ) < 0 : Dueto   L ,weknow n  14 L + p 224 L 3 +1 2  (8 L  6) sufwhereweassume L> 3 = 4 .Picking  = (1   ) L  (1   ) Ln +1 completestheproof. 37 S HENAND L I References RadosawAdamczak,AlexanderE.Litvak,AlainPajor,andNicoleTomczak-Jaegermann.Re- strictedisometrypropertyofmatriceswithindependentcolumnsandneighborlypolytopesby randomsampling. ConstructiveApproximation ,34(1):61Œ88,2011. AlekhAgarwal,SahandNegahban,andMartinJ.Wainwright.Fastglobalconvergenceofgradient methodsforhigh-dimensionalstatisticalrecovery. TheAnnalsofStatistics ,40(5):2452Œ2482, 2012. BubacarrBahandJaredTanner.Improvedboundsonrestrictedisometryconstantsforgaussian matrices. SIAMJournalonMatrixAnalysisApplications ,31(5):2882Œ2898,2010. BubacarrBahandJaredTanner.Boundsofrestrictedisometryconstantsinextremeasymptotics: FormulaeforGaussianmatrices. LinearAlgebraanditsApplications ,441:88Œ109,2014. SohailBahmani,BhikshaRaj,andPetrosT.Boufounos.Greedysparsity-constrainedoptimization. JournalofMachineLearningResearch ,14(1):807Œ841,2013. RichardBaraniuk,MarkDavenport,RonaldDeVore,andMichaelB.Wakin.Asimpleproofofthe restrictedisometrypropertyforrandommatrices. ConstructiveApproximation ,28(3):253Œ263, 2008. PierreC.Bellec,GuillaumeLecu ´ e,andAlexandreB.Tsybakov.Slopemeetslasso:improvedoracle boundsandoptimality. CoRR ,abs/1605.08651,2016. PeterJ.Bickel,Ya'acovRitov,andAlexandreB.Tsybakov.Simultaneousanalysisoflassoand dantzigselector. TheAnnalsofStatistics ,pages1705Œ1732,2009. JeffreyD.BlanchardandJaredTanner.Performancecomparisonsofgreedyalgorithmsincom- pressedsensing. NumericalLinearAlgebrawithApplications ,22(2):254Œ282,2015. ThomasBlumensathandMikeE.Davies.Iterativethresholdingforsparseapproximations. Journal ofFourierAnalysisandApplications ,14(5-6):629Œ654,2008. ThomasBlumensathandMikeE.Davies.Iterativehardthresholdingforcompressedsensing. Ap- pliedandComputationalHarmonicAnalysis ,27(3):265Œ274,2009. TonyT.CaiandAnruZhang.SharpRIPboundforsparsesignalandlow-rankmatrixrecovery. AppliedandComputationalHarmonicAnalysis ,35(1):74Œ93,2013. TonyT.Cai,LieWang,andGuangwuXu.Newboundsforrestrictedisometryconstants. IEEE TransactionsonInformationTheory ,56(9):4388Œ4394,2010. EmmanuelJ.Cand ˚ es.Therestrictedisometrypropertyanditsimplicationsforcompressedsensing. ComptesRendusMathematique ,346(9):589Œ592,2008. EmmanuelJ.Cand ˚ esandTerenceTao.Decodingbylinearprogramming. IEEETransactionson InformationTheory ,51(12):4203Œ4215,2005. 38 AT IGHT B OUNDOF H ARD T HRESHOLDING EmmanuelJ.Cand ˚ esandTerenceTao.TheDantzigselector:Statisticalestimationwhen p ismuch largerthan n . TheAnnalsofStatistics ,35(6):2313Œ2351,2007. EmmanuelJ.Cand ˚ esandMichaelB.Wakin.Anintroductiontocompressivesampling. IEEESignal ProcessingMagazine ,25(2):21Œ30,2008. EmmanuelJ.Cand ˚ es,JustinK.Romberg,andTerenceTao.Robustuncertaintyprinciples:ex- actsignalreconstructionfromhighlyincompletefrequencyinformation. IEEETransactionson InformationTheory ,52(2):489Œ509,2006. VenkatChandrasekaran,BenjaminRecht,PabloA.Parrilo,andAlanS.Willsky.Theconvexge- ometryoflinearinverseproblems. FoundationsofComputationalMathematics ,12(6):805Œ849, 2012. ScottShaobingChen,DavidL.Donoho,andMichaelA.Saunders.Atomicdecompositionbybasis pursuit. SIAMJournalonComputing ,20(1):33Œ61,1998. WeiDaiandOlgicaMilenkovic.Subspacepursuitforcompressivesensingsignalreconstruction. IEEETransactionsonInformationTheory ,55(5):2230Œ2249,2009. IngridDaubechies,MichelDefrise,andChristineDeMol.Aniterativethresholdingalgorithm forlinearinverseproblemswithasparsityconstraint. CommunicationsonPureandApplied Mathematics ,57(11):1413Œ1457,2004. AaronDefazio,FrancisR.Bach,andSimonLacoste-Julien.SAGA:afastincrementalgradient methodwithsupportfornon-stronglyconvexcompositeobjectives.In Proceedingsofthe28th AnnualConferenceonNeuralInformationProcessingSystems ,pages1646Œ1654,2014. DavidL.Donoho.Compressedsensing. IEEETransactionsonInformationTheory ,52(4):1289Œ 1306,2006. DavidL.DonohoandJaredTanner.Preciseundersamplingtheorems. ProceedingsoftheIEEE ,98 (6):913Œ924,2010. DavidL.DonohoandYaakovTsaig.Fastsolutionof ` 1 -normminimizationproblemswhenthe solutionmaybesparse. IEEETransactionsonInformationTheory ,54(11):4789Œ4812,2008. DavidL.Donoho,MichaelElad,andVladimirN.Temlyakov.Stablerecoveryofsparseovercom- pleterepresentationsinthepresenceofnoise. IEEETransactionsonInformationTheory ,52(1): 6Œ18,2006. DavidL.Donoho,IainJohnstone,andAndreaMontanari.Accuratepredictionofphasetransitions incompressedsensingviaaconnectiontominimaxdenoising. IEEETransactionsonInformation Theory ,59(6):3396Œ3433,2013. JohnC.DuchiandYoramSinger.Efonlineandbatchlearningusingforwardbackward splitting. JournalofMachineLearningResearch ,10:2899Œ2934,2009. BradleyEfron,TrevorHastie,IainJohnstone,andRobertTibshirani.Leastangleregression. The Annalsofstatistics ,32(2):407Œ499,2004. 39 S HENAND L I SimonFoucart.Hardthresholdingpursuit:Analgorithmforcompressivesensing. SIAMJournal onNumericalAnalysis ,49(6):2543Œ2563,2011. SimonFoucart.Sparserecoveryalgorithms:Sufconditionsintermsofrestrictedisometry constants.In ApproximationTheoryXIII:SanAntonio2010 ,pages65Œ77.Springer,NewYork, NY,2012. SimonFoucartandHolgerRauhut. AMathematicalIntroductiontoCompressiveSensing .Applied andNumericalHarmonicAnalysis.Birkh ¨ auser,2013. PrateekJain,AmbujTewari,andInderjitS.Dhillon.Orthogonalmatchingpursuitwithreplacement. In Proceedingsofthe25thAnnualConferenceonNeuralInformationProcessingSystems ,pages 1215Œ1223,2011. PrateekJain,AmbujTewari,andPurushottamKar.Oniterativehardthresholdingmethodsforhigh- dimensionalM-estimation.In Proceedingsofthe28thAnnualConferenceonNeuralInformation ProcessingSystems ,pages685Œ693,2014. RieJohnsonandTongZhang.Acceleratingstochasticgradientdescentusingpredictivevariance reduction.In Proceedingsofthe27thAnnualConferenceonNeuralInformationProcessing Systems ,pages315Œ323,2013. JohnLangford,LihongLi,andTongZhang.Sparseonlinelearningviatruncatedgradient. Journal ofMachineLearningResearch ,10:777Œ801,2009. PingLi,Cun-HuiZhang,andTongZhang.Compressedcountingmeetscompressedsensing.In ProceedingsofThe27thConferenceonLearningTheory ,pages1058Œ1077,2014. XingguoLi,TuoZhao,RamanArora,HanLiu,andJarvisHaupt.Stochasticvariancereduced optimizationfornonconvexsparselearning. CoRR ,abs/1605.02711,2016. Po-LingLohandMartinJ.Wainwright.High-dimensionalregressionwithnoisyandmissingdata: Provableguaranteeswithnon-convexity. TheAnnalsofStatistics ,40(3):1637Œ1664,2012. Po-LingLohandMartinJ.Wainwright.Regularizedm-estimatorswithnonconvexity:statistical andalgorithmictheoryforlocaloptima. JournalofMachineLearningResearch ,16:559Œ616, 2015. ZongmingMa.Sparseprincipalcomponentanalysisanditerativethresholding. TheAnnalsof Statistics ,41(2):772Œ801,2013. QunMo.Asharprestrictedisometryconstantboundoforthogonalmatchingpursuit. CoRR , abs/1501.01708,2015. QunMoandYiShen.Aremarkontherestrictedisometrypropertyinorthogonalmatchingpursuit. IEEETransactionsonInformationTheory ,58(6):3654Œ3656,2012. DeannaNeedellandJoelA.Tropp.CoSaMP:Iterativesignalrecoveryfromincompleteandinac- curatesamples. AppliedandComputationalHarmonicAnalysis ,26(3):301Œ321,2009. 40 AT IGHT B OUNDOF H ARD T HRESHOLDING DeannaNeedellandRomanVershynin.Signalrecoveryfromincompleteandinaccuratemeasure- mentsviaregularizedorthogonalmatchingpursuit. IEEEJournalofSelectedTopicsinSignal Processing ,4(2):310Œ316,2010. SahandNegahban,PradeepRavikumar,MartinJ.Wainwright,andBinYu.Aframeworkfor high-dimensionalanalysisof M -estimatorswithdecomposableregularizers.In Proceedingsof the23rdAnnualConferenceonNeuralInformationProcessingSystems ,pages1348Œ1356,2009. YuriiNesterov. IntroductoryLecturesonConvexOptimization:ABasicCourse ,volume87of AppliedOptimization .SpringerUS,2004. NamH.Nguyen,DeannaNeedell,andTinaWoolf.Linearconvergenceofstochasticiterativegreedy algorithmswithsparseconstraints. CoRR ,abs/1407.0088,2014. BrunoA.OlshausenandDavidJ.Field.Sparsecodingwithanovercompletebasisset:Astrategy employedbyV1? Visionresearch ,37(23):3311Œ3325,1997. ArtOwenandYiZhou.Safeandeffectiveimportancesampling. JournaloftheAmericanStatistical Association ,95(449):135Œ143,2000. YagyenshC.Pati,RaminRezaiifar,andPerinkulamS.Krishnaprasad.Orthogonalmatchingpursuit: Recursivefunctionapproximationwithapplicationstowaveletdecomposition.In Conference RecordofThe27thAsilomarConferenceonSignals,SystemsandComputers ,pages40Œ44,1993. GarveshRaskutti,MartinJ.Wainwright,andBinYu.Minimaxratesofestimationforhigh- dimensionallinearregressionover ` q -balls. IEEETransactionsonInformationTheory ,57(10): 6976Œ6994,2011. NicolasLeRoux,MarkW.Schmidt,andFrancisR.Bach.Astochasticgradientmethodwithanex- ponentialconvergenceratefortrainingsets.In Proceedingsofthe26thAnnualConference onNeuralInformationProcessingSystems ,pages2672Œ2680,2012. JieShenandPingLi.Atightboundofhardthresholding. CoRR ,abs/1605.01656,2016. JieShenandPingLi.Ontheiterationcomplexityofsupportrecoveryviahardthresholdingpursuit. In Proceedingsofthe34thInternationalConferenceonMachineLearning ,pages3115Œ3124, 2017a. JieShenandPingLi.Partialhardthresholding:TowardsAprincipledanalysisofsupportrecovery. In Proceedingsofthe31stAnnualConferenceonNeuralInformationProcessingSystems ,pages 3127Œ3137,2017b. RobertTibshirani.RegressionshrinkageandselectionviatheLasso. JournaloftheRoyalStatistical Society:SeriesB(Methodological) ,58(1):267Œ288,1996. JoelA.Tropp.Greedisgood:algorithmicresultsforsparseapproximation. IEEETransactionson InformationTheory ,50(10):2231Œ2242,2004. JoelA.TroppandAnnaC.Gilbert.Signalrecoveryfromrandommeasurementsviaorthogonal matchingpursuit. IEEETransactionsonInformationTheory ,53(12):4655Œ4666,2007. 41 S HENAND L I JoelA.TroppandStephenJ.Wright.Computationalmethodsforsparsesolutionoflinearinverse problems. ProceedingsoftheIEEE ,98(6):948Œ958,2010. MartinJ.Wainwright.Sharpthresholdsforhigh-dimensionalandnoisysparsityrecoveryusing ` 1 - constrainedquadraticprogramming(Lasso). IEEETransactionsonInformationTheory ,55(5): 2183Œ2202,2009. JianWangandByonghyoShim.Ontherecoverylimitofsparsesignalsusingorthogonalmatching pursuit. IEEETransactionsonSignalProcessing ,60(9):4973Œ4976,2012. LinXiao.Dualaveragingmethodsforregularizedstochasticlearningandonlineoptimization. JournalofMachineLearningResearch ,11:2543Œ2596,2010. LinXiaoandTongZhang.Aproximalstochasticgradientmethodwithprogressivevariancereduc- tion. SIAMJournalonOptimization ,24(4):2057Œ2075,2014. Xiao-TongYuanandTongZhang.Truncatedpowermethodforsparseeigenvalueproblems. Journal ofMachineLearningResearch ,14(1):899Œ925,2013. Xiao-TongYuan,PingLi,andTongZhang.Gradienthardthresholdingpursuit. JournalofMachine LearningResearch ,18(166):1Œ43,2018. TongZhang.SparserecoverywithorthogonalmatchingpursuitunderRIP. IEEETransactionson InformationTheory ,57(9):6215Œ6221,2011. 42  
PublishedasaconferencepaperatICLR2015 D EEP C APTIONINGWITH M ULTIMODAL R ECURRENT N EURAL N ETWORKS ( M -RNN) JunhuaMao UniversityofCalifornia,LosAngeles;BaiduResearch mjhustc@ucla.edu WeiXu&YiYang&JiangWang&ZhihengHuang BaiduResearch f wei.xu,yangyi05,wangjiang03,huangzhiheng g @baidu.com AlanYuille UniversityofCalifornia,LosAngeles yuille@stat.ucla.edu A BSTRACT Inthispaper,wepresentamultimodalRecurrentNeuralNetwork(m-RNN)model forgeneratingnovelimagecaptions.Itdirectlymodelstheprobabilitydistribution ofgeneratingawordgivenpreviouswordsandanimage.Imagecaptionsaregen- eratedaccordingtothisdistribution.Themodelconsistsoftwosub-networks:a deeprecurrentneuralnetworkforsentencesandadeepconvolutionalnetworkfor images.Thesetwosub-networksinteractwitheachotherinamultimodallayer toformthewholem-RNNmodel.Theeffectivenessofourmodelisvalidatedon fourbenchmarkdatasets(IAPRTC-12,Flickr8K,Flickr30KandMSCOCO). Ourmodeloutperformsthestate-of-the-artmethods.Inaddition,weapplythe m-RNNmodeltoretrievaltasksforretrievingimagesorsentences,andachieves performanceimprovementoverthestate-of-the-artmethodswhichdi- rectlyoptimizetherankingobjectivefunctionforretrieval.Theprojectpageof thisworkis: www.stat.ucla.edu/ Ÿ junhua.mao/m-RNN.html . 1 1I NTRODUCTION Obtainingsentenceleveldescriptionsforimagesisbecominganimportanttaskandithasmanyap- plications,suchasearlychildhoodeducation,imageretrieval,andnavigationfortheblind.Thanks totherapiddevelopmentofcomputervisionandnaturallanguageprocessingtechnologies,recent workhasmadeprogressonthistask(seeabriefreviewinSection2).Manyprevious methodstreatitasaretrievaltask.Theylearnajointembeddingtomapthefeaturesofbothsen- tencesandimagestothesamesemanticspace.Thesemethodsgenerateimagecaptionsbyretrieving themfromasentencedatabase.Thus,theylacktheabilityofgeneratingnovelsentencesordescrib- ingimagesthatcontainnovelcombinationsofobjectsandscenes. Inthiswork,weproposeamultimodalRecurrentNeuralNetworks(m-RNN)model 2 toaddress boththetaskofgeneratingnovelsentencesdescriptionsforimages,andthetaskofimageand sentenceretrieval.Thewholem-RNNmodelcontainsalanguagemodelpart,avisionpartanda multimodalpart.Thelanguagemodelpartlearnsadensefeatureembeddingforeachwordinthe 1 Mostrecently,weadoptasimplestrategytoboosttheperformanceofimagecaptioningtask. MoredetailsareshowninSection8.Thecodeandrelateddata(e.g.imagefeaturesandhypotheses sentencesgeneratedbythem-RNNmodel)areavailableat https://github.com/mjhucla/mRNN-CR . 2 ApreviousversionofthisworkappearsintheNIPS2014DeepLearningWorkshopwiththetitleﬁExplain ImageswithMultimodalRecurrentNeuralNetworksﬂ http://arxiv.org/abs/1410.1090 (Maoetal. (2014)).WeobservedsubsequentarXivpaperswhichalsouserecurrentneuralnetworksinthistopicandcite ourwork.Wegratefullyacknowledgethem. 1 arXiv:1412.6632v5  [cs.CV]  11 Jun 2015PublishedasaconferencepaperatICLR2015 Figure1:Examplesofthegeneratedandtwotop-rankedretrievedsentencesgiventhequeryimage fromIAPRTC-12dataset.Thesentencescanwelldescribethecontentoftheimages.Weshowa failurecaseinthefourthimage,wherethemodelmistakenlytreatsthelakeastheskyandmisses allthepeople.MoreexamplesfromtheMSCOCOdatasetcanbefoundontheprojectpage: www.stat.ucla.edu/ Ÿ junhua.mao/m-RNN.html . dictionaryandstoresthesemantictemporalcontextinrecurrentlayers.Thevisionpartcontainsa deepConvolutionalNeuralNetwork(CNN)whichgeneratestheimagerepresentation.Themulti- modalpartconnectsthelanguagemodelandthedeepCNNtogetherbyaone-layerrepresentation. Ourm-RNNmodelislearnedusingalog-likelihoodcostfunction(seedetailsinSection4).The errorscanbebackpropagatedtothethreepartsofthem-RNNmodeltoupdatethemodelparameters simultaneously. Intheexperiments,wevalidateourmodelonfourbenchmarkdatasets:IAPRTC-12(Grubinger etal.(2006)),Flickr8K(Rashtchianetal.(2010)),Flickr30K(Youngetal.(2014))andMSCOCO (Linetal.(2014)).Weshowthatourmethodachievesstate-of-the-artperformance, outperformingalltheothermethodsforthethreetasks:generatingnovelsentences,retrievingim- agesgivenasentenceandretrievingsentencesgivenanimage.Ourframeworkisgeneralandcan befurtherimprovedbyincorporatingmorepowerfuldeeprepresentationsforimagesandsentences. 2R ELATED W ORK Deepmodelforcomputervisionandnaturallanguage. Themethodsbasedonthedeepneural networkdevelopedrapidlyinrecentyearsinboththeofcomputervisionandnaturallan- guage.Forcomputervision,Krizhevskyetal.(2012)proposeadeepConvolutionalNeuralNet- works(CNN)with8layers(denotedasAlexNet)andoutperformpreviousmethodsbyalarge marginintheimagetaskofImageNetchallenge(Russakovskyetal.(2014)).This networkstructureiswidelyusedincomputervision,e.g.Girshicketal.(2014)designaobjectde- tectionframework(RCNN)basedonthiswork.Recently,Simonyan&Zisserman(2014)proposea CNNwithover16layers(denotedasVggNet)andperformssubstantiallybetterthantheAlexNet. Fornaturallanguage,theRecurrentNeuralNetwork(RNN)showsthestate-of-the-artperformance inmanytasks,suchasspeechrecognitionandwordembeddinglearning(Mikolovetal.(2010;2011; 2013)).Recently,RNNshavebeensuccessfullyappliedtomachinetranslationtoextractsemantic informationfromthesourcesentenceandgeneratetargetsentences(e.g.Kalchbrenner&Blunsom (2013),Choetal.(2014)andSutskeveretal.(2014)). Image-sentenceretrieval. Manypreviousmethodstreatthetaskofdescribingimagesasaretrieval taskandformulatetheproblemasarankingorembeddinglearningproblem(Hodoshetal.(2013); Fromeetal.(2013);Socheretal.(2014)).Theyextractthewordandsentencefeatures(e.g. Socheretal.(2014)usesdependencytreeRecursiveNeuralNetworktoextractsentencefeatures) aswellastheimagefeatures.Thentheyoptimizearankingcosttolearnanembeddingmodelthat mapsboththesentencefeatureandtheimagefeaturetoacommonsemanticfeaturespace.Inthis way,theycandirectlycalculatethedistancebetweenimagesandsentences.Recently,Karpathy etal.(2014)showthatobjectlevelimagefeaturesbasedonobjectdetectionresultscangenerate betterresultsthanimagefeaturesextractedatthegloballevel. 2 PublishedasaconferencepaperatICLR2015 Figure2:IllustrationofthesimpleRecurrentNeuralNetwork(RNN)andourmultimodalRecurrent NeuralNetwork(m-RNN)architecture.(a).ThesimpleRNN.(b).Ourm-RNNmodel.Theinputs ofourmodelareanimageanditscorrespondingsentencedescriptions. w 1 , w 2 ,..., w L representsthe wordsinasentence.Weaddastartsign w start andanendsign w end toallthetrainingsentences.The modelestimatestheprobabilitydistributionofthenextwordgivenpreviouswordsandtheimage. Itconsistsofvelayers(i.e.twowordembeddinglayers,arecurrentlayer,amultimodallayerand asoftmaxlayer)andadeepCNNineachtimeframe.Thenumberaboveeachlayerindicatesthe dimensionofthelayer.Theweightsaresharedamongallthetimeframes.(Bestviewedincolor) Generatingnovelsentencedescriptionsforimages. Therearegenerallythreecategoriesofmeth- odsforthistask.Thecategoryassumesaruleofthelanguagegrammar.Theyparse thesentenceanddivideitintoseveralparts(Mitchelletal.(2012);Gupta&Mannem(2012)).Each partisassociatedwithanobjectoranattributeintheimage(e.g.Kulkarnietal.(2011)usesaCon- ditionalRandomFieldmodelandFarhadietal.(2010)usesaMarkovRandomFieldmodel).This kindofmethodgeneratessentencesthataresyntacticallycorrect.Thesecondcategoryretrieves similarcaptionedimages,andgeneratesnewdescriptionsbygeneralizingandre-composingthere- trievedcaptions(Kuznetsovaetal.(2014)).Thethirdcategoryofmethods,whichismorerelated toourmethod,learnsaprobabilitydensityoverthespaceofmultimodalinputs(i.e.sentencesand images),usingforexample,DeepBoltzmannMachines(Srivastava&Salakhutdinov(2012)),and topicmodels(Barnardetal.(2003);Jiaetal.(2011)).Theygeneratesentenceswithricherandmore xiblestructurethanthegroup.Theprobabilityofgeneratingsentencesusingthemodelcan serveastheafmetricforretrieval.Ourmethodfallsintothiscategory.Morecloselyrelated toourtasksandmethodistheworkofKirosetal.(2014b),whichisbuiltonaLog-BiLinearmodel (Mnih&Hinton(2007))anduseAlexNettoextractvisualfeatures.Itneedsaedlengthofcontext (i.e.vewords),whereasinourmodel,thetemporalcontextisstoredinarecurrentarchitecture, whichallowsarbitrarycontextlength. ShortlyafterMaoetal.(2014),severalpapersappearwithrecordbreakingresults(e.g.Kirosetal. (2014a);Karpathy&Fei-Fei(2014);Vinyalsetal.(2014);Donahueetal.(2014);Fangetal.(2014); Chen&Zitnick(2014)).Manyofthemarebuiltonrecurrentneuralnetworks.Itdemonstratesthe effectivenessofstoringcontextinformationinarecurrentlayer.Ourworkhastwomajordifference fromthesemethods.Firstly,weincorporateatwo-layerwordembeddingsysteminthem-RNN networkstructurewhichlearnsthewordrepresentationmoreefthanthesingle-layerword embedding.Secondly,wedonotusetherecurrentlayertostorethevisualinformation.Theimage representationisinputtedtothem-RNNmodelalongwitheverywordinthesentencedescription. Itutilizesofthecapacityoftherecurrentlayermoreef,andallowsustoachievestate-of- the-artperformanceusingarelativelysmalldimensionalrecurrentlayer.Intheexperiments,we showthatthesetwostrategiesleadtobetterperformance.Ourmethodisstillthebest-performing approachforalmostalltheevaluationmetrics. 3M ODEL A RCHITECTURE 3.1S IMPLERECURRENTNEURALNETWORK WeintroducethesimpleRecurrentNeuralNetwork(RNN)orElmannetwork(Elman (1990)).ItsarchitectureisshowninFigure2(a).Ithasthreetypesoflayersineachtimeframe: 3 PublishedasaconferencepaperatICLR2015 theinputwordlayer w ,therecurrentlayer r andtheoutputlayer y .Theactivationofinput,re- currentandoutputlayersattime t isdenotedas w ( t ) , r ( t ) ,and y ( t ) respectively. w ( t ) denotes thecurrentwordvector,whichcanbeasimple1-of-Ncodingrepresentation h ( t ) (i.e.theone-hot representation,whichisbinaryandhasthesamedimensionasthevocabularysizewithonlyone non-zeroelement)Mikolovetal.(2010). y ( t ) canbecalculatedasfollows: x ( t )=[ w ( t ) r ( t  1)]; r ( t )= f 1 ( U  x ( t )); y ( t )= g 1 ( V  r ( t )); (1) where x ( t ) isavectorthatconcatenates w ( t ) and r ( t  1) , f 1 ( : ) and g 1 ( : ) areelement-wisesigmoid andsoftmaxfunctionrespectively,and U , V areweightswhichwillbelearned. ThesizeoftheRNNisadaptivetothelengthoftheinputsequence.Therecurrentlayersconnect thesub-networksindifferenttimeframes.Accordingly,whenwedobackpropagation,weneedto propagatetheerrorthroughrecurrentconnectionsbackintime(Rumelhartetal.(1988)). 3.2O URM -RNN MODEL ThestructureofourmultimodalRecurrentNeuralNetwork(m-RNN)isshowninFigure2(b).It hasvelayersineachtimeframe:twowordembeddinglayers,therecurrentlayer,themultimodal layer,andthesoftmaxlayer). Thetwowordembeddinglayersembedtheone-hotinputintoadensewordrepresentation.Iten- codesboththesyntacticandsemanticmeaningofthewords.Thesemanticallyrelevantwordscanbe foundbycalculatingtheEuclideandistancebetweentwodensewordvectorsinembeddinglayers. Mostofthesentence-imagemultimodalmodels(Karpathyetal.(2014);Fromeetal.(2013);Socher etal.(2014);Kirosetal.(2014b))usepre-computedwordembeddingvectorsastheinitializationof theirmodel.Incontrast,werandomlyinitializeourwordembeddinglayersandlearnthemfromthe trainingdata.Weshowthatthisrandominitializationissufforourarchitecturetogenerate thestate-of-the-artresult.WetreattheactivationofthewordembeddinglayerII(seeFigure2(b)) asthewordrepresentation,whichisoneofthethreedirectinputsofthemultimodallayer. Afterthetwowordembeddinglayers,wehavearecurrentlayerwith256dimensions.Thecalcula- tionoftherecurrentlayerisslightlydifferentfromthecalculationforthesimpleRNN.Insteadof concatenatingthewordrepresentationattime t (denotedas w ( t ) )andtherecurrentlayeractivation attime t  1 (denotedas r ( t  1) ),wemap r ( t  1) intothesamevectorspaceas w ( t ) andadd themtogether: r ( t )= f 2 ( U r  r ( t  1)+ w ( t )); (2) whereﬁ+ﬂrepresentselement-wiseaddition.Weset f 2 ( : ) tobetheRLinearUnit(ReLU), inspiredbyitstherecentsuccesswhentrainingverydeepstructureincomputervision(Nair &Hinton(2010);Krizhevskyetal.(2012)).ThisdiffersfromthesimpleRNNwherethesigmoid functionisadopted(seeSection3.1).ReLUisfaster,andhardertosaturateorovthedatathan non-linearfunctionslikethesigmoid.Whenthebackpropagationthroughtime(BPTT)isconducted fortheRNNwithsigmoidfunction,thevanishingorexplodinggradientproblemappearssinceeven thesimplestRNNmodelcanhavealargetemporaldepth 3 .Previouswork(Mikolovetal.(2010; 2011))useheuristics,suchasthetruncatedBPTT,toavoidthisproblem.ThetruncatedBPTT stopstheBPTTafter k timesteps,where k isahyperparameter.Becauseofthegood propertiesofReLU,wedonotneedtostoptheBPTTatanearlystage,whichleadstobetterand moreefutilizationofthedatathanthetruncatedBPTT. Aftertherecurrentlayer,wesetupa512dimensionalmultimodallayerthatconnectsthelanguage modelpartandthevisionpartofthem-RNNmodel(seeFigure2(b)).Thislayerhasthreeinputs: theword-embeddinglayerII,therecurrentlayerandtheimagerepresentation.Fortheimagerep- resentation,hereweusetheactivationofthe7 th layerofAlexNet(Krizhevskyetal.(2012))or15 th layerofVggNet(Simonyan&Zisserman(2014)),thoughourframeworkcanuseanyimagefea- tures.Wemaptheactivationofthethreelayerstothesamemultimodalfeaturespaceandaddthem togethertoobtaintheactivationofthemultimodallayer: m ( t )= g 2 ( V w  w ( t )+ V r  r ( t )+ V I  I ); (3) 3 WetriedSigmoidandScaledHyperbolicTangentfunctionasthenon-linearfunctionsforRNNinthe experimentsbuttheyleadtothegradientexplosionproblemeasily. 4 PublishedasaconferencepaperatICLR2015 whereﬁ+ﬂdenoteselement-wiseaddition, m denotesthemultimodallayerfeaturevector, I denotes theimagefeature. g 2 ( : ) istheelement-wisescaledhyperbolictangentfunction(LeCunetal.(2012)): g 2 ( x )=1 : 7159  tanh( 2 3 x ) (4) Thisfunctionforcesthegradientsintothemostnon-linearvaluerangeandleadstoafastertraining processthanthebasichyperbolictangentfunction. BoththesimpleRNNandm-RNNmodelshaveasoftmaxlayerthatgeneratestheprobabilitydis- tributionofthenextword.Thedimensionofthislayeristhevocabularysize M ,whichisdifferent fordifferentdatasets. 4T RAININGTHEM -RNN Totrainourm-RNNmodelweadoptalog-likelihoodcostfunction.Itisrelatedtothe Perplexity of thesentencesinthetrainingsetgiventheircorrespondingimages.Perplexityisastandardmeasure forevaluatinglanguagemodel.Theperplexityforonewordsequence(i.e.asentence) w 1: L is calculatedasfollows: log 2 PPL ( w 1: L j I )=  1 L L X n =1 log 2 P ( w n j w 1: n  1 ; I ) (5) where L isthelengthofthewordsequence, PPL ( w 1: L j I ) denotestheperplexityofthesentence w 1: L giventheimage I . P ( w n j w 1: n  1 ; I ) istheprobabilityofgeneratingtheword w n given I and previouswords w 1: n  1 .ItcorrespondstotheactivationoftheSoftMaxlayerofourmodel. Thecostfunctionofourmodelistheaveragelog-likelihoodofthewordsgiventheircontextwords andcorrespondingimagesinthetrainingsentencesplusaregularizationterm.Itcanbecalculated bytheperplexity: C = 1 N N s X i =1 L i  log 2 PPL ( w ( i ) 1: L i j I ( i ) )+   k  k 2 2 (6) where N s and N denotesthenumberofsentencesandthenumberofwordsinthetrainingset receptively, L i denotesthelengthof i th sentences,and  representsthemodelparameters. Ourtrainingobjectiveistominimizethiscostfunction,whichisequivalenttomaximizetheproba- bilityofgeneratingthesentencesinthetrainingsetusingthemodel.Thecostfunctionisdifferen- tiableandweusebackpropagationtolearnthemodelparameters. 5S ENTENCE G ENERATION ,I MAGE R ETRIEVALAND S ENTENCE R ETRIEVAL Weusethetrainedm-RNNmodelforthreetasks:1)Sentencesgeneration,2)Imageretrieval(re- trievingmostrelevantimagestothegivensentence),3)Sentenceretrieval(retrievingmostrelevant sentencestothegivenimage). Thesentencegenerationprocessisstraightforward.Startingfromthestartsign w start orarbitrary numberofreferencewords(e.g.wecaninputtheKwordsinthereferencesentencetothe modelandthenstarttogeneratenewwords),ourmodelcancalculatetheprobabilitydistribution ofthenextword: P ( w n j w 1: n  1 ; I ) .Thenwecansamplefromthisprobabilitydistributiontopick thenextword.Inpractice,wethatselectingthewordwiththemaximumprobabilityperforms slightlybetterthansampling.Afterthat,weinputthepickedwordtothemodelandcontinuethe processuntilthemodeloutputstheendsign w end . Fortheretrievaltasks,weuseourmodeltocalculatetheprobabilityofgeneratingasentence w 1: L givenanimage I : P ( w 1: L j I )= Q n P ( w n j w 1: n  1 ; I ) .Theprobabilitycanbetreatedasanaf measurementbetweensentencesandimages. Fortheimageretrievaltask,giventhequerysentence w Q 1: L ,werankthedatasetimages I D accord- ingtotheprobability P ( w Q 1: L j I D ) andretrievedthetoprankedimages.Thisisequivalenttothe perplexity-basedimageretrievalinKirosetal.(2014b). 5 PublishedasaconferencepaperatICLR2015 Thesentenceretrievaltaskistrickierbecausetheremightbesomesentencesthathavehighproba- bilityorperplexityforanyimagequery(e.g.sentencesconsistofmanyfrequentlyappearedwords). Tosolvethisproblem,Kirosetal.(2014b)usestheperplexityofasentenceconditionedonthe averagedimagefeatureacrossthetrainingsetasthereferenceperplexitytonormalizetheoriginal perplexity.Differentfromthem,weusethe normalizedprobability wherethenormalizationfactor isthemarginalprobabilityof w D 1: L : P ( w D 1: L j I Q ) =P ( w D 1: L ); P ( w D 1: L )= P I 0 P ( w D 1: L j I 0 )  P ( I 0 ) (7) where w D 1: L denotesthesentenceinthedataset, I Q denotesthequeryimage,and I 0 areimages sampledfromthetrainingset.Weapproximate P ( I 0 ) byaconstantandignorethisterm.This strategyleadstoamuchbetterperformancethanthatinKirosetal.(2014b)intheexperiments. Thenormalizedprobabilityisequivalenttotheprobability P ( I Q j w D 1: L ) ,whichissymmetrictothe probability P ( w Q 1: L j I D ) usedintheimageretrievaltask. 6L EARNINGOF S ENTENCEAND I MAGE F EATURES Thearchitectureofourmodelallowsthegradientsfromthelossfunctiontobebackpropagatedto boththelanguagemodelingpart(i.e.thewordembeddinglayersandtherecurrentlayer)andthe visionpart(e.g.theAlexNetorVggNet). Forthelanguagepart,asmentionedabove,werandomlyinitializethelanguagemodelinglayersand learntheirparameters.Forthevisionpart,weusethepre-trainedAlexNet(Krizhevskyetal.(2012)) ortheVggNet(Simonyan&Zisserman(2014))onImageNetdataset(Russakovskyetal.(2014)). Recently,Karpathyetal.(2014)showthatusingtheRCNNobjectdetectionresults(Girshicketal. (2014))combinedwiththeAlexNetfeaturesperformsbetterthansimplytreatingtheimageasa wholeframe.Intheexperiments,weshowthatourmethodperformsmuchbetterthanKarpathy etal.(2014)whenthesameimagefeaturesareused,andisbetterthanorcomparabletotheirresults evenwhentheyusemoresophisticatedfeaturesbasedonobjectdetection. WecanupdatetheCNNinthevisionpartofourmodelaccordingtothegradientbackpropagated fromthemultimodallayer.Inthispaper,wetheimagefeaturesandthedeepCNNnetworkinthe trainingstageduetoashortageofdata.Infuturework,wewillapplyourmethodonlargedatasets (e.g.thecompleteMSCOCOdataset,whichhasnotyetbeenreleased)andtheparameters ofthedeepCNNnetworkinthetrainingstage. Them-RNNmodelistrainedusingBaidu'sinternaldeeplearningplatformPADDLE,whichallows ustoexploremanydifferentmodelarchitecturesinashortperiod.Thehyperparameters,suchas layerdimensionsandthechoiceofthenon-linearactivationfunctions,aretunedviacross-validation onFlickr8Kdatasetandarethenedacrossalltheexperiments.Ittakes25msonaverageto generateasentence(excludingimagefeatureextractionstage)onasinglecoreCPU. 7E XPERIMENTS 7.1D ATASETS Wetestourmethodonfourbenchmarkdatasetswithsentencelevelannotations:IAPRTC-12(Grub- ingeretal.(2006)),Flickr8K(Rashtchianetal.(2010)),Flickr30K(Youngetal.(2014))andMS COCO(Linetal.(2014)). IAPRTC-12 .Thisdatasetconsistsofaround20,000imagestakenfromdifferentlocationsaround theworld.Itcontainsimagesofdifferentsportsandactions,people,animals,cities,landscapes, etc.Foreachimage,itprovidesatleastonesentenceannotation.Onaverage,thereareabout1.7 sentenceannotationsforoneimage.Weadoptthestandardseparationoftrainingandtestingsetas previousworks(Guillauminetal.(2010);Kirosetal.(2014b))with17,665imagesfortrainingand 1962imagesfortesting. Flickr8K .Thisdatasetconsistsof8,000imagesextractedfromFlickr.Foreachimage,itprovides vesentenceannotations.Weadoptthestandardseparationoftraining,validationandtestingset providedbythedataset.Thereare6,000imagesfortraining,1,000imagesforvalidationand1,000 imagesfortesting. 6 PublishedasaconferencepaperatICLR2015 Flickr30K .ThisdatasetisarecentextensionofFlickr8K.Foreachimage,italsoprovidesve sentencesannotations.Itconsistsof158,915crowd-sourcedcaptionsdescribing31,783images. ThegrammarandstylefortheannotationsofthisdatasetissimilartoFlickr8K.Wefollowthe previouswork(Karpathyetal.(2014))whichused1,000imagesfortesting.Thisdataset,aswellas theFlick8Kdataset,wereoriginallyusedfortheimage-sentenceretrievaltasks. MSCOCO .Thecurrentreleaseofthisrecentlyproposeddatasetcontains82,783trainingimages and40,504validationimages.Foreachimage,itprovidesvesentencesannotations.Werandomly sampled4,000imagesforvalidationand1,000imagesfortestingfromtheircurrentlyreleased validationset.ThedatasetpartitionofMSCOCOandFlickr30Kisavailableintheprojectpage 4 . 7.2E VALUATIONMETRICS SentenceGeneration .Followingpreviousworks,weusethesentenceperplexity(seeEqu.5)and BLEUscores(i.e.B-1,B-2,B-3,andB-4)(Papinenietal.(2002))astheevaluationmetrics.BLEU scoreswereoriginallydesignedforautomaticmachinetranslationwheretheyratethequalityofa translatedsentencesgivenseveralreferencesentences.Similarly,wecantreatthesentencegener- ationtaskastheﬁtranslationﬂofthecontentofimagestosentences.BLEUremainsthestandard evaluationmetricforsentencegenerationmethodsforimages,thoughithasdrawbacks.Forsome images,thereferencesentencesmightnotcontainallthepossibledescriptionsintheimageand BLEUmightpenalizesomecorrectlygeneratedsentences.Pleaseseemoredetailsofthecalcula- tionofBLEUscoresforthistaskinthesupplementarymaterialsection10.3 5 . SentenceRetrievalandImageRetrieval .Weadoptthesameevaluationmetricsaspreviousworks (Socheretal.(2014);Fromeetal.(2013);Karpathyetal.(2014))forboththetasksofsentences retrievalandimageretrieval.WeuseR@K(K=1,5,10)asthemeasurement.R@Kistherecall rateofacorrectlyretrievedgroundtruthgiventopKcandidates.HigherR@Kusuallymeansbetter retrievalperformance.Sincewecaremostaboutthetop-rankedretrievedresults,theR@Kscores withsmallerKaremoreimportant. TheMedrisanothermetricweuse,whichisthemedianrankoftheretrievedgroundtruth sentenceorimage.LowerMedrusuallymeansbetterperformance.ForIAPRTC-12datasets, weuseadditionalevaluationmetricstoconductafaircomparisonwithpreviouswork(Kirosetal. (2014b)).Pleaseseethedetailsinthesupplementarymaterialsection10.3. 7.3R ESULTSON IAPRTC-12 Theresultsofthesentencegenerationtask 6 areshowninTable1.Ours-RNN-Baseservesasa baselinemethodforourm-RNNmodel.Ithasthesamearchitectureasm-RNNexceptthatitdoes nothavetheimagerepresentationinput. Toconductafaircomparison,wefollowthesameexperimentalsettingsofKirosetal.(2014b) tocalculatetheBLEUscoresandperplexity.Thesetwoevaluationmetricsarenotnecessarily correlatedtoeachotherforthefollowingreasons.AsmentionedinSection4,perplexityiscalculated accordingtotheconditionalprobabilityofthewordinasentencegivenallofitspreviousreference words.Therefore,astronglanguagemodelthatsuccessfullycapturesthedistributionsofwordsin sentencescanhavealowperplexitywithouttheimagecontent.Butthecontentofthegenerated sentencesmightbeuncorrelatedtoimages.FromTable1,wecanseethatalthoughourbaseline methodofRNNgeneratesalowperplexity,itsBLEUscoreislow,indicatingthatitfailstogenerate sentencesthatareconsistentwiththecontentofimages. Table1showsthatourm-RNNmodelperformsmuchbetterthanourbaselineRNNmodelandthe state-of-the-artmethodsbothintermsoftheperplexityandBLEUscore. 4 www.stat.ucla.edu/ Ÿ junhua.mao/m-RNN.html 5 TheBLEUoutputtedbyourimplementationisslightlylowerthantherecentlyreleasedMSCOCOcaption evaluationtoolbox(Chenetal.(2015))becauseofdifferenttokenizationmethodsofthesentences.Were- evaluateourmethodusingthetoolboxinthecurrentversionofthepaper. 6 Kirosetal.(2014b)furtherimprovedtheirresultsafterthepublication.Wecompareourresultswiththeir updatedoneshere. 7 PublishedasaconferencepaperatICLR2015 PPL B-1B-2B-3B-4 LBL,Mnih&Hinton(2007)9.290.3210.1450.064- MLBLB-AlexNet,Kirosetal.(2014b)9.860.3930.2110.112- MLBLF-AlexNet,Kirosetal.(2014b)9.900.3870.2090.115- Guptaetal.(2012)-0.150.060.01- Gupta&Mannem(2012)-0.330.180.07- Ours-RNN-Base7.770.3070.1770.0960.043 Ours-m-RNN-AlexNet 6.920.4820.3570.2690.208 Table1:ResultsofthesentencegenerationtaskontheIAPRTC-12dataset.ﬁBﬂisshortforBLEU. SentenceRetrival(ImagetoText)ImageRetrival(TexttoImage) R@1R@5R@10MedrR@1R@5R@10Medr Ours-m-RNN20.943.854.4813.231.240.821 Table2:R@Kandmedianrank(Medr)forIAPRTC-12dataset. SentenceRetrival(ImagetoText)ImageRetrival(TexttoImage) R@1R@5R@10MedrR@1R@5R@10Medr Random0.10.51.06310.10.51.0500 SDT-RNN-AlexNet4.518.028.6326.118.529.029 Socher-avg-RCNN6.022.734.0236.621.631.725 DeViSE-avg-RCNN4.816.527.3285.920.129.629 DeepFE-AlexNet5.919.227.3345.217.626.532 DeepFE-RCNN12.632.944.0149.729.6 42.515 Ours-m-RNN-AlexNet 14.537.248.51111.531.0 42.4 15 Table3:ResultsofR@Kandmedianrank(Medr)forFlickr8Kdataset.ﬁ-AlexNetﬂdenotesthe imagerepresentationbasedonAlexNetextractedfromthewholeimageframe.ﬁ-RCNNﬂdenotes theimagerepresentationextractedfrompossibleobjectsdetectedbytheRCNNalgorithm. Fortheretrievaltasks,sincetherearenopubliclyavailableresultsofR@KandMedrinthisdataset, wereportR@KscoresofourmethodinTable2forfuturecomparisons.Theresultshowsthat 20.9%top-rankedretrievedsentencesand13.2%top-rankedretrievedimagesaregroundtruth.We alsoadoptadditionalevaluationmetricstocompareourmethodwithKirosetal.(2014b),seesup- plementarymaterialSection10.2. 7.4R ESULTSON F LICKR 8K Thisdatasetwaswidelyusedasabenchmarkdatasetforimageandsentenceretrieval.TheR@K andMedrofdifferentmethodsareshowninTable3.Wecompareourmodelwithseveralstate-of- the-artmethods:SDT-RNN(Socheretal.(2014)),DeViSE(Fromeetal.(2013)),DeepFE(Karpathy etal.(2014))withvariousimagerepresentations.Ourmodeloutperformsthesemethodsbyalarge marginwhenusingthesameimagerepresentation(e.g.AlexNet).Wealsolisttheperformanceof methodsusingmoresophisticatedfeaturesinTable3.ﬁ-avg-RCNNﬂdenotesmethodswithfeatures oftheaverageCNNactivationofallobjectsaboveadetectionthreshold.DeepFE-RCNN Karpathyetal.(2014)usesafragmentmappingstrategytobetterexploittheobjectdetectionresults. Theresultsshowthatusingthesefeaturesimprovestheperformance.Evenwithoutthehelpfrom theobjectdetectionmethods,however,ourmethodperformsbetterthanthesemethodsinalmostall theevaluationmetrics.Wewilldevelopourframeworkusingbetterimagefeaturesbasedonobject detectioninthefuturework. The PPL ,B-1,B-2,B-3andB-4ofthegeneratedsentencesusingourm-RNN-AlexNetmodelin thisdatasetare24.39,0.565,0.386,0.256,and0.170respectively. 8 PublishedasaconferencepaperatICLR2015 SentenceRetrival(ImagetoText)ImageRetrival(TexttoImage) R@1R@5R@10MedrR@1R@5R@10Medr Flickr30K Random0.10.61.16310.10.51.0500 DeViSE-avg-RCNN4.816.527.3285.920.129.629 DeepFE-RCNN16.440.254.7810.331.444.513 RVR12.127.847.81112.733.144.912.5 MNLM-AlexNet14.839.250.91011.834.046.313 MNLM-VggNet23.050.762.9516.842.056.58 NIC17.056.0-717.0 57.0 -7 LRCN14.034.947.011---- DeepVS22.248.261.44.815.237.750.59.2 Ours-m-RNN-AlexNet18.440.250.91012.631.241.516 Ours-m-RNN-VggNet 35.463.873.7322.8 50.7 63.15 MSCOCO Random0.10.61.16310.10.51.0500 DeepVS-RCNN29.462.075.92.520.9 52.8 69.24 Ours-m-RNN-VggNet 41.073.083.5229.0 42.2 77.03 Table4:ResultsofR@Kandmedianrank(Medr)forFlickr30KdatasetandMSCOCOdataset. Flickr30KMSCOCO PPL B-1B-2B-3B-4 PPL B-1B-2B-3B-4 RVR----0.13----0.19 DeepVS-AlexNet-0.470.210.09--0.530.280.15- DeepVS-VggNet21.200.500.300.15-19.640.570.370.19- NIC- 0.66 ---- 0.67 --- LRCN-0.590.390.250.16-0.630.440.310.21 DMSM---------0.21 Ours-m-RNN-AlexNet35.110.540.360.230.15----- Ours-m-RNN-VggNet 20.72 0.60 0.410.280.1913.600.670.490.350.25 Table5:ResultsofgeneratedsentencesontheFlickr30KdatasetandMSCOCOdataset. Ourm-RNNMNLMNICLRCNRVRDeepVS RNNDim.2563005121000(  4 )100300-600 LSTMNoYesYesYesNoNo Table6:Propertiesoftherecurrentlayersfortheveveryrecentmethods.LRCNhasastackof four1000dimensionalLSTMlayers.Weachievesstate-of-the-artperformanceusingarelatively smalldimensionalrecurrentlayer.LSTM(Hochreiter&Schmidhuber(1997))canbetreatedasa sophisticatedversionoftheRNN. 7.5R ESULTSON F LICKR 30K AND MSCOCO Wecompareourmethodwithseveralstate-of-the-artmethodsinthesetworecentlyreleaseddataset (Notethatthelastsixmethodsappearveryrecently,weusetheresultsreportedintheirpapers): DeViSE(Fromeetal.(2013)),DeepFE(Karpathyetal.(2014)),MNLM(Kirosetal.(2014a)), DMSM(Fangetal.(2014)),NIC(Vinyalsetal.(2014)),LRCN(Donahueetal.(2014)),RVR (Chen&Zitnick(2014)),andDeepVS(Karpathy&Fei-Fei(2014)).Theresultsoftheretrieval tasksandthesentencegenerationtask 7 areshowninTable4andTable5respectively.Wealso summarizesomeofthepropertiesoftherecurrentlayersadoptedintheveveryrecentmethodsin Table6. 7 WeonlyselectthewordwithmaximumprobabilityeachtimeinthesentencegenerationprocessinTable 5whilemanycomparingmethods(e.g.DMSM,NIC,LRCN)usesabeamsearchschemethatkeepsthebestK candidates.Thebeamsearchschemewillleadtobetterperformanceinpracticeusingthesamemodel. 9 PublishedasaconferencepaperatICLR2015 B1B2B3B4CIDErROUGE LMETEOR m-RNN-greedy-c50.6680.4880.3420.2390.7290.4890.221 m-RNN-greedy-c400.8450.7300.5980.4730.7400.6160.291 m-RNN-beam-c50.6800.5060.3690.2720.7910.4990.225 m-RNN-beam-c400.8650.7600.6410.5290.7890.6400.304 Table7:ResultsoftheMSCOCOtestsetevaluatedbyMSCOCOevaluationserver OurmethodwithVggNetimagerepresentation(Simonyan&Zisserman(2014))outperformsthe state-of-the-artmethods,includingtheveryrecentlyreleasedmethods,inalmostalltheevaluation metrics.Notethatthedimensionoftherecurrentlayerofourmodelisrelativelysmallcompared tothecompetingmethods.Itshowstheadvantageandefyofourmethodthatdirectlyinputs thevisualinformationtothemultimodallayerinsteadofstoringitintherecurrentlayer.Them- RNNmodelwithVggNetperformsbetterthanthatwithAlexNet,whichindicatestheimportance ofstrongimagerepresentationsinthistask.71%ofthegeneratedsentencesforMSCOCOdatasets arenovel(i.e.differentfromtrainingsentences). WealsovalidateourmethodonthetestsetofMSCOCObytheirevaluationserver(Chenetal. (2015)).TheresultsareshowninTable7.Weevaluateourmodelwithgreedyinference(select thewordwiththemaximumprobabilityeachtime)aswellaswiththebeamsearchinference.ﬁ- c5ﬂrepresentsresultsusing5referencesentencesandﬁ-c40ﬂrepresentsresultsusing40reference sentences. Tofurthervalidatetheimportanceofdifferentcomponentsofthem-RNNmodel,wetrainsev- eralvariantsoftheoriginalm-RNNmodelandcomparetheirperformance.Inparticular,weshow thatthetwo-layerwordembeddingsystemoutperformsthesingle-layerversionandthestrategyof directlyinputtingthevisualinformationtothemultimodallayersubstantiallyimprovestheperfor- mance(about5%forB-1).Duetothelimitedspace,weputthedetailsoftheseexperimentsin Section10.1inthesupplementarymaterialafterthemainpaper. 8N EAREST N EIGHBORAS R EFERENCE Recently,Devlinetal.(2015b)proposedanearestneighborapproachthatretrievesthecaptions ofthe k nearestimagesinthetrainingset,ranksthesecaptionsaccordingtotheconsensusofthe captionw.r.t.totherestofthecaptions,andoutputthetoprankedone. Inspiredbythismethod,weadoptthem-RNNmodelwiththetransposedweightsharingstrat- egy(Maoetal.(2015),denotedasm-RNN-shared)togenerate n hypothesesusingabeamsearch scheme.,wekeepthe n bestcandidatesinthesentencegenerationprocessuntilthe modelgeneratestheendsign w end .These n bestcandidatesareapproximatelythe n mostprobable sentencesgeneratedbythemodel,andcanbetreatedasthe n hypotheses.Inourexperiments,weset n =10 sinceitgivesusadivsetofhypotheseswithouttoomuchoutliersonourvalidation set. 8 Aftergeneratingthehypothesesofatargetimage,weretrieveitsnearestneighborsintheimage featurespaceonthetrainingset(seedetailsinSection8.1).Thenwecalculatetheﬁconsensusﬂ scores(Devlinetal.(2015a))ofthehypothesesw.r.t.tothegroundtruthcaptionsofthenearest neighborimages,andrerankthehypothesesaccordingtothesescores(seedetailsinSection8.2). 8.1I MAGEFEATURESFORTHENEARESTNEIGHBORIMAGESEARCH Wetrytwotypesofimagefeaturesforthenearestneighborimagesearch 9 .Theoneisthe originalimagefeaturesextractedbytheVggNet(Simonyan&Zisserman(2014)).Weresize theimagesothatitsshortsideis256pixels.Thenweextractfeaturesonten 224  224 windows 8 Ifwedirectlyoutputthetophypothesesgeneratedbythemodel,then n =5 givesusthebestperformance. Butifwewanttorerankthehypotheses,then n =10 givesusabetterresultonthevalidationset. 9 WereleasebothtypesofthefeaturesonMSCOCO2014train,valandtestsets.Pleaserefertothereadme at https://github.com/mjhucla/mRNN-CR toseehowtodownloadandusethem. 10 PublishedasaconferencepaperatICLR2015 Figure3:Thesampleimagesandtheirnearestneighborsretrievedbytwotypesoffeatures.Com- paredtotheoriginalVggNetfeatures,thefeaturesbythem-RNNmodelarebetterforcap- turingricherandmoreaccuratevisualinformation. (thefourcorners,thecenterandtheirmirroredversions)ontheresizedimage.Finally,weaverage poolthetenfeaturestomakeita4,096dimensionalfeature. Thesecondtypeisthefeaturebyourm-RNNmodel.Itcanbecalculatedas: I r = g 2 ( V I  I ) , where V I istheweightmatrixbetweentheimagerepresentationandthemultimodallayer(see Equation3),and g 2 ( : ) isthescaledhyperbolictangentfunction. WeshowthesampleimagesandtheirnearestneighborsinFigure3.Wethatcomparedtothe originalVggNetfeatures,thefeaturesbythem-RNNmodelcapturericherandmoreaccurate visualinformation.E.g.,thetargetimageinthesecondrowcontainsanoldwomanwithabunchof bananas.TheoriginalVggNetfeaturesdonotretrieveimageswithbananasinthem. 8.2C ONSENSUS R ERANKING Supposewehavegetthe k nearestneighborimagesinthetrainingsetasthereference.Wefollow Devlinetal.(2015a)tocalculatetheconsensusscoreofahypotheses.ThedifferenceisthatDevlin etal.(2015a)treatthecaptionsofthe k nearestneighborimagesasthehypotheseswhileourhy- pothesesaregeneratedbythem-RNNmodel.More,foreachhypothesis,wecalculate themeansimilaritybetweenthishypothesisandallthecaptionsofthe k nearestneighborimages. MSCOCOvalforconsensusreranking B1B2B3B4CIDErROUGE LMETEOR m-RNN-shared0.6860.5110.3750.2800.8420.5000.228 m-RNN-shared-NNref-BLEU0.7180.5500.4090.3050.9090.5190.235 m-RNN-shared-NNref-CIDEr0.7140.5430.4060.3040.9380.5190.239 m-RNN-shared-NNref-BLEU-Orcale0.7920.6630.5430.4431.2350.6020.287 m-RNN-shared-NNref-CIDEr-Oracle0.7840.6480.5290.4301.2720.5930.287 MSCOCO2014testserver B1B2B3B4CIDErROUGE LMETEOR m-RNN-shared0.6850.5120.3760.2790.8190.5040.229 m-RNN-shared-NNref-BLEU0.7200.5530.4100.3020.8860.5240.238 m-RNN-shared-NNref-CIDEr0.7160.5450.4040.2990.9170.5210.242 Table8:Resultsofm-RNN-sharedmodelafterapplyingconsensusrerankingusingnearestneigh- borsasreferences(m-RNN-shared-NNref),comparedwiththoseoftheoriginalm-RNNmodelon ourvalidationsetandMSCOCOtestserver. 11 PublishedasaconferencepaperatICLR2015 Theconsensusscoreofthishypothesisisthemeansimilarityscoreofthe m nearestcaptions.The similaritybetweenahypothesisandoneofitsnearestneighborreferencecaptionsisby asentence-levelBLEUscore(Papinenietal.(2002))orasentence-levelCIDEr(Vedantametal. (2014)).Wecross-validatethehyperparamters k and m .FortheBLEU-basedsimilarity,theopti- mal k and m are60and175respectively.FortheCIDEr-basedsimilarity,theoptimal k and m are 60and125respectively. 8.3E XPERIMENTS WeshowtheresultsofourmodelonourvalidationsetandtheMSCOCOtestingserverinTable 8.ForBLEU-basedconsensusreranking,wegetanimprovementof3.5pointsonourvalidation setand3.3pointsontheMSCOCOtest2014setintermsofBLEU4score.FortheCIDEr-based consensusreranking,wegetanimprovementof9.4pointsonourvalidationsetand9.8pointson theMSCOCOtest2014setintermsofCIDEr. 8.4D ISCUSSION WeshowtherankofthetenhypothesesbeforeandafterrerankinginFigure4.Althoughthehy- pothesesaresimilartoeachother,therearesomevariancesamongthem(E.g.,someofthemcapture moredetailsoftheimages.Someofthemmightbepartiallywrong).Thererankingprocessisable toimprovetherankofgoodcaptions. Wealsoshowtheoracleperformanceofthetenhypotheses,whichistheupperboundofthecon- sensusreranking.More,foreachimageinourvalidationset,wererankthehypotheses accordingtothescores(BLEUorCIDEr)w.r.ttothegroundtruthcaptions.Theresultsofthisoracle rerankingareshowninTable8(seerowswithﬁ-oracleﬂ).Theoracleperformanceissurprisingly high,indicatingthatthereisstillroomforimprovement,bothforthem-RNNmodelitselfandthe rerankingstrategy. 9C ONCLUSION WeproposeamultimodalRecurrentNeuralNetwork(m-RNN)frameworkthatperformsatthe state-of-the-artinthreetasks:sentencegeneration,sentenceretrievalgivenqueryimageandimage Figure4:Theoriginalrankofthehypothesesandtherankafterconsensusreranking(CIDEr). 12 PublishedasaconferencepaperatICLR2015 retrievalgivenquerysentence.ThemodelconsistsofadeepRNN,adeepCNNandthesetwo sub-networksinteractwitheachotherinamultimodallayer.Ourm-RNNispowerfulofconnecting imagesandsentencesandisxibletoincorporatemorecompleximagerepresentationsandmore sophisticatedlanguagemodels. A CKNOWLEDGMENTS WethankAndrewNg,KaiYu,ChangHuang,DuohaoQin,HaoyuanGao,JasonEisnerforuseful discussionsandtechnicalsupport.Wealsothankthecommentsandsuggestionsoftheanonymous reviewersfromICLR2015andNIPS2014DeepLearningWorkshop.WeacknowledgetheCenter forMinds,BrainsandMachines(CBMM),partiallyfundedbyNSFSTCawardCCF-1231216,and ARO62250-CS. R EFERENCES Barnard,Kobus,Duygulu,Pinar,Forsyth,David,DeFreitas,Nando,Blei,DavidM,andJordan, MichaelI.Matchingwordsandpictures. JMLR ,3:1107Œ1135,2003. Chen,X.,Fang,H.,Lin,TY,Vedantam,R.,Gupta,S.,Dollr,P.,andZitnick,C.L.Microsoftcoco captions:Datacollectionandevaluationserver. arXivpreprintarXiv:1504.00325 ,2015. Chen,XinleiandZitnick,CLawrence.Learningarecurrentvisualrepresentationforimagecaption generation. arXivpreprintarXiv:1411.5654 ,2014. Cho,Kyunghyun,vanMerrienboer,Bart,Gulcehre,Caglar,Bougares,Fethi,Schwenk,Holger, andBengio,Yoshua.Learningphraserepresentationsusingrnnencoder-decoderforstatistical machinetranslation. arXivpreprintarXiv:1406.1078 ,2014. Devlin,Jacob,Cheng,Hao,Fang,Hao,Gupta,Saurabh,Deng,Li,He,Xiaodong,Zweig,Geoffrey, andMitchell,Margaret.Languagemodelsforimagecaptioning:Thequirksandwhatworks. arXivpreprintarXiv:1505.01809 ,2015a. Devlin,Jacob,Gupta,Saurabh,Girshick,Ross,Mitchell,Margaret,andZitnick,CLawrence.Ex- ploringnearestneighborapproachesforimagecaptioning. arXivpreprintarXiv:1505.04467 , 2015b. Donahue,Jeff,Hendricks,LisaAnne,Guadarrama,Sergio,Rohrbach,Marcus,Venugopalan,Sub- hashini,Saenko,Kate,andDarrell,Trevor.Long-termrecurrentconvolutionalnetworksforvisual recognitionanddescription. arXivpreprintarXiv:1411.4389 ,2014. Elman,JeffreyL.Findingstructureintime. Cognitivescience ,14(2):179Œ211,1990. Fang,Hao,Gupta,Saurabh,Iandola,Forrest,Srivastava,Rupesh,Deng,Li,Doll ´ ar,Piotr,Gao, Jianfeng,He,Xiaodong,Mitchell,Margaret,Platt,John,etal.Fromcaptionstovisualconcepts andback. arXivpreprintarXiv:1411.4952 ,2014. Farhadi,Ali,Hejrati,Mohsen,Sadeghi,MohammadAmin,Young,Peter,Rashtchian,Cyrus,Hock- enmaier,Julia,andForsyth,David.Everypicturetellsastory:Generatingsentencesfromimages. In ECCV ,pp.15Œ29.2010. Frome,Andrea,Corrado,GregS,Shlens,Jon,Bengio,Samy,Dean,Jeff,Mikolov,Tomas,etal. Devise:Adeepvisual-semanticembeddingmodel.In NIPS ,pp.2121Œ2129,2013. Girshick,R.,Donahue,J.,Darrell,T.,andMalik,J.Richfeaturehierarchiesforaccurateobject detectionandsemanticsegmentation.In CVPR ,2014. Grubinger,Michael,Clough,Paul,M ¨ uller,Henning,andDeselaers,Thomas.Theiaprtc-12bench- mark:Anewevaluationresourceforvisualinformationsystems.In InternationalWorkshop OntoImage ,pp.13Œ23,2006. Guillaumin,Matthieu,Verbeek,Jakob,andSchmid,Cordelia.Multipleinstancemetriclearning fromautomaticallylabeledbagsoffaces.In ECCV ,pp.634Œ647,2010. 13 PublishedasaconferencepaperatICLR2015 Gupta,AnkushandMannem,Prashanth.Fromimageannotationtoimagedescription.In ICONIP , 2012. Gupta,Ankush,Verma,Yashaswi,andJawahar,CV.Choosinglinguisticsovervisiontodescribe images.In AAAI ,2012. Hochreiter,SeppandSchmidhuber,J ¨ urgen.Longshort-termmemory. Neuralcomputation ,9(8): 1735Œ1780,1997. Hodosh,Micah,Young,Peter,andHockenmaier,Julia.Framingimagedescriptionasaranking task:Data,modelsandevaluationmetrics. JAIR ,47:853Œ899,2013. Jia,Yangqing,Salzmann,Mathieu,andDarrell,Trevor.Learningcross-modalitysimilarityfor multinomialdata.In ICCV ,pp.2407Œ2414,2011. Kalchbrenner,NalandBlunsom,Phil.Recurrentcontinuoustranslationmodels.In EMNLP ,pp. 1700Œ1709,2013. Karpathy,AndrejandFei-Fei,Li.Deepvisual-semanticalignmentsforgeneratingimagedescrip- tions. arXivpreprintarXiv:1412.2306 ,2014. Karpathy,Andrej,Joulin,Armand,andFei-Fei,Li.Deepfragmentembeddingsforbidirectional imagesentencemapping.In arXiv:1406.5679 ,2014. Kiros,Ryan,Salakhutdinov,Ruslan,andZemel,RichardS.Unifyingvisual-semanticembeddings withmultimodalneurallanguagemodels. arXivpreprintarXiv:1411.2539 ,2014a. Kiros,Ryan,Zemel,R,andSalakhutdinov,Ruslan.Multimodalneurallanguagemodels.In ICML , 2014b. Krizhevsky,Alex,Sutskever,Ilya,andHinton,GeoffreyE.Imagenetwithdeepcon- volutionalneuralnetworks.In NIPS ,pp.1097Œ1105,2012. Kulkarni,Girish,Premraj,Visruth,Dhar,Sagnik,Li,Siming,Choi,Yejin,Berg,AlexanderC,and Berg,TamaraL.Babytalk:Understandingandgeneratingimagedescriptions.In CVPR ,2011. Kuznetsova,Polina,Ordonez,Vicente,Berg,TamaraL,andChoi,Yejin.Treetalk:Compositionand compressionoftreesforimagedescriptions. TransactionsoftheAssociationforComputational Linguistics ,2(10):351Œ362,2014. LeCun,YannA,Bottou,L ´ eon,Orr,GenevieveB,andM ¨ uller,Klaus-Robert.Efbackprop.In Neuralnetworks:Tricksofthetrade ,pp.9Œ48.Springer,2012. Lin,Tsung-Yi,Maire,Michael,Belongie,Serge,Hays,James,Perona,Pietro,Ramanan,Deva, Doll ´ ar,Piotr,andZitnick,CLawrence.Microsoftcoco:Commonobjectsincontext. arXiv preprintarXiv:1405.0312 ,2014. Mao,Junhua,Xu,Wei,Yang,Yi,Wang,Jiang,andYuille,AlanL.Explainimageswithmultimodal recurrentneuralnetworks. NIPSDeepLearningWorkshop ,2014. Mao,Junhua,Xu,Wei,Yang,Yi,Wang,Jiang,Huang,Zhiheng,andYuille,Alan.Learninglikea child:Fastnovelvisualconceptlearningfromsentencedescriptionsofimages. arXivpreprint arXiv:1504.06692 ,2015. Mikolov,Tomas, ´ at,Martin,Burget,Lukas,Cernock ˚ y,Jan,andKhudanpur,Sanjeev.Recur- rentneuralnetworkbasedlanguagemodel.In INTERSPEECH ,pp.1045Œ1048,2010. Mikolov,Tomas,Kombrink,Stefan,Burget,Lukas,Cernocky,JH,andKhudanpur,Sanjeev.Exten- sionsofrecurrentneuralnetworklanguagemodel.In ICASSP ,pp.5528Œ5531,2011. Mikolov,Tomas,Sutskever,Ilya,Chen,Kai,Corrado,GregS,andDean,Jeff.Distributedrepresen- tationsofwordsandphrasesandtheircompositionality.In NIPS ,pp.3111Œ3119,2013. 14 PublishedasaconferencepaperatICLR2015 Mitchell,Margaret,Han,Xufeng,Dodge,Jesse,Mensch,Alyssa,Goyal,Amit,Berg,Alex,Ya- maguchi,Kota,Berg,Tamara,Stratos,Karl,andDaum ´ eIII,Hal.Midge:Generatingimage descriptionsfromcomputervisiondetections.In EACL ,2012. Mnih,AndriyandHinton,Geoffrey.Threenewgraphicalmodelsforstatisticallanguagemodelling. In ICML ,pp.641Œ648.ACM,2007. Nair,VinodandHinton,GeoffreyE.linearunitsimproverestrictedboltzmannmachines. In ICML ,pp.807Œ814,2010. Papineni,Kishore,Roukos,Salim,Ward,Todd,andZhu,Wei-Jing.Bleu:amethodforautomatic evaluationofmachinetranslation.In ACL ,pp.311Œ318,2002. Rashtchian,Cyrus,Young,Peter,Hodosh,Micah,andHockenmaier,Julia.Collectingimageanno- tationsusingamazon'smechanicalturk.In NAACL-HLTworkshop2010 ,pp.139Œ147,2010. Rumelhart,DavidE,Hinton,GeoffreyE,andWilliams,RonaldJ.Learningrepresentationsby back-propagatingerrors. Cognitivemodeling ,1988. Russakovsky,Olga,Deng,Jia,Su,Hao,Krause,Jonathan,Satheesh,Sanjeev,Ma,Sean,Huang, Zhiheng,Karpathy,Andrej,Khosla,Aditya,Bernstein,Michael,Berg,AlexanderC.,andFei-Fei, Li.ImageNetLargeScaleVisualRecognitionChallenge,2014. Simonyan,KarenandZisserman,Andrew.Verydeepconvolutionalnetworksforlarge-scaleimage recognition. arXivpreprintarXiv:1409.1556 ,2014. Socher,Richard,Le,Q,Manning,C,andNg,A.Groundedcompositionalsemanticsforand describingimageswithsentences.In TACL ,2014. Srivastava,NitishandSalakhutdinov,Ruslan.Multimodallearningwithdeepboltzmannmachines. In NIPS ,pp.2222Œ2230,2012. Sutskever,Ilya,Vinyals,Oriol,andLe,QuocVV.Sequencetosequencelearningwithneuralnet- works.In NIPS ,pp.3104Œ3112,2014. Vedantam,Ramakrishna,Zitnick,CLawrence,andParikh,Devi.Cider:Consensus-basedimage descriptionevaluation. arXivpreprintarXiv:1411.5726 ,2014. Vinyals,Oriol,Toshev,Alexander,Bengio,Samy,andErhan,Dumitru.Showandtell:Aneural imagecaptiongenerator. arXivpreprintarXiv:1411.4555 ,2014. Young,Peter,Lai,Alice,Hodosh,Micah,andHockenmaier,Julia.Fromimagedescriptionstovisual denotations:Newsimilaritymetricsforsemanticinferenceovereventdescriptions.In ACL ,pp. 479Œ488,2014. 10S UPPLEMENTARY M ATERIAL 10.1E FFECTIVENESSOFTHEDIFFERENTCOMPONENTSOFTHEM -RNN MODEL B-1B-2B-3B-4 m-RNN0.6000.4120.2780.187 m-RNN-NoEmbInput0.5920.4080.2770.188 m-RNN-OneLayerEmb0.5940.4060.2740.184 m-RNN-EmbOneInput0.5900.4060.2740.185 m-RNN-visInRnn0.4660.2670.1570.101 m-RNN-visInRnn-both0.5460.3330.1910.120 m-RNN-visInRnn-both-shared0.4780.2790.1710.110 Table9:Performancecomparisonofdifferentversionsofm-RNNmodelsontheFlickr30Kdataset. AllthemodelsadoptVggNetastheimagerepresentation.SeeFigure5fordetailsofthemodels. 15 PublishedasaconferencepaperatICLR2015 Figure5:Illustrationofthesevenvariantsofthem-RNNmodels. Inthissection,wecomparedifferentvariantsofourm-RNNmodeltoshowtheeffectivenessofthe two-layerwordembeddingandthestrategytoinputthevisualinformationtothemultimodallayer. Thewordembeddingsystem. Intuitively,thetwowordembeddinglayerscapturehigh-levelse- manticmeaningsofwordsmoreefthanthesinglelayerwordembedding.Asaninputto themultimodallayer,itoffersusefulinformationforpredictingthenextworddistribution. Tovalidateitsefy,wetrainthreedifferentm-RNNnetworks:m-RNN-NoEmbInput,m-RNN- OneLayerEmb,m-RNN-EmbOneInput.TheyareillustratedinFigure5.ﬁm-RNN-NoEmbInputﬂ denotesthem-RNNmodelwhoseconnectionbetweenthewordembeddinglayerIIandthemul- timodallayeriscutoff.Thusthemultimodallayerhasonlytwoinputs:therecurrentlayerand theimagerepresentation.ﬁm-RNN-OneLayerEmbﬂdenotesthem-RNNmodelwhosetwoword embeddinglayersarereplacedbyasingle256dimensionalword-embeddinglayer.Therearemuch moreparametersoftheword-embeddinglayersinthem-RNN-OneLayerEmbthanthoseinthe originalm-RNN( 256  M v.s. 128  M +128  256 )ifthedictionarysize M islarge.ﬁm-RNN- EmbOneInputﬂdenotesthem-RNNmodelwhoseconnectionbetweenthewordembeddinglayerII andthemultimodallayerisreplacedbytheconnectionbetweenthewordembeddinglayerIandthe multimodallayer.TheperformancecomparisonsareshowninTable9. Table9showsthattheoriginalm-RNNmodelwiththetwowordembeddinglayersandthecon- nectionbetweenwordembeddinglayerIIandmultimodallayerperformsthebest.Itvthe effectivenessofthetwowordembeddinglayers. Howtoconnectthevisionandthelanguagepartofthemodel. Wetrainthreevariantsofm-RNN modelswheretheimagerepresentationisinputtedintotherecurrentlayer:m-RNN-VisualInRNN, m-RNN-VisualInRNN-both,andm-RNN-VisualInRNN-Both-Shared.Form-RNN-VisualInRNN, weonlyinputtheimagerepresentationtothewordembeddinglayerIIwhileforthelatertwomod- els,weinputtheimagerepresentationtoboththemultimodallayerandwordembeddinglayerII. 16 PublishedasaconferencepaperatICLR2015 Theweightsofthetwoconnections V (1) I , V (2) I aresharedform-RNN-VisualInRNN-Both-Shared. PleaseseedetailsofthesemodelsinFigure5.Table9showsthattheoriginalm-RNNmodel performsmuchbetterthanthesemodels,indicatingthatitiseffectivetodirectlyinputthevisual informationtothemultimodallayer. Inpractice,wethatitishardertotrainthesevariantsthantotraintheoriginalm-RNNmodel andwehavetokeepthelearningrateverysmalltoavoidtheexplodinggradientproblem.Increasing thedimensionoftherecurrentlayerorreplacingRNNwithLSTM(asophisticatedversionofRNN Hochreiter&Schmidhuber(1997))mightsolvetheproblem.Wewillexplorethisissueinfuture work. 10.2A DDITIONALRETRIEVALPERFORMANCECOMPARISONSON IAPRTC-12 Fortheretrievalresultsinthisdataset,inadditiontotheR@KandMedr,wealsoadoptexactly thesameevaluationmetricsasKirosetal.(2014b)andplotthemeannumberofmatchesofthe retrievedgroundtruthsentencesorimageswithrespecttothepercentageoftheretrievedsentences orimagesforthetestingset.Forthesentenceretrievaltask,Kirosetal.(2014b)usesashortlistof 100imageswhicharethenearestneighborsofthequeryimageinthefeaturespace.Thisshortlist strategymakesthetaskharderbecausesimilarimagesmighthavesimilardescriptionsanditisoften hardertosubtledifferencesamongthesentencesandpickthemostsuitableone. Therecallaccuracycurveswithrespecttothepercentageofretrievedimages(sentenceretrieval task)orsentences(sentenceretrievaltask)areshowninFigure6.Themethod,bowdecaf,isa strongimagebasedbag-of-wordsbaseline(Kirosetal.(2014b)).Thesecondandthethirdmodels (Kirosetal.(2014b))areallmultimodaldeepmodels.Ourm-RNNmodeloutperforms thesethreemethodsinthistask. 10.3T HECALCULATIONOF BLEU SCORE TheBLEUscorewasproposedbyPapinenietal.(2002)andwasoriginallyusedasaevaluation metricformachinetranslation.TocalculateBLEU-N(i.e.B-Ninthepaperwhere N =1,2,3,4)score, wecomputethe  n-gramprecision(Papinenietal.(2002)), p n .Thenwecomputethe geometricmeanof p n uptolength N andmultiplyitbyabrevitypenaltyBP: BP =min(1 ;e 1  r c ) (8) B-N = BP  e 1 N P N n =1 log p n (9) where r isthelengthofthereferencesentenceand c isthelengthofthegeneratedsentence.We usethesamestrategyasFangetal.(2014)where p n , r ,and c arecomputedoverthewholetesting corpus.Whentherearemultiplereferencesentences,thelengthofthereferencethatisclosest (longerorshorter)tothelengthofthecandidateisusedtocompute r . (a)ImagetoTextCurve (b)TexttoImageCurve Figure6:Retrievalrecallcurvefor(a).Sentenceretrievaltask(b).ImageretrievaltaskonIAPR TC-12dataset.Thebehavioronthefarleft(i.e.topfewretrievals)ismostimportant. 17  
Proceedingsofthe51stAnnualMeetingoftheAssociationforComputationalLinguistics ,pages312Œ317, Bulgaria,August4-92013. c  2013AssociationforComputationalLinguistics 312 313 314 315 316 317  
OntheGranularityofDialogStrategies:InsightsfromLarge-scaleAnalysesof TwoCommercialTravelInformationSpokenDialogSystems ZengtaoJiao 1 ,ZhuoranWang 2 ; 3 ,GuanchunWang 1 ,HaoTian 1 ,HuaWu 1 ,HaifengWang 1 1 BaiduInc.,No.10,Shangdi10thStreet,HaidianDistrict,Beijing,P.R.China 2 Heriot-WattUniversity,Edinburgh,UK 3 ToshibaResearchEuropeLtd.,Cambridge,UK Abstract Thispaperempiricallyanalyzeslargeamountsofrealuserdi- alogscollectedfromtwodeploymentsofspokendialogsys- tems(SDS)inatravelinformationmobileappwidelyusedin China.Thedatastudiedhereconsistof288Kdialogsfroma bookingSDSand2.42Mdialogsfromitsextendedver- sionthathandlesbothandhotelreservations.Basedon suchdata,weinvestigateuserbehaviorsandtheunderlying intentionsbehindtheirbehaviors.Wealsoexaminehowsuch userintentions/behaviorsvarywithdifferentlocation-related andtime-of-day-relatedfactors.Theinthisresearch canbeusedtodesignmoregranulardialogstrategiesandal- lowricherdialogstatefeaturizationsinordertobetteraddress realuserdemandsforSDSinsimilardomains. Introduction Withgrowingdemandsfornaturalhuman-machineinter- faces,spokendialogsystems(SDS)havebeenincreasingly deployedinvariouscommercialapplicationsoverthelast decade.RecentadvancesinSDShaveshownthatstatis- ticalapproachessuchaspartiallyobservableMarkovde- cisionprocesses(POMDPs)inconjunctionwithreinforce- mentlearningcanresultinmorerobustand naturaldialogs(Youngetal.2013).Thedialogpoliciesin suchstatisticalSDSareusuallylearntbasedoneitheruser simulations(Schatzmannetal.2007;Youngetal.2010)or crowdworkers(Gasicetal.2013).However,theintentions andbehaviorsofrealusersmayvarywithmanyenvironmen- tal,regionalorsociologicalfactors,whichposesthefollow- inghigh-levelquestions:(1)Towhatgranularityshoulddi- alogpoliciesbedesignedinordertocapturethediversityof realuserintentionsinrealworldscenarios?(2)Whatexter- nalfactorsinadditiontoobserveddialogsthemselvescould helpanSDStodiscriminatesuchdiverseuserintentions? Toaddresstheabovequestions,weempiricallyanalyze twolargecollectionsofrealuserdialogsobtainedrespec- tivelyfromtwotravelinformationSDSpubliclydeployedin China.Thepurposeoftheanalyseshereistobetterunder- standhiddenuserintentionsembeddedin(mostlyimplicit) userbehaviorpatterns.Severalinterestingphenomenaare basedonwhichwediscusspossiblewaysofus- Copyright c  2015,AssociationfortheAdvancementofal Intelligence(www.aaai.org).Allrightsreserved. ingconsequentialtoimprovethegranularitydesign ofdialogmodelingforSDSinsimilardomains. ComparingtopreviousempiricalSDSstudiesbasedon publicdeployments(Williams2011;2012),thisworkisfo- cusedonuserbehaviorsinsteadofmodel-relatedsystem characteristics.Tothebestofourknowledge,thisisthe paperonhumansubjectresearchforSDSthatinvestigates datasetsaslargeasmillionsofdialogs. DescriptionsoftheSystemsandData ThetwoSDSusedfordatacollectioninthispapercorre- spondtotwodeploymentsofatravelassistantmobileapp, developedbyBaiduandwidelyusedinChina.Thesys- tem(SDS1)isdesignedforbookingonly,whilethe secondsystem(SDS2)extendstheapplicationdomainsto bookingandhotelreservation.BothSDSarerule- based,followingtheRavenClawarchitecture(Bohusand Rudnicky2009),andareinChineselanguage.One-best ASRhypothesesareusedinbothsystemswhenmakingde- cisions,wheretheASRwordaccuracyis90.0%.TheSDS canprovideexternallinksforactualorderplacements,but cannotaccessfurthertransactioninformation. Thefundamentaldialogmanagementmechanisminour SDScanbeinterpretedasaprocedure,wherere- quiredslotsandoptionalslotsareforeachtasksep- arately.Ineverydialog,alltherequiredslotsmustbe beforethesystemcanexecuteadatabasesearchanddisplay thequeryresultstotheuser.Forbooking,thereare 3requiredslots,namelydeparturecity,destinationcityand departuredate,and14optionalslotssuchastimeofday, seatgrade,airline,etc.Forhotelreservation,the3required slotsaredestinationcity,check-inandcheck-outdates,with 9optionalslotsincludinglandmark,star-rating,etc. Thedataunderstudyinthispaperconsistof288Kdialogs with610KturnscollectedfromSDS1duringJanuarytoMay 2014(denotedasDS1)and2.42Mdialogswith5.72Mturns collectedfromSDS2duringJanuarytoJune2014(denoted asDS2).InDS2,90%ofthedialogsareforsingletasks, with60%forbookingand30%forhotelreservation. Thereare2%ofthedialogsinDS2thatcontaincompound andhotelbookinggoals,whilenoclearusergoalsare intheremaining8%ofthedialogsinDS2.The estimatedtaskcompletion(ETC)rate 1 forSDS1andSDS2 are77.0%and78.5%,respectively. Whatmakesdialogsfail? Westartfromsomesimplestatisticstoexaminethemissing (required)slotinformationthatresultsindialogfailuresin ourSDS,ofwhichtheresultsareshowninTable1 2 . Flight Departure Destination Date DS1 0.087 0.394 0.578 DS2 0.115 0.214 0.750 Hotel Destination Check-in Check-out DS2 0.214 0.749 0.722 Table1:Theproportionsofmissingvaluesforrequiredslots intheincompletedialogsinDS1andDS2. Firstly,itcanbefoundthatthelackofdateinformation resultsinmostofthedialogfailuresinbothsystemsand forbothtasks.Thisisintuitivelyunderstandable,aswhen planningatrip,ausershouldalreadyhavehis/herdeparture anddestinationinmind,butmaynotalwayshaveaclear ideaoftheexacttraveldate.Inmanycases,usersmayjust wanttobrowsethroughpossibleschedulesandcomparethe pricesofticketsorhotelrooms.Nevertheless, ingthoserequiredslotsisanecessarymechanismto databasequeriesfortheeaseofinformationaccess.Onthe otherhand,inmanysituations,e.g.whenusergoalisclear andedorwhentimecostisofconcern,usersmayprefer morebriefdialogsthancontinuousbrowsing.Betterdialog strategiesshouldbeabletoidentifydifferentuserintentions andtoaddressthemincorrespondingmanners. Itposesafurtherquestionherethathowoftenuserstend toexplicitlystatetheirintentions.Takingtheintentionof browsingtravelplansasanexample,byidentifyingrel- evantuseractsusingtemplate-basedapproaches,we that,inbothDS1andDS2,thereareonlyaround0.11% ofthedialogsthatcontainexplicituserstatementsofﬁdate unplannedﬂorﬁtobrowseﬂ.Similarlyandprobablyeven worse,onecanhardlyexpectausertoexpresshis/herde- mandofbriefinteractionsbyexplicitlyurgingthesystem. Suchimplicituserbehaviorssuggestthattoobtainmore granulardialogstrategies,extraknowledgeorfactorsshould 1 Throughoutthispaper, estimatedtaskcompletionrate stands forthepercentageofdialogswhereallrequiredslotsareThe ETCratetheabilityandwillingnessoftheuserstocom- pletetheirtravelbookingtaskswiththeSDS.Sincethispaperaims toanalyzemacro-levelpatternsofuserbehaviors,itisimpractical tocomputetrueusergoalsatisfactionratesbymanuallylabeling thedata. 2 Notethat,asourSDSwillinitializethedepartureplaceaccord- ingtouser'sGPSlocationifsuchinformationisavailable,mostof thoseincompletedialogscanstillhavetheirdepartureslots bydefault.Therefore,inTable1,theproportionsoftheincomplete dialogscausedbymissingdeparturevaluesaremuchlowerthan thosecausedbymissingotherslotvalues. beconsideredtoenablethesystemtoinferhiddenuserin- tentions. Whatfactorsmayimplyuserintentions? Intuitively,onecouldimaginethatauser'sdestinationwill tosomeextentthepurposeofthetrip.Inaddition,the waythatauserinteractswithanSDSmayalsovaryduring differenttimeperiodsofaday(e.g.busyhoursandleisure hours).Therefore,weexaminethecorrelationsofuserin- tentionswithlocationandtime-of-dayrelatedfactorsasfol- lows. characteristics Startingfromthebookingproblem,weanalyzethe correlationsbetweenETCratesanduser'sdepartureand destinationcitiesinTable2,wherethemostinteresting ingsarehighlighted. Thereisacommonphenomenoninbothdatasetsthat, somemostpopulartouristcities 3 inChina,suchasLhasa, Sanya,Lijiang,etc.,demonstrateexactlyoppositeeffectsto theETCrateswhenbeingthedepartureorthedestination locations.Toexplainthis,onecanimaginethatauserus- ingthebookingsystematatouristplacewouldtend tohaveacleargoalinmind(e.g.searchingforaback home),whilstinmanycasestheuserssearchingfor totouristplacesmayjustwanttobrowseandtocom- parepriceswithoutanyedplan,especiallywhen itcomestothetraveldate. Asthetruebrowsinguserintentionsareunobservable, wetheaboveassumptionfromthefollowingper- spectives.Firstly,foreachdestinationcity,wecomputethe percentageoftheincompletedialogswheredatevaluesare missing(named datemissingrate )andthepercentageofthe completeddialogswhereuserscontinuethedialogs(e.g.to searchforalternativeoptions)aftertheresultsfortheir queriesbeingdisplayed(named perceivedbrowsingrate ), ofwhichtheresultsareillustratedinTable3.Notehere,we onlyshowthestatisticsbasedonDS2,astheexamplesof interestinDS1aretoosparse.Itcanbefoundthatthelack ofdateinformationresultsinmorefrequentdialogfailures fortheuserstravelingtothosetouristcitiesthanforusersto mostoftheotherdestinations,whiletheusersgoingtothose touristdestinationstendtobrowsemoreoftenforalternative optionsaftertheirinitialgoalswereInaddition, whenthosetouristcitiesaretheperceiveddestinations,the percentageofdialogswhereusersexplicitlystateﬁdateun- plannedﬂorﬁtobrowseﬂincreasestoaround0.15%,which isslightlyhigherthantheoveralllevel(0.11%),thoughthe absoluteoccurrencefrequencyofsuchuseractsisstilllow. Alltheabovesuggestthatchar- acteristicsofuser'sdestination,suchaswhetheritisapop- ulartourismplace,canuser'sintention.Takingsuch informationintoaccountwhendesigningdialogstrategies couldimprovetheperformanceand/orusabilityoftheSDS. Suchstrategiescouldalsogeneralizetodestinationcities 3 Allthehighlightedcitiesarefamousforthelandscapessur- roundingthem,wherethereisnoothernotableindustryorbusiness excepttourism. DS1Departure DS1Destination DS2Departure DS2Destination Rank City ETC Rank City ETC Rank City ETC Rank City ETC 1 HongKong 1.000 1 Taipei 1.000 1 Xidai  0.995 1 Chongqing 0.997 2 Lhasa 0.999 2 Jinjiang 1.000 2 HongKong 0.993 2 Taipei 0.996 3 Jieyang 0.982 3 Chongqing 1.000 3 Lhasa 0.991 3 Qingdao 0.996 4 Guilin 0.955 4 Xi'an 1.000 4 Kunming 0.970 4 Dalian 0.996 5 Kunming 0.954 5 Shanghai 1.000 5 Haikou 0.967 5 Xi'an 0.996 6 Urumqi 0.950 6 Beijing 1.000 6 Lijiang 0.965 6 Shenyang 0.996 7 Haikou 0.950 7 Lanzhou 1.000 7 Xieyang 0.965 7 Harbin 0.996 8 Yinchuan 0.943 8 HongKong 1.000 8 Yinchuan 0.959 8 Nanjing 0.995 9 Lijiang 0.938 9 Shenyang 1.000 9 Sanya 0.959 9 Lanzhou 0.995    10 Guilin 0.959 10 Tianjin 0.995 13 Sanya 0.930             41 Kunming 0.994 43 Haikou 0.989 43 Haikou 0.994       103 Dongguan 0.822 53 Sanya 0.982 50 Taizhou 0.669 46 Xidai  0.991 104 Cixi 0.817 54 Lijiang 0.980 51 Yiwu 0.663 47 Sanya 0.984 105 Nan'an 0.810 55 Zhangjiajie 0.975 52 Suzhou 0.550 48 Lijiang 0.968 106 Fuqing 0.808 56 Dali 0.971 53 Dongguan 0.342 49 Lhasa 0.955 107 Foshan 0.804 57 Lhasa 0.965 Total:53 Total:49 Total:107 Total:57 Table2:ETCratesofdepartureanddestinationcitiesinDS1andDS2.Citieswithlessthan500and1000occurrencesin DS1andDS2areout,respectively.(  XidaiistheabbreviationforXishuangbanna,oralsoknownasSibsongbanna.) DateMissing PerceivedBrowsing Rank City Rate Rank City Rate 1 Huangshan 0.958 2 Dali 0.186 3 Dali 0.935 6 Zhangjiajie 0.173 7 Lijiang 0.902 7 Lijiang 0.166 8 Lhasa 0.898 8 Sanya 0.153 10 Zhangjiajie 0.880 10 Lhasa 0.150 Overall 0.755 Overall 0.126 Table3:Datemissingratesandperceivedbrowsingratesfor somehighlightedtouristdestinationsinDS2(incomparison totherespectiveoverallratesforallapplicabledialogs). thathaveothertypesofcharacteristics.Forexample, usersgoingtoindustry-intensivecitieswouldusuallyhave acleartravelplaninmind,andthereforemightprefermore eftravelbookingproceduresthanbrowsing.However, manybigcitiesmayhavesimultaneousbusinessandtourism functions.Insuchcases,itmightbetooassertivetoestimate userintentionspredictively.Nevertheless,insteadofﬁguess- ingﬂthepurposeofauser'strip,askingstraightforwardly (e.g.ﬁOkay,toLhasa.Isitforbusinessorleisure?ﬂ) couldbemorepreferable.ButamoreelegantSDSshould beabletodecidewhetherorwhentoaskuser'stravelpur- poseaccordingtowheretheuserisgoingto.Personalized statisticswillalsobehelpfulforsuchdecisions. Remark1 Onemightarguethatthisisatrivialproblem,as state-of-the-artstatisticalSDScanlearnsuchdecisionsus- ingreinforcementlearningbyinteractingdirectlywithhu- manusers(Gasicetal.2013).However,duetothecom- monlyusedsummaryspacemethods(e.g.(Youngetal.2010; ThomsonandYoung2010)),thediscriminationsofindivid- ualslotvaluesareusuallyeliminatedwhenlearningdia- logpolicies.Inaddition,theextenttowhichcrowdsourcing testerswillbehavethesameasrealusersisalsoanopen question.Therefore,improvingthegranularityofdialog staterepresentationsanddialogstrategydesignsintravel informationSDSbyconsideringcharacter- isticsisoneofthemainsuggestionsofthisresearch. Remark2 Althoughaproperrule-basedimplementation (suchasours)canalreadyresultinratherhighoverallETC rates,improvingthegranularityofdialogstrategieswillstill beanissueofimportance,sinceETCdoesnot directlyrelatetousersatisfaction.Moreover,recommending optionsinamoresuitablemannertotheuserswhodonot havecleartravelplansisanindispensablefunctionfora commercialtravelinformationapplication. However,theabovephenomenaaredomain-dependent. Thestatisticsinhotelreservationdialogsindicateadiffer- entpatternofuserbehaviors,whereuserstargetingtourist citiesdemonstratehigherETCratesbutlessbrowsinginten- tions.Thiscouldbepartiallyduetothefactthatthepricesof hotelsusuallydonotchangeasmuchasthepricesof ticketsdoondifferentdates. Time-of-day-relatedfactors Asmentionedabove,differenttimeperiodsduringaday couldalsobethefactorsthataffectuser'sbehaviorswhenin- teractingwithanSDS.Weplotthetask-dependentETCrates Figure1:ETCrateswithrespectto dailytimeperiods(hours). Figure2:Perceivedbrowsingratewith respecttodailytimeperiods(hours). Figure3:Averagedialoglengthswith respecttodailytimeperiods(hours). inDS1andDS2withrespecttodailytimeperiods(hours) inFigure1. ForbookingandhotelreservationdialogsinDS2, itcanbefoundthatthelowestETCratesoccursinearly morningaround6 ˘ 7am,whileforDS1bookingdi- alogsonly)thereisalsoadecreaseinETCrate around6am,thoughtherelativetrendappearslessobvi- ousthanthatinDS2.Moreinterestingly,highestETCrates forallthetasksandsystemsareobservedaroundmidnight. Possibleexplanationstothesephenomenacouldbeasfol- lows.Firstly,itcanbeunderstoodthatpeopleusingthesys- teminﬁabnormalﬂtimeperiods(suchasthemidnight)may havestrongrequirementsandmotivationstohavetheirjour- neys/hotelsbooked.Onthecontrary,intheearlymorning, usersmayprefermoreeforbriefdialogs,andthere- foretendtobelesspatientwiththesystems. Toproveouraboveassumptions,wecarryoutatime- dependentcalculationoftheperceivedbrowsingratesand theaveragelengthsofthedialogsasshowninFigure2and 3,respectively.Bothstatisticsindicatethatshorterdialogs andlessbrowsingoperationsoccurinthemorning,while theoppositeuserbehaviorsareobservedatlatenight. Remark3 Tothebestofourknowledge,time-of-day- relatedfactorshaveneverbeendiscussedinpreviouswork onSDS.Buttheheresuggestthatincludingsuchin- formationindialogstrategydesignand/orfordialogpolicy learningcouldbeapossibledirectiontoimprovetheusabil- ityofSDS. Inaddition,onecaninFigure1thatthereareno- tabledropsintheETCratesforbookingdialogsin bothDS1andDS2during6pmto8pm.Userswithuncer- taingoalstryingtobrowsefortravelplanoptionscouldbe themainreason,asobviouspeakscanbeobservedforDS1 inbothFigure2and3.However,thereisnoclearevidenceto explainthesamephenomenoninDS2.Onepossibilitycould bethattheproportionofmistakenlytriggereddialogsmight increaseduringthoseﬁentertainingﬂhours. Conclusion Basedonlarge-scaleanalysesofrealuserbehaviorsob- servedfromtwocommercialtravelinformationspokendi- alogsystems,thispaperproposesseveralpossibleimprove- mentstothegranularitydesignofdialogstrategyanddialog modelingforSDSinsimilardomains.Themaincontribu- tionsofthisworkaretheinvestigationsonthecorrelations betweenuserbehaviorsandcharacteristics, aswellastime-of-day-relatedfactors.Thefeasibilitiesofthe proposedimprovementsarealsodiscussedbasedonempir- icalevidences.PracticalimplementationsofSDSfollowing suchinstructionstobetteraddressrealuserdemandsinreal worldscenarioswillbethefocusofourfutureresearch. Acknowledgements TheresearchinthispaperissupportedbyChina's973Pro- gram(No.2014CB340505).ZWissupportedinpartbya SICSAPECEgrant. References Bohus,D.,andRudnicky,A.I.2009.TheRavenClawdialog managementframework:Architectureandsystems. Comp. SpeechLang. 23(3):332Œ361. Gasic,M.;Breslin,C.;Henderson,M.;Kim,D.;Szummer, M.;Thomson,B.;Tsiakoulis,P.;andYoung,S.2013.On- linepolicyoptimisationofBayesianspokendialoguesys- temsviahumaninteraction.In ICASSP . Schatzmann,J.;Thomson,B.;Weilhammer,K.;Ye,H.;and Young,S.2007.Agenda-basedusersimulationforboot- strappingaPOMDPdialoguesystem.In NAACL-HLT . Thomson,B.,andYoung,S.2010.Bayesianupdateofdia- loguestate:APOMDPframeworkforspokendialoguesys- tems. Comp.SpeechLang. 24(4):562Œ588. Williams,J.D.2011.Anempiricalevaluationofastatistical dialogsysteminpublicuse.In SIGDIAL . Williams,J.D.2012.Challengesandopportunitiesforstate trackinginstatisticalspokendialogsystems:Resultsfrom twopublicdeployments. IEEEJ.SelectedTopicsSig.Proc. 6(8):959Œ970. Young,S.;Gasic,M.;Keizer,S.;Mairesse,F.;Schatzmann, J.;Thomson,B.;andYu,K.2010.TheHiddenInformation Statemodel:apracticalframeworkforPOMDP-basedspo- kendialoguemanagement. Comp.SpeechLang. 24(2):150Œ 174. Young,S.;Gasic,M.;Thomson,B.;andWilliams,J.2013. POMDP-basedstatisticalspokendialoguesystems:are- view. Proc.IEEE PP(99):1Œ20. View publication statsView publication stats 
ConnectingEmergingRelationshipsfromNewsviaTensorFactorization JingyuanZhang à,Chun-TaLu !,BokaiCao !,YiChang €andPhilipS.Yu !¤àBigDataLab(BDL),BaiduResearch,Sunnyvale,CA,USA !DepartmentofComputerScience,UniversityofIllinoisatChicago,IL,USA €HuaweiResearchAmerica,SantaClara,CA,USA ¤InstituteforDataScience,TsinghuaUniversity,Beijing,China {zhangjingyuan03}@baidu.com,{clu29,caobokai,psyu }@uic.edu,{yichang }@huawei.comAbstractÑKnowledgegraphs(KGs)havebeenwidelyused torepresentrelationshipsamongentities,whileKGscannot capturenewrelationshipsbetweenentitiesemergingalongtime. Sincenewsoftenprovidesthelatestinformationregarding thenewentitiesandrelationships,thereisanopportunityto connectemergingrelationshipsfromnewstimely.However,itis achallengingtaskduetothesourceheterogeneityofstructured KGsandunstructurednewstexts.Inordertoaddresstheissue, weproposeatensor-basedframeworktocapturethecomplex interactionsamongmultipletypesofrelations,entitiesand textdescriptions.WefurtherdevelopanefÞcientText-Aware MUlti-RElationallearningmethod(TAMURE)thatcanlearn theembeddingrepresentationsofentitiesandrelationtypes frombothKGsandnews,byjointlyfactorizingtheinteraction parameters.Furthermore,thecomplexityofTAMUREislinear inthenumberofparameters,whichmakesitsuitableto large-scaleKGsandnewstexts.Extensiveexperimentsvia TensorFlowdemonstratetheeffectivenessoftheproposed TAMUREmodelcomparedwithninestate-of-the-artmethods onreal-worlddatasets. Keywords -InformationExtraction;EmergingRelationships; TensorFactorization;Embedding I.I NTRODUCTION Knowledgegraphs(KGs),suchasFreebase 1andDB- pedia2,havebeenwidelyusedtorepresentrelationships betweenentitiesintheformoftriplets( h,r,t ).Here handtaretwoentities(headandtail)and risarelation. Eachtripletisarelationinstance 3.Entitiescanbepersons, organizations,locations,etc.,andexamplesofrelationscan beperson-afÞliationandorganization-location.TheKGs withrelationsandentitiesareusefulsourcesformanyreal- wordapplicationsininformationextraction,naturallanguage understandingandinformationretrieval.However,current KGshavelimitedcoverageofreal-worldrelationships[1], especiallyfornewentitiesthatarisewithnewrelationships emergingovertime[2,3]. Fortunately,withthelatestinformationinnews,thereis anopportunitytoconnectemergingrelationshipsfromnews timely.ConsiderFigure1asanexample,whereanemerg- ingrelationshipappearswhenthepublisherÒWaltDisney 1https://www.freebase.com/ 2http://wiki.dbpedia.org/ 3Forsimplicity,arelationmeansarelationtypeinKGs,andarelation- shipmeansatripleinstance( h,r,t ).Knowledge Graph (KG)Byron Howard Rich Moore Walt DisneyStudiosZootopiaNew Entity            Relationships in KG            Emerging relationships directedBy directedBy workAtworkAtcooperateWith producedBy Figure1.Anexampleofemergingrelationships. StudiosÓproducesanewmovieÒZootopiaÓ.Althoughthere isnoinformationregardingZootopiainKGs,manypieces ofnewsaretalkingaboutthisnewmovie,thepublisher andthedirectors.Detectingsuchemergingrelationshipshas manybeneÞtsinpractice.Forexample,thecurrentKGscan beexpandedandupdatedwithemergingrelationships.In addition,emergingrelationshipscanhelpnewsrelatedtasks, suchasnewsretrievalandranking,eventdetection,etc.. However,learningemergingrelationshipsfromtextde- scriptionsisachallengingtaskduetothesourcehetero- geneityofstructuredKGsandunstructurednewstexts. Althoughmanyresearchworkstrytomitigatetheproblemof knowledgesparsityinKGs[4Ð9],themainfocusofthese worksistoutilizethestructuralinformationtoÞllinthe missingrelationshipsinKGs.Forinstance,PathRanking Algorithm(PRA)[10,11]completesKGsbyperforming randomwalktechniques;someotherstudiesembedentities andrelationsintoalow-dimensionalspaceandinfermissing relationshipsbytranslatingrelationsfromheadentitiesto tailentitiesinKGs[9,12,13].However,itisnontrivialto incorporatenewstextsintothesestructure-basedmethodsto connectemergingrelationships. Fromtheotheraspect,thereareseveralstudiesattempting toembedlarge-scaletexts[14Ð17].Forexample,LINEis proposedin[16]toembedtextsintoalow-dimensional spacebyconstructingahomogeneouswordco-occurrence networkfromtexts.LaterPTEisproposedin[17]toimprove b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926!"#$%&'() !"#$%&'(* !"#$%&'(! +"$,( "'%-. -$/#("'%-. -"0- Figure2.Tensorsbuiltforrelations,theassociatedheadentities,tail entitiesandnewstexts. theLINEmethodbybuildingaheterogeneoustextnetwork. Thesemethodsfocusonembeddingeverysinglewordin textsbutignoringthesemanticrelationsamongwords. Therefore,theywouldfailtocaptureemergingrelationships fromnewstexts.Recently,someworktriestoembedthe KGandthetextsjointly[3,18].However,themethodin [18]embedstheKGandtextsseparately.Sotheirindirect inferenceaccordingtotextscannothelpconnectemerging relationshipseffectively.Thoughthemethodin[3]aimsto detectemergingrelationships,itfailstodistinguishamong differentrelationtypes. Toaddresstheissue,inthispaper,weproposeatensor- basedframeworktocombineKGsandnewstextseffectively fordetectingnotonlyemergingrelationshipconnection,but alsotherelationtype.Afourth-ordertensorstructureisused tocapturethehiddenconnectionbetweenmultiplerelations inKGsandmultipletextdescriptionsofrelationsinnews. SpeciÞcally,wemodelthemultimodalinteractionsamong headentities,relations,tailentities,andtextdescriptionsas atensorstructure,bytakingthetensorproductoffeature spacesofentities,relationsandtexts.Figure2showsthe tensorstructureconstructedfromKGsandnews.Asthe interactions(i.e.,tensorproduct)ofentities,relationsand textdescriptionscanreßecttheconnectionbetweenKGsand news,weusethemtolearntheembeddingrepresentations ofentitiesandrelations.Inthismanner,itcandealwith multiplerelationtypeswithoutdifÞculty. Noteworthily,directlylearningfromthetensorstructure wouldbeproblematic.First,thespacecomplexityofbuild- ingthefourth-ordertensorispolynomialtothenumbers ofentities,relationsandtextdescriptions,makingitchal- lengingtoÞtthetensorintomemory.Second,duetothe largenumberofinteractionparameters,itistime-consuming todecomposethebuilttensordirectlyanditisproneto overÞtting.Lastbutnotleast,howtolearnameaningful representationofagivennewssentence,whichconsistsof theheadandtailentitieswiththeothertextdescriptions, andconnecttherepresentationtotherelationtypesinKGs ischallenging. Inordertosolvetheabovechallenges,wefurtherde- velopa Text- Aware MUlti-RElationallearningmethod (TAMURE)tolearntheembeddingsofentitiesandrela- tiontypesfrombothnewsandKGs.Byfactorizingthe interactionparameters,theproposedTAMUREmethodcan efÞcientlylearnthelatentrepresentationofentitiesandtext descriptions,withoutphysicallybuildingthetensor.Since theparametersarelearnedjointlythroughthefactorization, italsomakestheparameterestimationmoreaccurateunder sparsityandrendersthemodelwiththecapacitytoavoid overÞtting.Insummary,ourcontributionsareasfollows: ¥Weformulateanewtask,connectingemergingre- lationships,whichistodiscoverrelationtypeswith newentitiesbyfusinginformationfromheterogeneous sources,i.e.,thestructuredKGsandtheunstructured newstexts. ¥Weintroduceanoveltensor-basedframeworktocon- nectemergingrelationshipsfromnews.Thenewstexts andKGsareincorporatedintoanelegantfourth-order tensorformulation,wherethecomplexmultipleinterac- tionsamongrelationtypes,entitiesandtextdescriptions areembeddedwithinthetensorstructure. ¥TheproposedTAMUREmethodcaneffectivelyrecog- nizeemergingrelationshipsfromnews,bycapturing notonlythefourth-butalsolower-orderinteractions inthebuilttensor.Thelower-orderinteractionscan explorehiddencompatibilityamongentities,relations andtextdescriptions(seesectionIII-B2fordetails). Furthermore,thecomplexityofTAMUREislinearin thenumberofparameters,whichmakesitsuitableto large-scaleapplications. ¥WedemonstratetheeffectivenessofTAMUREby comparingitwithninestate-of-the-artmethodsvia TensorFlowonreal-worldKGandnewsdata. Therestofthepaperisorganizedasfollows.SectionII formulatestheproblem;SectionIIIintroducesthedetails oftheproposedTAMUREmethod;SectionIVpresentsthe experimentalsetupandtheresults;SectionVbrießyreviews relatedwork;andSectionVIconcludesthisstudy. II.P RELIMINARY Inthispaper,westudytheproblemofconnectingemerg- ingrelationshipsfromnews.Beforeproceeding,weÞrst introducetherelatedconcepts,andthenstatetheproblem ofemergingrelationshipdetectionfromnews.TableIlists basicsymbolsthatwillbeusedthroughoutthepaper. A.BasicConcepts DeÞnition1: Entity,RelationandRelationship: Anentityecanrepresentaperson,anorganization,oralo- cation,etc..Arelationcanbeaperson-afÞliationtypeoran organization-locationtype.ArelationshipisdeÞnedinthe formoftriplets( h,r,t ),where hisaheadentity, tisatail entityand risarelation.Foreachpossibletriple( h,r,t ),weuse y!{0,1}toindicatewhetherthetripleexists. DeÞnition2: KnowledgeGraph(KG): Aknowledge graph isdenotedasadirectedgraph Gkg=(Ekg,Ekg),b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926TableI LISTOFBASICSYMBOLS .SymbolDeÞnitionanddescription (h,r,t )arelationship (h,r,t,d )arelationshipwithatextdescription dxeachlowercaseletterrepresentsascale xeachboldfacelowercaseletterrepresentsavector Xeachboldfaceuppercaseletterrepresentsamatrix Xeachcalligraphicletterrepresentsatensor,setorspace [1: M]asetofintegersintherangeof 1toMinclusively "á,á#denotesinnerproduct $denotestensorproduct(outerproduct) !denotesHadamard(element-wise)product whereEkgisthesetofentitiesand Ekgisthesetofknown relationships.Eachdirectededgein Ekgcanberepresented byatriplet( h,r,t ),wheretheentities handt!Ekg,and therelation risonetypeoftherelationsintherelationset R.Nowadays,duetotherapidgrowthofreal-worldknowl- edge,largevolumesof emergingrelationships arearising withtime.AnemergingrelationshipisdeÞnedasfollows: DeÞnition3: EmergingRelationship: Anemergingre- lationship( h,r,t )exists,ifitslabel y=1intherealworld andatleastoneentityisnotincludedinthegivenKG(i.e., h"!Ekgort"!Ekg).Forexample,inFigure1,( Zootopia,producedBy,Walt DisneyStudios )isanemergingrelationshipwith y=1sinceZootopiaisanewmovieentityanditisproducedbythe publisherentity WaltDisneyStudios .Similarly,( Zootopia,directedBy,ByronHoward )and( Zootopia,directedBy,Rich Moore )arealsoexamplesofemergingrelationships. Thekeyofthisworkistoapplythetensorstructuretofuse theKGandthenewsforconnectingemergingrelationships. Inthefollowing,weintroducesomerelatedconceptsand notationsaboutthetensor. DeÞnition4: Tensor: Tensorsarehigherorderarraysthat generalizethenotionofvectors(Þrstorder)andmatrices (secondorder).Following[19],an M-thordertensoris denotedby X!RI1!ááá!IManditselementsby xi1,ááá,iM.Anindexisdenotedbyalowercaseletter,spanningthe rangefrom 1totheuppercaseletteroftheindex, e.g. ,i=1,2,ááá ,I.Allvectorsarecolumnvectorsunless otherwisespeciÞed. Foranarbitrarymatrix X!RI!J,its i-throwand j-thcolumnvectoraredenotedby xiandxj,respectively.The innerproductoftwosame-sizedtensors X,Y!RI1!ááá!IMisdeÞnedby !X,Y"=#I1i1=1ááá #IMi1=1xi1,ááá,iMyi1,ááá,iM.Theouterproductof Mvectors x(m)!RImform![1: M]isan M-thordertensoranddeÞnedelementwiseby $x(1)#ááá #x(M)%i1,ááá,iM=x(1)i1ááá x(M)iMforallvaluesof theindices.Inparticular,for X=x(1)#ááá #x(M)andY=y(1)#ááá #y(M),itholdsthat !X,Y"=M&m=1!x(m),y(m)"=M&m=1x(m)Ty(m).(1)Forageneraltensor X!RI1!ááá!IM,itsCANDECOM/ PARAFAC(CP)factorization[19Ð21]is X=K'k=1x(1)k#ááá #x(M)k=!X(1),..., X(M)",(2)wherefor m![1: M],X(m)=[x(m)1,ááá ,x(m)K]arefactor matricesofsize Im$K,Kisthenumberoffactors,and !á"isusedforshorthand. B.ProblemStatement GivenalargecollectionofnewsandtheexistingKG Gkg,thetaskofconnectingemergingrelationshipsaims todeterminetheexistenceofmultipletypesofrelations amongentities.Assumewecanextractasetofrelationship candidates,eachofwhichisrepresentedbyatriplet (h,r,t )andisassociatedwithavectoroftextdescriptions dfromnews.Sincetheentitiesinthecandidatesmaynotexist intheentityset EkgoftheKG,wedenote Enews asthesetofentitiesthatappearinthenews,anddenote E=Ekg%Enews asthesetofalltheentities.Thetask ofconnectingemergingrelationshipsistolearnascore functionf:(h,r,t, d)&{0,1}thatcorrectlypredictsthe labelofthetestinstance,wheretheentities handt!E,r!Rand(h,r,t )"!Ekg.III.P ROPOSED METHOD Inthissection,weÞrstintroducehowtodesignthetensor- basedscorefunctionforfusingtheKGandthenewstexts. ThenwederiveanefÞcient Text- Aware MUlti-RElationallearningmethod(TAMURE)thatlearnstheembeddingsof entitiesandrelationtypesinlinearcomplexity. A.Tensor-basedScoreFunction Webeginbyintroducinghowtoextractentitiesand relationsfromnews.Wethendescribehowtodesignthe scorefunctionifonlyoneofthesources(theKGorthenews texts)isavailable.Afterthat,weshowthatbothsourcescan beintegratedintoaneleganttensor-basedmodel. Givenalargecollectionofnews,entitiescanbeextracted viapopularNamedEntityRecognition(NER)techniquesin [22,23].Theentitiesthatcannotbeexactlymatchedtothe KGarenewentities.ForexistingrelationshipsintheKG,we canassociatetheirrelationtypeswithtextdescriptionsfrom news.Givenemergingrelationships,weaimtodetermine theirrelationtypeswiththehelpoftextdescriptionsfrom news. WithoutconsideringtheexistenceoftheKG,themost commonapproachisusingalinearscorefunctionforeach b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926relationtype rbasedonthetextdescriptionvector d!RIDextractedfromnews: fnews (r,d)=ID'i=1ur,i di=dTur,(3)whereur!RIDistheweightvectorforrelationtype r.Forlearningmultiplerelationtypesatthesametime, welet IR=|R|denotethenumberofrelationtypesand U!RID!IRdenotetheweightmatrixtobelearned,whose columnsarethevector ur.Let er!RIRdenotetherelation typeindicatorvector er=[0 ,ááá ,0()*+r-1 ,1,0,ááá ,0]T.Wecanobservethat Uisactuallytheweightmatrix ofabilinearfeaturemapformodelingthesecond-order interactionsbetweenthetextdescriptionvectorandthe relationtype: fnews (r,d)=dTUer='U,d#er(.(4)GiventheKGonly,wecanuseasimilarscorefunction formodelingtheinteractionsbetweentheheadentity,tail entityandrelationtype.Let IE=|E|denotethenumber ofentities,andlet eh!RIEandet!RIEdenotethe headandtailentityindicators,respectively.Wecanform themultilinearscorefunctionby fKG(h,r,t )=eThVret='V,er#eh#et(.(5)whereVr!RIE!IEistheweightmatrixbetweenheadenti- tiesandtailentitiesforrelationtype r,and V!RIR!IE!IEisastackedtensorwitheachslice Vr,:,:=Vr.Obviously,wecanfusethenewsandtheKGtogetherby formulatingthescorefunctionasfollows: f(h,r,t, d)='W,er#eh#et#d(,(6)whereW!RIR!IE!IE!IDistheweighttensortobe learned.Itisworthytobenotedthatsomeexistingrelationships intheKGmightnothavethetextdescription d,such thatthetextdescription d=0.Besides,inorderto includerelationshipsthatareextractedfromthenewsbut notavailableintheKG,weaddaÒco-occurrenceÓrelation typetotheexistingrelationsduringthelearningprocessand donottestthisrelationtypeinthetestingphase. B.Text-awareMulti-relationalLearning NowthenewstextsandtheKGhavebeenincorporated intoaneleganttensorformulation,suchthatthecomplex multipleinteractionsamongrelationtypes,entitiesandtext descriptionsareembeddedwithinthetensorstructure.How- ever,thespacecomplexityofbuildingthefourth-ordertensor isO(IR$IE$IE$ID).Withlargevolumesofemerging relationshipsinnews,itisimpracticaltophysicallybuild thetensor.Moreover,directlylearningtheweighttensor Wwouldbeproblematic.First,duetothelargenumber ofparameters IR$IE$IE$ID,thelearningprocedure ispronetooverÞttingandlesseffectivecoupledwithits sparsecounterpart er#eh#et#d.Second,sincetheweight parametersarelearnedindependently,itcannotmodelthe interactionsthatneverappear.Toaddresssuchissues,we proposeanefÞcientmethodbasedontensorfactorization. 1)EfÞcientTensorDecompositionFramework: Assumethattheeffectofinteractionshasalowrank,theweight tensorWcanbefactorizedas W=!M(r),M(h),M(t),M(d)",whereM(r)!RIR!KandM(h),M(t)!RIE!Krepresenttheembeddingmatricesforrelationtypes,headentitiesand tailentities,respectively; M(d)!RID!Krepresentsthe weightmatrixfortextdescriptions.FromEq.1andEq.6, wecaneasilyderivethat f(h,r,t, d)=K'k=1,M(r):,k#M(h):,k#M(t):,k#M(d):,k,er#eh#et#d-=K'k=1.eTrM(r):,k/.eThM(h):,k/.eTtM(t):,k/.dTM(d):,k/=.eTrM(r)/T..eThM(h)/).eTtM(t)/).dTM(d)//=rT.h)t)(dTM(d))/,(7)where)istheHadamard(elementwise)product, r=eTrM(r),h=eThM(h)andt=eTtM(t)aretheembedding vectorslearnedfortherelationtype r,headentity handtail entityt,respectively. 2)Lower-orderConstraintonRelationTypes: Onecan noticethatEq.7onlymodelsthefourth-orderinteractions betweenthespeciÞcrelationtype r,entities handt,and theassociatedtextdescription d.However,thelower-order interactionscanalsobediscriminativefordeterminingthe existenceoftherelationship.Forexample,iftheheadentity isaperson,e.g.,ÒRichMooreÓ,therelationtype risunlikely tobeÒproducedByÓnomatterwhichtailentity tischosen inthesampleinstance (h,r,t, d).Inthiscase,eventhe pairwiseinteractionbetween handrcanbediscriminative. Thus,weconsidertoincorporatethelower-orderinteractions inthepredictivemodel.Thiscanbedonebyaddingbias vectorsinEq.7asfollows: f(h,r,t, d)=rT.(h+bh))(t+bt)).dTM(d)+bv//,(8)wherebv!RID,bhandbt!RIEarethebiasvectors thatareindependenttothegiveninstance.Toillustratewhy thebiasvectorscanhelpmodelthelower-orderinterac- tions,wecandecomposeEq.8intotwoparts.TheÞrst partrT$h)(t+bt))$dTM(d)+bv%%modelsthefourth- orderinteractionsbetween (h,r,t, d),whilethesecondpart b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926!eteherdrM(t)M(h)M(d)M(r)f(h,r,t, d)!!111h+bht+btdTM(d)+bvFigure3.TheworkßowofTAMURE. rT$bh)(t+bt))$dTM(d)+bv%%withthebiasvector bhmodelsthethird-orderaswellasthelower-orderinter- actionsbetween (r,t, d)withouttheheadentity hinvolved. Othertypesofthethird-orderinteractionsandthelower- orderinteractionscanbederivedinasimilarway. WenamethemodelinEq.8as Text- Aware MUlti-RElationallearningmethod(TAMURE).Theworkßow ofTAMUREisillustratedinFigure3.Aftermapping theheadentity,tailentityandtextdescriptionextracted fromnewsintoacommonembeddingspacevia (h+bt))(t+bt))$dTM(d)+bv%,TAMURElearnsameaningful representationtoconnectrelationtypesinKGswithnews textsvia rT$(h+bh))(t+bt))$dTM(d)+bv%%.Clearly,theparametersoftheinteractionsamongmultiple relationtypes,entities,textdescriptionsarejointlyfactor- ized.ThejointfactorizationbeneÞtsparameterestimation undersparsity,sincedependenciesexistwhentheinterac- tionssharethesameentitiesortextdescriptions.Therefore, themodelparameterscanbeeffectivelylearnedwithout directobservationsofsuchinteractionsespeciallyinhighly sparsedata.Further,sincetheinteractionsaremodeledwith thebiasvectors,thisjointfactorizationmodelcaneasilydeal withmissinginformationandevenincompleteinformation forrelationships. AnotherappealingpropertyofTAMUREcomesfromthe maincharacteristicsofmultilinearanalysis.Afterfactorizing theweighttensor W,thereisnoneedtoconstructthe inputtensorphysically.Moreover,themodelcomplexityis O(K(IR+IE+ID)),whichislinearinthenumberof parameters.Thismultilinearpropertycanhelpsavememory andalsospeedupthelearningprocedure. C.LearningProcedureofTAMURE Giventhetrainingset D={(h,r,t, d)},TAMURE learnsvectorembeddingsoftheentities,relationtypesand textdescriptionsviathescorefunction f.Followingthe samestrategyasin[8,12],weminimizeamargin-based rankingcriterionoverthetrainingset: Algorithm1 TheTAMUREalgorithm Input:Thetrainingset D,entities Eandrelationtypes R,margin !,embedding dimensionK.Output:Embeddingsofentities,relationtypesandtexts. 1:Initializeembeddings h,r,t"uniform(#1!K,1!K)foreach h$E,t$Eandr$R2:Initializeweightparametersrandomly 3:Normalizeentityembeddings 4:whilenotconvergence do5://sampleamini-batchofsize mDbatch"sample(D,m)6:Sbatch"%7:for (h,r,t, d)$Dbatchdo8://Sampleacorruptedinstance (h",r,t ",d")"sample(D")9:Sbatch"&{!(h,r,t, d),(h",r,t ",d")"}10:endfor 11:Updateembeddingsandweightparametersw.r.t.thegradientofEq.9on Sbatch12:Constrainentityembeddingswiththemax-normregularization 13:endwhile L='(h,r,t, d)$D'(h",r,t ",d")$D"max(0 ,f(h,r,t, d)+!*f(h',r,t ',d')),(9)where!>0isamarginhyper-parameter,and D'={(h',r,t, d')|h'!E}%{(h,r,t ',d')|t'!E}.(10)Thesetofcorruptedrelationships D'iscomposedoftraining datawitheithertheheadortailreplacedbyarandomentity (butnotbothatthesametime).Forarelationshipfromthe KG,thecorrupteddataischeckedtomakesurethatitisnot intheKG.Noticethatsincethetextdescription disalways associatedwiththeentitypairs,whenoneoftheentitiesis replaced,thetextdescriptionvectorwillalsobereplaced accordingly.ThelossfunctioninEq.9favorshigherscores fortrainingdatathanforcorrupteddata. ThelearningprocessofTAMUREiscarriedoutbyAdam optimizer[24]inmini-batchmode,withtheadditionalmax- normregularizationconstraint[12,25],whichconstrainsthe L2-normoftheembeddingsoftheentitiestobenolarger than1.Noregularizationornormconstraintsaregiventothe relationtypeandtextdescriptionembeddings.Thedetailed optimizationprocedureisdescribedinAlgorithm1.Ateach mainiterationofthealgorithm,asmallsetofdataissampled fromthetrainingsetandserversasthetrainingdataofthe mini-batch.Foreachexistingrelationship,asinglecorrupted relationshipissampledaccordingly.Theparametersarethen updatedbytakingagradientstepwithaconstantlearning rate.Beforethenextiteration,theembeddingvectorsof theentitiesarenormalizedviathemax-normregularization constraint.IV.E XPERIMENTS Inthissection,weconductextensiveexperimentstoeval- uatetheproposedTAMUREmethod.Afterintroducingthe datasetsandtheexperimentalsettings,wecomparedifferent baselinemethods. b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926A.DataProcessing WeconnectemergingrelationshipsfromtheClueWeb12 corpusintheFB15k-237 4dataset[26].FB15k-237contains 200millionsentencesintheClueWeb12corpuscoupled withFreebasementionannotations.Therearearound3.9 milliontextdescriptionscorrespondingtotherelationtypes inFreebase.Thesetextsarerepresentedasfulllexicalized dependencypaths.Forexample,giventhenewstextÒBarack Obamaisthe44thandcurrentPresidentofUnitedStates.Ó, thedependencypathfortherelationbetweenentityÒBarack ObamaÓandÒUnitedStatesÓisrepresentedasÒSUBJECT nsubj+*** president prep**&ofobj*&OBJECTÓ.Intheexperiment, weextractedn-gram(n=1,2,3)featuresfromthelexical- izeddependencypathsandconcatenatedthemtogetherasthe featuresofthetextdescriptions.Intheprocessofn-gram featureextraction,weconsideredthelexicaldependencies suchasÒnsubjÓandÒprepÓaswords.Thosen-gramfeatures withfrequencieslessthan10areremovedintheexperiment. InFB15k-237,FreebaseisusedastheKG.Asubsetof entitiesandrelationsareextractedfromFreebase.Almost allentitiesoccurintheClueWeb12corpus. Inordertoevaluatetheeffectivenessoftheproposed TAMUREmethod,werandomlyselecthalfoftheentitiesin FB15k-237asnewentitiestoavoidthehigh-costofhuman labeling.Thoserelationshipsassociatedwithnewentitiesare emergingrelationships.Intheexperiment,relationshipswith existingentitiesareconsideredasthetrainingsetfromthe KG.Theemergingrelationshipsareequallysplitintothe validationsetandthetestingset.Wegeneratethenegative emergingrelationshipsforthevalidationandtestingsets byreplacingeachrelationtypewitharandomone.The validationsetisusedtoÞndthethresholdforeachrelation bythresholdingthereal-valuedoutputscoresoftheproposed TAMUREmodelasin[8].Thestatisticsofthedatasetare summarizedinTableII. B.ComparedMethods InordertoshowthattheproposedTAMUREmodelcan effectivelyconnectemergingrelationships,wecomparethe followingninemethods: ¥TransE:ItistheclassicKGembeddingmodelby treatingrelationtypesastranslationsfromheadentities totailentities[12].Eachentityisembeddedintoa low-dimensionalvectorspaceandeachrelationtypeis representedasatranslationvector.Thescorefunction ofTransEis f(h,r,t )=||h+r*t||"2.¥Skip-Gram:Itisthestate-of-the-artwordembedding model[14].ItconsiderstheKGandnewstextstogether asalargecorpusandlearnsembeddingvectorsforeach wordwhereeachentityorrelationtypeisregardedas aword. 4https://www.microsoft.com/en-us/download/details.aspx?id=52312 ¥DeepWalk:Itisanembeddingmodelforhomogeneous graphswithbinaryedges[15].Itlearnsembeddings ofnodesbyapplyingtruncatedrandomwalksonthe graph.Byviewingentities,relationtypesandwordsas thesametypeofnodes,wecanbuildahomogeneous graphfromnewstextsandKGs.Thenweapplythe DeepWalkmodelonthisgraph. ¥LINE:Itisthe Large-scale InformationNetwork Embeddingmethod(LINE)[16].SimilartoDeepWalk, LINEtreatsentities,relationtypesandwordsasone typeofnodesbutconsiderstheweightsofedges whenlearningtheembeddings.Theweightsarethe frequenciesoftwonodesco-occurringinnewsorKGs. ¥PTE:Itisthe Predictive Text Embeddingmethod (PTE)[17].Itlearnsembeddingsofnodesfromahet- erogeneousgraphbuiltfromnewstextsandKGs.The heterogeneousgraphincludesfourtypesofgraphs:a word-wordco-occurrencegraph,anentity-entitygraph, aword-entitygraphandanentity-relationgraph. ¥TransE+SG:ItisbasedonthepTransEmethodfor relationshipinferencefromnewstextsandknowledge graphs[18].ItÞrstappliesTransEforentityembed- dingsfromKGsandSkip-Gramforwordembeddings fromnewstexts.Thenthetwomodelsarecombined viaaligningtheembeddingsintothesamespace.Since thedatasetFB15k-237hasalreadyannotatedentitiesin news,thereisnoneedtoapplythealignmentmodel. ¥RESCAL:Itisarelationallearningapproachbased ontensorfactorization[27].ItfocusesontheKGand embedsrelationtypesintoamatrixspacethatoperates asabilinearoperatoronentityembeddings.RESCAL appliesamoreßexibletensordecompositionthanCP, astherelationembeddingmatrixintroducesinteraction termsforentityembeddings. ¥TAMURE-KG:Itistheproposedtensor-basedframe- workontheKGonly.WeÞrstbuildathird-order tensoraboutheadentities,relationtypesandtailen- titiesfromtheKG.Thentheproposedmulti-relational factorizationmodelisappliedtolearnembeddings. ThescorefunctionofTAMURE-KGis f(h,r,t )=rT((h+bh))(t+bt)).¥TAMURE-HI:Itistheproposedtensor-basedframe- workwiththehigh-order(fourth-order)interactions only.Thelower-orderconstraintonrelationtypesis notconsideredinthepredictivemodelofTAMURE- HI.ThescorefunctionisshowninEq.7. ¥TAMURE:Itisthe Text- Aware MUlti-RElationallearningmodel(TAMURE)proposedinthispaper.We buildafourth-ordertensorstructuretocombinethe KGandnewstextstogetherforconnectingemerging relationships.WeimplementTAMUREviaTensorFlow.Forfaircom- parisons,wesetthedimensionalityofembeddingsas20for b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926TableII STATISTICSOFTHEDATASET .News EntitiesRelationshipsTrain/Validation/Test #Texts #Text Descriptions#Entities #Entities #Existing Entities#Emerging Entities#Relationships #Existing Relationships#Emerging Relationships#Training Relationships#Validation Relationships#Testing Relationships200,000,0003,978,01413,937 14,5417,2707,271 310,11682,062228,054 82,062114,027114,027 alltheabovemethods.Foragivenentity,itsembedding vectoristhesamewhentheentityappearsasthehead orasthetailofarelationtype.TheAdamalgorithm[24] withinTensorFlowisappliedastheoptimizerforTransE, RESCAL,TAMURE-KG,TAMURE-HIandTAMURE.The learningrateforAdamoptimizerissetas0.01,themini- batchsizeissetas1000,themaximumnumberofepochs issetas20,andthemarginissetas1intheexperiments. Forthetextembeddingbaselines,wefollowthesettingsin [14Ð17].Toevaluatetheperformanceofthecomparedapproaches, weturntheproposedTAMUREmodelintoabinaryclassi- Þerasin[8]bythresholdingthereal-valuedoutputscores wherethethresholdsforeachrelationtypearefoundonthe validationset.Forthetextembeddingbaselines(i.e.,Skip- Gram,DeepWalk,LINEandPTE),wefollow[17]toapply thelogisticregressionmodelintheLibLinearpackage 5afterlearningtheembeddings.Theembeddingconcatenationof headentities,relationtypesandtailentitiesisconsideredas thefeatureduringtheclassiÞcationphase.Sincetheremight bemultiplerelationtypesbetweentwoentities,wemeasure theperformancebyadoptingÞvepopularmulti-labeleval- uationmetricsintheliterature: MicroF1 andMacroF1 thatevaluateaclassiÞerÕslabelsetpredictionperformance andconsiderthemicro/macro-averageof precision andrecall onallbinarylabelswithequalimportance[28]; AverageAccuracy (AvgAcc ),AverageAUC (AvgAuc )andHammingLoss (HL)thatevaluatetheaverageaccuracy, AUCanderrorrateoverallthebinarylabels(relationtypes) [29].C.PerformanceEvaluation Inthissection,wereporttheperformanceofthecompared methods.Thetestingemergingrelationshipscanbecatego- rizedintotwogroups:oneisaboutthosewithonlyone emergingentityandtheotherisaboutthoserelationships withbothentitiesnotintheKG.Inordertoshowthe effectivenessoftheproposedTAMUREmethod,wenot onlyshowtheperformanceoverallemergingrelationships, butalsoshowtheresultsforeachgroupofemerging relationships.Theperformancewiththerankofeachmethod isreportedinTableIII.Itcanbeobservedthatthepro- posedTAMUREmethodconsistentlyoutperformsallthe otherninebaselinesregardlessofthegroupsofemerging relationships.SpeciÞcally,comparedwiththesecondbest baseline,TAMUREachievesanimprovementof47%onthe 5http://www.csie.ntu.edu.tw/ !cjlin/liblinear/(a)Relationshipswithoneemergingentity (b)Relationshipswithtwoemergingentities (c)Alltestingrelationships Figure4.Theperformancewithdifferentpercentagesofemergingentities. Forthe Micro -F1andMacro -F1metrics,thelargerthevalue,thebetter theperformance.Forthe HLmetric,thesmallerthebetter. HLmetricand24%onthe AvgAcc metric.Furthermore,in thecasewherebothentitiesareemergingentities,TAMURE achievesasigniÞcantperformanceimprovement(42%and 27%higherthanthesecondbestbaselineon HLandAvgAuc ,respectively),showingthecapabilityofTAMURE capturingtheinteractionsbetweentherelationtypesinKGs andthetextdescriptionsinnews. ComparedwiththeKGembeddingbaselines,TAMURE performsthebestsinceitincorporatesthenewstextsand theKGintoanelegantfourth-ordertensorformulationto capturecomplexinteractionsamongrelationtypes,entities andtextdescriptions.Forthoseemergingrelationshipswith onlyonenewentity,TransEachievesthesecondbestresults. b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926TableIII THECLASSIFICATIONPERFORMANCE ÒVALUE (RANK )ÓONCONNECTINGEMERGINGRELATIONSHIPS .Ò%ÓINDICATESTHELARGERTHEVALUE ,THE BETTERTHEPERFORMANCE .Ò&ÓINDICATESTHESMALLERTHEBETTER .(a)ResultsonemergingrelationshipswithonlyoneentitynotintheKG. MethodsKGEmbedding TextEmbedding KG+TextEmbedding CriteriaTransERESCALTAMURE-KG Skip-GramDeepWalkLINEPTE TransE+SGTAMURE-HITAMURE Micro-F1 %0.8239( 2)0.7923( 5)0.8001( 4)0.0723( 10)0.1019( 9)0.2792( 8)0.4263( 7)0.7893( 6)0.8012( 3)0.8738(1)Macro-F1 %0.7022( 2)0.6746( 5)0.6810( 4)0.1189( 10)0.1603( 9)0.4296( 7)0.3215( 8)0.6654( 6)0.6881( 3)0.7827(1)AvgAcc %0.5905( 2)0.5635( 3)0.5623( 4)0.3777( 9)0.3907( 8)0.5184( 7)0.3072( 10)0.5188( 6)0.5622( 5)0.7240(1)AvgAuc %0.6425( 2)0.4860( 6)0.5759( 3)0.1420( 10)0.1860( 9)0.4752( 7)0.1947( 8)0.5187( 5)0.5634( 4)0.7370(1)HL&0.4096( 2)0.4365( 3)0.4377( 4)0.6223( 9)0.6093( 8)0.4816( 7)0.6928( 10)0.4812( 6)0.4378( 5)0.2760(1)(b)ResultsonemergingrelationshipswithbothentitiesnotintheKG. MethodsKGEmbedding TextEmbedding KG+TextEmbedding CriteriaTransERESCALTAMURE-KG Skip-GramDeepWalkLINEPTE TransE+SGTAMURE-HITAMURE Micro-F1 %0.7625( 6)0.7742( 3)0.7704( 4)0.1703( 10)0.2540( 9)0.3751( 8)0.6125( 7)0.7656( 5)0.7946( 2)0.8461(1)Macro-F1 %0.6450( 6)0.6564( 3)0.6502( 5)0.1936( 10)0.2637( 9)0.5103( 8)0.5748( 7)0.6513( 4)0.6827( 2)0.7555(1)AvgAcc %0.5090( 4)0.5035( 6)0.5080( 5)0.3243( 10)0.3438( 9)0.5025( 7)0.4266( 8)0.5150( 3)0.5575( 2)0.6885(1)AvgAuc %0.5028( 6)0.5060( 4)0.5049( 5)0.1577( 10)0.2025( 9)0.4758( 7)0.2045( 8)0.5199( 3)0.5583( 2)0.7081(1)HL&0.4910( 4)0.4965( 6)0.4920( 5)0.6757( 10)0.6562( 9)0.4975( 7)0.5734( 8)0.4850( 3)0.4425( 2)0.3115(1)(c)Resultsonallemergingrelationships. MethodsKGEmbedding TextEmbedding KG+TextEmbedding CriteriaTransERESCALTAMURE-KG Skip-GramDeepWalkLINEPTE TransE+SGTAMURE-HITAMURE Micro-F1 %0.8033( 2)0.7863( 5)0.7902( 4)0.1083( 10)0.1599( 9)0.3139( 8)0.4948( 7)0.7819( 6)0.7957( 3)0.8647(1)Macro-F1 %0.6877( 2)0.6675( 5)0.6700( 4)0.1414( 10)0.1918( 9)0.4459( 7)0.4129( 8)0.6632( 6)0.6830( 3)0.7754(1)AvgAcc %0.5723( 2)0.5442( 5)0.5467( 4)0.3658( 9)0.3796( 8)0.5138( 7)0.3439( 10)0.5210( 6)0.5580( 3)0.7168(1)AvgAuc %0.6050( 2)0.4932( 6)0.5517( 4)0.1531( 10)0.1966( 9)0.4768( 7)0.2514( 8)0.5073( 5)0.5606( 3)0.7326(1)HL&0.4277( 2)0.4558( 5)0.4533( 4)0.6342( 9)0.6204( 8)0.4862( 7)0.6562( 10)0.4791( 6)0.4420( 3)0.2832(1)However,withoutthehelpofnewstexts,TransEcannot performwelliftherelationshiphasbothentitiesnotin theKG.SinceemergingrelationshipsusuallyÞrstappear innews,itisimportantandnecessarytoconsidernews duringthedetectionprocess.WithspeciÞcdesignsforthe KGembedding,itisdifÞculttoincorporatenewstextsinto TransEandRESCAL.Incontrast,theproposedtensor-based framework(TAMURE-KG)canbeeasilyadaptedtoadd newsinformation(TAMURE),therebyconnectingemerging relationshipseffectively. Anotherobservationisthattextembeddingbaselinesdo notperformwellforconnectingemergingrelationships.The reasonliesinthetwo-steplearningofemergingrelationships forthesemodels.Firsttheembeddingvectorsarelearned andthentheemergingrelationshipsareclassiÞedbased ontheembeddingvectors.Withtwoseparatesteps,itis difÞculttoconnectemergingrelationshipswiththelearning ofembeddings.ThoughPTEconsiderstheheterogeneity amongentities,relationtypesandwordsinnewstexts,it stillperformsworsethantheKGembeddingmodels. Furthermore,comparedwithTransE+SG,TAMURE-HI andTAMUREstillperformbetter.TransE+SGcombinesthe KGandnewstextstogether,therebyachievingreasonable performanceonemergingrelationshipswithbothentitiesnot intheKG.However,TransE+SGembedstheKGandthe newsseparately.Theinteractionsbetweentherelationtypes inKGsandthetextdescriptionsinnewsarenotcaptured. Hence,itcannothelpdetectemergingrelationshipseffec- tively.TheproposedTAMURE-HIandTAMUEmethodsin thispaperbuildsafourth-ordertensortocapturethehidden connectionsbetweentheKGandthenewstexts,andthereby achievingbetterperformance.SinceTAMUREconsidersthe lower-orderinteractionsinthebuilttensor,itoutperforms TAMURE-HItoalargeextent. Insummary,withatensorstructure,theproposed TAMUREhelpsconnectemergingrelationshipsfromnews effectivelyandoutperformsalltheninebaselinemethods. D.EffectsofEmergingEntities Asmentionedpreviously,werandomlyselecthalfof theentitiesinFB15k-237asnewentitiestoevaluatethe effectivenessofTAMURE.Inthefollowing,weassessthe performanceofTAMUREwithdifferentpercentagesof newentities,intherangeof10%to90%.Weselectthe bestÞvemodelsaccordingtoTableIII(TransE,RESCAL, TAMURE-KG,TAMURE-HIandTAMURE)andrepresent theirperformanceinFigure4.Duetospacelimit,we b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926Figure5.Theperformancewithdifferentnumbersofepochs. onlyshowtheperformanceon Micro -F1andHL.Similar performanceisachievedontheothermetricsof Macro -F1,AvgAcc andAvgAuc .FromFigure4,wecanobservethatTAMUREsigniÞ- cantlyoutperformsotherbaselinesregardlessofhowmany percentagesofemergingentitiesweselect.Inaddition, TAMUREcanachieveastableperformancewhenthereare lessthan70%ofemergingentities.Withlessthan30%of existingentitiesintheKG,littleinformationcanbeprovided forconnectingemergingrelationshipsfromnews,thereby resultinginadropofperformanceforTAMURE.However, TAMUREstillachievesthebestresultcomparedwithother baselinemethodssinceitcapturesthehiddenconnections betweenrelationtypesintheKGandtextdescriptionsin thenews. E.ParameterAnalysis Inthefollowing,weanalyzetheperformanceof TAMUREwithdifferentembeddingsizesandepochs. 1)InßuenceofEmbeddingSize: Wedemonstratethe performanceofTAMUREwithdifferentembeddingsizes byÞxingtheotherparameters.InFigure6,weshowthe resultson Micro -F1andMacro -F1metrics,whereÒOneÓ meanstheresultsforemergingrelationshipswithonlyone entitynotintheKG,ÒTwoÓisabouttherelationships withneitherentitiesintheKG,andÒAllÓisaboutallthe emergingrelationships.Itcanbeobservedthat,withalarger embeddingsize,TAMUREachievesbetterperformanceon bothMicro -F1andMacro -F1.However,whentheembed- dingsizeislargerthan20,theperformanceofTAMURE becomesmorestablewithsmallimprovement.Therefore, inourexperiment,wesettheembeddingsizeas20.Dueto spacelimit,wedonotshowtheresultson AvgAcc ,AvgAuc andHLmetrics,onwhichsimilarpatternscanbeobserved. 2)InßuenceofEpochs: Wenowinvestigatetheper- formanceofTAMUREwithdifferentnumbersofepochs. Figure5showstheconvergenceprocessofthetrainingloss ofTAMURE,withdifferentembeddingsizesanddifferent percentagesofemergingentities,respectively.ÒEmbed kÓon theleftofFigure5indicatestheresultswithembeddingsize k.WecanobservethatthetraininglossdropsquicklyatÞrst fewepochsandbecomesstableafter10epochs,regardless Figure6.Theperformancewithdifferentembeddingsizes. oftheembeddingsizeorthepercentageofemergingentities. Itdemonstratesafastconvergenceoftheproposedmethod TAMURE.Inourexperiment,wesetthenumberofepochs as20toachieveastableperformance. V.R ELATED WORK Knowledgegraphs(KGs)ofreal-worldrelationsabout entitiesareusefulsourcesforalotofimportantapplications ininformationextraction,naturallanguageunderstanding andinformationretrieval[30,31].However,knowledge graph(KG)isincompletewithlargeamountsofmissing relationtypes[1]. DuetothelimitedcoverageofKGs,thetaskofKG completionhasreceivedalotofattention[6,7,9,13,32]. Someworklearnsembeddingrepresentationsofentities andrelationtypesintheKGandusetheseembeddingsto infermissingrelationships[4,5,8,12,33].Inaddition, somestudiespredictmissingrelationshipsfromagraph view[10,11,34,35].Forinstance,thePathRanking Algorithm(PRA)[10,11]performslinkpredictioninthe KGviaarandomwalkinferencetechnique.Recently,the researchwork[35]usesarecursiveneuralnetworktocreate embeddedrepresentationsofpathslearnedfrom[10,11].In ourwork,theemergingrelationshipshavenewentitiesthat arenotincludedintheKG.Hence,itisimpossibletoapply thesetechniquesdirectly. Furthermore,someworktriestoembedtheKGandthe textsjointly[3,18].However,themethodin[18]embeds theKGandtextsseparately.Sotheirindirectinference accordingtotextscannothelpdetectemergingrelationships effectively.Thoughthemethodin[3]aimstoconnect emergingrelationships,itfailstodistinguishamongdifferent relationtypes.Incontrast,ourproposedTAMUREmodel handlesmulti-labelrelationtypesbybuildingafourth-order tensorstructure. Ourworkisalsorelatedtotheproblemofinformation networkmodelingandmining[36Ð39].Recently,thereare someworkaboutembeddinglarge-scaletexts[14Ð17].For example,DeepWalkandLINEareproposedin[15]and[16] toembedtextsintoalow-dimensionalspacebyconstructing ahomogeneouswordco-occurrencenetworkfromtexts. LaterPTEisproposedin[17]toimprovetheLINEmethod b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926bybuildingaheterogeneoustextnetwork.Thesemethods focusonembeddingeverysinglewordintextsbutignoring thesemanticrelationtypesamongwords.Therefore,they cannothelpconnectemergingrelationshipsfromnewstexts effectively. VI.C ONCLUSION Inthispaper,weformulateanewtaskofconnecting emergingrelationshipsfromnewsandproposeanovel tensor-basedframeworktocombineKGsandnewstexts effectivelyforconnectingemergingrelationships.Withan efÞcient Text- Aware MUlti-RElationallearningmethod (TAMURE),thecomplexinteractionsamongrelationtypes, entitiesandtextdescriptionsarejointlyfactorizedwithout physicallybuildingthetensor.Extensiveexperimentsvia TensorFlowdemonstratetheeffectivenessoftheproposed TAMUREmodelcomparedwithninestate-of-the-artmeth- odsonreal-worlddatasets. ACKNOWLEDGEMENT ThisworkissupportedinpartbyNSFthroughgrantsIIS- 1526499,CNS-1626432,andNSFC61672313.Wegrate- fullyacknowledgethesupportofNVIDIACorporationwith thedonationoftheTitanXGPUusedforthisresearch. REFERENCES [1]X.Dong,E.Gabrilovich,G.Heitz,W.Horn,N.Lao,K.Murphy, T.Strohmann,S.Sun,andW.Zhang,ÒKnowledgevault:Aweb-scale approachtoprobabilisticknowledgefusion,Óin KDD,2014. [2]J.Hoffart,Y.Altun,andG.Weikum,ÒDiscoveringemergingentities withambiguousnames,Óin WWW,2014. [3]J.Zhang,C.Lu,M.Zhou,S.Xie,Y.Chang,andP.Yu,ÒHeer: Heterogeneousgraphembeddingforemergingrelationdetectionfrom news,Óin IEEEBigData ,2016. [4]K.Chang,W.Yih,B.Yang,andC.Meek,ÒTypedtensordecompo- sitionofknowledgebasesforrelationextraction,Óin EMNLP,2014. [5]A.Bordes,J.Weston,R.Collobert,andY.Bengio,ÒLearningstruc- turedembeddingsofknowledgebases,Óin AAAI,2011. [6]A.Garc «õa-Dur«an,A.Bordes,andN.Usunier,ÒEffectiveblendingof twoandthree-wayinteractionsformodelingmulti-relationaldata,Óin ECML/PKDD,2014. [7]M.Nickel,X.Jiang,andV.Tresp,ÒReducingtherankinrelational factorizationmodelsbyincludingobservablepatterns,Óin NIPS,2014. [8]R.Socher,D.Chen,C.Manning,andA.Ng,ÒReasoningwithneural tensornetworksforknowledgebasecompletion,Óin NIPS,2013. [9]Z.Wang,J.Zhang,J.Feng,andZ.Chen,ÒKnowledgegraphembed- dingbytranslatingonhyperplanes,Óin AAAI,2014. [10]M.GardnerandT.Mitchell,ÒEfÞcientandexpressiveknowledgebase completionusingsubgraphfeatureextraction,Óin EMNLP,2015. [11]N.Lao,T.Mitchell,andW.Cohen,ÒRandomwalkinferenceand learninginalargescaleknowledgebase,Óin EMNLP,2011. [12]A.Bordes,N.Usunier,A.Garcia-Duran,J.Weston,and O.Yakhnenko,ÒTranslatingembeddingsformodelingmulti-relational data,Óin NIPS,2013. [13]Y.Lin,Z.Liu,M.Sun,Y.Liu,andX.Zhu,ÒLearningentityand relationembeddingsforknowledgegraphcompletion.Óin AAAI,2015. [14]T.Mikolov,I.Sutskever,K.Chen,G.Corrado,andJ.Dean,ÒDis- tributedrepresentationsofwordsandphrasesandtheircomposition- ality,Óin NIPS,2013. [15]B.Perozzi,R.Al-Rfou,andS.Skiena,ÒDeepwalk:Onlinelearning ofsocialrepresentations,Óin KDD,2014. [16]J.Tang,M.Qu,M.Wang,M.Zhang,J.Yan,andQ.Mei,ÒLine: Large-scaleinformationnetworkembedding,Óin WWW,2015. [17]J.Tang,M.Qu,andQ.Mei,ÒPte:Predictivetextembeddingthrough large-scaleheterogeneoustextnetworks,Óin KDD,2015. [18]Z.Wang,J.Zhang,J.Feng,andZ.Chen,ÒKnowledgegraphandtext jointlyembedding,Óin EMNLP,2014. [19]T.KoldaandB.Bader,ÒTensordecompositionsandapplications,Ó SIAMreview ,2009. [20]B.Cao,H.Zhou,G.Li,andP.S.Yu,ÒMulti-viewmachines,Óin WSDM,2016. [21]C.-T.Lu,L.He,W.Shao,B.Cao,andP.S.Yu,ÒMultilinear factorizationmachinesformulti-taskmulti-viewlearning,Óin WSDM,2017.[22]D.NadeauandS.Sekine,ÒAsurveyofnamedentityrecognitionand classiÞcation,Ó LingvisticaeInvestigationes ,vol.30,no.1,2007. [23]StanfordNamedEntityRecognizer ,http://nlp.stanford.edu/software/ CRF-NER.shtml. [24]D.KingmaandJ.Ba,ÒAdam:Amethodforstochasticoptimization,Ó inICLR,2015. [25]N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,and R.Salakhutdinov,ÒDropout:asimplewaytopreventneuralnetworks fromoverÞtting.Ó JMLR,2014. [26]K.Toutanova,D.Chen,P.Pantel,P.Choudhury,andM.Gamon, ÒRepresentingtextforjointembeddingoftextandknowledgebases,Ó inACL ,2015. [27]M.Nickel,V.Tresp,andH.Kriegel,ÒAthree-waymodelforcollective learningonmulti-relationaldata,Óin ICML,2011. [28]N.GhamrawiandA.McCallum,ÒCollectivemulti-labelclassiÞca- tion,Óin CIKM,2005. [29]G.Tsoumakas,I.Katakis,andI.Vlahavas,ÒMiningmulti-labeldata,Ó inDataMiningandKnowledgeDiscoveryHandbook ,2009. [30]G.Angeli,J.Tibshirani,J.Wu,andC.Manning,ÒCombiningdistant andpartialsupervisionforrelationextraction,Óin EMNLP,2014. [31]D.Zeng,K.Liu,Y.Chen,andJ.Zhao,ÒDistantsupervisionfor relationextractionviapiecewiseconvolutionalneuralnetworks,Óin EMNLP,2015. [32]M.Gardner,P.Talukdar,J.Krishnamurthy,andT.Mitchell,ÒIn- corporatingvectorspacesimilarityinrandomwalkinferenceover knowledgebases,Óin EMNLP,2014. [33]G.Ji,S.He,L.Xu,K.Liu,andJ.Zhao,ÒKnowledgegraphembedding viadynamicmappingmatrix,Óin ACL ,2015. [34]M.Gardner,P.Talukdar,B.Kisiel,andT.Mitchell,ÒImproving learningandinferenceinalargeknowledge-baseusinglatentsyntactic cues,Óin EMNLP,2013. [35]A.Neelakantan,B.Roth,andA.McCallum,ÒCompositionalvector spacemodelsforknowledgebasecompletion,Óin ACL ,2015. [36]B.Cao,X.Kong,J.Zhang,P.S.Yu,andA.B.Ragin,ÒMining brainnetworksusingmultiplesideviewsforneurologicaldisorder identiÞcation,Óin ICDM,2015. [37]J.Zhang,X.Kong,L.Jie,Y.Chang,andP.Yu,ÒNcr:Ascalable network-basedapproachtoco-rankinginquestion-and-answersites,Ó inCIKM,2014. [38]J.Zhang,L.Jie,A.Rahman,S.Xie,Y.Chang,andP.Yu,ÒLearning entitytypesfromquerylogsviagraph-basedmodeling,Óin CIKM,2015.[39]J.Zhang,B.Cao,S.Xie,C.-T.Lu,P.S.Yu,andA.B.Ragin, ÒIdentifyingconnectivitypatternsforbraindiseasesviamulti-side- viewguideddeeparchitectures,Óin SDM,2016. b731afe6-WzJw94iD6zoJQHfhBpOQZhzxQ0g7xVUUBgaHnQ==-500ddfd78926 
NaturalLanguageEngineering 24(4):641–647. cCambridgeUniversityPress2018.Thisisan OpenAccessarticle,distributedunderthetermsoftheCreativeCommonsAttributionlicence (http://creativecommons.org/licenses/by/4.0/),whichpermitsunrestrictedre-use,distribution, andreproductioninanymedium,providedtheoriginalworkisproperlycited. doi:10.1017/S1351324918000189641Emergingtrends:ArtiﬁcialIntelligence,China andmynewjobatBaidu KENNETHWARDCHURCH Baidu,Sunnyvale,CA,94089,USA e-mail:kennethchurch@baidu.com(Received24April2018;accepted25April2018 )Abstract ItisamazingthatIhavewrittenformorethanayearonEmergingTrendswithoutmentioning China’sinvestmentsinArtiﬁcialIntelligence(AI).NowthatIhavemovedtoBaidu,Iwould liketotakethisopportunitytosharesomeofmypersonalobservationswithwhat’shappening inChinaoverthepast25years.ThetopuniversitiesinChinahavealwaysbeenverygood,  buttheyarebettertodaythantheywere25yearsago,andtheyareonatrajectorytobecome thebiggestandthebestintheworld.Chinaisinvestingbigtimeinwhatwedo,bothin theprivatesectorandthepublicsector.Kai-FuLeeisbullishonhisinvestmentsinAIand China.ThereisaboldgovernmentplanforAIwithspeciﬁcmilestonesforparitywiththe Westin2020,majorbreakthroughsby2025andtheenvyoftheworldby2030. 1MynewpositionatBaidu ItisamazingthatIhavewrittenformorethanayearonEmergingTrendswithout mentioningChina.Mylifehaschangeddramaticallysincemylastcolumn.Inow workforBaidu.IfyoudonotspeakChinese,youmaynothaveheardofBaidu,but youwillsoon.ChinesespeakersknowthatBaiduisthelargestsearchcompanyin  China.TheChinesemarketissolargethatitispossibleforacompanywithrelatively  littlebrandawarenessoutsideChinatobethesecondlargestsearchcompanyinthe  world. 1Baidudoesmanythingsinadditiontosearch.Somearemorerelevanttowhat wedo(suchasspeech-to-speechtranslation), 2andsomearelessrelevanttowhat wedo(suchasself-drivingcars). 32EducationinChina WhenIwaspresidentoftheACL,Iperformedamarketinganalysistoseewherethe ACLmembershipwasalreadystrong,andwheretherewereopportunitiestogrowthe 1https://en.wikipedia.org/wiki/Baidu2https://www.theverge.com/circuitbreaker/2017/9/25/16363250/baidu-translator-gadget-announced3https://www.wired.com/2015/12/baidus-self-driving-car-has-hit-the-road/642K.W.Church ACLmembership.Foreachcountry,Ilookedatthreevariables:ACLmembership, populationandgraduatestudents.CountrieswithmoreACLmemberspercapita  wereconsideredrelativelystrong,andcountrieswithmorestudentsinareasof  interest(appropriatelynormalized)wereconsideredopportunitiesforgrowth.It  shouldcomeasnosurprisethatChinawasidentiﬁedasthetopopportunityfor  growth. ThetopuniversitiesinChinahavealwaysbeenverygood,buttheyarebigger andbettertodaythantheywerethen,andtheywillcontinuetoimprove.Theﬁrst  timeIvisitedTsinghua,the‘MIT’ofChina,thingsweresobadthatIhadtostep  overtheceiling,whichhadfallendownontotheﬂoor.Nevertheless,Iwasimpressed  withtheaudience.Therestoftheworldwassobusywithsillystart-upsthatnoone  hadtimetothinkaboutwhattheyweredoing.WhereasinChina,sincetherewas  nopossibilitytomakeaquickbuck,topstudentsattopuniversitieshadthetime  toreadtheliteratureandthinkdeepthoughts.Theycoulddreamaboutstudying  abroad,andafewhadalreadydoneso. Sincethen,Chineseuniversitieswentthroughamajorexpansion.Towardtheend oftheexpansion,Ihadanopportunityin2008togiveaninvitedtalktoanumber  ofdeansinChina.Isuggestedthatmyson,whowasinmiddleschoolatthetime, mightgethisPhDinChinabecausethatmightbetheplacetogoinhisday. Wecomefromalonglineofacademics.MyfatherandhisfatherandIreceived  ourPhDsinAmericabecausethatwastheplacetogoinourday,butmygreat  grandfatherwenttoGermanyforhisPhDbecausethatwastheplacetogoin  hisday.ThedeanscouldnotimaginethatChinamightbetheplacetogointhe  foreseeablefuture,butthenextgenerationofstudentsaremorebullishonChinese  universitiesthanthelastgenerationofdeans. ThenarrativeisthatschoolsinAmericaarebetterandmorediversethanschools inChina.Whilethatmaystillbethecase,thegapisclosing.IamtoldthatTsinghua  isnowevenhardertogetintothanMIT.Thebestofabillionpeoplearereally  good.Tables1and2 4showthattopschoolsinChinahavemoreinternational 4Howmanystudentsarethereintopschools,especiallyareasthatwecareabout?Itis hardtocomeupwithacompellingapplestoapplescomparison.Shouldwecountboth undergraduateandgraduatestudents?Whichareasdowecareabout?Thelistofschools inTables1and2comefrom: (1)https://www.usnews.com/education/best-global-universities/search?region=&country=china&subject=computer-science&name=(2)https://www.usnews.com/best-graduate-schools/top-engineering-schools/computer-engineering-rankings?int=9d0e08&int=a06908Estimatesoftheothercolumnscomefromthosepages,aswellas: (1)https://admission.princeton.edu/how-apply/admission-statistics(2)https://news.harvard.edu/gazette/story/2013/03/college-admits-2029-5-8-percent-of-applicants/(3)https://collegeadmissions.uchicago.edu/apply/applicants/international(4)https://www.yale.edu/about-yale/yale-facts(4)https://undergrad.admissions.columbia.edu/ask/faq/topic/405(5)http://web.mit.edu/facts/enrollment.html(6)https://diversityandaccess.stanford.edu/diversity/diversity-facts(7)https://www.upenn.edu/about/factsEmergingtrends 643Table1. Topschoolsintheworld SchoolStudents%International Princeton8,18113 Harvard20,32410  UniversityofChicago13,32224 Yale12,45836 Columbia25,08419  MIT11,37629 Stanford16,9149 UniversityofPennsylvania21,82624 Average 16,18621 Table2. TopschoolsinChinahavelotsofstudents,andmorediversitythanIhad appreciated SchoolStudents%International TsinghuaUniversity42,0899 HuazhongUniversityofScienceandTechnology56,1405 ZhejiangUniversity50,0516  ShanghaiJiaoTongUniversity37,2886 SoutheastUniversity32,4645 PekingUniversity42,13614  HarbinInstituteofTechnology29,3959 Average 41,3668 studentsthanIwouldhavethoughtbasedonmyadmittedlylimitedexperience. Moreimportantly,thesetablesshowthatthetopschoolsinChinahavemore  studentsthantopschoolselsewhere(41kstudents/school 16kstudents/school). Thisisespeciallytrueinareasofinteresttothiscommunity.TopschoolsinChina  areparticularlystronginengineering. Numberscanbeabitmind-numbing.Ibegantoappreciatethegrowthopportunity whenIgaveatalkatasummerschoolinBeijingin2011.Thelargeauditoriumwas  completelysoldout,eventhoughtheeventwasputtogetherwithshortnoticeatthe  lastminute.IcameawaywiththeimpressionthatthereweremoreNLPstudents  withinafewmilesofthatauditoriumthanintherestoftheworld. TheexperienceremindedmeofakeynoteIgaveatEACL-1993ontherevival ofempiricism.Itwasclearthatempiricismwasabouttotakeowhenthetalkdid  betterwiththenextgenerationofstudentsthanthelastgeneration(ofoldfogies). Thestudentsarethefuture.Weshouldinvestinwhatevertheyaredoing.Back in1993,thatwasempiricism.Thesedays,therearelotsofyoungpeopleinChina  workingondeepnets. 644K.W.Church 3Chinaisinvestingbigtimeinwhatwedo AmazingthingsarehappeninginArtiﬁcialIntelligencearoundtheworld,and especiallyinChina.Manyoftheseinvestmentsarehavingapositiveimpacton  researchinareasthatwecareaboutsuchasnaturallanguageprocessingand  speech. InarecentCommencementAddressatColumbia, 5Kai-FuLeepointedoutthat enrollmentinAIatColumbiahasincreasedfrom80to800overthe34yearssince hewasanundergraduateatColumbia.Hecalledoutafewofthedrivingforces behindthisincreaseinenrollment,someofwhichheisinvestingin:phones,banking  (loans),facerecognition(payments),self-drivingcarsandmedicine.Aschairman  andCEOofSinovationVentures,a$1BfundthatinvestsinAIcompaniesinChina,  Kai-FuisbullishonhisinvestmentsinAIandChina.InChina,peoplelikehimare  treatedlikerockstars. 6Kai-Fuhasconsiderableexperienceinareasclosetoours.Aftergraduatingfrom ColumbiainAI,heearnedaPhDatCMUinspeech.Sincethen,hehasheld  executivepositionsatApple,MicrosoftandGoogle. Kai-Fuhasﬁrst-handknowledgeworkingforAmericancompaniesinChina. InaspeechattheDavosWorldEconomicForumonthedevelopmentofChina,  EuropeanandAmericanventurecapitalandtechnologycompanies, 7helistedfour reasonswhyAmericancompaniesencountersomanyinChina.Twoof  hisfourreasonsare(1)talent: China’stoptalent,fromundergraduatestoprofessionalmanagers,usedtothinkthatforeign companieswerethebestjobs.MostnowseeChina’sbigcompanies–suchasBaidu,Alibaba andTencent–medium-sizedcompanies–suchasMeituan,DidiandXiaomi–andstartups suchasVipKidandFace++–asbetteroptions.Withoutthebesttalent,thedevelopment prospectsofforeigncompaniesinChinaisnotoptimistic. and(2)leadership: Theheads(generalmanagersorCEOs)ofAmericancompaniesinChinaarealwaysforeigners  whobarelyknowanythingaboutthecountry.Theyusuallystarttheirjobsinsmallercountries, getpromotedtogeneralmanagerofChinageneralandreturntoheadquartersfortheirnext  promotion.Buttheirmainskillissales,andtheyneitherknownorcareaboutproduct technology.Theydon’tunderstandChinese,don’tspeakChineseandhavenolong-term commitmenttoChina.Andtheyaremainlyresponsiblefortheheadquartersindex,andtheir  biggestwishistoprotecttheirjob.Theyonlyreportgoodnewsandignorethebad.These peoplearevulnerabletotheﬁghterswhoheadupdomesticChinesecompanies. Inadditiontotheprivatesector,theChinesegovernmentisalsoinvestingbigtime inwhatwedo.TechnologyReview,MIT’salumnimagazine, 8citesaboldChinese 5http://engineering.columbia.edu/kai-fu-lee-speech6‘MeettheChinesetechgurubiggerthanJustinTimberlake.’ https://edition.cnn.com/2015/09/25/asia/china-innovation-kai-fu-lee/index.html7https://pandaily.com/kai-fu-lee-american-companies-struggle-china/8https://www.technologyreview.com/s/609038/chinas-ai-awakeningEmergingtrends 645governmentAIplan 9withspeciﬁcmilestonesforparitywiththeWestin2020,major breakthroughsby2025andtheenvyoftheworldby2030. TheChinesegovernmenthasdeliveredinthepastonambitiousplanslikethis. TechnologyReviewmentionshigh-speedtrainsasaprecedentwhereasimilarly  ambitiousplanproducedimpressiveresults.Sotoo,weshouldexpectimpressive  resultsfromtheChineseinvestmentinAI(andwhatwedo).TechnologyReview  quotesAndrewNg,whowasatBaiduatthetime: WhentheChinesegovernmentannouncesaplanlikethis,ithassigniﬁcantimplicationsfor thecountryandtheeconomy...It’saverystrongsignaltoeveryonethatthingswillhappen. 4Istheglasshalffull,orhalfempty? TheTechnologyReviewarticlementionedaboveissubtitled:‘TheWestshouldn’t fearChina’sAIrevolution.Itshouldcopyit.’Thereisconsiderableexcitement  aroundtheworldaboutAI,buttherearealsofears,especiallyintheWest,largely  involvingprivacyandjobsecurity, 10andeventhefutureofhumanity. 11TechnologyReviewsuggeststhejobsecurityquestionplaysintheWest andtheEast. [China’s]politicalandbusinessleadersarebettingthatAIcanjump-startitseconomy.In recentdecades,aboomingmanufacturingsector...havehelpedbringhundredsofmillions ofpeopleoutofpoverty,creatingbusinessempiresandtransformingChinesesociety.But  manufacturinggrowthisslowing...Applyingartiﬁcialintelligencemaybethenextstep... WhilemanyintheWestfretaboutAIeliminatingjobsandworseningwealthandincome inequality,Chinaseemstobelieveitcanbringaboutpreciselytheoppositeoutcomes. TheWestandtheEasthavemuchincommon,buttherearealsomany ThereismorefearintheWestandmoreoptimismintheEast.Thistrendholdsfor  manythingsthatgowellbeyondjobsecurity. 5Facerecognition Considerfacerecognitiontechnology,atechnologythatcanbeusedtodomany things,somegoodandsomenotsogood.Itiseasytobepessimisticaboutthe  downsides,butthereareplentyofupsidestobeoptimisticabout.WhiletheWest  ﬁndsitcreepytoliveinaworldwithmillionsofcameras, 12thepeoplethatItalk toinChinabelievethatcamerasreducecrimeandmakepeoplefeelsafer. Iamremindedofthehistoryofansweringmachines.Whenansweringmachines wereﬁrstintroduced,itwasconsideredtabootorecordphonecalls.Buteventually peoplebecameusedtoansweringmachines,anditsoonbecamerudenottohave 9http://www.gov.cn/zhengce/content/2017-07/20/content5211996.htm10https://www.cnbc.com/2017/04/27/kai-fu-lee-robots-will-replace-half-of-all-jobs.html11https://pandaily.com/kai-fu-lee-i-disagree-with-elon-musks-ai-will-destroy-humanity/12http://www.dailymail.co.uk/news/article-4918342/China-installs-20-million-AI-equipped-street-cameras.html646K.W.Church anansweringmachine.Nowitseemsmorepolitetosendatextmessageoremail. Culturalattitudestowardtechnologyareconstantlyevolvingacrossspaceandtime.  Whatisconsideredokayornotokayatonepointintimeandspace,mayormay  notbeconsideredokayatanothertimeoranotherplace. Therearelotsofwaystodogoodthingswithanynewtechnology.Someofthem workbetterinthecourtofpublicopinion,andsomeofthemaremoresigniﬁcant  commercially.Inthecaseoffacerecognitiontechnology,therehavebeenafew  highlypublicizedcaseswherefacerecognitionhashelpedreunitechildrenwiththeir  parents.13,14 Securityapplicationsimpactmorepeople.Manyofususefacerecognitionto unlockourphones.Baiduusesfacerecognitiontounlockthedoorstothebuilding.  AfterIlinkmyfacetomybadge,Idonotneedtobringmybadgetogetintothe  building;allIneedismyface.UnlikeAmericanExpressproducts, 15itishardto leavehomewithoutmyface. 6Electronicpayments Paymentsareanevenlargercommercialopportunity.Cashisnotusedmuchin Chinathesedays,andcreditcardsnevertookMostpeoplepayforthingswith  barcodesonphonesviamobilepaymentapps,suchasWeChatandAlipay.Face  recognitionmaysoonreplacebarcodes. WhenIcheckoutoftheBaiducafeteria,Ipayformylunchbyshowingmyface tothecamera,andthatisthat.Facerecognitionissaferthanacreditcard.Anyone canstealacreditcard(withorwithoutachip),butitishardtostealmyface. Therealproblemwithcreditcardsisfriction,aspointedoutbyKai-FuLeein theDavosspeechmentionedabove. ThenumberofdailymobiletransactionsonWeChatandAlipayexceeded300million.Such vigorousexpansionofmobilepaymentswillbringahugechangetoconsumptioninChina...  Creditcardcompaniesearn3percentfrommerchants,return2percenttousersandleave themselveswith1percent.Creditcardsareconnectedtotheinterestsofthesepeople,and itishardtodeterAmericansfromusingcreditcards.ButAmericaandEuropemustﬁnda  moreopenmodel. WhenIﬁrstvisitedChina25yearsago,everythingwasnegotiable,andcon- sequently,ittookalongtimetobuyanything.Inadditiontotheprice,wealso  hadtonegotiatecurrency.Thereweremanyillegalcurrencies,aswellastwolegal  currencies,oneforlocals(RMBs)andanotherforforeigners(FMBs).Intheory,  thetwocurrencieswereworththesamething,butinpractice,FMBswereworth  morebecausetheycouldbeusedin‘friendshipstores’thatsoldgoodsthatwerenot availableelsewhere.But,FMBswerehardtouseinruralareaswherepeoplehadnot 13https://news.filehippo.com/2013/06/facial-recognition-app-to-find-missing-children/14https://nakedsecurity.sophos.com/2017/06/02/facial-recognition-reunites-missing-boy-with-his-dad-after-four-years/15http://www.thedrum.com/news/2016/07/03/marketing-moments-11-american-express-dont-leave-home-without-itEmergingtrends 647seenthem.FMBswerealsohardtousebecausetheycameinlargedenominations. Onetimeweowedataxidriverasmallfee,andallwehadwasalargenotein  FMBs.Thedriverdidnothaveenoughchange,buthewantedourFMBssomuch  thathenegotiatedwithalineoftaxidriverstoborrowenoughRMBstomake  change.Needlesstosay,ittookalongtimetopayforthattaxiride. TherewasanothertimewhenIwantedanalarmclock.Myhostwashorriﬁed whenIacceptedtheinitialbidof$3.Thevendorwasalsodisappointed,sohethrew  inafreebattery.Iprobablyshouldhavenegotiatedharderbecausethatishowthe  gameisplayed,butIneededtheclock,anddidnothavemuchtime. Whenmysonwasinmiddleschool,heattractedalargecrowdbynegotiating (remarkably)withanexperiencedvendor.Bothheandthecrowdenjoyed  thegame,butattheendoftheday,thevendorspentmoretimeonthetransaction  thanitwasworth,andmysonendedupwithasouvenirthatheneverwanted. EverytimeIgotoChina,itisacountry.Thesedays,thereisless negotiating,andalmostnocash.Iactuallyusedcashrecentlytobuyacupof  Itcanstillbedone(barely),butthevendormadeitclearthatthat’snotthewaythe  gameisplayedthesedays. Evenmorerecently,Baidusentabunchofusonateambuildingtripwhere weplayedlotsofgames.Someofgamesincludednegotiationsofvariouskinds, includinganoldfashioned‘blackmarket.’ Negotiatingcanbefun,evenifitdoesnotmakemuchsenseeconomically.From apracticalpointofview,itisimportanttocomeupwithsimple(fast)methods  topayforthingswithaslittlefrictionaspossible.Kai-FuLeeiscorrectwhenhe  pointsoutthatChinanowplaysthepaymentgamebetterthanwedo. 7Concludingremarks IwastalkingwithsomeonefromGermanytheotherday.WhenImentionedthat ‘MadeinJapan’usedtobeassociatedwithpoorquality,hementionedthatthesame  usedtobetruefor‘MadeinGermany.’Ittookafewdecadesfortheeconomies  inJapanandGermanytorecoverfromWorldWarII.Sotoo,astheeconomy  improvesinChina,wewillquicklyforgetthat‘MadeinChina’usedtobeassociated  withpoorquality. Thefutureiswiththenextgeneration.ThetopuniversitiesinChinahavealways beenverygood,buttheyarebettertodaythantheywere25yearsago,andtheyare  onatrajectorytobecomethebiggestandthebestintheworld. Chinaisinvestingbigtime(andlongterm)inwhatwedo,bothintheprivate sectoraswellasthepublicsector.Kai-FuisbullishonhisinvestmentsinAI  andChina,aswellasChinesetalent.TheChinesegovernmenthasannounceda boldplanforAIwithspeciﬁcmilestonesforparitywiththeWestin2020,major breakthroughsby2025andtheenvyoftheworldby2030.  
AGenericInvertedIndexFrameworkforSimilarity SearchontheGPU JingboZhou  ,QiGuo y ,H.V.Jagadish # ,Lubo  sKr  c ´ al y ,SiyuanLiu z WenhaoLuan y ,AnthonyK.H.Tung y ,YuejiYang yx ,YuxinZheng yx y NationalUniversityofSingapore # Univ.ofMichigan,AnnArbor z NanyangTechnologicalUniversity  BusinessIntelligenceLab,BaiduResearch x TencentInc. y f jzhou,qiguo,krcal,luan1,atung,yueji,yuxin g @comp.nus.edu.sg # jag@umich.edu, x sliu019@e.ntu.edu.sg Abstract ŠWeproposeanovelgenericinvertedindexframe- workontheGPU(calledGENIE),aimingtoreducethepro- grammingcomplexityoftheGPUforparallelsimilaritysearch  ofdifferentdatatypes.Noteverydatatypeandsimilarity measurearesupportedbyGENIE,butmanypopularones are.WepresentthesystemdesignofGENIE,anddemonstrate  similaritysearchwithGENIEonseveraldatatypesalongwith atheoreticalanalysisofsearchresults.Anewconceptoflocality sensitivehashing(LSH)named ˝ -ANNsearch,andanoveldata structurec-PQontheGPUarealsoproposedforachieving thispurpose.Extensiveexperimentsondifferentreal-lifedatasets demonstratetheandeffectivenessofourframework.  Theimplementedsystemhasbeenreleasedasopensource 1 . I.I NTRODUCTION Thereisoftenaneedtosupportahighthroughputof queriesonindexstructuresatscale.Thesequeriescould arisefromamultiplicityofﬁusersﬂ,bothhumansandclient  applications.Evenasingleapplicationcouldsometimesissue  alargenumberofqueries.Forexample,imagematching  isoftendonebyextractinghundredsofhighdimensional  SIFT(scale-invariantfeaturetransform)featuresandmatching themagainstSIFTfeaturesinthedatabase.Parallelizationfor similaritysearchisrequiredforhighperformanceonmodern  hardwarearchitectures[1],[2],[3],[4]. SolutionsmaybeimplementeditonGraphicsProcessing Units(GPUs).GPUshaveexperiencedatremendousgrowthin termsofcomputationalpowerandmemorycapacityinrecent years.OneadvancedGPUintheconsumermarket,theNvidia  GTXTitanX,has12GBofDDR5memoryatapriceof1000  USdollarswhileanadvancedserverclassGPU,theNvidia  K80,has24GBofDDR5memoryatapriceof5000US dollars.Furthermore,mostPCsallowtwotofourGPUstobe installed,bringingthetotalamountofGPUmemoryinaPC  tobecomparablewitharegularCPUmemory. However,GPUprogrammingisnoteasy.Effectivelyex- ploitingparallelismisevenharder,particularlyaswewor- ryabouttheuniquefeaturesoftheGPUincludingthe Single-Instruction-Multiple-Data(SIMD)architecture,concur-  rentcontrol,coherentbranchingandcoalescingmemoryac- z SiyuanLiuhasdonehisworkontheprojectasaninternatNUS. 1 https://github.com/SeSaMe-NUS/genie cess.Whilecapableprogrammerscouldtaketheirindexstruc-  tureofchoiceandcreateaGPU-basedparallelimplementa-  tion,doingsowillrequireconsiderableeffortandskill. Ourgoalistoaddressthisparallelsimilaritysearchproblem ontheGPUinagenericfashion.Tothisend,wedevelop  anefandparallelizableGPU-basedGen ericI nverted Inde xframework,calledGENIE(wealsonameoursystem asGENIE),forsimilaritysearchusinganabstract match- countmodel weGENIEisdesignedtosupportparallel computationofthematch-countmodel,butthesystemis  genericinthatawidevarietyofdatatypesandsimilarity  measurescanbesupported. Wedonotclaimthateverydatatypeandsimilaritymeasure aresupportedbyGENIE 2 Œjustthatmanyare,aswewill demonstrateinthepaper,includingmostthatwehavecome  acrossinpractice.Asananalogy,considerthemap-reduce model,implementedinasoftwarepackagelikeHadoop.Not allcomputationscanbeexpressedinthemap-reducemodel,  butmanycan.Forthosethatcan,Hadooptakescareof  parallelismandscalingout,greatlyreducingtheprogrammer's  burden.Inasimilarway,oursystem,GENIE,canabsorbthe  burdenofparallelGPU-basedimplementationofsimilarity searchmethodsandindexstructures. Ourproposedmatch-countmodelthecommon operationsonthegenericinvertedindexframeworkwhich hasenoughxibilitytobeinstantiatedfordifferentdata  types.Theinsightforthispossibilityisthatmanydatatypes  canbetransformedintoaformthatcanbesearchedbyan  inverted-index-likestructure.Suchtransformationcanbedone  bythe LocalitySensitiveHashing (LSH)[5],[6]schemeunder severalsimilaritymeasuresorbythe ShotgunandAssembly (SA)[7],[8]schemeforcomplexstructureddata.Wepresent  adetaileddiscussionofthisinSectionII-B. ThechallengeofGENIEistodesignanef indexarchitectureforthematch-countmodel.Wepropose  aninvertedindexstructureontheGPUwhichcandividethe  queryprocessingtomanysmalltaskstoworkina  mannertofullyutilizeGPU'sparallelcomputationpower. 2 Notethatwenamedoursystemasﬁgenericinvertedindexﬂ,butnot ﬁgeneralinvertedindexﬂ. GENIEalsoexploitsGPU'spropertieslikecoalescingmemory accessandcoherencebranchingduringtheindexscanning. WeproposeanoveldatastructureontheGPU,called CountPriorityQueue(c-PQforshort),whichcan  reducethetimecostforsimilaritysearch.DuetotheSIMD  architecture,anotherchallengeofGENIEishowtoselect  thetop-kcandidatesfromthecandidateset,whichiswidely  consideredamainbottleneckforsimilaritysearchontheGPU inpreviousstudy[9],[4](whichiscalledk-selectionin[9] andshort-listsearchin[4]).Existingmethodsusuallyadopta  sortingmethodwhichisanexpensiveoperation,orapriority  queueontheGPUwhichhavewarpdivergenceproblemand  irregularmemorymovement.Ournoveldesignofc-PQcan  keeponlyafewcandidatesonahashtableontheGPU,and weonlyneedtoscanthehashtableoncetoobtainthequery result.Thereforethismajorbottleneckforsimilaritysearchon  theGPUcanbeovercome. WeoptimizedataplacementontheGPUtoimprovethe throughputofGENIE.Thenovelstructureofc-PQcanalso reducethememoryrequirementformultiplequeries.There- foreGENIEcansubstantiallyincreasethenumberofqueries  withinabatchontheGPU.Weproposeatailoredhashtable  ontheGPUtoreducehashBesides,toovercome  thelimitedmemorysizeoftheGPU,weintroduceamultiple  loadingstrategytoprocesslargedata. WedescribehowtoprocessdatausinganLSHscheme byGENIE.Weproposeanewconcept,calledTolerance-  ApproximateNearestNeighbour( ˝ -ANN)search,whichisin thesamespiritasthepopular c -ANNsearch.Thenweprove that,GENIEcansupportthe ˝ -ANNsearchforanysimilarity measurethathasagenericLSHscheme. ForcomplexdatatypeswithoutLSHtransformation,anoth- erchoiceistoadopttheSAschemetoprocessthedata.We  willshowcasethisbyperformingsimilaritysearchonsequence  data,shortdocumentdataandrelationaldatausingGENIE. Wesummarizeourcontributionsasfollows:  Weproposeagenericinvertedindexframework(GENIE) ontheGPU,whichcanabsorbtheburdenofparallel  GPU-basedimplementationofsimilaritysearchforany  datatypethatcanbeexpressedinthematch-countmodel.  WepresentthesystemdesignofGENIE.Especially,  wedevisethenoveldatastructurec-PQto increasethethroughputforqueryprocessingontheGPU.  WeexhibitanapproachtoadoptingLSHschemefor similaritysearchunderGENIE.Weproposethenew  conceptof ˝ -ANN,anddemonstratethatGENIEcan effectivelysupport ˝ -ANNsearchundertheLSHscheme.  Weshowcasethesimilaritysearchoncomplexdata  structuresbyGENIEundertheSAscheme.  Weconductcomprehensiveexperimentsondifferent  typesofreal-lifedatasetstodemonstratetheeffectiveness  andefyofGENIE. Weencloseallproofsandsupplementarymaterialsinour techniquereport[10]. II.P RELIMINARIESAND O VERVIEW Inthissection,wegiveanoverviewofGENIEincluding itsmainconceptsandcomputationalframework.Weuse  relationaldatashowninFig.1asarunningexample. A.Match-countmodel Givenauniverse U ,an object O i containsasetofelements in U ,i.e. O i = f o i; 1 ;:::;o i;r gˆ U .Asetofsuchdataobjects formsa dataset DS = f O 1 ;:::O n g .A query Q i isasetof items f q i; 1 ;:::;q i;s g ,whereeachitem q i;j isasetofelements from U ,i.e. q i;j ˆ U ( q i;j isasubsetof U ).A queryset is as QS = f Q 1 ;:::;Q m g . Fig.1.Anexampleonarelationaltable. Example2.1: Givenarelationaltable,theuniverse U isaset of orderedpairs ( d;v ) where d isanattributeofthistable and v isavalueofthisattribute.An l  dimensionalrelational tuple p =( v 1 ;:::;v l ) isrepresentedasanobject O = f ( d 1 ;v 1 ) ;:::; ( d l ;v l ) g .AsillustratedinFig.1,the O 1 inthere- lationaltableisrepresentedas O 1 = f ( A; 1) ; ( B; 2) ; ( C; 1) g . Aqueryontherelationaltableusuallyasetof ranges R =([ v L 1 ;v U 1 ] ,..., [ v L l ;v U l ]) .Thenitcanberepresented as Q = f r 1 ;r 2 ;::;r l g ,where r i =( d i ; [ v L i ;v U i ]) aset ofpairs ( d i ;v ) withvalue v 2 [ v L i ;v U i ] .Aswecanseefrom Fig.1,query Q 1 toretrievethetupleswithconditions 1  A  2 ; 1  B  1 and 2  C  3 canberepresentedas Q 1 = f ( A; [1 ; 2]) ; ( B; [1 ; 1]) ; ( C; [2 ; 3]) g . Informally,givenaquery Q andanobject O ,thematch- countmodel MC (  ;  ) returnsthenumberofelements o i 2 O containedbyatleastonequeryitemof Q .Wegiveaformal ofthematch-countmodelasfollows. 2.1(match-countmodel): Givenaquery Q = f r 1 ;r 2 ;::;r l g andanobject O = f o 1 ;:::;o s g ,wemapeach queryitem r i toanaturalintegerusing C :( r i ;O ) ! N , where C ( r i ;O ) returnsthenumberofelements o j 2 O containedbytheitem r i (whichisalsoasubsetof U ). Finallytheoutputofthematch-countmodelisthesumof theintegers MC ( Q;O )= P r i 2 Q C ( r i ;O ) .Forexample, inFig.1,for Q 1 and O 1 wehave C (( A; [1 ; 2]) ;O 1 )= 1 ;C (( B; [1 ; 1]) ;O 1 )=0 andC (( C; [2 ; 3]) ;O 1 )=0 ,then wehave MC ( Q 1 ;O 1 )=1+0+0=1 : InGENIE,weaimtorankalltheobjectsinadatasetwith respecttothequery Q accordingtothemodel MC (  ;  ) to obtainthetop- k objectsofquery Q . GENIEessentiallyisaninvertedindexontheGPUto efsupportthematch-countmodelbetweenobjects  andqueries.Fig.1showsanillustrationofsuchhighlevel  invertedindex.Weencodeattributesandallpossible  valuesasorderedpairs(continuousvaluedattributesare discretized).Thenweconstructaninvertedindexwherethe keyword isjusttheencodedpairandthe postingslist comprises allobjectshavingthiskeyword.Givenaquery,wecanquickly mapeachqueryitemtothecorrespondingkeywords(ordered pairs).Afterthat,byscanningthepostingslists,wecan  calculatethematchcountsbetweenthequeryandallobjects. B.GENIEwithLSHandSA Theinvertedindexwithmatch-countmodelhasthexi- bilitytosupportsimilaritysearchofmanydatatypes.Justas map-reducemodelcannothandleallcomputationtasks,wedo  notexpectthatalldatatypescanbesupported.However,at  leastmanypopulardatatypescanbesupportedwithLSHor  SAasweaddressfurtherbelow.Weillustratetherelationships  amongGENIE,LSHandSAinFig.2.Howtoorganizedata structuresasinvertedindexeshasbeenextensivelyinvestigated bypreviousliterature[2],[11],[12]anditisbeyondthescope  ofthispaper. 1)TransformedbyLSH: Themostcommondatatype,high dimensionalpoint,canbetransformedbyanLSHscheme  [6].Insuchascheme,multiplehashfunctionsareusedto  hashdatapointsintodifferentbucketsandpointsthatare  frequentlyhashedtothesamebucketaredeemedtobesimilar.  Hencewecanbuildaninvertedindexwhereeachpostingslist  correspondstoalistofpointshashedtoaparticularbucket. Givenaquerypoint,ANNsearchcanbeperformedby hashingthequerypointandthenscanningthecorresponding  postingslisttoretrievedatapointsthatappearinmanyofthese  buckets.Meanwhile,sets,featuresketchesandgeometries  typicallyhavekernelizedsimilarityfunctions[5],including Jaccardkernelforsets,RadialBasisFunction(RBF)kernel forfeaturesketches,andGeodesickernelforhyperplanes.We  presenttheindexbuildingmethodunderLSHschemeand  theoreticalanalysisinSectionIV. 2)TransformedbySA: Thedatawithcomplexstructure, includingdocuments,sequences,treesandgraphs,canbe  transformedwiththeSA[7],[8]scheme.,the  datawillbebrokendownintosmallersub-units(ﬁshotgunﬂ),  suchaswordsfordocuments,n-gramsforsequences[2],  binarybranchesfortrees[12]andstarsforgraph[11].After thedecomposition,wecanbuildaninvertedindexwitha postingslistforeachuniquesub-unit.Dataobjectscontaining  aparticularsub-unitarestoredinthepostingslist.Atquery  time,queryobjectswillalsobebrokendownintoasetof  smallsub-unitsandthecorrespondingpostingslistswillbe  accessedtodataobjectsthatsharecommonsub-unitswith thequeryobject.Thematch-countmodelreturnsthematching resultbetweenthequeryandobjectssharingkeywords,which  isanimportantintermediateresultforsimilaritysearch.This  approachhasbeenwidelyusedforsimilaritysearchofcom-  plexstructureddata[2],[12],[11].Morediscussionaboutit  ispresentedinSectionV. Fig.2.TherelationsamongGENIE,LSHandSA. III.I NVERTED I NDEXONTHE GPU Wepresenttheindexstructureandthedatawof GENIE.ThenwepresentCountPriorityQueue(c-PQfor  short),whichisapriorityqueue-likestructureontheGPU  memoryfacilitatingthesearch.Finally,weproposeamultiple loadingmethodtohandlelargedataset. A.Invertedindexandqueryprocessing Theinvertedindexisresidentintheglobalmemoryof theGPU.Fig.3illustratesanoverviewofsuchanindex structure.Allpostingslistsarestoredinalarge ListArray intheGPU'sglobalmemory.Thereisalsoa PositionMap in theCPUmemorywhichstoresstartingandendingpositions  ofeachpostingslistforeachkeywordintheListArray.When processingqueries,weusethePositionMaptolookupthe correspondingpostingslistaddressforeachkeyword.This  look-upoperationisonlyrequiredonceforeachqueryandour  experimentalsodemonstratesthatitstimecostisnegligible. Fig.3.Overviewoftheinvertedindexanddataw. Fig.3showstheprocessofmultiplequeriesontheGPU. Eachqueryhasasetofitemswhichparticularranges  onsomeattributes.Whenweinvokeaquery,weobtain  itspostingslists'addressesbythePositionMap,thenwe useoneblockoftheGPU(AblockontheGPUorganizes asmallbatchofthreads(upto2048)andcontrolsthe  cooperationamongthethreads.)toscanthecorresponding  postingslistsforeachqueryitem,wherethethreadsofeach  blockparallelaccesspartsofthepostingslists.Foraquery  Q i = f q i; 1 ;q i; 2 ;:::;q i;s g with s queryitems,ifthereare m queries,therewillbeabout m  s blocksworkingontheGPU inparallel.Duringtheprocess,afterscanninganobjectin  thepostingslist,eachthreadwillupdatethe CountTable to updatethenumberofoccurrencesoftheobjectinthescanned  postingslists.Therefore,thesystemworksina  mannertoprocessmultiplequerieswhichfullyutilizesthe parallelcomputationalcapabilityoftheGPU. Intheinvertedindex,theremaybesomeextremelylong postingslists,whichcanbeabottleneckforoursystem.We alsoconsiderhowtobalancetheworkloadforeachblock  bybreakinglongpostingslistsintoshortsub-lists.Werefer readersto[10]formoredetailsaboutloadbalancing. B.CountPriorityQueue Weproposeanoveldatastructure,calledCountPriority Queue(c-PQforshort)toreplacetheCountTableonthe GPU,whichaimstoimprovetheefyandthroughputof  GENIE.c-PQhastwostrongpoints:1)Thoughhowtoretrieve  thetop-kresultfromallcandidatesisthemajorbottleneckfor  similaritysearchontheGPU[4],c-PQcanthistask  withsmallcost;and2)c-PQcanreducethespace requirementofGENIE. OnemajorobstacleofGENIEishowtoselecttop- k countobjectsfromtheCountTable.Thisproblemisalso consideredamajorbottleneckforsimilaritysearchontheGPU inpreviousstudy[4].Itisdesirabletouseapriorityqueuefor  thisproblem.However,theparallelpriorityqueueusuallyhas  warpdivergenceproblemandirregularmemorymovement,  whichcannotrunefonGPUarchitectures[13]. Thekeyideaofc-PQistouseatwo-leveldatastructureto storethecountresults,anduseadevicetoschedulethedata  allocationbetweenlevels.Ournoveldesigncanguaranteethat  onlyafewofcandidatesarestoredintheupperlevelstructure whileallobjectsinlowerlevelstructurecanbeabandoned. Thenweonlyneedtoscantheupperlevelstructuretoselect  thequeryresult.Wewilldescribethestructureandmechanism  ofc-PQ,andprovealltheclaimsinTheorem3.1. AnothermajorproblemofGENIEisitslargespacecost, sincetheCountTablemustallocateintegertostorethecount  foreachobjectforeachquery.Takingadatasetwith10M  pointsasanexample,ifwewanttosubmitabatchofone thousandqueries,therequiredspaceoftheCountTableis about40GB(byallocatingoneintegerforcountvalue,thesize  is 1 k ( queries )  10 M ( points )  4( bytes )=40 GB ),which exceedsthememorylimitofthecurrentavailableGPUs. Toreducespacecost,wecanusebitmapstructureto avoidexplicitlystoringid.Second,weonlyneedtoallocate  several(insteadof32)bitstoencodethecountforeachobject  inbitmapstructure.Thereasonisthatthemaximumcountfor eachobjectisbounded(i.e.thereisamaximumvalueofthe count)sincethecountvaluecannotbelargerthanthenumber  ofpostingslistsintheindex.Actually,weusuallycaninfera  muchsmallercountboundthanthenumberofpostingslists.  Forexample,forhighdimensionalpoints,themaximumcount  valueisjustthenumberofitsdimensions. Fig.4.Anillustrationofthec-PQ. 1)Thestructureandmechanismofc-PQ: Fig.4shows themainstructuresofc-PQwhichhasthreecomponents.In  thelowerlevel,wecreatea BitmapCounter whichallocates severalbits(upto32bits)foreachobjectswhoseidis  correspondingtothebeginningaddressofthebits.Intheupper  level,thereisa HashTable whoseentryisapairofobject idanditscountvalue.Then,apivotaldevice,called Gate , determineswhichid-valuepairintheBitmapCounterwill  beinsertedintotheHashTable.TheGatehastwomembers:  a ZipperArray andathresholdcalled AuditThreshold .Inthe followingcontext,wedenotetheBitmapCounteras  BC ,theHashTableas HT ,theAuditThresholdas AT and theZipperArrayas ZA . TheGatehastwofunctions.First,onlyafewobjectsinthe BCcanpasstheGatetotheHT,whileallobjectsremaining  intheBCcannotbetop k objectsandthuscanbesafely abandoned.Second,theATintheGatejustkeepstrackofthe  thresholdforthetop- k result,andweonlyneedtoscanthe HToncetoselecttheobjectswithcounterlargerthan AT  1 astop- k results(SeeTheorem3.1). TheZAandATintheGateworktogethertorestrictobjects goingfromtheBCtotheHT.Thesizeofthe ZA inGate isequaltothemaximumvalueofthecount(basedonthe  countvaluebound). ZA[i] (ZAis1-basedindexingarray,i.e. theindexstartsfrom1)recordstheminimumvaluebetween  thenumberofobjectswhosecounthavereached i (denotedas zc i )andvalue k ,i.e. ZA [ i ]= min ( zc i ;k ) .The AT inGate recordstheminimumindexofZAwhosevalueissmallerthan  k (i.e. ZA [ AT ] <k and ZA [ AT  1]  k ). TheintuitionbehindthemechanismoftheGateisthat, iftherearealready k objectswhosecounthasreached i (i.e. ZA [ i ]== k )intheHT,thereisnoneedtoinsertmoreobjects whosecountislessorequalto i intotheHTsincethereare already k candidatesifthetop- k countthresholdisjust i . Therefore,the AT increaseby1when ZA [ AT ]== k . WepresenttheupdateprocessperthreadontheGPUof c-PQinAlgorithm1,whichisalsoillustratedinFig.4.For  eachquery,weuseoneblockoftheGPUtoparallelprocess  onequeryitem.Forallinvertedlistsmatchedbyaqueryitem,  thethreadsofthisblockaccesstheobjectsandupdatec-PQ  withAlgorithm1.Notethattheaddoperationisatomic.When thecountofanobjectisupdatedintheBC,weimmediately checkwhethertheobject'scountislargerthanthe AT (line 3).Ifitis,wewillinsert(orupdate)anentryintotheHT  whosekeyistheobjectidandwhosevalueistheobject's  count.Meanwhile,wewillupdatethe ZA (line5).If ZA[AT] islargerthan k ,wealsoincreasethe AT byoneunit(line7). Algorithm1: UpdateontheCountPriorityQueue // Forathreadinablock,itaccesses object O i intheinvertedindex,then makesfollowingupdates. 11 val i = BC [ O i ]+1 22 BC [ O i ]= val i 3 if val i  AT then 44 Putentry ( O i ;val i ) intotheHT 55 ZA [ val i ]+=1 6 while ZA [ AT ]  k do 77 AT +=1 WepresentTheorem3.1toelaboratethepropertiesofc-PQ. Theorem3.1: Afterscanningtheinvertedindexand updatingc-PQ,thetop- k candidatesarestoredintheHT,and thenumberofobjectsintheHTis O ( k  AT ) .Supposethe matchcountofthe k -thobject O k ofaquery Q is MC k = MC ( Q;O k ) ,thenwehave MC k = AT  1 . AccordingtoTheorem3.1,wecanselectthetop- k objects byscanningtheHTandselectingobjectswithmatchcount  greaterthan( AT  1 )only.Iftherearemultipleobjectswith matchcountequalto( AT  1 ),webreaktiesrandomly. Wegiveanexampletoshowupdateprocessofc-PQwith datainFig.1andtheresultshowninFig.4. Example3.1: Givenadataset f O 1 ;O 2 ;O 3 g andaquery Q 1 inFig.1,wewanttothetop-1resultof Q 1 fromthe objects,i.e. k =1 .Sincethenumberofattributesofthetableis 3 ,themaximumvalueofcountis 3 .Initiallywehave AT =1 , ZA =[0 ; 0 ; 0] , BC = f O 1 :0 ;O 2 :0 ;O 3 :0 g and HT = ; . Foreasyexplanation,weassumethepostingslistsmatched by Q 1 arescannedwiththeorderof ( A; [1 ; 2]) ; ( B; [1 ; 1]) and ( C; [2 ; 3]) .(OntheGPUtheyareprocessedwithmultiple blocksinparallelwithrandomorder.) AsshowninAlgorithm1,whenscanningthepostings list ( A; [1 ; 2]) ,westaccess O 1 andget BC ( O 1 )=1 . Since BC ( O 1 )  AT (=1) ,wehave HT ( O 1 )=1 and ZA [1]=0+1=1 (notethatZAis1-basedarray,thus aftertheupdating ZA =[1 ; 0 ; 0] ).Since ZA [ AT ]  k ( k =1 and AT =1 ),thenwehave AT =1+1=2 .Thenwe update BC ( O 2 )=1 and BC ( O 3 )=1 withoutchanging otherparameterssinceboththevaluesof O 2 and O 3 are smallerthan AT . Withthesamemethod,afterprocessing ( B; [1 ; 1]) ,weonly have BC ( O 2 )=2 , HT ( O 2 )=2 , ZA =[1 ; 1 ; 0] , AT =3 and BC = f O 1 :1 ;O 2 :2 ;O 3 :1 g . Thelastpostingslisttoprocessis ( C; [2 ; 3]) .Thereisno O 1 init.For O 2 ,wehave BC ( O 2 )=3 .Since BC ( O 2 )  AT , wehave HT ( O 2 )=3 , ZA =[1 ; 1 ; 1] and AT =4 .Wealso have BC ( O 3 )=2 . Finally,wehave HT = f O 1 :1 ;O 2 :3 g and AT =4 . ByTheorem3.1,weknowthatthecountoftop-1resultis3 (AT-1=4-1=3).Wecanthenscanthehashtable HT toselect theobjectequalto 3 whichisjust O 2 . 2)HashTablewithRobinHoodScheme: Here weexplainthedesignoftheHT.Weproposea  RobinHoodSchemetoimplementahashtable ontheGPUwhichisdifferentfromexistingwork[14].  AccordingtoTheorem3.1,thesizeoftheHTcanbesetas  O ( k  max count value ) .Weadoptalock-freesynchroniza- tionmechanismstudiedin[15]tohandletheracecondition  problem.MoredetailsabouttheHTcanbefoundin[10]. ThevitalinsighttoimprovetheefyoftheRobin HoodSchemeinthec-PQisthatallentrieswithvalues  smallerthan( AT  1 )intheHTcannotbetop- k candidates (seeTheorem3.1).Ifthevalueofanentryissmallerthan  ( AT  1 ),wecandirectlyoverwritetheentryregardlessof hashingThuswecanreducetheprobe  timesofinsertionoftheHT,sincemanyinsertedkeysbecome  expiredwiththeincreaseof AT . C.Indexinglargedatawithmultipleloadings Wealsodeviseamultipleloadingmethodtoincreasethe capacityofGENIEutilizingtheadvantageofthehighGPU  memorybandwidth.Theremaybesomecasesthatthedata  indexistoolargetobeintotheGPUmemory.Forthis  problem,wesplitthewholedatasetintoseveralparts,andthen buildinvertedindexforeachpartintheCPUmemory.When abatchofqueriesissubmitted,wetransfereachindexpart  intotheGPUmemoryinturn,andrunthequeryprocessing  introducedbefore.Afteraround,wecollectall  resultsfromeachpart,andmergethemtogetaquery results.SomenecessarycomputationisdoneintheCPUsuch asthetop- k resultsamongthetop- k resultsof eachdatapart.Fig.5illustratesthismultipleloadingmethod. Fig.5.AnillustrationofGENIEwithmultipleloadings. D.TheutilityoftheGPUforGENIE ThedesignofGENIEutilizesthepropertyoftheSIMD architectureandthefeaturesoftheGPUfromseveralper- spectives.First,GENIEdividesthequeryprocesstosuf  numberofsmalltaskstofullyutilizestheparallelcomputation poweroftheGPU.Weuseoneblocktohandleonequeryitem anduseeachthreadoftheblocktoprocessanobjectinthe  postingslist,therefore,thesimilaritysearchcanbeprocessed  inasmanneraspossible,sothatalltheprocessors  ontheGPUarehighlyutilized. Second,c-PQcanthetop-kselectiontaskwitha smallnumberofhomogenousoperations(scanningtheHash  Tableonlyonce)suitablefortheSIMDarchitecture.Thisdata  structureavoidstheexpensiveoperationslikesort[4],[9]and  datadependentmemoryassessmovementontheGPU[13]  fortop-kselection.Thispointisintheexperimentof  SectionVI-B1andSectionVI-C,withadiscussionaboutits competitorsinSectionVI-B4. Third,thedesignofGENIEtriestoperformcoalescing memoryaccessandcoherentbranching.Inc-PQ,mostopera- tionsarelimitedintheBCandonlyafewdataarepassedto  theHT,whichminimizesthebranchdivergence.Besides,since  weusemanythreadstoprocessapostingslist,thethreadshave coalescingmemoryaccesspatterns. Fourth,themultipleloadingmethodtakestheadvantageof thehighGPUmemorybandwidthwhichisusually5-10times higherthantheCPUmemorybandwidth.Ourexperimentin  SectionVI-B3alsodemonstratesthatsuchindextransferstep onlytakesaverysmallportionofthetotaltimecost. IV.G ENERIC ANNS EARCHWITH LSH WetshowthatGENIEcansupporttheANNsearchfor anysimilaritymeasurewhichhasanLSHscheme,followed byanerrorboundanalysisforANNsearchonGENIE. A.BuildingindexforANNsearchonGENIE Inthissection,weshowhowtouseGENIEtosupport similaritysearchafterprocessingdatabyLSHscheme. 1)ANNsearchwithLSHonGENIE: Accordingtothe in[5],ahashingfunction h (  ) issaidtobelocality sensitiveifit Pr [ h ( p )= h ( q )]= sim ( p;q ) (1) whichmeansthecollisionprobabilityisequaltothesimilarity measure.Here sim (  ;  ) isafunctionthatmapsapairofpoints toanumberin [0 ; 1] where sim ( p;q )=1 means p and q are identical.LSHisoneofthemostpopularsolutionsforthe  ANNsearchproblem[16],[5],[17]. Wecanusetheindexingmethodforrelationtableshown inFig.1tobuildinvertedindexforLSH.Wetreateach hashfunctionasanattribute,andthehashsignatureasthe valueforeachdatapoint.Thekeywordintheinvertedindex forpoint p underhashfunction h i (  ) isapair ( i;h i ( p )) andthepostingslistofthepair ( i;h i ( p )) isasetofpoints whosehashvalueby h i (  ) is h i ( p ) (i.e. h i ( p 0 )= h i ( p ) if p 0 and p inthesamepostingslist.).Givenaquerypoint q , wealsoconvert q withthesametransformationprocess,i.e. Q =[ h 1 ( q ) ;h 2 ( q ) ;:::;h m ( q )] . AswewillproveinSectionIV-B,thetopresultreturnedby GENIEaccordingtothematch-countmodelontheinverted  indexisjusttheANNsearchresult.Anysimilaritymeasure associatedwithanLSHfamilybyEqn.1canbe  supportedbyGENIE.ForANNsearchinhighdimensional  space,weusuallyresortto ( r 1 ;r 2 ;ˆ 1 ;ˆ 2 ) -sensitivehashing functionfamily.Wegiveaspecialdiscussionaboutitin[10].  Wewillalsoanalyzetheerrorboundbetweentheestimate ^ s andtherealsimilaritymeasure s = sim ( p;q ) inSectionIV-B. 2)Re-hashingforLSHwithlargesignaturespace: A possibleproblemisthatthehashsignatureofLSHfunctions mayhaveahugenumberofvalueswithacceptableerror byparameters.Forexample,thesignatureofthe  RandomBinningHashingfunctionintroducedlatercanbe  thousandsofbitsbysettinggoodparametersforsearcherror.  Butitisnotreasonabletodiscretizethehashsignatureintoa setofbucketsaccordingtotheofLSHinEqn.1. Toreducethenumberofpossiblesignatures,weproposea re-hashingmechanismillustratedinFig.6.Afterobtainingthe LSHsignature h i (  ) ,wefurtherprojectthesignaturesintoa smallsetofbucketswitharandomprojectionfunction r i (  ) . Thus,wecanconvertapointtoanobjectbythetransforma-  tion: O i =[ r 1 ( h 1 ( p i )) ;r 2 ( h 2 ( p i )) ;:::;r m ( h m ( p i ))] .Notethat re-hashingisnotnecessaryifthesignaturespaceofselected LSHcanbesmallenough. Fig.6.Re-hashingmechanism where h (  ) isaLSHfunctionand r (  ) isarandomprojectionfunction. Fig.7.Similarity(s)v.s.thenumber  ofminimumrequiredLSHfunctions (m)withconstraint Pr [ j c=m  s j  ]  1   where  =  =0 : 06 . 3)Casestudy:ANNinLaplaciankernelspace: Wetake theANNsearchonashift-invariantkernelspaceasacase study,whichhasimportantapplicationsformachinelearning. Theauthorsin[18]proposeanLSHfamily,calledRandom  BinningHashing(RBH),forLaplaciankernel k ( p;q )= exp ( k p  q k 1 =˙ ) .Thoughthismethodiswell-known fordimensionreduction,asfarasweknow,ithasnotbeen  appliedtoANNsearch.Onepossiblereasonisthatithasa hugehashsignaturespace.AbriefintroductiontoRBHcanbe foundin[10].Inexperiment,wedemonstratethatGENIEcan  supportANNsearchinLaplaciankernelspacebasedonRBH.  Toreducethehashsignaturespace,weusethere-hashing  mechanismtoprojecteachsignatureintoasetofbuckets. B.Theoreticalanalysis TointegrateLSHmethodsintoGENIErequiresatheoretical analysisforLSHundermatch-countmodel.Forthispurpose,  weproposearevisedoftheANNsearch,called  Tolerance-ApproximateNearestNeighborsearch( ˝ -ANN). 4.1( ˝ -ANN): Givenasetof n points P = f p 1 ;p 2 ;::;p n g inaspace S underasimilaritymeasure sim ( p i ;q ) ,the ˝ -ANNsearchreturnsapoint p suchthat j sim ( p;q )  sim ( p  ;q ) j ˝ withhighprobabilitywhere p  isthetruenearestneighbor. Thisconceptissimilartothepopularof c -ANN [16]whichistoapoint p sothat sim ( p;q )  c  sim ( p  ;q ) withhighprobability.Someexistingworks,like [19],havealsousedaconceptsimilartotion4.1though withoutexplicit 1)Errorboundand ˝ -ANN: Weprovethatthetopreturnof GENIEforaquery q isthe ˝ -ANNof q .Givenapoint p and aquery q withasetofLSHfunctions H = f h 1 ;h 2 ;:::;h m g , supposethereare c functionsin H satisfying h i ( p )= h i ( q ) (where c isjustthereturnofmatch-countmodel).Weprovein Theorem4.1thatthereturnofmatch-countmodelonGENIE  canbeprobabilisticallyboundedw.r.tthesimilaritybetween  p and q ,i.e. j c=m  sim ( p;q ) j < withhighprobability. Theorem4.1: Givenasimilaritymeasure sim (  ;  ) ,anLSH family h (  ) ,wecangetanewhashfunction f ( x )= r ( h ( x )) , where r (  ) isarandomprojectionfunctionfromLSHsignature toadomain R : U ! [0 ;D ) . Forasetofhashfunctions f i (  )= r i ( h i (  )) ; 1  i  m with m =2 ln (3  )  2 ,wecanconvertapoint p and aquery q toanobjectandaqueryofthematch-count model,whichare O p =[ f 1 ( p ) ;f 2 ( p ) ;:::;f m ( p )] and Q q = [ f 1 ( q ) ;f 2 ( q ) ;:::;f m ( q )] ,thenwehave j MC ( Q q ;O p ) =m  sim ( p;q ) j < +1 =D withprobabilityatleast 1   . Nowweintroduceanimportanttheoremwhichclaimsthat, givenaquerypoint q andproperof m stated inTheorem4.1,thetopresultreturnedbyGENIEisjustthe ˝ -ANNof q . Theorem4.2: Givenaquery q andasetofpoints P = f p 1 ;p 2 ;::;p n g ,wecanconvertthemtotheobjectsof ourmatch-countmodelbytransformation O p i =[ r 1 ( h 1 ( p i )) , r 2 ( h 2 ( p i )) ;:::;r m ( h m ( p i ))] which j MC ( Q q ;O p i ) =m  sim ( p i ;q ) j  withtheprobabilityat least 1   .SupposethetrueNNof q is p  ,andthetop resultbasedonthematch-countmodelis p ,thenwehave j sim ( p  ;q )  sim ( p;q ) j 2  withprobabilityatleast 1  2  . 2)Numberofhashfunctionsinpractice: Theorem4.1 providesaruletosetthenumberofLSHfunctionsas O ( 1  2 ) whichmaybeverylarge.ItisNOTaproblemforGENIEto supportsuchanumberofhashfunctionssincetheGPUisa  parallelarchitecturesuitableforthemassivequantityofsimple  tasks.Thequestionhoweveristhat:Dowereallyneedsuch  alargenumberofhashfunctionsinpracticalapplications? Beforeexploitingthis,weexplainthatthecollision probabilityofahashfunction f i (  ) canbeapproximatedwith thecollisionprobabilityofanLSHfunction h i (  ) if D islarge enough.Thecollisionprobabilityof f i (  ) canbefactorizedas collisionscausedby h i (  ) andcollisionscausedby r i (  ) : Pr [ f i ( p )= f i ( q )]= Pr [ r i ( h i ( p ))= r i ( h i ( q ))] (2)  Pr [ h i ( p )= h i ( q )]+ Pr [ r i ( h i ( p ))= r i ( h i ( q ))] (3) = s +1 =D (4) where s = sim ( p;q ) .Thus,wehave s  Pr [ f i ( p )= f i ( q )]  s +1 =D .Suppose r (  ) canre-hash h i (  ) intoavery largedomain [0 ;D ) ,wecanclaimthat Pr [ f i ( p )= f i ( q )] t s . Forsimplicity,letusdenote c = MC ( Q q ;O p ) .Anestimation of s bymaximumlikelihoodestimation(MLE)canbe[19]: s = MC ( Q q ;O p ) =m = c=m (5) Eqn.5canbefurtherbythefollowingequation: Pr [ j c m  s j  ]= Pr [( s   )  m  c  ( s +  )  m ] (6) = d ( s +  ) m e X c = b ( s   ) m c  m c  s c (1  s ) m  c (7) Eqn.7showsthattheprobabilityoferrorbounddependson thesimilaritymeasure s = sim ( p;q ) [19].Therefore,thereis noclosed-formexpressionforsucherrorbound. Nevertheless,Eqn.7providesapracticalsolutiontoesti- mateatightererrorboundofthematch-countmodel.Ifwe ed  and  ,forasimilaritymeasure s ,wecaninferthe numberofrequiredhashfunctions m subjecttotheconstraint Pr [ j c=m  s j  ]  1   accordingtoEqn.7.Fig.7 visualizesthenumberofminimumrequiredLSHfunctionsfor differentsimilaritymeasurewithrespecttoaedparameter   =  =0 : 06 bythismethod.Asimilarhasalsobeen illustratedin[19].Fig.7showsthatthelargestnumberof  hashfunctions,beingm=237,appearsat s =0 : 5 ,whichis muchsmallerthantheoneestimatedbyTheorem4.1(which is m =2 ln (3  )  2 =2174 ).Weshouldnotethattheresult showninFig.7isdataindependent.Thus,insteadofusing Theorem4.1,wecaneffectivelyestimatetheactuallyrequired numberofLSHfunctionsusingthesimulationresultbasedon Eqn.7(likeFig.7). V.S EARCHINGONDATAWITH SA GENIEalsoprovidesachoiceofadoptingtheﬁShotgun andAssemblyﬂ(SA)schemeforsimilaritysearch.Givena dataset,wespliteachobjectintosmallunits.Thenwebuild invertedindexwhereeachuniqueunitisakeyword,andthe  correspondingpostingslistisalistofobjectscontainingthis  uniqueunit.Whenaquerycomes,itisalsobrokendownas  asetofsuchsmallunits.Afterthat,GENIEcaneffectively  calculatethenumberofcommonunitsbetweenthequery objectanddataobjects. Thereturnofmatch-countmodelcaneitherbeasimilarity measure(e.g.documentsearchwherethecountisjustthe  innerproductbetweenthespacevectorofdocuments),orbe consideredasalowerboundofadistance(e.g.editdistance)to  candidates[2],[12].Wewilldemonstratehowtoperform  similaritysearchonsequencedata,shortdocumentdataand  relationaldatausingGENIE. A.Searchingonsequencedata Inthissection,weshowhowtouseGENIEtosupport similaritysearchbytheSAschemewithanexampleof  sequencesimilaritysearchundereditdistance. 1)ShotgunŒdecompositionandindex: Wedecompose thesequence S intoasetof n -gramsusingalength- n sliding window.Givenasequence S andaninteger n ,the n -gramis alength- n subsequence s of S .Sincethesame n -grammay appearmultipletimesinasequence,weintroducethe ordered n -gram ,whichisapair( n -gram, i )where i denotesthe i -th same n -graminthesequence.Therefore,wedecomposethe sequence S intoasetofordered n -gram G ( S ) .InGENIE, webuildaninvertedindexbytreatingtheordered n -gramas akeywordandputtingitssequenceidinthepostingslist. Example5.1: Forasequence S = f aabaab g , thesetof ordered3-grams of S is G ( S )= f ( aab; 0) ; ( aba; 0) ; ( baa; 0) ; ( aab; 1) g where ( aab; 0) denotes thestsubsequence aab in S ,and ( aab; 1) denotesthe secondsubsequence aab in S . 2)AssemblyŒcombinationand Duringquery process,wealsodecomposeaquerysequence Q intoasetof ordered n -grams usingslidingwindows,i.e. G ( Q ) .GENIE canretrievecandidateswithtop- k largecountintheindex. Weintroducethefollowinglemma: Lemma5.1: Supposethesame n -gram s i n appears c i  s times insequence S and c i  q timesinsequence Q ,thentheresult returnedbythematch-countmodelis MC ( G ( S ) ;G ( Q ))= P s i n min f c i s ;c i  q g . Withrespecttotheeditdistance,theresultofthematch- countmodelthefollowingtheorem. Theorem5.1: Iftheeditdistancebetween S and Q is ˝ ,then thereturnofthematch-countmodelhas MC ( G ( S ) ;G ( Q ))  max fj Q j ; j S jg n +1  ˝  n .[20] AccordingtoTheorem5.1,wecanusetheresultofthe match-countmodelasanindicatorforselectingcandidates forthequery.Ourstrategyistoretrieve K candidatesfrom GENIEaccordingtomatchcountwithalarge K ( K >>k ) . Thenwecanemployavprocesstocalculatethe  editdistancebetweenthequery Q andthe K candidatesto obtainthe k -thmostsimilarsequence S k 0 .Thedetailofthe vprocessisshowninAlgorithm2in[10]. Withthismethod,wecanknowwhethertherealtop- k sequencesarecorrectlyreturnedbyGENIE,thoughwe cannotguaranteethereturnedtop- k candidatesarethereal top- k datasequenceforallqueries.Inotherwords,afterthe vwecanknowwhether S k 0 isthereal k -thmost similarsequenceofQaccrodingtothefollowingtheorem. Theorem5.2: Forthe K -thcandidates S K returnedby GENIEaccordingtocount,supposethematchcountbetween  S K andquery Q is c K = MC ( G ( S K ) ;G ( Q )) .Amongthe K candidates,afteremployingthealgorithm,wecan  obtaintheeditdistancebetween k -thmostsimilarsequence (amongthe K candidates)and Q is ˝ k 0 = ed ( Q;S k 0 ) .If c K < j Q j n +1  ˝ k 0  n ,thentherealtop- k resultsare correctlyreturnedbyGENIE. Apossiblesolutionforsequencesimilaritysearchisto repeatthesearchprocessbyGENIEwithlarger K ,untilthe conditioninLemma5.2isIntheworstcaseitmay needtoscanthewholedatasetbeforeretrievingtherealtop-  k sequencesundereditdistance.However,asshowninour experiment,itcanworkwellinpracticeforneareditdistance  similaritysearchinsomeapplications. B.Searchingonshortdocumentdata Inthisapplication,bothquerydocumentsandobjectdoc- umentsarebrokendownintoﬁwordsﬂ.Webuildaninverted indexwithGENIEwherethekeywordisaﬁwordﬂfromthe  document,andthepostingslistisalistofdocumentids. WecanexplaintheresultreturnedbyGENIEonshortdoc- umentdatabythedocumentvectorspacemodel.Documents  canberepresentedbyabinaryvectorspacemodelwhereeach  wordrepresentsaseparatedimensioninthevector.Ifaword occursinthedocument,itsvalueinthevectorisone,otherwise itiszero.Theoutputofthematch-countmodel,whichisthe  numberofwordco-occurringinboththequeryandtheobject,  isjustthe innerproduct betweenthebinarysparsevectorof thequerydocumentandtheoneoftheobjectdocument. C.Searchingonrelationaldata GENIEcanalsobeusedtosupportqueriesonrelational data.InFig.1,wehaveshownhowtobuildaninverted indexforrelationaltuples.Forattributeswithcontinuous  value,weassumethattheycanbediscretizedtoanacceptable granularitylevel.Arangeselectionqueryonarelationaltable isasetofrangesonattributesoftherelationaltable. Thetop- k resultreturnedbyGENIEonrelationaltables canbeconsideredaspecialcaseofthetraditionaltop- k selectionquery.Thetop- k selectionqueryselectsthe k tuples inarelationaltablewiththelargestrankingscore  function F (  ) (SQL ORDERBY F (  ) ).InGENIE,weuse aspecialrankingscorefunctionbythematch-count  model,whichisespeciallyusefulfortableshavingboth  categoricalandnumericalattributes. VI.E XPERIMENTS A.Settings 1)Datasets: Weusevereal-lifedatasetstoevaluateour system.Eachdatasetcorrespondstoonesimilaritymeasure  respectivelyintroducedinSectionIVandSectionV. [ OCR ] 3 Thisisadatasetforopticalcharacterrecognition.It contains3.5Mdatapointsandeachpointhas1156dimensions.  Thesizeofthisdatasetis3.94GB.Werandomlyselect10K  pointsfromthedatasetasquery/testset(andremovethem  fromthedataset).WeuseRBHtogeneratetheLSHsignature, whichisfurtherre-hashedintoanintegerdomainof[0,8192). [ SIFT ] 4 Thisdataset[21]contains4.5MSIFTfeatures whichare128-dimensionalpoints.Itstotalsizeis1.49GB. Werandomlyselect10Kfeaturesasquerysetandremove themfromthedataset.Weselectthehashfunctionsfrom E2LSHfamily[6]andeachfunctiontransformsafeatureinto 3 http://largescale.ml.tu-berlin.de/instructions/ 4 http://lear.inrialpes.fr/ ˘ jegou/data.php 67buckets.Thesettingofbucketwidthfollowstheroutinein [6].WeusethisdatasettoevaluatetheANNsearchinhigh dimensionalspace. [ SIFT LARGE ] 5 Toevaluatethescalabilityofoursystem, wealsoextract36millionsSIFTfeaturesbyourselvesfrom  theILSVRC-2010imagedataset.Thesizeofthisdatasetis  14.75GB.Weusethesamemethodasdescribedabovefor  SIFTtoprocessthedata. [ DBLP ] 6 Thisdatasetisobtainedbyextractingarticletitles fromtheDBLPwebsite.Thetotalnumberofsequencesis  5.0Mandthesizeofthisdatasetis0.94GB.Werandomly  choose10Ksequencesasatestdata,andthenmodify20%  ofthecharactersofthesequences.Thisdatasetistoserve theexperimentofsequencesimilaritysearchinSectionV-A. Specially,weset K =32 and k =1 bydefault. [ Tweets ] 7 Thisdatasethas6.8Mtweets.Weremovestop wordsfromthetweets.Thedatasetiscrawledbyourcollab-  oratorsfromTwitterforthreemonthsbykeepingthetweets  containingasetofkeywords. 8 Thedatasizeis0.46GB.We reserve10Ktweetsasaqueryset.Itisusedtostudytheshort  documentsimilaritysearch(seeSectionV-B). [ Adult ] 9 Thisdatasethascensusinformation[22]which contains49Krowswith14attributes(mixedofnumerical  andcategoricalones).Fornumericaldata,wediscretizeall valueinto1024intervalsofequalwidth.Wefurtherdupli- cateeveryrow20times.Thus,thereare0.98Minstances  (withsizebeing5.8GB).Weselect10Ktuplesasqueries.  Fornumericalattributes,thequeryitemrangeisas  [ discretized value  50 ;discretized value +50] .Weuseit tostudytheselectionfromrelationaldata(seeSectionV-C). 2)Competitors: Weusethefollowingcompetitorsasbase- linestoevaluatetheperformanceofGENIE. [ GPU-LSH ] WeuseGPU-LSH[4],[23]asacompetitor ofGENIEforANNsearchinhighdimensionalspaceandits  sourcecodeispubliclyavailable 10 .Furthermore,sincethere isnoGPU-basedLSHmethodforANNsearchinLaplacian  kernelspace,westilluseGPU-LSHmethodasacompetitor  forANNsearchofGENIE.Wetheparametersof GPU-LSHtomakesureitsANNsearchresultshavesimilar qualityasGENIE,whichisdiscussedinSectionVI-D1.We  onlyuse1MdatapointsforGPU-LSHonOCRdatasetsince  GPU-LSHcannotaffordmoredatapoints. [ GPU-SPQ ] Weimplementedapriorityqueue-likemethod onGPUasacompetitor.Wescanthewholedatasetto computematch-countvaluesbetweenqueriesandallpoints, andstorethesecomputedresultsinanarray.Thenweuse  aGPU-basedfastk-selection[9]methodtoextractthetop-  k candidatesfromthearrayforeachquery.Wenamethis top- k calculationmethodasSPQ(whichdenotesGPUfast 5 http://image-net.org/challenges/LSVRC/2010/index#data 6 http://dblp.uni-trier.de/xml/ 7 https://dev.twitter.com/rest/public 8 ThekeywordsincludeﬁSingaporeﬂ,ﬁCityﬂ,ﬁfoodjointﬂandﬁrestaurantﬂ,etc.Itis crawledforaresearchproject. 9 http://archive.ics.uci.edu/ml/datasets/Adult 10 http://gamma.cs.unc.edu/KNN/ k-s electionfromanarrayasap riorityq ueue).Wegivean introductiontoSPQin[10].NotethatforANNsearch,we scanontheLSHsignatures(notoriginaldata). [ CPU-Idx ] WeimplementedaninvertedindexontheCPU memory.Whileaccessingtheinvertedindexinmemory,we useanarraytorecordthevalueofmatch-countmodelforeach  object.Thenweuseapartialquickselectionfunction(with   n + klogn ) worst-caseperformance)inC++STLtogetthe k largest-countcandidateobjects. [ CPU-LSH ] WeuseCPU-LSH[24]forANNsearchinhigh dimensionalspaceasacompetitorafterobtainingitssource  codefromauthors'website 11 .Weusethesuggestionmethod inthepapertodeterminetheparameters. [ AppGram ] Thisisoneofthestate-of-the-artmethodsfor sequencesimilaritysearchundereditdistanceontheCPU[2].  WeuseAppGramasabaselineforcomparingtherunningtime  ofGENIEforsequencesimilaritysearch.NotethatAppGram  andGENIEarenotcompletelycomparable,sinceAppGram  triesitsbesttothetruekNNs,whileGENIEonlydoes  oneroundsearchprocessintheexperiment.Thussometrue kNNsmaybemissed(thoughweknowwhichqueriesdonot havetruetop- k ,andanothersearchprocesscanbeissuedto explorethetruekNNs).Wegivemorediscussionaboutthis  inSectionVI-D2. [ GEN-SPQ ] ThisisavariantofGENIEbutusingSPQ insteadofc-PQ.WestillbuildinvertedindexontheGPU  foreachdataset.However,insteadofusingc-PQ(seeSection  III-B),weuseSPQ(whichisthesamewiththeoneforGPU- SPQ)toextractcandidatesfromtheCountTable. 3)Environment: WeconductedtheexperimentsonaCPU- GPUplatform.TheGPUusedisNVIDIAGeForceGTX TITANXwith12GBmemory.GPUcodeswereimplemented inCUDA7.OtherprogramswereinC++onCentOS6.5 server(with64GBRAMandtheCPUofIntelCorei7-3820). Unlessotherwiseweset k =100 andsetthe submittedquerynumberperbatchtotheGPUas1024.All  thereportedresultsaretheaverageofrunningresultsoften  times.Bydefault,wedonotenabletheloadbalancingfunction  sinceitisnotnecessarywhenthequerynumberislarge foronebatchprocess(theexperimentaboutloadbalancing canbefoundin[10]).ForANNsearch,weusethemethod  introducedinSectionIV-B1todeterminethenumberofLSH  hashfunctionswithsetting  =  =0 : 06 ,thereforethenumber ofhashfunctionsis m =237 . B.EfofGENIE 1)Searchtimeformultiplequeries: Wecomparetherun- ningtimeamongGENIEanditscompetitors.Wedonot  includeindexbuildingtimeforallcompetitorssinceindex  buildingcanbedoneofTheindexbuildingtimeof GENIEisdiscussedinSectionVI-B2. Weshowthetotalrunningtimewithrespecttodifferent numbersofqueriesinFig.8(y-axisislog-scaled).Ourmethod outperformsGPU-SPQbymorethanoneorderofmagnitude,  anditcanachievemorethantwoordersofmagnitudeover 11 http://ss.sysu.edu.cn/ ˘ fjl/c2lsh/C2LSH Source Code.tar.gz TABLEI T IMEPROFILINGOFDIFFERENTSTAGESOF GENIE FOR 1024 QUERIES ( THEUNITOFTIMEIS second ). Stage OCR SIFT DBLP Tweets Adult Indexbuild 81.39 47.73 147.34 12.10 1.06 Indexloading 0.53 0.34 0.20 0.088 0.011 Query transfer 0.015 0.018 0.0004 0.0004 0.0004 match 2.60 7.04 0.85 1.19 1.82 selection 0.004 0.003 0.11* 0.003 0.009 *Thisincludesvtimewhichisthemajorcost. GPU-SPQandAppGramforsequencesearch.Furthermore, GPU-SPQcanonlyrunlessthan256queriesinparallel (exceptforAdultdataset)foronebatchprocess,butGENIE  cansupportmorethan1000queriesinparallel. AswecanseefromFig.8,GENIEcanalsooutperform GPU-LSHaboutoneorderofmagnitude.Therunningtimeof  GPU-LSHisrelativelystablewithvaryingnumbersofqueries.  ThisisbecauseGPU-LSHusesonethreadtoprocessone  query,thus,GPU-LSHachievesitsbestperformancewhen thereare1024queries(whichisthemaximumnumberof threadsperblockontheGPU).Notethatweonlyuse1M  datapointsforGPU-LSHonOCRdataset. Fig.9conveystherunningtimeofGENIEanditscom- petitorswithvaryingnumbersofdatapointsforeachdataset.  Sincemostofthecompetitorscannotrun1024queriesforone  batch,wethequerynumberas512inthisexperiment.The  runningtimeofGENIEisgraduallyincreasedwiththegrowth  ofdatasize.Nevertheless,therunningtimeofGPU-LSHis relativelystableonalldatasetswithrespecttothedatasize. ThepossiblereasonisthatGPU-LSHusesmanyLSHhash  tablesandLSHhashfunctionstobreakthedatapointsinto  shortblocks,therefore,thetimeforaccessingtheLSHindex  ontheGPUbecomesthemaincostofqueryprocessing. Fig.11showstherunningtimeofGENIEandGPU-LSHfor alargernumber(upto65536)ofqueriesonSIFTdata.Though  GPU-LSHcansupporttenthousandsofqueriesperbatch,  GENIEcanalsosupportthesuchlargenumberofqueries withbreakingquerysetintoseveralsmallbatches.Withsetting 1024queriesasabatchforGENIE,wecanseethatthetime  costofGPU-LSHtoprocess65536querieswithonebatchis  1329seconds,whileGENIEcanprocessthesamenumberof  queries(with64batches)in441seconds. 2)Timepr TableIshowsthetimecostfordifferent stagesofGENIE.TheﬁIndex-buildﬂrepresentstherunning timetobuildtheinvertedindexontheCPU.Thisisanone- timecost,andwedonotcountitinthequerytime.The  ﬁIndex-loadingﬂdisplaysthetimecosttoswaptheinverted  indexfromtheCPUtotheGPU.TherowsofﬁQueryﬂdisplay  thetimeforsimilaritysearchwith1024queriesperbatch.  TheﬁQuery-transferﬂisthetimecosttotransferqueriesand otherinformationfromtheCPUtotheGPU.TheﬁQuery- selectionﬂcontainsthetimeforselectingcandidatesfromc-PQ  andsendingbackthecandidatestotheCPU(ForDBLPdata,  italsoincludesthetimeofvTheﬁQuery-matchﬂ  isthetimecostforscanninginvertedindexwhichdominates  thecostforsimilaritysearch.Thisourdesignchoice ofusingGPUtoacceleratethistask. (a)OCR(b)SIFT(c)DBLP(d)Tweets(e)Adult Fig.8.Totalrunningtimeformultiplequeries. (a)OCR(b)SIFT(c)DBLP(d)Tweets(e)Adult Fig.9.Varyingdatasizeformultiplequeries(Thequerynumberis512). TABLEII R UNNINGTIMEOF GENIE WITHMULTIPLELOADINGSON SIFT LARGE DATASETFOR 1024 QUERIES ( UNIT : SECOND ) SIFT LARGE 6M 12M 24M 36M GENIE 4.32 8.62 17.26 25.90 GPU-LSH 47.71 48.82 (97.64)* (146.46)* CPU-LSH 2817 5644 12333 20197 *Itisestimatedwithmultipleloadingmethod. TABLEIII E XTRARUNNINGTIMECOSTOF GENIE WITHMULTIPLE LOADINGSWITHTHESAMESETTINGOF T ABLE II( UNIT : SECOND ) SIFT LARGE 6M 12M 24M 36M Indexloading 0.50 1.03 2.01 3.02 Resultmerging 0 0.04 0.10 0.22 GENIE total 4.32 8.62 17.26 25.90 3)Searchingonlargedatawithmultipleloadings: Ifthe datasetistoolargetobeprocessedwithlimitedGPUmemory, weadoptamultipleloadingmethod(seeSectionIII-D).Table IIshowsthescalabilityofGENIEwithdifferentdatasizeson SIFT LARGEdataset.Inthisexperiment,wesetthedatapart foreachloadingas6Mdatapoints.Byresortingtomultiple  loadings,GENIEcanthequeryprocessfor1024queries  with25.90secondson36MSIFTdatapoints.Italsoshows  thatGENIEwithmultipleloadingscanscaleuplinearlywith thenumberofdatapoints.SinceGPU-LSHcannothandle datasetswithlargerthan12Mpoints,weestimatetherunning  timeofGPU-LSHon24Mand36Mdatapointswiththesame  multipleloadingmethodbutwithoutincludingindexloading  andresultmergetime.WecanseethatGPU-LSHhasalmost sixtimesofrunningtimeofGENIEforthesamedataset. GENIEwithmultipleloadingshastwoextrasteps:1)index loading:swappingindexofeachdatapartintotheGPU memoryand2)resultmerging:mergingthequeryresultsof  eachdataparttoobtaintheresult.Therunningtimecost  ofeachextrastepisshowninTableIII.Wecanseethatthe  extrastepsonlytakeasmallportionofthetotaltimecost. 4)Discussion: Herewegiveabriefdiscussionaboutthe rootcausesthatGENIEoutperformsothermethods.Itisnot  surprisingthatGENIEcanoutperformalltheCPU-basedal-  gorithmslikeCPU-LSH,CPI-IdxandAppGram. ThekeyreasonthatGENIEcanoutperformGPU-LSHis TABLEIV M EMORYCONSUMPTIONPERQUERY ( UNIT :MB) dataset OCR SIFT DBLP Tweets Adult GENIE 4.9 6.5 7.2 10.2 1.4 GEN-SPQ 41.0 49.1 47.3 61.4 8.8 duetothenovelstructureofc-PQforcandidateselection.The  mainbottleneckofGPU-LSHistoselecttop-kcandidates  fromthecandidatesetgeneratedbyLSH,whosemethod  essentiallyistosortallcandidateswhichisanexpensive  computation.Meanwhile,c-PQcanobtainthecandidatesby  scanningthesmallHashTableonce. GENIEoutperformsGPU-SPQwithsimilarreasons.GPU- SPQusesak-selectionalgorithmontheGPUwhichrequires  multipleiterationstoscanallcandidates.WhereasGENIEonly  needstoscantheHashTableoncewhosesize(whichis O ( k  AT ) )ismuchsmallerthanthecandidatesetofGPU-SPQ. C.Effectivenessofc-PQ c-PQcanreducethememoryrequirementand therunningtimecostforGENIE.InFig.10,GEN-SPQ  representstherunningtimeofGENIEwithoutc-PQ.Wecan  seethat,whenthenumberofqueriesisthesame,withthehelp  ofc-PQtherunningtimeofGENIEdecreases sinceitavoidsselectingcandidatesfromalargeCountTable. FromTableIVwecanseethatGENIEreducesmemory consumptionperqueryto 1 = 5 s 1 = 10 oftheoneofGEN- SPQ.Toevaluatethememoryconsumption,withthe  datasize,wegraduallyincreasethenumberofqueriesto themaximumquerynumberhandledbyourGPU,thenwe  calculatethememoryconsumptionperquerybyusing12GB  todividethemaximumquerynumber. D.EffectivenessofGENIE Inthissection,weevaluatetheeffectivenessofGENIE undertheLSHschemeandtheSAscheme. 1)ANNSearchwithGENIE: Herewediscussthequality oftheANNsearchwithGENIEandGPU-LSH.Theused evaluationmetricis approximationratio ,whichis ashowmanytimesfartherareportedneighboriscompared  totherealnearestneighbor.Whentoevaluatetherunning  time,wetheparametersofGPU-LSHandGENIE toensurethattheyhavesimilarapproximationratio.Adetailed (a)OCR(b)SIFT(c)DBLP(d)Tweets(e)Adult Fig.10.Theeffectivenessofc-PQ. TABLEV P REDICTIONRESULTOF OCR DATABY 1NN method precision recall F1-score accuracy GENIE 0.8446 0.8348 0.8356 0.8374 GPU-LSH 0.7875 0.7730 0.7738 0.7783 descriptionofapproximationratioandtheparametersetting methodcanbefoundin[10]. Fig.11.Runningtimewithalarge numberofqueriesonSIFTdata Fig.12.Approximationratiov.s.  valueofkonSIFTdata Fig.12showstheapproximationratioofGPU-LSHand GENIE,whereGENIEhasstableapproximationratiowith varying k ;whereasGPU-LSHhaslargeapproximationratio when k issmall.TheincreaseofapproximationratioofGPU- LSHwithsmaller k isacommonphenomenonwhichalso appearsinsomeperviousLSHmethodslike[25].Thereason  isthatthesemethodsusuallyadoptsomeearly-stopconditions,  thuswithlarger k theycanaccessmorepointstoimprovethe approximationratio[25]. Weuseasimilarmethodtodeterminetheparametersfor GPU-LSHandGENIEontheOCRdataset.GPU-LSHuses  GPU'sconstantmemorytostorerandomvectorsforLSH.  Thus,thenumberofhashfunctionsonOCRdatacannotbe largerthan8otherwisetheconstantmemoryovws.Weuse only1MdatapointsfromtheOCRdatasetforGPU-LSHsince  itcannotworkonalargerdataset.Weincreasethenumber  ofhashtables(withthenumberofhashfunctions  as8)untilitcanachievesimilarpredictionperformanceas GENIEasreportedinTableVwherethenumberofhash tablesforGPU-LSHissetas100.Notethattheprediction  performanceofGPU-LSHisslightlyworsethantheoneof  GENIE.ItispossibletoimprovetheperformanceofGPU-  LSHbyincreasingthenumberofhashtables,whichwill  dramaticallyincreasetherunningtimeforqueries. 2)SequencesimilaritysearchwithGENIE: After thesearchonGENIE,wecanidentifythatsomequeriesdo  notobtainrealtop- k results(seeSectionV-A).TableVIshows thepercentofthequeriesobtainingcorrecttop-1searchresults  withoneroundofthesearchprocessfor1024queries.Aswe  seefromTableVI,withlessthan10%GENIE  canreturncorrectresultsforalmostallqueries.Evenwith40%  GENIEcanstillreturncorrectresultsformore than95%ofqueries.Atypicalapplicationofsuchsequence TABLEVI A CCURACYOFTOP -1 SEARCHON DBLP DATASETON GENIE ( QUERYLENGTH =40 AND K =32 ) Percentof 0.1 0.2 0.3 0.4 Accuracy 1.0 0.999 0.995 0.954 Latencytiem(s) 1.3 1.2 1.1 1.2 similaritysearchis(typing)sequenceerrorcorrection,where  GENIEcanreturnthemostsimilarwords(withinminimum editdistanceinadatabase)forthe1024querieswitha  latencytimeof1second(asshowninFig.8andTableI).  Notethattheonesecondisthewholelatencytimefor1024  queriesincludingindexloading,querytransfer,matchingin  GENIEandvAppGrammaydothisjobwithbetter accuracy,butithasmuchlargerlatency.Adiscussionon multipleroundsearchandhowtoset K isin[10]. VII.R ELATEDWORK A.Similaritysearchondifferentdata Duetotheﬁcurseofdimensionalityﬂ,spatialindexmethods providelittleimprovementoveralinearscanalgorithmwhen  dimensionalityishigh.Itisoftenunnecessarytotheexact  nearestneighbour,leadingtothedevelopmentofLSHscheme  forANNsearchinhighdimensionalspace[16].Werefer  interestedreaderstoasurveyofLSH[17]. Thesimilaritybetweensets,featuresketchesandgeometries isoftenknownonlyimplicitly,thusthecomputablekernel  functionisadoptedforsimilaritysearch.Toscaleupthe  similaritysearch,theLSH-basedANNsearchinsuchkernel  spaceshasdrawnconsiderableattention.Charikar[5]inves-  tigatesseveralLSHfamiliesforkernelizedsimilaritysearch.  Wangetal.[17]giveagoodsurveyabouttheLSHschemeon differentdatatypes.GENIEcansupportthesimilaritysearch inanarbitrarykernelspaceifithasanLSHscheme. Thereisawealthofliteratureconcerningsimilaritysearch oncomplexstructureddata,andalargenumberofindexes  havebeendevised.ManyofthemadopttheSAscheme[7],  [8].Differentdatatypesarebrokendownintodifferenttypes  ofsub-units.Examplesincludewordsfordocuments,n-grams forsequences[2],binarybranchesfortrees[12]andstars forgraphs[11].Sometimes,avstepisnecessary  tocomputetherealdistance(e.g.editdistance)betweenthe  candidatesandthequeryobject[2],[11],[12]. B.Parallelizingsimilaritysearch Parallelismcanbeadoptedtoimprovethethroughputfor similaritysearch.Therearealsosomeproposedindexstruc- turesongraphsandtreesthatcanbeparallelized[26],[11].  However,indexestailoredtospecialdatatypescannotbe easilyextendedtosupportotherdatatypes.Thereareafew GPU-basedmethodsforANNsearchusingLSH.Panetal.[4], [23]proposeasearchingmethodontheGPUusingabi-level LSHalgorithm,whichspeciallydesignedforANNsearchin the l p space.HoweverGENIEcangenerallysupportLSHfor ANNsearchundervarioussimilaritymeasures. C.DatastructuresandmodelsontheGPU Therearesomeworks[3],[27]aboutinvertedindexonthe GPUtodesignspecializedalgorithmsforacceleratingsome importantoperationsonsearchengines.Aninverted-likeindex ontheGPUisalsostudiedforcontinuoustimeseriessearch  [28].Asfarasweknown,thereisnoexistingworkoninverted  indexframeworkforgenericsimilaritysearchontheGPU. Tree-baseddatastructuresarealsoinvestigatedtoutilize theparallelcapabilityoftheGPU[1].SomeGPUsystemsfor  key-valuestorearealsostudied[29],[30]. D.Frequentitemalgorithm SomepreviousworkrelatedtoCountPriorityQueue(c-PQ) ofGENIEisfrequentitemalgorithm[31],whichcan becategorizedascounter-basedapproach(likeLossyCounting  [32]andSpaceSaving[33])andsketch-basedapproach(like  Count-Min[34]andCount-Sketch[35]).However,bothap-  proachesareapproximationmethods,whereasc-PQcanreturn theexacttop- k frequentitems.Moreover,severalfrequent itemalgorithms(likeSpaceSavingandCount-Min) requirepriorityqueue-likeoperations,makingthemnontrivial  beimplementedontheGPU. VIII.C ONCLUSION Inthispaper,wepresentedGENIE,agenericinvertedindex framework,whichtriestoreducetheprogrammerburdenby  providingagenericfashionforsimilaritysearchontheGPU  fordatatypesandsimilaritymeasuresthatcanbemodeled inthematch-countmodel.Severaltechniquesaredevisedto improvetheparallelismandscalingoutoftheGPU,like c - PQtoreducethetimecostandthemultipleloadingmethod  forhandlinglargedatasets.WealsoprovedthatGENIEcan  support ˝ -ANNsearchforanysimilaritymeasuresatisfying theLSHscheme,aswellassimilaritysearchonoriginaldata  withtheSAscheme.Inparticular,weinvestigatedhowto  useGENIEtosupportANNsearchinkernelspaceandin  highdimensionalspace,similaritysearchonsequencedata  anddocumentdata,andtop- k selectiononrelationaldata. Extensiveexperimentsonvariousdatasetsdemonstratethe efyandeffectivenessofGENIE. A CKNOWLEDGMENTS ThisresearchwascarriedoutattheSeSaMeCentre.Itis supportedbytheSingaporeNRFunderitsIRC@SGFunding InitiativeandadministeredbytheIDMPO.TheworkbyH.V.  JagadishwaspartiallysupportedbytheUSNationalScience FoundationunderGrantsIIS-1250880andIIS-1741022. R EFERENCES [1] L.Luo,M.D.Wong,andL.Leong,ﬁParallelimplementationofr-trees onthegpu,ﬂin ASP-DAC ,2012,pp.353Œ358. [2] X.Wang,X.Ding,A.K.Tung,andZ.Zhang,ﬁEfandeffective knnsequencesearchwithapproximaten-grams,ﬂ PVLDB ,vol.7,no.1, pp.1Œ12,2013. [3] S.Ding,J.He,H.Yan,andT.Suel,ﬁUsinggraphicsprocessorsforhigh performanceirqueryprocessing,ﬂin WWW ,2009,pp.421Œ430. [4] J.PanandD.Manocha,ﬁFastgpu-basedlocalitysensitivehashingfor k-nearestneighborcomputation,ﬂin GIS ,2011,pp.211Œ220. [5] M.S.Charikar,ﬁSimilarityestimationtechniquesfromroundingalgo- rithms,ﬂin STOC ,2002,pp.380Œ388. [6] M.Datar,N.Immorlica,P.Indyk,andV.S.Mirrokni,ﬁLocality-sensitive hashingschemebasedonp-stabledistributions,ﬂin SoCG ,2004. [7] S.Aparicio,J.Chapman,E.Stupka,N.Putnam,J.-m.Chia etal. , ﬁWhole-genomeshotgunassemblyandanalysisofthegenomeoffugu rubripes,ﬂ Science ,vol.297,no.5585,pp.1301Œ1310,2002. [8] X.She,Z.Jiang,R.A.Clark,G.Liu,Z.Cheng,E.Tuzun,D.M.Church, G.Sutton,A.L.Halpern,andE.E.Eichler,ﬁShotgunsequenceassembly andrecentsegmentalduplicationswithinthehumangenome,ﬂ Nature , vol.431,no.7011,pp.927Œ930,2004. [9] T.Alabi,J.D.Blanchard,B.Gordon,andR.Steinbach,ﬁFastk-selection algorithmsforgraphicsprocessingunits,ﬂ JEA ,vol.17,pp.4Œ2,2012. [10] J.Zhou,Q.Guo,H.V.Jagadish,L.Kr  c ´ al,S.Liu,W.Luan,A.Tung, Y.Yang,andY.Zheng,ﬁAgenericinvertedindexframeworkfor  similaritysearchonthegpuŒtechincalreport,ﬂ arXiv:1603.08390 ,2018. [11] X.Yan,P.S.Yu,andJ.Han,ﬁSubstructuresimilaritysearchingraph databases,ﬂin SIGMOD ,2005,pp.766Œ777. [12] R.Yang,P.Kalnis,andA.K.Tung,ﬁSimilarityevaluationontree- structureddata,ﬂin SIGMOD ,2005,pp.754Œ765. [13] X.He,D.Agarwal,andS.K.Prasad,ﬁDesignandimplementationof aparallelpriorityqueueonmany-corearchitectures,ﬂin HiPC ,2012. [14] I.Garc ´ S.Lefebvre,S.Hornus,andA.Lasram,ﬁCoherentparallel hashing,ﬂ ACMTOG ,vol.30,no.6,p.161,2011. [15] M.MoazeniandM.Sarrafzadeh,ﬁLock-freehashtableongraphics processors,ﬂin SAAHPC ,2012,pp.133Œ136. [16] P.IndykandR.Motwani,ﬁApproximatenearestneighbors:towards removingthecurseofdimensionality,ﬂin STOC ,1998,pp.604Œ613. [17] J.Wang,H.T.Shen,J.Song,andJ.Ji,ﬁHashingforsimilaritysearch: Asurvey,ﬂ arXiv:1408.2927 ,2014. [18] A.RahimiandB.Recht,ﬁRandomfeaturesforlarge-scalekernel machines,ﬂin NIPS ,2007,pp.1177Œ1184. [19] V.SatuluriandS.Parthasarathy,ﬁBayesianlocalitysensitivehashingfor fastsimilaritysearch,ﬂ PVLDB ,vol.5,no.5,pp.430Œ441,2012. [20] E.SutinenandJ.Tarhio,ﬁFiltrationwithq-samplesinapproximatestring matching,ﬂin CombinatorialPatternMatching ,1996,pp.50Œ63. [21] H.Jegou,M.Douze,andC.Schmid,ﬁHammingembeddingandweak geometricconsistencyforlargescaleimagesearch,ﬂin ECCV ,2008. [22] M.Lichman.(2016)UCImachinelearningrepository.http://archive.ics. uci.edu/ml. [23] J.PanandD.Manocha,ﬁBi-levellocalitysensitivehashingfork-nearest neighborcomputation,ﬂin ICDE ,2012,pp.378Œ389. [24] J.Gan,J.Feng,Q.Fang,andW.Ng,ﬁLocality-sensitivehashingscheme basedondynamiccollisioncounting,ﬂin SIMOD ,2012,pp.541Œ552. [25] Y.Sun,W.Wang,J.Qin,Y.Zhang,andX.Lin,ﬁSrs:solvingc- approximatenearestneighborqueriesinhighdimensionaleuclidean spacewithatinyindex,ﬂ PVLDB ,vol.8,no.1,pp.1Œ12,2014. [26] S.TatikondaandS.Parthasarathy,ﬁHashingtree-structureddata:Meth- odsandapplications,ﬂin ICDE ,2010,pp.429Œ440. [27] N.Ao,F.Zhang,D.Wu,D.S.Stones,G.Wang,X.Liu,J.Liu, andS.Lin,ﬁEfparallellistsintersectionandindexcompression  algorithmsusinggraphicsprocessingunits,ﬂ PVLDB ,pp.470Œ481,2011. [28] J.ZhouandA.K.Tung,ﬁSmiler:Asemi-lazytimeseriesprediction systemforsensors,ﬂin SIGMOD ,2015,pp.1871Œ1886. [29] K.Zhang,K.Wang,Y.Yuan,L.Guo,R.Lee,andX.Zhang,ﬁMega-kv: Acaseforgpustomaximizethethroughputofin-memorykey-value  stores,ﬂ PVLDB ,vol.8,no.11,pp.1226Œ1237,2015. [30] T.H.Hetherington,T.G.Rogers,L.Hsu,M.O'Connor,andT.M. Aamodt,ﬁCharacterizingandevaluatingakey-valuestoreapplication  onheterogeneouscpu-gpusystems,ﬂin ISPASS ,2012,pp.88Œ98. [31] G.CormodeandM.Hadjieleftheriou,ﬁFindingthefrequentitemsin streamsofdata,ﬂ CACM ,vol.52,no.10,pp.97Œ105,2009. [32] G.S.MankuandR.Motwani,ﬁApproximatefrequencycountsoverdata streams,ﬂin VLDB ,2002,pp.346Œ357. [33] A.Metwally,D.Agrawal,andA.E.Abbadi,ﬁAnintegratedef solutionforcomputingfrequentandtop- k elementsindatastreams,ﬂ TODS ,vol.31,no.3,pp.1095Œ1133,2006. [34] G.CormodeandS.Muthukrishnan,ﬁAnimproveddatastreamsummary: thecount-minsketchanditsapplications,ﬂ JournalofAlgorithms , vol.55,no.1,pp.58Œ75,2005. [35] M.Charikar,K.Chen,andM.Farach-Colton,ﬁFindingfrequentitems indatastreams,ﬂ TCS ,vol.312,no.1,pp.3Œ15,2004.  
PublishedasaconferencepaperatICLR2018 ANEWMETHODOFREGIONEMBEDDINGFORTEXT CLASSIFICATION ChaoQiao !à,BoHuang €à,GuochengNiu à,DarenLi à,DaxiangDong à¤,WeiHe à,DianhaiYu à¤,HuaWu à¤àBaiduInc.,Beijing,China ¤NationalEngineeringLaboratoryofDeepLearningTechnologyandApplication,China {qiaochao,huangbo02,niuguocheng,lidaren, daxiangdong,hewei06,yudianhai,wu hua}@baidu.comABSTRACT TorepresentatextasabagofproperlyidentiÞedÒphrasesÓandusetherepresen- tationforprocessingthetextisprovedtobeuseful.Thekeyquestionhereishow toidentifythephrasesandrepresentthem.Thetraditionalmethodofutilizing n-gramscanberegardedasanapproximationoftheapproach.Suchamethodcan sufferfromdatasparsity,however,particularlywhenthelengthofn-gramislarge. Inthispaper,weproposeanewmethodoflearningandutilizingtask-speciÞcdis- tributedrepresentationsofn-grams,referredtoasÒregionembeddingsÓ.Without lossofgeneralityweaddresstextclassiÞcation.WespeciÞcallyproposetwomod- elsforregionembeddings.Inourmodels,therepresentationofawordhastwo parts,theembeddingoftheworditself,andaweightingmatrixtointeractwiththe localcontext,referredtoaslocalcontextunit.Theregionembeddingsarelearned andusedintheclassiÞcationtask,asparametersoftheneuralnetworkclassiÞer. Experimentalresultsshowthatourproposedmethodoutperformsexistingmeth- odsintextclassiÞcationonseveralbenchmarkdatasets.Theresultsalsoindicate thatourmethodcanindeedcapturethesalientphrasalexpressionsinthetexts. 1INTRODUCTION TextclassiÞcationisanimportanttaskformanyapplications,includingtopiccategorization,search queryclassiÞcation,andsentimentanalysis,whichhasbeenstudiedforyears.Asimpleyeteffec- tiveapproachfortextclassiÞcationistorepresentdocumentsasbag-of-words,andtrainaclassiÞer onthebasisoftherepresentationsusingmethodssuchaslogisticregression,supportvectorma- chines(Joachims,1998;Fanetal.,2008),andnaiveBayes(McCallumetal.,1998).Although bag-of-wordsmethodsareeffectiveandefÞcient,theyalsohavelimitations.Therepresentationsdo nottakeintoaccountthewordorderinformationwhichhasbeenprovedtobeusefulatleastinsome applicationssuchassentimentanalysis(Pangetal.,2002). TomakeeffectiveuseofwordorderinformationfortextclassiÞcation,peoplehavetraditionally exploitedn-grams,i.e.,shortsequencesofwordsinthetexts.Previousworkshowsthattheuseof n-gramsiseffectiveinthetextclassiÞcationtask(Pangetal.,2002;Wang&Manning,2012;Joulin etal.,2016).Althoughn-gramsareveryuseful,theyhavecertainlimitations.1)Thenumberof n-gramsincreasesexponentiallywhenthelengthofn-gram nincreases.ThismakesitdifÞcultto exploitlargen-grams(e.g., n>4).2)Sincethenumberofparametersinann-grammodelisvery large,theestimationoftheparametersusuallysuffersfromthedatasparsityproblem. Recently,themethodofFastTexthasbeenproposed(Joulinetal.,2016),whichcanlearnanduse distributedembeddingsofn-grams.MorespeciÞcally,theembeddingofann-gramisdeÞnedasa low-dimensionalvectorrepresentationofthen-gram.Notethatthen-gramsinavocabularycanalso berepresentedasone-hotvectorsWang&Manning(2012). !chao.qiao@outlook.com€bohuang0321@gmail.com1PublishedasaconferencepaperatICLR2018 Inthispaper,weproposetolearnembeddingsofn-gramsforaspeciÞctask(e.g.,classiÞcation), whicharemorecompactandthuseasytoobtain.Wecalltheembeddingsregionembeddings,fol- lowingtheworkinJohnson&Zhang(2015).OurmethodsigniÞcantlydiffersfromtheirmethod, however,inthesensethattheregionembeddingsinourmethodaretask-dependentandacquired fromsupervisedlearning,whilethoseintheirmethodaretask-independentandacquiredfromun- supervisedlearning.OurmethodisalsolargelydifferentfromFastText,asitlearnsrichermodels forregionembeddings. Intuitively,themeaningofawordisdeÞnedbythemeaningofitselfaswellasthemeaningsof wordsinthesurroundingcontext.The extended embeddingofawordinann-gramthusconsistsof twoparts,theembeddingoftheworditselfandamatrixtointeractwiththelocalcontext,named ÒlocalcontextunitÓ.Theembeddingofawordisacolumnvector,andthelocalcontextunitofa wordisamatrixinwhichthecolumnsareusedtointeractwithwordsinthelocalcontext.The regionembeddingofann-gramisthenconstructedbytheextendedembeddingsofallwordsinthe n-gram.Inthispaper,weintroducetwomodelsforregionembeddings. ForthetextclassiÞcationtask,adocumentisviewedasabagofregionembeddings,andthebagof regionembeddingsisfedintoaclassiÞer.Theparametersofthelocalcontextunitsandwordem- beddingsaretrainedtogetherwiththeparametersoftheclassiÞerwhichisafullyconnectedneural network.Ourmodelsachievebetterresultsthanthestate-of-the-artmethodsonseveralbenchmark datasetsoftextclassiÞcation.Experimentsshowthatourproposedmodelscanreallycaptureim- portantinformationforthetask. 2RELATED WORKTextclassiÞcationhasbeenstudiedforyears,traditionalapproachesfocusedonfeatureengineering andusingdifferenttypesofmachinelearningalgorithms.Forfeatureengineering,bag-of-words featuresareefÞcientandpopular.Inaddition,thehand-craftedn-gramsorphrasesareaddedto makeuseofwordorderintextdata,whichhasbeenshowneffectiveonWang&Manning(2012). Formachinelearningalgorithms,linearclassiÞersarewidelyused,suchasnaivebayes(McCallum etal.,1998),logisticregressionandsupportvectormachines(Joachims,1998;Fanetal.,2008). However,thesemodelscommonlysufferthedatasparsityproblem. Recently,severalneuralmodelshavebeenproposed,thepre-trainedwordembeddingsof word2vec (Mikolovetal.,2013)havebeenwidelyusedasinputstodeepneuralmodelssuchas recursivetensornetworks(Socheretal.,2013).Ontheotherhand,somesimpleandefÞcient modelswhichcandirectlylearntaskspeciÞcwordembeddingsorÞne-tuneonpre-trainedword embeddingshavebeenproposedrecently,suchasDeepAveragingNetworks(Iyyeretal.,2015), FastText(Joulinetal.,2016).Severalneuralmodelshavebeenproposedtomakeuseofwordorder information,mostmodelsarebasedonconvolutionalneuralnetwork(CNN)(Kim,2014;Johnson &Zhang,2014;Zhangetal.,2015)andrecurrentneuralnetwork(RNN)(Tangetal.,2015;Lai etal.,2015;Yogatamaetal.,2017).Morerecently,theTransformer(Vaswanietal.,2017),ase- quencetransductionmodelbasedsolelyonattentionmechanismshasbeenproposed.Although TransformerwasnotdesignedforthetextclassiÞcationtask,ithassimilaritieswithourwork.Inthe restofthissection,wewillbrießyintroduceFastText,CNNandTransformer,whicharethemost relevanttoourwork. FastText FastTextaveragesthewordembeddingstorepresentadocument,andusesafullcon- nectedlinearlayerastheclassiÞer.ThewordembeddingsaretrainedforeachtaskspeciÞcally. Toutilizethelocalwordorderinformationofsmallregions,FastTextuseshand-craftedn-gramsas featuresinadditiontosinglewords.Withthesimplearchitecture,FastTexthasbeenprovedtobe effectiveandhighlyefÞcientontextclassiÞcationtasks.Similarly,ourmodelsusebagofregionem- beddingstorepresentadocument,andusethesamelinearclassiÞer.Differently,ourmodelsdirectly learnthesemanticsofregionsbasedonwordsequence,hand-craftedfeaturesarenotrequired. CNNCNNisafeed-forwardnetworkwithconvolutionallayersinterleavedwithpoolinglayers, whichareoriginallyusedforimageprocessingtasks.Fornaturallanguageprocessing,wordsare commonlyconvertedtovectors.CNNdirectlyappliesconvolutionallayeronwordvectors,both wordvectorsandtheshared(wordindependent)kernelsaretheparametersofCNN,whichcanbe 2PublishedasaconferencepaperatICLR2018 learnedtocapturethepredictivestructuresofsmallregions.TheessenceofCNNistolearnembed- dingsforsmallÞxedsizeregions,eachkerneloftheconvolutionallayertriestocaptureaspeciÞc semanticorstructuralfeature.OurpurposeissimilarwithCNN,whichtriestolearntaskspeciÞc representationsofregions.UnlikeCNN,weapplylocalcontextunitsonwordvectors,whichare worddependent,moreover,theconvolutionkernelsextractthepredictivefeaturesbyapplyingcon- volutionoperationonwordsequences,whileweuselocalcontextunitsasdistinctlinearprojection functionsoncontextwordsincorrespondingrelativepositionstogetregionrepresentations. Transformer Vaswanietal.(2017)proposedasequencetransductionmodel,theTransformer, basedsolelyonattentionmechanisms.BothTransformerandourmethodcancapturewordorder informationwithoutanyCNNorRNNcomponent,andthescalarformofcontextunits(introduced inourablationexperiments)canberegardedasakindoflocalattention.Therearealsosome differenceshere:themotivationweproposedlocalcontextunitsistoaddresswordspeciÞcinßuence betweenwordanditscontext,whileVaswanietal.(2017)hasproposedaparallelablesequence transductionframeworkbasedentirelyonattention;Toutilizepositioninformation,inourmethod, wordsareinteractedwithcontextwordsatdifferentrelativepositionsbycorrespondingcolumnsin theircontextunits,whileTransformeruseÞxed sinandcos functionbasedpositionencoding. 3METHOD Inthispaper,wefocusonlearningtherepresentationsofsmalltextregionswhichpreservethelocal internalstructuralinformationfortextclassiÞcation.Theregionsinadocumentcanbeconsidered asÞxedlengthcontiguoussubsequencesofthedocument.MorespeciÞcally,with wistandingfor thei-th(startingfrom0)wordofthedocument,weuse region (i,c )todenotethe 2!c+1lengthregionwithmiddleword wi.Forinstance,givenasentencesuchas Thefoodisnotverygoodinthis hotel,region (3,2)meansthesubsequence foodisnotverygood .Inthiswork,weusetheinteractionsbetweenwordsandtheirlocalcontextbasedonwordembed- dingsaswellasthelocalcontextunitstoproduceregionembeddings.Intherestofthissection, wewillintroducethelocalcontextunitsÞrstly,andtwoarchitecturestogeneratetheregionembed- dingsthroughlocalcontextunitswillbeintroduced,Þnallywewillintroducehowweusetheregion embeddingsontextclassiÞcation. (a)Word-ContextRegionEmbedding (b)Context-WordRegionEmbedding Figure1:Architecturesofregionembeddingusinglocalcontextunitsindifferentperspectives 3.1L OCALCONTEXTUNIT Innaturallanguageprocessing,wordsarecommonlyconvertedtolowdimensionalvectors(word embeddings)astheinputstoneuralnetworks.Moreformally,theembedding ewofword wis3PublishedasaconferencepaperatICLR2018 representedbyacolumninamatrix E"Rh"vwithalookuplayer,where visthesizeofthe vocabulary, histheembeddingsize. ToutilizetheinformationofwordsÕrelativepositionsandlocalcontext,welearnalocalcontextunit foreachwordinadditiontothewordembedding,andboththeunitandwordembeddingarelearned asmodelparameters.Formally,wedeÞnethelocalcontextunit Kwi"Rh"(2"c+1) ofwiasa matrixwhichcanbelookedupinthetensor U"Rh"(2"c+1) "vbywiÕsindexinthevocabulary. Eachcolumnin Kwicanbeusedtointeractwiththecontextwordincorrespondingrelativeposition ofwi.Infact,thecolumnsofaunitmatrixcanberegardedasdistinctivelinearprojectionfunctions ontheembeddingsofwordsinthelocalcontext.Theparametersoftheseprojectionfunctions(i.e., columnsofeachunitmatrix)canbelearnedtocapturethesemanticandsyntacticinßuenceofthe wordtoitscontext.Wordembeddingsareusedasinputstotheprojectionfunctions,andwecallthe outputsprojectedwordembeddings. Formally,let piwi+tbetheprojectedwordembeddingof wi+tini-thwordÕsview,and Kwi,tbethe (c+t)-thcolumnin Kwi(#c<=t<=c),giventheunit Kwiofwiandtheembedding ewi+tofwi+t,weuseanelement-wisemultiplication(denotedby $)tocompute piwi+t:piwi+t=Kwi,t$ewi+t(1)Foracontextwordinaparticularrelativepositionof wi,thereisacorrespondinglinearprojection function(aparticularcolumnof Kwi),thusourproposedlocalcontextunitscanutilizethelocal orderedwordinformationinanovelway.Notethatthemiddlecolumn Kwi,0ofKwicanbe regardedasalinearprojectionfunctionon ewiitself,whichtransforms ewitothesamespaceas otherprojectedembeddings. 3.2W ORD-CONTEXTREGIONEMBEDDING Weproposedtwoarchitecturestoperformtheregionembeddingfromdifferentperspectives.We considerthesemanticsofagivenregionisderivedfromthemutualinßuencesofthewordsinthis region.Inthispaper,theregionscanberegardedassnapshotsofawindowslidingonadocument, whosemiddlewordsarecontiguous,hencewecancomposethesemanticsofagiveregiononlyby themiddlewordÕsinßuencesonthecontextwords,orthecontextwordsÕinßuencesonthemiddle word. IntheÞrstproposedarchitecture,wefocusonaddressingthemiddlewordÕsinßuencesonthecontext words.Forexample,inthesentence Thefoodisnotverygoodinthishotel ,theoccurrenceofword notmightbringasemanticreversaltothelocalregion. Weusethelocalcontextunitofthemiddlewordandtheoriginalwordembeddingsinaregion toperformtheprojectedembeddingsinaword-to-contextview,wheretheprojectedembeddings canreßectthemiddlewordÕsinßuencesonthecontextwords.Oncetheprojectedembeddingsare obtained,amaxpoolingoperationisappliedtoextractthemostpredictivefeaturesintheregion. Theoutputofthemaxpoolingoperationcanberegardedasataskrelatedregionembeddingina word-to-contextview,i.e.Word-Contextregionembedding. Formally,weusethecontextunit Kwiofmiddleword wiandembeddingsofallwordsinaregion region (i,c )tocomputetheprojectedembeddingmatrixbyequation(1),thentheWord-Contextre- gionembedding r(i,c)canbeobtainedthroughamaxpoolingoperationontheprojectedembedding matrix:r(i,c)=max ([piwi!cpiwi!c+1...piwi+c!1piwi+c])(2)wheremax standingforthemaxpoolingoperationonthecolumndimensionoftheinputmatrix. Finally,weget r(i,c)asavectorrepresentationof region (i,c )withdimension h.Figure1ashows thedetailsoftheÞrstmodelarchitecture. Forinstance,inthesentence Thefoodisnotverygoodinthishotel ,theprojectedwordembeddings inthe region (3,2)arecomposedbytheelement-wisemultiplicationsbetweencolumnsinthelocal 4PublishedasaconferencepaperatICLR2018 contextunitof notandwordembeddingsof food,is,not,veryandgood.Theembedding r3,2ofregion (3,2)canbeobtainedbymaxpoolingontheprojectedwordembeddingmatrix. 3.3C ONTEXT -WORDREGIONEMBEDDING Thesecondarchitecturegoesasadifferentview,whichaddressesthelocalcontextwordsÕinßuences onthemiddlewordintheregion,andwecallthisContext-Wordregionembedding.Similarly,fora region (i,c ),theprojectedembeddingsarecomputedbytheoriginalwordembeddingofthemiddle wordandthecontextunitsofallwordsintheregion,thentheContext-Wordregionembeddingcan beobtainedbyamaxpoolingoperationthroughthecolumndimensionoftheprojectedembedding matrix:r(i,c)=max ([pi#cwipi#c+1wi...pi+c#1wipi+cwi])(3)Figure1bshowsthedetailsofthesecondmodelarchitecture.ourtwomodelstakedifferentways toproducetheprojectedwordembeddings,theWord-Contextmodelusescontextunitsofmiddle wordsandwordembeddingsofcontextwords,whiletheContext-Wordmodelusescontextunitsof contextwordsandwordembeddingsofthemiddleword. 3.4R EGIONEMBEDDINGFORTEXTCLASSIFICATION FortextclassiÞcation,documentsareusuallyvariable-sized,whichneedtoberepresentedasÞxed sizevectors.Inordertoshowtheeffectivenessofourproposedregionembeddingmodels,we justsumuptheembeddingsofallregionstorepresentadocument,andfeedittoanupperFull- ConnectedlayerfortextclassiÞcationtask. Formally,themodelcanberepresentedasfollowing: f(x;E,U,W,b)=g(W!(n!i=0r(i,c))+b)(4)wherexdenotestheinputtextsequence, Wandbdenotetheweightmatrixandbiasofthefully connectedlayerrespectively,gdenotesthesoftmaxfunctionoftheoutputlayer, !denotesthesoft- signfunctionand ndenotesthenumberofregionsinadocument, ristheregionembeddingwhich canbecomputedbytheequation(2)or(3). E,U,Wandbcanbeupdatedinthetrainingperiod. 4EXPERIMENTS Wereportexperimentswithproposedmodelsincomparisonwithpreviousmodels. 4.1D ATASETS WeusepubliclyavailabledatasetsfromZhangetal.(2015)toevaluateourmodels.Therearein total8textclassiÞcationdatasets,correspondingtosentimentanalysis,newsclassiÞcation,question- answer,ontologyextractiontasks,respectively.Table1showsthedescriptivestatisticsofdatasets usedinourexperiments.Toguaranteecomparableindications,sameevaluationprotocolofZhang etal.(2015)isemployed. 4.2B ASELINES OurmodelsarecomparedwithseveralwidelyusedsupervisedtextclassiÞcationmodels.Wereport then-gramsandTFIDFbaselinesfromZhangetal.(2015),aswellasthecharacterlevelcon- volutionalmodel(char-CNN)ofZhang&LeCun(2015),thecharacterbasedconvolutionrecur- rentnetwork(char-CRNN)ofXiao&Cho(2016),theverydeepconvolutionalnetwork(VDCNN) ofConneauetal.(2016),theDiscriminativeLSTM(D-LSTM)ofYogatamaetal.(2017)andthe bigramFastText(bigram-FastText)ofJoulinetal.(2016). 5PublishedasaconferencepaperatICLR2018 Table1:StatisticsofDatasets DatasetClassesAverage LengthsTrain SamplesTest SamplesTasks YelpReviewPolarity 2156560,00038,000SentimentAnalysisYelpReviewFull 5158650,00050,000AmazonReviewPolarity 2913,000,000650,000AmazonReviewFull 5933,600,000400,000AGÕsNews 444120,0007,600News ClassiÞcationSogouNews 5579450,00060,000Yahoo!Answers 101121,400,00060,000QuestionAnswerDBPedia1455560,00070,000OntologyExtraction4.3I MPLEMENTATION DETAILS Fordatapreprocessing,allthetextsofdatasetsaretokenizedbyStanfordtokenizerandallwordsare convertedtolowercase.Wordsthatappearonlyinonedocumentaretreatedasout-of-vocabulary (OOV)items,andallstopwordsaswellassymbolsarekept.Additionally,lengthof cpaddingare addedtoboththeheadandtailofeachdocument. Forourmodels,optimalhyperparametersaretunedwith 10%ofthetrainingsetonYelpReviewFull dataset,andidenticalhyperparametersareappliedtoalldatasets:thedimensionofwordembedding is128,theregionsizeis 7whichmeanstheshapeoflocalcontextunitmatrixofeachwordis 128!7,theinitiallearningrateissetto 1!10#4,andthebatchsizeis 16.Foroptimization,theembeddings ofwordsandtheunitsarerandomlyinitializedwithGaussianDistribution.Adam(Kingma&Ba, 2014)isusedastheoptimizer.Wedonotuseanyextraregularizationmethods,likeL2normalization ordropout.AlgorithmsareentirelyimplementedwithTensorFlowandtrainedonNVIDIATesla P40GPUs.Thecode 1ispubliclyavailableontheInternet. 4.4R ESULTS Table2:TestSetAccuracy[%]ComparedtootherMethodsonseveralDatasets ModelYelpP.YelpF.Amz.P.Amz.F.AGSogouYah.A.DBP BoW92.258.090.454.688.892.968.996.6 ngrams95.656.392.054.392.097.168.598.6 ngramsTFIDF95.454.891.552.492.497.2 68.598.7 char-CNN94.762.094.559.687.295.171.298.3 char-CRNN94.561.894.159.291.495.271.798.6 bigram-FastText95.7 63.994.660.292.5 96.872.398.6 VDCNN95.7 64.795.763.091.396.873.498.7 D-LSTM92.659.6--92.194.973.7 98.7W.C.region.emb 96.464.9 95.160.9 92.897.673.798.9 C.W.region.emb96.264.595.360.8 92.897.373.4 98.9Table2isthesummaryoftheexperimentalresults.Weuseunderscorestorepresentthebestpub- lishedresults,andboldthebestrecords.Onsixdatasetsofeight,ourmodelsbeatormatchthe state-of-the-artwithaperformancegainhighestto0.7%.Webeatallthepreviousmodelsonall datasetsexceptVDCNN,whilethelatterperformsalmostbestonallclassiÞcationtasksbefore.As aresult,weslightlywinVDCNNonsixdatasetsandlostintwoofAmazondatasets.Detailed experimentalresultsincludingbestperformanceepochandtrainingtimeforalllisteddatasetsare reportedinAppendixA. 1https://github.com/text-representation/local-context-unit 6PublishedasaconferencepaperatICLR2018 Furthermore,theupperlayerstructureofourmodelsonlyusesasummingupoperation,whichis moreconciseandrobustthananyotherdeeporcomplexmodels.Infact,bothofourtwoproposed modelsareeffectiveagainstpreviousmodels. 4.5E XPLORATORY EXPERIMENTS Inthissubsection,wearegoingtodoasetofexploratoryexperimentstostudytheeffectofeach componentofourmodel.Typicalcaseswillbeanalyzedtovalidatepropertiesofvariousaspectsof ourmodels.Consideringthelimitationofpaperspace,weonlyanalyzedtheWord-Contextregion embeddingmodelinourexploratoryexperiments. 4.5.1E FFECTOFREGIONSIZEANDEMBEDDINGSIZE OurmethodusesaÞxedsizeofregionascontextualinformationjustlikeCNN.Sotheselection ofregionsizereallymatters.Asmallregionmaylosesomelongdistancepatterns,whereaslarge regionswillbringintomorenoises.Luckily,ourmodelsseemtobefairlyinsensitivetowardskinds ofdatasets.Actually,wejustuseidenticalregionsize7foralldatasetsanditisabletooutperform thebestpublishedresultsever. Figure2adescribestheperformanceonYelpReviewFullwithdifferentregionsizes,andwhenthe sizeequalsto 1,theresultisquiteclosetounigramFastText(accuracy60.7%),butstillgetsa0.6% promotion.Intuitively,themiddlewordcannotinßuenceotherwordsexceptitselfwhenthesize equalsto 1.Theperformanceincreaseswiththegrowthofregionsizeupto7. (a)(b)Figure2:Effectofthehyperparameters( regionsize andembeddingsize )onYelpReviewFull dataset.(a)showsthecomparisonofsingleÞxedregionsize7andmultisizescombination[3,5,7] and(b)showstheeffectofdifferentsettingsofembeddingsizeamongfourkindsofmodels,unigram FastText,bigramFastText,CNNandours.Weuseregionsize7forCNNandours. Furthermore,weexperimentourmodelswiththecombinationofmultiregionsizes.Hereweuse theapproachofsharingcontextunitsamongeachregion,wheretheparametersoflocalcontext unitsofsmallerregionsarejustthesliceofthelongestone.Regionembeddingsofdifferentsizes areconcatenatedforÞnalclassiÞcation.InÞgure2a,thecombinationofmultiregionsizes 3,5,7isslightlybetterthanthebestsingleregionsize7.Theeffectivenessofmulti-sizecombinationcanbe explainedbythedifferenceofinßuencerangesbetweenwords.Forexample,insentimentanalysis, word veryonlyemphasizesthenextwordwhile however maylaystressonawiderangeofthe followingwords. Inadditiontotheanalysisofregionsizes,wefurtherstudytheinßuenceofwordembeddingdi- mensions.Figure2bliststhecomparativeresultsonYelpReviewFullwithdifferentembedding dimensions.TheresultshowsthatourmodelismorerobusttooverÞttingthanFastTextandCNN withthewordembeddingdimensionincreasing.Infact,theamountofparametersinourmodels isrelativelylarge.SincewelearnaspeciÞcunitforeachword,underthesamewordembedding dimension,ourparametersizehasbeenexpandedby regionsize times,theparametersnumberis v!h+v!(2!c+1) !h+h!m+m,where misthenumberofclasses.SpeciÞcnumbers parametersfordifferentregionsizesarelistedinAppendixA.Noticethatthesizesofparameters arerelativelyconsistentamong 1024inFastText ,1024inCNN and128inours .7PublishedasaconferencepaperatICLR2018 4.5.2E FFECTOF CONTEXTUNIT Inthissection,weexploresomecomparativeexperimentstoshowtheeffectivenessofourproposed wordspeciÞccontextunit.TheexperimentsareemployedbasedonunigramFastTextbaseline, whichhassimilarupperlayerstructurewithourmodels.Table3illustratestheresults. Table3:ComparativedecompositionresultsonYelpReviewFulldataset.ForFast- Text(Unigram),embeddingdimensionis10.ForFastText(Win-pool),W.C.region.emb(Scalar)and W.C.region.emb(ourmodel),regionsizeis7andembeddingdimensionis128 .DecompositionPerformance(%) FastText(Unigram)60.73 FastText(Win-pool)61.01(+ 0.28)W.C.region.emb(Scalar)63.18(+ 2.45)W.C.region.emb(Ourmodel)64.9(+ 4.17)Firstly,weremovetheentirecontextunitsfromourmodel,whichmeansitisjustavariantversionof unigramFastText,wecallitFastText(Win-pool).ThedifferenceisthatFastTextsumsuptheword embeddingsdirectlywhileFastText(Win-pool)sumsupthewindowpooledembeddingsinastride of1.Ityieldsaslightlyaccuracygainof0.28%thanunigramFastText. Secondly,weapplyasimpliÞedscalarversionofcontextunitstoFastText(Win-pool).Distinguish- able,thecontextunitofeachwordhastheshapewith 1!(2!c+1) ,henceitcanberegarded asabroadcastingoperationoncorrespondingwordembeddingsofitslocalcontext.Wenamethis methodW.C.region.emb(Scalar).Comparedtothenon-scalarmethod,ityieldsahugeparameter sizereduction,butitalreadyyieldsasigniÞcantgainof2.45%. Furthermore,W.C.region.emb(ourmodel)isthevariantversionofW.C.region.emb(Scalar)where eachcolumnofscalarcontextunitisexpandedtoadensevector.EachwordÕscontextunithasa shapewith h!(2!c+1) .Addingthelowdimensionaldensecontextunitimprovestheperformance by4.17%.Wecansensemuchfromtheprocedureofdecomposition,withthehelpofcontextunit, evenasimplerscalarversionpromotesalot. Tohaveabetterunderstandingofwhatcontextunitactuallycapture,heatmapsareplottedforchosen wordsamples.Representativeadversarialconjunctionslike however ,but ,modiÞerslike very,good,badandnounslike food,morningarelistedinFigure3. ForeachrowoftheÞgure,theintensityofthesurroundingcolorboxreßectstheemphasisdegree intheviewofthemiddleword.Qualitativebutnotfullyrigorous,anormalizedL2-normofeach columnincontextunitisusedtorendertheshade.Regionsize7isadopteddefault,annotation li(i%3)isdenotedasleftcolumnsofthespeciÞccontextunit,while ri(i%3)denotedastheright part.WhattheÞgurereßectsareconsistentwithintuitivepriorsofhumanbeings.Intheperspectiveof however ,rightcontextsplaythekeyroleforclassiÞcationpolaritybecauseoftheemotionalreversal, thecolorisindeeddeeperin rithanli,sodoes but .Forword very,r1ismoreprominentthanthe restofall,whichcapturessomemodiÞedpatternslike veryhappy orverysad .Forword good,tendencieswillbecompletelydifferentforpatternslike notgood ,verygood andnotthatgood ,whichareintensivenegative,intensivepositiveandslightlyhesitated,separately,thepositionof l1willbestrengthenedasaresult,sodoesword bad.TherearesigniÞcantdifferencesbetweentwo nounsfoodandmorning.Theheatmapofword foodimpliespatternslike deliciousfood orfoodwasmediocre ,whiletheword morninghasfewervaluablepatternsforclassiÞcation. Actually,fromthemotivationofwordspeciÞedcontextunits,wewouldliketobelievethisfeature helpscapturesyntacticandsemanticinßuencesofwordsonsurroundingwordsatrelativepositions. 4.5.3V ISUALIZATION Inthissubsection,wewilltrytovisualizethecontributionofeachwordandselectedphraseto classiÞcation.DetailedvisualizationtechniqueshavebeenintroducedinLietal.(2015).Herewe generalizeittothecolorrenderingofmulti-categoryversion.Noticethatforourmodel,notthe 8PublishedasaconferencepaperatICLR2018 Figure3:HeatmapsofchosenwordstrainedonYelpReviewPolarity,whichisabinarysentiment analysisdataset.Eachrowrepresentthecontextunitofthemiddleword.Regionsizeis7and embeddingsizeis128. originalembeddingactshere,buttheaccumulationoftheprojectedembeddingofeachwordonits surroundingwords. Table4:VisualizationofchosensamplesonYelpReviewPolaritydataset.Greendenotespositive contributionwhilereddenotesnegative.Twomethodsarecomparedwithoutcontextunit(NoC-unit) andwithcontextunit(WithC-unit). MethodSentenceSamples PhraseNoC-unit getyourwallet ready,thepricesarecrazyhighpricesarecrazyhigh WithC-unit getyourwallet ready,thepricesarecrazyhighpricesarecrazyhigh NoC-unit nothingremarkable,but notbadeitherbutnotbadeither WithC-unit nothingremarkable,but notbadeitherbutnotbadeither Forclarity,wechooseabinaryclassiÞcationtaskofsentimentanalysis.InTable4,welisttwocases inYelpReviewPolaritydataset,inwhichourmodelbehavesasexpected.WordsandartiÞcially selectedphrasesarehighlightedgreeniftheyarepositivefactors,rediftheyarenegative.The intensityofthecolorindicatesthedegreeofthepolarity. Tohaveabettercomparison,theresultsofwithandwithoutcontextunitmethodsarebothvisualized. Weabbreviatethemas WithC-unit andNoC-unit ,respectively.Forsentence getyourwalletready, thepricesarecrazyhigh ,ifnocontextunitisadopted,thewordcolorreßectsitswordembedding, whichiscontext-free.Thepolarityof crazy ispositive,and highisnegative.Becausetheintensity ofcrazy ishigherthan high,thepolarityofphrase pricesarecrazyhigh istotallypositive,whichis amistake.Butwithcontextunit,thingshavechangedquiteabit,thepolaritiesofwordsarecontext dependent.Undertheinßuenceof high,thepositivepolarityof crazy vanishesandphrase pricesarecrazyhigh performsnegativeoverall.Foranothercase nothingremarkable,butnotbadeither ,thingsseemmoreinteresting.Withoutcontext-unit, remarkable ispositive,while nothing,not,bad performnegative,respectively.Butwithcontextunit,thepolarityofthepartaheadof but weakens, meanwhilethepolaritiesof notandbadßips.Asaresult,phrase butnotbadeither performspositive overall. 5CONCLUSION ThispaperproposedtwonovelarchitecturesfortextclassiÞcationtasks,whichlearntaskspeciÞcre- gionembeddingswithouthandcraftedfeatures.ToutilizethewordspeciÞcinßuencesofeachword onitscontextwords,alocalcontextunitforeachwordislearnedinadditiontowordembedding. Ourmodelsachievestate-of-the-artperformancesonsixbenchmarktextclassiÞcationdatasets,and 9PublishedasaconferencepaperatICLR2018 thevisualizationexperimentsshowthatourproposedlocalcontextunitcancapturethesemantic andsyntacticinformationforeachword. Noticedthepowerofthelocalcontextunitonlearningtaskrelatedregionembeddings,weare interestedinitsabilitytounsupervisedandsemi-supervisedlearning.Atthesametime,wearealso curiousaboutwhetherwecanachievebetterresultsbyintroducingmorecomplexupperlayerson textclassiÞcation,andothernaturallanguageprocessingtasks. ACKNOWLEDGMENTS ThispaperissupportedbyNationalBasicResearchProgramofChina(973program No.2014CB340505).Wegratefullythanktheanonymousreviewersfortheirinsightfulcomments. REFERENCES AlexisConneau,HolgerSchwenk,Lo ¬õcBarrault,andYannLecun.Verydeepconvolutionalnet- worksfornaturallanguageprocessing. arXivpreprintarXiv:1606.01781 ,2016. Rong-EnFan,Kai-WeiChang,Cho-JuiHsieh,Xiang-RuiWang,andChih-JenLin.Liblinear:A libraryforlargelinearclassiÞcation. Journalofmachinelearningresearch ,9(Aug):1871Ð1874, 2008.MohitIyyer,VarunManjunatha,JordanBoyd-Graber,andHalDaum «eIII.Deepunorderedcompo- sitionrivalssyntacticmethodsfortextclassiÞcation.In Proceedingsofthe53rdAnnualMeeting oftheAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceon NaturalLanguageProcessing(Volume1:LongPapers) ,volume1,pp.1681Ð1691,2015. ThorstenJoachims.Makinglarge-scalesvmlearningpractical.Technicalreport,TechnicalReport, SFB475:Komplexit ¬atsreduktioninMultivariatenDatenstrukturen,Universit ¬atDortmund,1998. RieJohnsonandTongZhang.Effectiveuseofwordorderfortextcategorizationwithconvolutional neuralnetworks. arXivpreprintarXiv:1412.1058 ,2014. RieJohnsonandTongZhang.Semi-supervisedconvolutionalneuralnetworksfortextcategorization viaregionembedding.In Advancesinneuralinformationprocessingsystems ,pp.919Ð927,2015. ArmandJoulin,EdouardGrave,PiotrBojanowski,andTomasMikolov.BagoftricksforefÞcient textclassiÞcation. arXivpreprintarXiv:1607.01759 ,2016. YoonKim.ConvolutionalneuralnetworksforsentenceclassiÞcation. arXivpreprint arXiv:1408.5882,2014. DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization. arXivpreprint arXiv:1412.6980,2014. SiweiLai,LihengXu,KangLiu,andJunZhao.Recurrentconvolutionalneuralnetworksfortext classiÞcation.In AAAI,volume333,pp.2267Ð2273,2015. JiweiLi,XinleiChen,EduardHovy,andDanJurafsky.Visualizingandunderstandingneuralmodels innlp. arXivpreprintarXiv:1506.01066 ,2015. AndrewMcCallum,KamalNigam,etal.Acomparisonofeventmodelsfornaivebayestextclas- siÞcation.In AAAI-98workshoponlearningfortextcategorization ,volume752,pp.41Ð48. Madison,WI,1998. TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.Distributedrepresen- tationsofwordsandphrasesandtheircompositionality.In Advancesinneuralinformationpro- cessingsystems ,pp.3111Ð3119,2013. BoPang,LillianLee,andShivakumarVaithyanathan.Thumbsup?:sentimentclassiÞcationusing machinelearningtechniques.In ProceedingsoftheACL-02conferenceonEmpiricalmethodsin naturallanguageprocessing-Volume10 ,pp.79Ð86.AssociationforComputationalLinguistics, 2002.10PublishedasaconferencepaperatICLR2018 RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherDManning,AndrewNg, andChristopherPotts.Recursivedeepmodelsforsemanticcompositionalityoverasentiment treebank.In Proceedingsofthe2013conferenceonempiricalmethodsinnaturallanguagepro- cessing,pp.1631Ð1642,2013. DuyuTang,BingQin,andTingLiu.Documentmodelingwithgatedrecurrentneuralnetworkfor sentimentclassiÞcation.In EMNLP,pp.1422Ð1432,2015. AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez, ! ukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InI.Guyon,U.V.Luxburg, S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(eds.), AdvancesinNeu- ralInformationProcessingSystems30 ,pp.6000Ð6010.CurranAssociates,Inc.,2017.URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .SidaWangandChristopherDManning.Baselinesandbigrams:Simple,goodsentimentandtopic classiÞcation.In Proceedingsofthe50thAnnualMeetingoftheAssociationforComputational Linguistics:ShortPapers-Volume2 ,pp.90Ð94.AssociationforComputationalLinguistics,2012. YijunXiaoandKyunghyunCho.EfÞcientcharacter-leveldocumentclassiÞcationbycombining convolutionandrecurrentlayers. arXivpreprintarXiv:1602.00367 ,2016. DaniYogatama,ChrisDyer,WangLing,andPhilBlunsom.Generativeanddiscriminativetext classiÞcationwithrecurrentneuralnetworks. stat,1050:6,2017. XiangZhangandYannLeCun.Textunderstandingfromscratch. arXivpreprintarXiv:1502.01710 ,2015.XiangZhang,JunboZhao,andYannLeCun.Character-levelconvolutionalnetworksfortextclas- siÞcation.In Advancesinneuralinformationprocessingsystems ,pp.649Ð657,2015. 11PublishedasaconferencepaperatICLR2018 APPENDIXADETAILEDEXPERIMENTALRESULTS Tohaveafurtherinsightaboutourreportedresults,welistthetrainingtimeandbesttestingperfor- manceepochforkindsofregionsizes( 5,7,9)indetail.Forallthe8datasetsinTable5,hyperparam- etersarekeptinlinewithsection4.3.Thedimensionofwordembeddingis 128,theregionsizeis 7,theinitiallearningrateissetto 1!10#4,andthebatchsizeis 16.Epochindexstartsfrom0and trainingtimeisreportedperepoch.Wechoosethemodelofword-contextregionembeddinghere. Table5:Experimentaldetailedrecordsonseveraldatasets DatasetVocabulary SizeW.CRegion SizeParameters NumberBestEpochTraining Time(Mins) Accuracy(%) YelpP.115298 588,549,12232796.34 7118,065,41033696.39 9147,581,69834396.38 YelpF.124273 595,442,30933464.73 7127,256,19724364.90 9159,070,08525264.74 Amz.P.394385 5302,887,938233695.07 7403,850,498240295.23 9504,813,058258995.06 Amz.F.356312 5273,648,261130060.83 7364,864,133139560.93 9456,080,005149061.05 AG42783 532,857,8606292.81 743,810,3084392.89 954,762,7565492.82 Sogou99394 576,335,23772797.6 7101,780,10193397.63 9127,224,965103997.56 Yah.A.361926 5277,960,458116073.42 7370,613,514121073.66 9463,266,570225673.68 DBP227863 5175,000,59033798.87 7233,333,51824898.89 9291,666,44636098.94 WealsoreportresultsofseveralrepeatedrunsinTable6toexcludetheeffectofrandomnessand ensurereproducibility.FiveindependentrunsareconductedoneachdatasetofYelp.PandYelp.F, wherebothperformancevariancesarewithin0.11%onaccuracy. Table6:PerformancevariancesthroughseveralrepeatedrunsonYelpDatasets DatasetTriesNum.W.CregionsizeBestEpochAccuracy(%)PerformanceVariance YelpP. 07396.39 %%0.1117496.36 27496.41 37396.38 47296.46 YelpF. 07264.90 %%0.1117264.94 27164.87 37164.86 47264.98 12 
ImprovedNeuralMachineTranslationwithSMTFeatures WeiHe,ZhongjunHe, HuaWu, andHaifengWang BaiduInc.No.10,Shangdi10thStreet,Beijing,100085,China {hewei06,hezhongjun,wu hua,wanghaifeng }@baidu.comAbstractNeuralmachinetranslation(NMT)conductsend-to-end translationwithasourcelanguageencoderandatargetlan-  guagedecoder,makingpromisingtranslationperformance. However,asanewlyemergedapproach,themethodhassome limitations.AnNMTsystemusuallyhastoapplyavocabu-  laryofcertainsizetoavoidthetime-consumingtrainingand decoding,thusitcausesaseriousout-of-vocabularyproblem. Furthermore,thedecoderlacksamechanismtoguaranteeall  thesourcewordstobetranslatedandusuallyfavorsshort translations,resultinginﬂuentbutinadequatetranslations.In ordertosolvetheaboveproblems,weincorporatestatisti- calmachinetranslation(SMT)features,suchasatranslation modelandan n-gramlanguagemodel,withtheNMTmodel underthelog-linearframework.Ourexperimentsshowthat theproposedmethodsigniﬁcantlyimprovesthetranslation qualityofthestate-of-the-artNMTsystemonChinese-to-  Englishtranslationtasks.Ourmethodproducesagainofup to2.33BLEUscoreonNISTopentestsets. Introduction Neuralnetworkshaverecentlybeenappliedtomachine translationandbeguntoshowpromisingresults.Sutskever,  Vinyals,andLe(2014)andBahdanau,Cho,andBen-  gio(2014)directlybuiltneuralnetworkstoperformend-to-  endtranslation,namedneuralmachinetranslation(NMT).  Typically,anNMTsystemcontainstwocomponents,anen-  coderthatconvertsasourcesentenceintoavector,anda decoderthatgeneratestargettranslationbasedonthevector. ThestrengthofNMTliesinthatthesemanticandstruc- turalinformationcanbelearnedbytakingglobalcontext intoconsideration.However,asanewlyemergedapproach,  theNMTmethodhassomelimitationsthatmayjeopardize  itsabilitytogeneratebettertranslation. 1.Toreducemodelcomplexity,anNMTsystemusually usesthetop- Nfrequentwordsinthetrainingcorpus andregardsotherwordsasunseenones,whichcauses  aseriousout-of-vocabulary(OOV)problem.WhenOOV  wordsoccurinthesentencestobetranslated,thetransla-  tionqualitywouldbebadlyhurt. Correspondingauthor:ZhongjunHehezhongjun@baidu.com. Copyright c2016,AssociationfortheAdvancementofArtiﬁcial Intelligence(www.aaai.org).Allrightsreserved. 2.TheNMTdecoderlacksamechanismtoguaranteethat allthesourcewordsaretranslatedandusuallyfavors  shorttranslations.Thissometimesresultsinaninadequate  translationthatdoesnotconveythecompletemeaningof  sourcesentence. 3.NMTmodelscannotmakeuseoflargeamountoftarget monolingualcorpus.Therefore,itisdifﬁcultforanNMT  systemtobeneﬁtfromtargetlanguagemodeltrainedon targetmonolingualcorpus,whichisproventobeuse- fulforimprovingtranslationqualityinstatisticalmachine  translation(SMT). Luongetal.(2015)usedadictionarytotranslatethe OOVwordsinapost-processingstep.Gulcehreetal.(2015)  proposedtwowaystointegratearecurrentneuralnetwork  (RNN)basedlanguagemodelintotheNMTmodel.How-  ever,thesemethodsonlyfocusononeoftheaboveNMT  problems.Intuitively,theseproblemscouldbealleviatedwithsome oftheSMTcomponents,suchasthetranslationtable,the n-gramlanguagemodel.Nevertheless,thecurrentNMT frameworksuffersfromafactthatitisdifﬁculttoaddef-  fectivefeaturesintothemodeltofurtherimprovetranslation  quality. Inthispaper,weproposetoimproveNMTbyintegrat- ingSMTfeatureswiththeNMTmodelunderthelog-linear  framework.Weincorporate3SMTfeatures,includingthe  translationmodel,thewordrewardfeatureandthe n-gramlanguagemodel.Thetranslationmodelistrainedonword-  alignedbilingualcorpuswiththeconventionalphrase-based  SMTapproach(Koehn,Och,andMarcu2003),andem-  ployedtoscorewordpairsandalleviatetheOOVproblem.  Thewordrewardfeaturecontrolsthelengthofthetransla-  tion.Andthe n-gramlanguagemodelaimstoenhancethe localﬂuencywhichistrainedontargetmonolingualsen-  tences.Comparedtopreviousmethods,ourmethodhasthefol- lowingadvantages: 1.Thelog-linearframeworkmakesanNMTsystembeeas- ilyextended.Itcanbeintegratedwitheffectivefeatures  usedinconventionalSMTmodels. 2.Weintegrateawordtranslationtableintothelog-linear frameworkwiththetranslationprobabilitiesestimated  fromtheword-alignedbilingualcorpuswhichistrained Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)dajia doubei chuanran releihengliu− x1− x2−  x3−  x4−  x5−  x6−  x7− h1−  h1−  h2−  h2−  h3−  h3−  h4−  h4−  h5−  h5−  h6−  h6−  h7−  h7Allthepeopleareinfectedandtears UNKEOS− y1− y2− y3− y4− y5− y6− y7− y8− y9− s0− s1− s2− s3− s4− s5− s6− s7− s8− s9− c1− c2− c3− c4− c5− c6− c7− c8− c9DECODERENCODER˜f:x :henc:c :s dec:y :˜e:Figure1:IllustrationoftheRNNEncoder-Decoder.NotethatthereisaUNKsymbolonthetargetsidebecauseoftheOOV problem.onIBMmodels(Brownetal.1993).Thetranslationta-  blecannotonlybeusedtorecoverthetranslationofsuch  wordsthataretakenasunknownwordsintheNMTsys-  tem,butalsoprovidesanotherwaytomeasurethecorre-  lationbetweensourceandtargetwords.Thetranslation  tableisintegratedintothedecodingprocedureratherthan  usedinapost-processstep(Luongetal.2015). 3.ToalleviatetheinadequatetranslationprobleminNMT, weuseawordrewardfeaturetomakethedecoderfavor  longtranslation,ratherthanusingacoveragevectorthat  guaranteesallsourcewordsaretranslated. 4.Weaddan n-gramlanguagemodeltothelog-linear framework,soastomakefulluseofthelarge-scalemono-  lingualcorpustofurtherimprovetranslationquality.The languagemodelscorealongwithotherfeaturesisusedto  selectcandidatesduringdecoding.Whileintheshallow  fusion,Gulcehreetal.(2015)usedthelanguagemodelto rescoretop- NcandidatesgeneratedbytheNMTdecoder. Ourapproachisdifferentfromtheconventionalmeth- odswhichintegratedneuralnetworksintoSMTsystems (Devlinetal.2014;Aulietal.2013;Choetal.2014;  Li,Liu,andSun2013;Zhaietal.2013).Themaindifference isthattheconventionalmethodsareconductedwithinan  SMTframework.OursystemisanNMTsystem,enhanced  byeffectiveSMTfeatures. Wecarriedoutexperimentswithanopen-sourceNMT systemGroundHog 1(Bahdanau,Cho,andBengio2014). 1https://github.com/lisa-groundhog/GroundHog ThesystembuildstwoRNNstoperformend-to-endtrans-  lation:oneasanencoderandtheotherasadecoder.We  trainedthesystemwithalargeamountcorpus(contain-  ingabout200millionsentencepairs)collectedfromthe  web.ExperimentsonChinese-to-Englishtranslationtasks  demonstratethattheproposedmethodachievessigniﬁcant  improvementsoverthestate-of-the-artNMTsystem. Background ThissectionbrieﬂyreviewstheRNNencoder-decoder,are-  centlyproposedNMTapproachbasedonrecurrentneural  network,andthelog-linearmodels,thedominantframework  forSMTinthelastdecade. RNNEncoder-Decoder Figure1showsthetranslationprocedureoftheRNN  encoder-decoder(Bahdanau,Cho,andBengio2014)for Chinese-to-Englishtranslation.Givenasourcesentence ˜f=f1,f2,...,f I,theencoderﬁrstencodes ˜fintoasequence ofvectors,thenthedecodergeneratesthetargettranslation ˜e=e1,e2,...,e Jbasedonthevectorsandthetargetwords previouslygenerated. TheencoderisabidirectionalRNN(SchusterandPaliwal 1997)withahiddenlayer.Attheencodingstep,theencoder  ﬁrstlyprojectstheinputsentence ˜fintowordvectors x =(x1,x2,...,x I),xiRKx,where Kxisthevocabularysize ofthesourcelanguage.Thenthenetworkupdatesthehidden statehiencateachstepby hienc=genc(xi,hi−1enc)(1)where,gencisanactivationfunction,e.g.the tanhfunction.hienc=[−− hienc,−− hienc]istheconcatenationoftheforward andbackwardhiddenstatescalculatedbasedonthesource sentence.Atthedecodingstep,theprobabilityoftheoutputse- quenceiscomputedas: p(y )=Jj=1p(yj|{yj−1,yj−2,...,y 1}x)(2)=Jj=1gdec(sjdec,yj−1,cj)(3)where,sj decisthehiddenstateatstep j,whichiscom- putedby, sj dec=gdec(sj−1dec,yj−1,cj)(4)gdecandgdecarenon-linearactivationfunctions.Thecon- textvector cjiscomputedasaweightedsumofthehidden statesoftheencoder: cj=Txi=1jihienc(5)where,theweight jicanbeconsideredasanassociation measurethathowwellatargetword yjistranslatedfroma sourceword xi.Bahdanau,Cho,andBengio(2014)used afeed-forwardneuralnetworktoparametrizeanalignment modeltoestimate .Thisisanimportantdifferencefromthe basicRNNencoder-decoderproposedbyChoetal.(2014),  whichencodesthesourcesentenceintoasinglevectorwith  ﬁxedlength,thusunabletoreﬂectthestrengthoftherela-  tionshipbetweensourceandtargetwords. FollowingChoetal.(2014),Bahdanau,Cho,andBen- gio(2014)alsousedtwotypesofhiddenunits, reset gatesandupdategates.The reset gatesallowthenetworkstoig- noretheinformationofsomeprevioushiddenstates,which  maybenoisyforthecurrentstate.The updategatescontrol thedegreeoftheinformationbeingtransferedtothecurrent  statefromthepreviousstates.Theroleofthetwokindsof  gatesisanalogoustothelong-short-term-memory(LSTM)  (Sutskever,Vinyals,andLe2014),butmuchsimpler. TheRNNencoder-decoderistrainedonbilingualcorpora andperformsanend-to-endtranslation.However,underthe  currentarchitecture,itisdifﬁculttoimprovethetranslation  qualitybyintegratingadditionaltranslationknowledge. Log-linearModels Thewidelyusedlog-linearframeworkinSMTwasintro-  ducedbyOchandNey(2002). p(˜e|˜f)=exp(mi=1iHi(˜f,˜e))˜eexp(m i=1iHi(˜f,˜e))(6)where,Hi(˜f,˜e)isafeaturefunctionand iistheweight. Thestrengthofthelog-linearmodelisthatfeaturescanbe easilyaddedintoit.Astandardphrase-basedSMT(Koehn, Och,andMarcu2003)typicallycontains8features:thebi-  directionaltranslationprobabilities p(f|e)andp(e|f),the x1x2...xIcjsjyj...yj−n+1...yj−1n-gramlangugemodel wordtranslatio nprobabilitiesRNNwordprediction jijip(yj|xi)Figure2:IllustrationoftheLog-linearNMT.Topredict thetargetword yj,weintroduceSMTfeatures,suchasthe wordtranslationprobabilities,the n-gramlanguagemodel, togetherwiththeprobabilitiesestimatedbyRNNs. bi-directionallexicalweights plex (f|e)andplex (e|f),the languagemodel,thereorderingmodel,thewordpenalty,and thephrasepenalty.Thesefeatureshavebeenproveneffec-  tivetoimprovetranslationquality. Recently,someresearchersintegratedneuralnetworksas acomponentintoSMTsystems,toimprovelanguagemod-  eling(Devlinetal.2014),translationmodeling(Aulietal.  2013;Choetal.2014),andphrasereordering(Li,Liu,and Sun2013;Zhaietal.2013). Theimportantdifferencebetweenourmethodandthepre- viousmethodsisthatweinsteadintegrateSMTfeatureswith theNMTmodelviathelog-linearframework,makingthe  NMTmodelextendable. Log-linearNMT WebelievethatintegratingSMTfeaturesmighthelpim-  provetranslationqualityfortheNMTsystems.WeuseFig-  ure2toillustrateouridea.Ateachsteptopredictatar-  getword yj,inadditiontotheprobabilitiesestimatedby RNN,weaddawordtranslationtableandan n-gramlan- guagemodel.Thetranslationtable,estimatingfromword-  alignedbilingualcorpus,canimprovelexicaltranslationand  translatethelow-frequencywordswhicharetakenasun-  knownwords.Thelanguagemodelcanmakefulluseoftar-  getmonolingualcorpustoimprovelocalﬂuency.Weusea  log-linearframeworktointegratetheseeffectivefeatures. FeatureDeﬁnition Ourmethodincludesthefollowingfeaturefunctions: 1.TheRNNencoder-decoderfeature.Thisfeatureisthe conditionalprobabilityestimatedbytheNMTmodelthat  predictsatargetwordbasedonthesourcesentenceand  previouslyproducedtargetwords. Hrnn =Jj=1log (g(yj−1,sj,cj))(7)2.Thebi-directionalwordtranslationprobabilities.Ateach stepofdecoding,weestimatethelexicaltranslationprob- abilitiesbetweentargetcandidatesandthecorresponding sourcewords. Htp1=Jj=1Ii=1jilog (p(yj|xi))(8)Htp2=Jj=1Ii=1jilog (p(xi|yj))(9)where,jiistheweightedsoftalignmentsbetweenthe targetword yjandassociatedsourcewords,estimated bytheRNNencode-decoder(Section RNNEncoder- Decoder).p(y|x)andp(x|y)isthewordtranslationprob- abilitiesestimatedfromword-alignedbilingualcorpus,  wherethewordalignmentistrainedwithGIZA++(Och  andNey2004)andthe“grow-diag-ﬁnal”(Koehn,Och,  andMarcu2003)method. Thewordtranslationprobabilitiesarecomputedasfol- lows: p(x|y)=N(x,y )xN(x,y)(10)p(y|x)=N(y,x )yN(y,x)(11)whereN(x,y )istheco-occurrenceofthecorresponding words xandy.3.Thestandard n-gramlanguagemodel. Hlm=Jj=1log (p(yj|yj−1,...,y j−n+1))(12)Thelanguagemodelistrainedontargetmonolingualcor-  pus.Thusthisfeatureallowsustomakeuseofalarge-  scalemonolingualcorpusoftargetlanguage. 4.Thewordrewardfeature. Hwp=Jj=11(13)Thefeatureisthenumberofwordsinthetargetsentence,  whichcouldcontrolanappropriatelengthoftranslation. ComparedwiththeoriginalRNNmodel,weadd threeadditionalfeaturesforeachstategeneratedbythe RNNdecoderduringdecoding.Thetranslationcanbe  generatedfromtheﬁnalstatewiththehighesttotalscore. HandlingtheOOVProblem Asmentioned,theNMTencoder-decoderusuallyfacesase-  riousOOVproblem.Thepost-processingmethod(Luong  etal.2015)didnotbeneﬁtfromthecontextualinformation  duringdecoding.Weinsteaduseawordtranslationtable, automaticallyextractedfromword-alignedbilingualcorpus,  totranslatetheOOVwordsduringdecoding. SeeFigure3forillustration.Inordertoproducethecor- recttranslationfortheOOVword,weﬁrstlyﬁnditscor- respondingsourceword.Accordingtothealignmentprob-  abilitiesestimatedbytheRNNmodel,the“UNK”symbol releihengliuandtearsUNK13=0.123=0.233=0.7hengliucrossﬂow0.6 hengliuover0.2 ···translationtable Figure3:Illustrationforrecoveringunknownwordusing translationtable. ijisthealignmentprobabilitiesestimated bytheRNNencoder-decoder. referstothesourceword“ hengliu”.Thenweobtaintrans- lationcandidatesfromthewordtranslationtable.Theﬁnal translationisdeterminedbytheproposedlog-linearmodel,  consideringrichcontextualinformationonboththesource  andtargetsidesduringdecoding. DecodingTheRNNdecoderutilizesabeamsearchalgorithm(Bah-  danau,Cho,andBengio2014)toproducetranslationthat  maximizestheconditionaltranslationprobability, ˆy=argmax ˜yp(˜y|˜x)(14)Givenasourcesentence,thedecoderpredictsandoutputs  targetsentenceswordbyword.Thedecodingstartsfroman  initialstate.Ateachtimestep,thedecoderselectstop- N states( Nisthebeamsize)andexpandsthemuntiltheend- of-sentence(EOS)symbolisproduced.Theﬁnaltranslation  isgeneratedbytracingbackfromtheﬁnalstatewiththe  highestscore. WemodiﬁedthedecoderofGroundHogtoperformtrans- lationunderthelog-linearframework.Ateachdecoding  state,theGroundHogonlyusethescoreestimatedbyRNN  (Equation.7)toselecttop- Ncandidatesfromthetargetvo- cabulary.Inourdecoder,weadditionallycalculatetheword  translationprobabilities,thelanguagemodelscoreandthe  currentsentencelengthateachstate.Foreachwordintar-  getvocabulary,wecalculateatotalscorewithEquation6.  Thenweusethescoretogeneratebettercandidatelistsince  morefeaturesareusedthantheoriginalNMTmodel.The  weightsofthelog-linearmodelsaretunedusingthestan-  dardminimum-error-rate-training(MERT)(Och2003)al-  gorithm.Tospeedupthedecoder,weuseapriorityqueue  tochoosethebestcandidatetobeextended(Huetal.2015)  ratherthanextendingallcandidatesateachstate. ExperimentsWecarriedoutexperimentsonChinese-to-Englishtransla-  tion.Thetrainingcorporaareautomaticallycrawledfrom  theweb,containingabout2.2billionChinesewordsand2.3  billionEnglishwords.Sofarasweknow,thisisthelargest  amountofcorpusthatisusedtotrainanNMTsystem. SystemDEVTEST GroundHog36.7230.61 +TM+WR37.5931.57 +LM38.1532.94 PBSMT33.8229.57 Table1:BLEUscoresondevelopmentandtestsets. TM=translationmodel,WR=wordreward,LM=language  model,PBSMT=phrasebasedSMT. WeusedNISTMT06asthedevelopmentsetandtested oursystemonNISTMT08.Theevaluationmetriciscase-  insensitiveBLEU-4(Papinenietal.,2002).Thefeature  weightsofthetranslationsystemaretunedwiththestan-  dardminimum-error-rate-training(MERT)(Och2003)to  maximizethesystems’BLEUscoreonthedevelopmentset. Weusetheopen-sourceNMTsystem,GroundHog(Bah- danau,Cho,andBengio2014),withdefaultsettingsasour baselinesystem.Wesetbeamsizeto10fordecoding.As  acomparison,wealsoreporttheperformanceofaphrase-  basedSMT(PBSMT)system,whichisare-implementation ofthestate-of-the-artphrase-basedsystem,Moses(Koehn  etal.2007).OurSMTsystemismuchmoreefﬁcientbothon  traininganddecodingonourlargebilingualcorpus,andthe  translationqualityiscomparablewithMoses.FortheSMT  system,wesetthestack-limitto200andthetranslation-  option-limitto20. Training Totrainthe GroundHog system,welimitthevocabulary to30Kmostfrequentwordsforboththesourceandtar-  getlanguages.Otherwordsarereplacedbyaspecialsym-  bol“UNK”.TheencoderconsistsofaforwardRNNand  abackwardRNN,andeachhas1000hiddenunits.The  decoderhas1000hiddenunits.Thewordembeddingsare 620-dimensional.Amini-batchstochasticgradientdescent (SGD)togetherwithAdadelta(Zeiler2012)areusedto  trainthenetworks.Eachmini-batchofSGDcontains50  sentencepairs.Adadeltaisusedtoadaptthelearningrate  ofparameters( =10 −6and=0.95).Weranboththe traininganddecodingonasinglemachinewithoneGPU  card(NVIDIATeslaK10).Thesystemistrainedwithabout  1,570,000updatesfortheRNNencoder. Forthe PBSMTsystem,weobtainedwordalignmentvia theGIZA++(OchandNey2004)andthe“grow-diag-ﬁnal”  (Koehn,Och,andMarcu2003)method.Wetraineda5-gram  languagemodel(Stolcke2002)withKN-discountonthe  targetsideofthebilingualcorpus.Thewordtranslationta-  bleandthelanguagemodelarethenusedasfeaturesbeing  integratedwiththe GroundHog system.ResultsTable1liststheresultsonNISTtestsets.Weobservedthat  theproposedmethodsigniﬁcantlyimprovesthetranslation  qualityoftheconventionalNMTsystem.Moreover,oursys-  temoutperformsthephrase-basedSMTsystemonthesame  largetrainingcorpus. Speciﬁcally,wecandrawthefollowingconclusionsfrom Table1: 1.Byaddingthewordtranslationtableandthewordre- wardfeatures,ourmethodobtainedsigniﬁcantimprove-  mentsoverthebaseline(theresultsareshownintherow  “+TM+WR”).Therearethreemainreasonsfortheim-  provements.Firstly,thetranslationprobabilitieshelpthe  NMTsystemtoperformbetterlexicaltranslation.Sec-  ondly,thetranslationtableisusedtorecovertransla-  tionsofunknownwords.Thirdly,thewordrewardfeature  makesthedecoderfavorslongtranslation.Theaverage  lengthsoftheoutputsonthetestsetofoursystemand  GroundHogare23.5and21.4,respectively.Thisindicates  thatourmethodalleviatestheinadequatetranslationprob-  lem.Furtheranalysesanddiscussionswillbedescribedin  thenextSection. 2.OurmethodallowstheNMTsystemtoincorporatead- ditionallanguagemodels.Weaddeda 5-gramlanguage modeltrainedonthetargetsideofthebilingualcorpus  tothe GroundHog system(theresultsareshowninthe row“ +LM”).Itisobservedthatourmethodobtainedfur-  therimprovementsonthetestset,asthe n-gramlanguage modelcaptureslocaltargetcontextualinformationand  improvetheﬂuency. Comparedwiththe GroundHog system,oursystem (GroundHog+TM+WR+LM )achievesanabsoluteimprove- mentof2.33pointsinBLEUscore,whichisstatistically  signiﬁcantatp=0.01level(RiezlerandMaxwell2005). AnalysisandDiscussion Inordertofurtherstudytheperformanceoftheproposed  method,wecomparedtheoutputsofthesystems. ImprovingLexicalTranslation TakingtheﬁrstsentenceinTable2asanexample,the GroundHogsystemomitsthetranslationof“ (chuanshu)transmission ”.Infact,thetargetwordsareinthevocabulary butnotselectedbytheRNNmodel. Byintegratingatranslationtable,ourmethodproduces thecorrecttranslationforthesourcewordsomitedbythe GroundHog.Thiscanbeattributedtothefactthatthetrans- lationtableconsistsofwordpairswithtranslationprob-  abilitiesestimatedfromtheword-alignedtrainingcorpus,  providinganotherwaytomeasuretherelationshipbetween  sourceandtargetwords.Inthisexample,thetranslationta-  blecontainsthewordpairs“ chuanshu,transmission ”with highprobabilities. Tofurtherimprovethequalityoflexicaltranslation,we employaconventional n-gram( n=5)languagemodeltoim- provethelocalﬂuency.Forexample,thereisanotheren- try“ chuanshu,transfer ”fortheChineseword“ chuanshu ”inthetranslationtable.The n-gramlanguagemodelcould helpthedecoderpredictthecorrecttranslation,because plm(transmission |seriesofhighspeed )isgreater thanplm(transfer |seriesofhighspeed ).SourceR400  PBSMTYes,liketheR400laptop,anexpansionslotofaseriesofhighspeed wirelesstransmission. GroundHogYes,liketheR400laptop,aseriesofhighspeedwirelessexpansionslot. OurMethod Yes,liketheR400laptop,aseriesofhighspeed transmissionofthe wirelessexpansionslot. Source  PBSMTAllofthepeoplehavebeeninfected,crossﬂowoftears. GroundHogAllthepeopleareinfected,andtearsUNK. OurMethod Allthepeopleareinfected,andtears crossﬂow .Table2:Translationexamples.Chinesewordsinboldarecorrectlytranslatedbyoursystem. SystemOOVPercentage DEVTEST PBSMT1.6%1.8% GroundHog4.4%4.6% OurMethod 0.8%0.9% Table3:StatisticsofthepercentagesoftheOOVwordsfor thePBSMT,GroundHogandourmethod. TranslatingtheOOVWords Table3showsthestatisticsoftheOOVwordsfor PBSMT,GroundHog andoursystemsonNIST06andNIST08test sets.ItisobservedthatallthesystemsconfronttheOOV  problembecausethesourcewordsdonotoccurinthetrain-  ingcorpusorthewordpairsarenotlearnedduetotheword  alignmenterror.However,theproblemismuchmoreserious  forthe GroundHog systemsinceitlimitsthevocabularysize toreducethemodelcomplexity. TheOOVwordsharmthetranslationquality.Asshownin thesecondexampleinTable2,thesourceword“ł(hengliu)”  isnottranslatedby GroundHog .Afterintegratingthetrans- lationtablewithinthelog-linearframework,thiswordwas  correctlytranslatedinto“ crossﬂow ”.AsdemonstratedinTable3,ourmethodreducesabout 82%oftheOOVwordsfortheNMTsystem.Moreover,  thenumberofOOVwordsinoursystemishalfofthat  inthePBSMTsystem.Asweknow,PBSMTsystemex-  tractsword/phrasetranslationsfromword-alignedbilingual  corpus.However,constrainedbytheinaccuratewordalign-  ment,notallwordsinthebilingualcorpusarecoveredinthe  phrasetable,causingOOVproblemsinthePBSMTsystem.  Ideally,theRNNencoder-decoderiscapableoftranslating  allthewordsaslongastheyareencodedinthevocabulary.  AsthevocabularyusedintheRNNencoder-decoderislim-  itedforpracticalreasons,addingwordtranslationtableinto  RNNencoder-decodercombinesthestrengthofbothRNN  andPBSMT,leadingtoafurtherreductionofOOVratio. Table4showstheeffectoftranslatingOOVwordswith thetranslationtable.Therow“ OurMethod ”sharesthesame  settingswithSection Experiment.Inthesesettings,thetrans- lationtableisnotonlyusedtoscorewordpairs,butalso SystemDEVTEST OurMethod 38.1532.94 -OOV 37.9032.57 Table4:EffectoftranslatingOOVwords.OurMethod=  GroundHog+TM+WR+LM,-OOVmeansthetranslationta-  bleisnotusedtorecoverOOVwords. Figure4:BLEUscoreswithrespecttothebeamsizes. usedtotranslateOOVwords.Therow“ -OOV ”meansthat  thetranslationtableisonlyusedtoscorewordpairsinthe  vocabularyoftheRNNencoder-decoder,butnotgenerate  newtranslationcandidatesfortheOOVwords.Itisobserved  thatbytranslatingOOVwords,weobtainedanabsoluteim-  provementof0.37pointsinBLEUscoresonNIST08. ImprovingCandidateList Figure4showstheperformancewithrespecttothebeam  sizeinthedecoder.Wevarybeamsizeattesttimewhile  keepingthefeatureweightsafterMERT.Wecanseethatthe  theBLEUscoreofGroundHogisnotimprovedasthebeam  sizeincreasesafter3.Ontheotherhand,theperformanceof  theproposedmethodisimprovedwiththeincreaseofbeam  size.Thereasonisthat,ourmethodusesmorefeaturesthan  GroundHogtogeneratecandidatelists.Withthebeamsize  increasing,morebettercandidatesareselectedfromthetar- getvocabularybythedecoder. ConclusionandFutureWork Inthispaper,weimproveNMTbyintegratingadditional SMTcomponents(e.g.thetranslationtable,thelanguage  model)underthelog-linearframework,whichmakesthe  NMTapproachbeeasilyextended.Thetranslationtable  istrainedonword-alignedbilingualcorpusviathestan-  dardphrase-basedSMTmethod,andthelanguagemodel  istrainedonmonolingualtargetsentences.Theproposed  methodalleviatesmajorlimitationsofthecurrentNMTar-  chitecture.Thetranslationtablerecoverstheomittedtransla-  tionsofsourcewordsandtheOOVwords,andthelanguage  modelincreaseslocalﬂuencybymakingfulluseofmono-  lingualcorpus.ExperimentsonChinese-to-Englishtrans-  lationtasksshowthatoursystemachievessigniﬁcantim-  provementsoverthebaselineonlargeamountofthetraining  corpuscrawledfromtheweb. Asanewapproach,NMTstillhasmoreroomforim- provement.CurrentRNNencoder-decoderisactuallya  word-basedtranslationsystem.Inthefuture,weplantoim-  proveNMTwithphrasepairs,whicharegoodatcapturing  localwordreordering,idiomtranslation,etc. Acknowledgements ThisresearchissupportedbytheNationalBasicResearch  ProgramofChina(973programNo.2014CB340505).We  wouldliketothankXuanLiuandtheanonymousreviewers  fortheirinsightfulcomments. References Auli,M.;Galley,M.;Quirk,C.;andZweig,G.2013.Joint  languageandtranslationmodelingwithrecurrentneuralnet- works.In Proceedingsofthe2013ConferenceonEmpirical MethodsinNaturalLanguageProcessing ,10441054. Bahdanau,D.;Cho,K.;andBengio,Y.2014.Neuralma- chinetranslationbyjointlylearningtoalignandtranslate.In arXiv:1409.0473[cs.CL] .Brown,P.F.;Pietra,S.A.D.;Pietra,V.J.D.;andMer- cer,R.L.1993.Themathematicsofstatisticalmachine translation:Parameterestimation. ComputationalLinguis- tics19(2):263–311.Cho,K.;vanMerrienboer,B.;Gulcehre,C.;Bahdanau,D.; Bougares,F.;Schwenk,H.;andBengio,Y.2014.Learning phraserepresentationsusingrnnencoder-decoderforstatis- ticalmachinetranslation.In Proceedingsofthe2014Con- ferenceonEmpiricalMethodsinNaturalLanguagePro-  cessing(EMNLP) ,17241734. Devlin,J.;Zbib,R.;Huang,Z.;Lamar,T.;Schwartz,R.;and Makhoul,J.2014.Fastandrobustneuralnetworkjointmod-  elsforstatisticalmachinetranslation.In Proceedingsofthe 52ndAnnualMeetingoftheAssociationforComputational  Linguistics,13701380. Gulcehre,C.;Firat,O.;Xu,K.;Cho,K.;Barrault,L.;Lin, H.-C.;Bougares,F.;Schwenk,H.;andBengio,Y.2015.On  usingmonolingualcorporainneuralmachinetranslation.In  arXiv:1503.03535[cs.CL] .Hu,X.;Li,W.;Lan,X.;Wu,H.;andWang,H.2015.Op- timizedbeamsearchwithconstrainedsoftmaxfornmt.In MTSummitXV .Koehn,P.;Hoang,H.;Birch,A.;Callison-Burch,C.;Fed- erico,M.;Bertoldi,N.;Cowan,B.;Shen,W.;Moran,C.;  Zens,R.;Dyer,C.;Bojar,O.;Constantin,A.;andHerbst,  E.2007.Moses:Opensourcetoolkitforstatisticalmachine  translation.In ACL2007demonstrationsession .Koehn,P.;Och,F.J.;andMarcu,D.2003.Statisticalphrase-  basedtranslation.In ProceedingsofHLT-NAACL2003 ,127–133.Li,P.;Liu,Y.;andSun,M.2013.Recursiveautoencoders foritg-basedtranslation.In ProceedingsoftheConfer- enceonEmpiricalMethodsinNaturalLanguageProcess-  ing,567577. Luong,M.-T.;Sutskever,I.;Le,Q.V.;Vinyals,O.;and Zaremba,W.2015.Addressingtherarewordproblemin neuralmachinetranslation.In Proceedingsofthe53rdAn- nualMeetingoftheAssociationforComputationalLinguis-  ticsandthe7thInternationalJointConferenceonNatural  LanguageProcessing(Volume1:LongPapers) ,11–19. Och,F.J.,andNey,H.2002.Discriminativetrainingand maximumentropymodelsforstatisticalmachinetranslation.  InProceedingsofthe40thAnnualMeetingoftheAssocia- tionforComputationalLinguistics ,295–302. Och,F.J.,andNey,H.2004.Thealignmenttemplateap- proachtostatisticalmachinetranslation.30:417–449. Och,F.J.2003.Minimumerrorratetraininginstatisti- calmachinetranslation.In Proceedingsofthe41stAnnual MeetingoftheAssociationforComputationalLinguistics ,160–167.Riezler,S.,andMaxwell,J.T.2005.Onsomepitfallsin automaticevaluationandsigniﬁcancetestingformt.In Pro- ceedingsoftheACLWorkshoponIntrinsicandExtrinsic EvaluationMeasuresforMachineTranslationand/orSum-  marization,5764. Schuster,M.,andPaliwal,K.K.1997.Bidirectionalrecur- rentneuralnetworks. SignalProcessing IEEETransactions on45(11),2673–2681. Stolcke,A.2002.Srilm–anextensiblelanguagemodeling toolkit.In ProceedingsoftheInternationalConferenceon SpokenlanguageProcessing ,volume2,901–904. Sutskever,I.;Vinyals,O.;andLe,Q.V.2014.Sequenceto sequencelearningwithneuralnetworks.In NIPS2014 .Zeiler,M.D.2012.Adadelta:Anadaptivelearningrate  method.In arXiv:1212.5701[cs.LG] .Zhai,F.;Zhang,J.;Zhou,Y.;andZong,C.2013.Rnn-based  derivationstructurepredictionforsmt.In Proceedingsofthe 52ndAnnualMeetingoftheAssociationforComputational Linguistics,779784.  
ActiveLearningforSpeechRecognition: thePowerofGradients JiajiHuang  ,RewonChild  ,VinayRao  f huangjiaji,rewon,vinay g @baidu.com HairongLiu,SanjeevSatheesh,AdamCoates f liuhairong,sanjeevsatheesh,adamcoates g @baidu.com BaiduSiliconValleyAILab 1195BordeauxDr Sunnyvale,CA,94089 Abstract Intrainingspeechrecognitionsystems,labelingaudioclipscanbeexpensive,and notalldataisequallyvaluable.Activelearningaimstolabelonlythemostin- formativesamplestoreducecost.Forspeechrecognition,scoresand otherlikelihood-basedactivelearningmethodshavebeenshowntobeeffective. Gradient-basedactivelearningmethods,however,arestillnotwell-understood. ThisworkinvestigatestheExpectedGradientLength( EGL )approachinactive learningforend-to-endspeechrecognition.Wejustify EGL fromavariancere- ductionperspective,andobservethat EGL 'smeasureofinformativenesspicks novelsamplesuncorrelatedwithscores.Experimentally,weshowthat EGL canreduceworderrorsby11%,oralternatively,reducethenumberofsam- plestolabelby50%,whencomparedtorandomsampling. 1Introduction State-of-the-artautomaticspeechrecognition(ASR)systems[1]havelargemodelcapacitiesand requirequantitiesoftrainingdatatogeneralize.Labelingthousandsofhoursofaudio, however,isexpensiveandtime-consuming.Anaturalquestiontoaskishowtoachievebetter generalizationwithfewertrainingexamples.Activelearningstudiesthisproblembyidentifying andlabelingonlythemostinformativedata,potentiallyreducingsamplecomplexity.Howmuch activelearningcanhelpinlarge-scale,end-to-endASRsystems,however,isstillanopenquestion. Thespeechrecognitioncommunityhasgenerallytheinformativenessofsamplesbycal- culatingscores.Inparticular,anutteranceisconsideredinformativeifthemostlikely predictionhassmallprobability[3],orifthepredictionsaredistributedveryuniformlyoverthela- bels[7].Thoughmeasuresworkwellinpractice,lessattentionhasbeenfocused ongradient-basedmethodslikeExpectedGradientLength( EGL )[4],wheretheinformativenessis measuredbythenormofthegradientincurredbytheinstance. EGL haspreviouslybeenas intuitivelymeasuringtheexpectedchangeinamodel'sparameters[4].Weformalizethisintuition fromtheperspectiveofasymptoticvariancereduction,andexperimentally,weshow EGL tobesu- periortomethodsonspeechrecognitiontasks.Additionally,weobservethatthe rankingofsamplesscoredby EGL isnotcorrelatedwiththatofscoring,suggesting EGL aspectsofaninstancethatscorescannotcapture.  Equalcontribution. 30thConferenceonNeuralInformationProcessingSystems(NIPS2016),Barcelona,Spain. arXiv:1612.03226v1  [cs.CL]  10 Dec 2016In[4], EGL wasappliedtoactivelearningonsequencelabelingtasks,butourworkisthewe knowoftoapply EGL tospeechrecognitioninparticular.Gradient-basedmethodshavealsofound applicationsoutsideactivelearning.Forexample,[9]suggeststhatinstochasticgradientdescent, samplingtraininginstanceswithprobabilitiesproportionaltotheirgradientlengthscanspeedup convergence.Fromtheperspectiveofvariancereduction,thisimportancesamplingproblemshares manysimilaritiestoproblemsfoundinactivelearning. 2ProblemFormulation Denote x asanutteranceand y thecorrespondinglabel(transcription).Aspeechrecognitionsystem modelstheconditionaldistribution p ( y j x ; ) ,where  aretheparametersinthemodel,and p ( y j x ; ) istypicallyimplementedbyaRecurrentNeuralNetwork(RNN).Atrainingsetisacollectionof ( x ;y ) pairs,denotedas f ( x i ;y i ) g n i =1 .Theparametersofthemodelareestimatedbyminimizingthe negativelog-likelihoodonthetrainingset: ^  n =min  1 n n X i =1 h ` ( x i ;y i ; ) ,  log p ( y i j x i ; ) i : (1) Activelearningseekstoaugmentthetrainingsetwithanewsetofutterancesandlabels f ( x  i ;y  ) g m i =1 inordertoachievegoodgeneralizationonaheld-outtestdataset.Inmanyappli- cations,thereisanunlabeledpool U whichiscostlytolabelinitsentirety. U is queried forthe ﬁmostinformativeﬂinstance(s) x  i ,forwhichthelabel(s) y  i arethenobtained.Wediscussseveral such querystrategies below. 2.1Scores scoringhasbeenusedextensivelyasaproxyfortheinformativenessoftrainingsamples. ,an x  i isconsideredinformativeifthepredictionsareuniformlydistributedoverallthe labels[7],orifthebestpredictionofitslabeliswithlowprobability[3].Bytakingtheinstances whichﬁconfuseﬂthemodel,thesemethodsmayeffectivelyexploreunder-sampledregionsofthe inputspace. 2.2ExpectedGradientLength Intuitively,aninstancecanbeconsideredinformativeifitresultsinlargechangesinmodelpa- rameters.Anaturalmeasureofthechangeisgradientlength, kr  ` ( x i ;y i ;  ) k .Motivatedbythis intuition,ExpectedGradientLength( EGL )[4]pickstheinstancesexpectedtohavethelargestgra- dientlength.Sincelabelsareunknownon U , EGL computestheexpectationofthegradientnorm overallpossiblelabelings.[4]interprets EGL asﬁexpectedmodelchangeﬂ.Inthefollowingsection, weformalizetheintuitionfor EGL andshowthatitfollowsnaturallyfromreducingthevarianceof anestimator. 2.3VarianceintheAsymptote Assumethejointdistributionof ( x ;y ) hasthefollowingform, p ( x ;y j  0 )= p ( y j x ; 0 ) p ( x ) ; where  0 isthetrueparameter,and p ( x ) isindependentof  0 .Byselectingasubsetofthetraining data,weareessentiallychoosinganotherdistribution q ( x ) sothatthe ( x ;y ) pairsaredrawnfrom q ( x ;y j  0 )= p ( y j x ; 0 ) q ( x ) : Statisticalsignalprocessingtheory[6]statesthefollowingasymptoticdistributionof ^  n , p n  ^  n   0  !N (0 ; I  1 q (  0 )]) ; (2) where I q (  0 ) def = E q ( x ;y )  r  log p ( x ;y j  0 ) r >  log p ( x ;y j  0 )  istheFisherInformationMatrix withrespectto q ( x ;y ) .Usingorderapproximationat ` ( x ;y ;  0 ) ,wehaveasymptotically, p n ( ` ( x ;y ; ^  n )  ` ( x ;y ;  0 )) !N (0 ; r >  ` ( x ;y ;  0 ) I  1 q (  0 ) r  ` ( x ;y ;  0 )) : (3) 2 Eq.(3)indicatesthattoreduce ` ( x ;y ; ^  n ) ontestdata,weneedtominimizetheexpectedvariance E p ( x ;y ) [ r >  ` ( x ;y ;  0 ) I  1 q (  0 ) r  ` ( x ;y ;  0 )] overthetestset.ThisiscalledFisherInformationRatio criteriain[8],whichitselfishardtooptimize.Aneasiersurrogateistomaximize tr( I q (  0 )) . SubstitutingEq.(2.3)into I q (  0 ) ,wehave I q (  0 )= E q ( x ;y )  r  log p ( y j x ; 0 ) r >  log p ( y j x ; 0 )  = E q ( x ;y )  r  ` ( x ;y ;  0 ) r >  ` ( x ;y ;  0 )  ; whichisequivalentto max q R q ( x ) R p ( y j x ; 0 ) kr  ` ( x ;y ;  0 ) k 2 dyd x : Apracticalissueisthatwedonotknow  0 inadvance.Wecouldinsteadsubstituteanestimate ^  0 fromapre-trainedmodel,whereitisreasonabletoassumethe ^  0 tobeclosetothetrue  0 .The batchselectionthenworksbytakingthesamplesthathavelargestgradientnorms, i  =argmax i X y p ( y j x i ; ^  0 ) kr  ` ( x i ;y ; ^  0 ) k 2 : (4) ForRNNs,thegradientsforeachpotentiallabelcanbeobtainedbyback-propagation.Another practicalissueisthat EGL marginalizesoverallpossiblelabelings,butinspeechrecognition,the numberoflabelingsscalesexponentiallyinthenumberoftimesteps.Therefore,weonlymarginalize overthe K mostprobablelabelings.Theyareobtainedbybeamsearchdecoding,asin[5].The EGL methodin[4]isalmostthesameasEq.(4),exceptthegradient'snormisnotsquaredin[4]. Herewehaveprovidedamoreformalcharacterizationof EGL tocomplementitsintuitiveinterpre- tationasﬁexpectedmodelchangeﬂin[4].Fornotationalconvenience,wedenoteEq.(4)as EGL in subsequentsections. 3Experiments Weempiricallyvalidate EGL onspeechrecognitiontasks.Inourexperiments,theRNNtakesin spectrogramsofutterances,passingthemthroughtwo2D-convolutionallayers,followedbyseven bi-directionalrecurrentlayersandafully-connectedlayerwithsoftmaxactivation.Allrecurrent layersarebatchnormalized.Ateachtimestep,thesoftmaxactivationsgiveaprobabilitydistribution overthecharacters.CTCloss[2]isthencomputedfromthetimestep-wiseprobabilities. Abasemodel, ^  0 ,istrainedon190hours( ˘ 100Kinstances)oftranscribedspeechdata.Then, itselectsasubsetofa1,700-hour( ˘ 1.1Minstances)unlabeleddataset.Wequerylabelsforthe selectedsubsetandincorporatethemintotraining.Learningratesaretunedonasmallvalidationset of2048instances.Thetrainedmodelisthentestedona156-hour( ˘ 100Kinstances)testsetand wereportCTCloss,CharacterErrorRate(CER)andWordErrorRate(WER). Thescoremethods[3,7]canbeeasilyextendedtooursetup.,fromthe probabilitiesoverthecharacters,wecancomputeanentropypertimestepandthenaveragethem. Thismethodisdenotedas entropy .Wecouldalsotakethemostlikelypredictionandcalculateits CTCloss,normalizedbynumberoftimesteps.Thismethodisdenotedas pCTC (predictedCTC)in thefollowingsections. (a)CTClossondevset (b)CERondevset (c)WERondevset Figure1: Performancemetricsatvariouspercentagesofqueries. EGL showsagreaterreductioninerrorfor smalleramountsofdata.Byallstrategiesconvergeasthequerypercentageapproaches100%. 3 Weimplement EGL bymarginalizingoverthemostlikely100labels,andcompareitwith:1)a random selectionbaseline,2) entropy ,and3) pCTC .Usingthesamebasemodel,eachmethod queriesavariablepercentageoftheunlabeleddataset.Thequeriesarethenincludedintotraining set,andthemodelcontinuestraininguntilconvergence.Fig.1reportsthemetrics(Exactvalues arereportedinTable1intheAppendix)onthetestsetasthequerypercentagevaries.Allthe activelearningmethodsoutperformthe random baseline.Moreover, EGL showsasteeper,more rapidreductioninerrorthanallotherapproaches.,whenquerying20%oftheunlabeled dataset, EGL has11.58%lowerCERand11.09%lowerWERrelativeto random .Theperformance of EGL atquerying20%isonparwith random at40%,suggestingthatusing EGL canleadtoan approximate50%decreaseindatalabeling. 3.1SimilaritybetweenQueryMethods Itisusefultounderstandhowthethreeactivelearningmethodsdifferinmeasuringtheinformative- nessofaninstance.Tocompareanytwomethods,wetakerankingsofinformativenessgivenby thesetwomethods,andplotthemina2-Dranking-vs-rankingcoordinatesystem.Aplotcloseto thediagonalimpliesthatthesetwomethodsevaluateinformativenessinaverysimilarway. Fig.2showstheranking-vs-rankingplotsbetween pCTC and entropy , EGL and entropy .Weob- servethat pCTC rankingsand entropy rankings(Fig.2a)areverycorrelated.Thisislikelybecause theyarebothrelatedtomodeluncertainty.Incontrast, EGL givesverydifferentrankingsfrom en- tropy (Fig.2b).Thissuggests EGL isabletoidentifyaspectsofaninstancethatuncertainty-based measurementscannotcapture. (a) pCTC rankingsvs entropy rankings (b) EGL rankingsvs entropy rankings Figure2: Thedifferenceinhowactivelearningmethodsrankinformativenessofsamples.Rankingsarenor- malizedto [0 ; 1] ,with1beingthemostinformative.In(a), pCTC and entropy areshowntobeverycorrelated. In(b), EGL appearsuncorrelatedwith entropy (and pCTC ).Datasampleshighlightedintheredcircleare consideredveryinformativeby EGL ,butuninformativeby entropy . Wefurtherinvestigatethesamplesforwhich EGL and entropy yieldvastlydifferentestimatesof informativeness,e.g.,theelementsintheredcircleinFig.2b.Theseparticularsamplesconsistof shortutterancescontainingsilence(withbackgroundnoise)orwords.Furtherinvestigationis requiredtounderstandwhetherthesesamplesarenoisyoutliersorwhethertheyareinfactimportant fortrainingend-to-endspeechrecognitionsystems. 4ConclusionandFutureWork Weformallyexplained EGL fromavariancereductionperspectiveandexperimentallytestedits performanceonend-to-endspeechrecognitionsystems.Initialexperimentsshowanotablegainover randomselection,andthatitoutperformsscoremethodsusedintheASRcommunity.We alsoshow EGL measuressampleinformativenessinaverydifferentwayfromscores, givingrisetoopenresearchquestions.Alltheexperimentsreportedherequeryallsamplesina singlebatch.Itisalsoworthconsideringtheeffectsofqueryingsamplesinasequentialmanner. Inthefuture,wewillfurthervalidatetheapproachwithsequentialqueriesandseektomakethe informativenessmeasurerobusttooutliers. 4 Appendix Table1:Performancemetricsatvariousquerypercentages(smallerisbetter,bestinbold) query CTC CER WER random entropy pCTC EGL random entropy pCTC EGL random entropy pCTC EGL 10% 35.97 32.88 33.10 33.24 12.70 11.59 11.73 11.36 31.47 28.29 28.62 28.15 20% 31.31 29.48 29.44 28.14 10.84 10.20 10.17 9.59 26.57 25.29 25.29 23.63 30% 28.80 27.27 27.02 26.43 9.81 9.44 9.22 9.12 24.08 23.32 22.88 22.50 40% 27.23 25.62 25.59 25.31 9.28 8.75 8.68 8.67 22.75 21.51 21.37 21.26 References [1] D.Amodei,S.Ananthanarayanan,R.Anubhai,etal.Deepspeech2:End-to-endspeech recognitioninenglishandmandarin.In 33rdInternationalConferenceonMachineLearning , 2016. [2] A.Graves,S.Fernandez,F.Gomez,andJ.Schmidhuber.Connectionisttemporal labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In Proceedingsofthe23rd internationalconferenceonMachinelearning ,2006. [3] G.RiccardiandD.Hakkani-Tur.Activelearning:Theoryandapplicationstoautomaticspeech recognition. IEEETransactionsonSpeechandAudioProcessing ,13(4):504Œ511,2005. [4] B.Settles.Activelearningliteraturesurvey.Technicalreport,UniversityofWisconsin,Madison, 2009. [5] B.SettlesandM.Craven.Ananalysisofactivelearningstrategiesforsequencelabelingtasks. In Proceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing.Associ- ationforComputationalLinguistics ,2008. [6] J.Sourati,M.Akcakaya,T.K.Leen,D.Erdogmus,andJ.G.Dy.Asymptoticanalysisof objectivesbasedoninformationinactivelearning. arXiv:1605.08798 ,2016. [7] B.Varadarajan,D.Yu,L.Deng,andA.Acero.Maximizingglobalentropyreductionforactive learninginspeechrecognition.In InternationalConferenceonAcoustics,SpeechandSignal Processing ,2009. [8] T.ZhangandF.Oles.Thevalueofunlabeleddataforproblems.In Proceedings ofthe17thInternationalConferenceonMachineLearning ,2000. [9] P.ZhaoandT.Zhang.Stochasticoptimizationwithimportancesamplingforregularizedloss minimization.In Proceedingsofthe32ndInternationalConferenceonMachineLearning ,2015. 5  
DeepSpeech:Scalingupend-to-end speechrecognition AwniHannun  ,CarlCase,JaredCasper,BryanCatanzaro,GregDiamos,ErichElsen, RyanPrenger,SanjeevSatheesh,ShubhoSengupta,AdamCoates,AndrewY.Ng BaiduResearchŒSiliconValleyAILab Abstract Wepresentastate-of-the-artspeechrecognitionsystemdevelopedusingend-to- enddeeplearning.Ourarchitectureissimplerthantraditionalspeech systems,whichrelyonlaboriouslyengineeredprocessingpipelines;thesetradi- tionalsystemsalsotendtoperformpoorlywhenusedinnoisyenvironments.In contrast,oursystemdoesnotneedhand-designedcomponentstomodelback- groundnoise,reverberation,orspeakervariation,butinsteaddirectlylearnsa functionthatisrobusttosucheffects.Wedonotneedaphonemedictionary, noreventheconceptofaﬁphoneme.ﬂKeytoourapproachisawell-optimized RNNtrainingsystemthatusesmultipleGPUs,aswellasasetofnoveldatasyn- thesistechniquesthatallowustoefobtainalargeamountofvarieddata fortraining.Oursystem,calledDeepSpeech,outperformspreviouslypublished resultsonthewidelystudiedSwitchboardHub5'00,achieving16.0%erroronthe fulltestset.DeepSpeechalsohandleschallengingnoisyenvironmentsbetterthan widelyused,state-of-the-artcommercialspeechsystems. 1Introduction Topspeechrecognitionsystemsrelyonsophisticatedpipelinescomposedofmultiplealgorithms andhand-engineeredprocessingstages.Inthispaper,wedescribeanend-to-endspeechsystem, calledﬁDeepSpeechﬂ,wheredeeplearningsupersedestheseprocessingstages.Combinedwitha languagemodel,thisapproachachieveshigherperformancethantraditionalmethodsonhardspeech recognitiontaskswhilealsobeingmuchsimpler.Theseresultsaremadepossiblebytrainingalarge recurrentneuralnetwork(RNN)usingmultipleGPUsandthousandsofhoursofdata.Becausethis systemlearnsdirectlyfromdata,wedonotrequirespecializedcomponentsforspeakeradaptation ornoiseInfact,insettingswhererobustnesstospeakervariationandnoisearecritical, oursystemexcels:DeepSpeechoutperformspreviouslypublishedmethodsontheSwitchboard Hub5'00corpus,achieving16.0%error,andperformsbetterthancommercialsystemsinnoisy speechrecognitiontests. Traditionalspeechsystemsusemanyheavilyengineeredprocessingstages,includingspecialized inputfeatures,acousticmodels,andHiddenMarkovModels(HMMs).Toimprovethesepipelines, domainexpertsmustinvestagreatdealofefforttuningtheirfeaturesandmodels.Theintroduction ofdeeplearningalgorithms[27,30,15,18,9]hasimprovedspeechsystemperformance,usually byimprovingacousticmodels.Whilethisimprovementhasbeendeeplearningstill playsonlyalimitedroleintraditionalspeechpipelines.Asaresult,toimproveperformanceona tasksuchasrecognizingspeechinanoisyenvironment,onemustlaboriouslyengineertherestof thesystemforrobustness.Incontrast,oursystemappliesdeeplearningend-to-endusingrecurrent neuralnetworks.Wetakeadvantageofthecapacityprovidedbydeeplearningsystemstolearn fromlargedatasetstoimproveouroverallperformance.Ourmodelistrainedend-to-endtoproduce  Contactauthor:awnihannun@baidu.com 1 arXiv:1412.5567v2  [cs.CL]  19 Dec 2014transcriptionsandthus,withsufdataandcomputingpower,canlearnrobustnesstonoiseor speakervariationonitsown. Tappingtheofend-to-enddeeplearning,however,posesseveralchallenges:(i)wemust innovativewaystobuildlarge,labeledtrainingsetsand(ii)wemustbeabletotrainnetworks thatarelargeenoughtoeffectivelyutilizeallofthisdata.Onechallengeforhandlinglabeleddata inspeechsystemsisthealignmentoftexttranscriptswithinputspeech.Thisproblemhas beenaddressedbyGraves,Fern ´ andez,GomezandSchmidhuber[13],thusenablingneuralnet- workstoeasilyconsumeunaligned,transcribedaudioduringtraining.Meanwhile,rapidtrainingof largeneuralnetworkshasbeentackledbyCoatesetal.[7],demonstratingthespeedadvantagesof multi-GPUcomputation.Weaimtoleveragetheseinsightstothevisionofagenericlearning system,basedonlargespeechdatasetsandscalableRNNtraining,thatcansurpassmorecompli- catedtraditionalmethods.ThisvisionisinspiredpartlybytheworkofLeeet.al.[27]whoapplied earlyunsupervisedfeaturelearningtechniquestoreplacehand-builtspeechfeatures. WehavechosenourRNNmodeltomapwelltoGPUsandweuseanovelmodelpar- titionschemetoimproveparallelization.Additionally,weproposeaprocessforassemblinglarge quantitiesoflabeledspeechdataexhibitingthedistortionsthatoursystemshouldlearntohandle. Usingacombinationofcollectedandsynthesizeddata,oursystemlearnsrobustnesstorealistic noiseandspeakervariation(includingLombardEffect[20]).Takentogether,theseideassufto buildanend-to-endspeechsystemthatisatoncesimplerthantraditionalpipelinesyetalsoperforms betterondifspeechtasks.DeepSpeechachievesanerrorrateof16.0%onthefullSwitchboard Hub5'00testsetŠthebestpublishedresult.Further,onanewnoisyspeechrecognitiondatasetof ourownconstruction,oursystemachievesaworderrorrateof19.1%wherethebestcommercial systemsachieve30.5%error. Intheremainderofthispaper,wewillintroducethekeyideasbehindourspeechrecognitionsystem. Webeginbydescribingthebasicrecurrentneuralnetworkmodelandtrainingframeworkthatwe useinSection2,followedbyadiscussionofGPUoptimizations(Section3),andourdatacapture andsynthesisstrategy(Section4).Weconcludewithourexperimentalresultsdemonstratingthe state-of-the-artperformanceofDeepSpeech(Section5),followedbyadiscussionofrelatedwork andourconclusions. 2RNNTrainingSetup Thecoreofoursystemisarecurrentneuralnetwork(RNN)trainedtoingestspeechspectrograms andgenerateEnglishtexttranscriptions.Letasingleutterance x andlabel y besampledfroma trainingset X = f ( x (1) ;y (1) ) ; ( x (2) ;y (2) ) ;::: g .Eachutterance, x ( i ) ,isatime-seriesoflength T ( i ) whereeverytime-sliceisavectorofaudiofeatures, x ( i ) t ;t =1 ;:::;T ( i ) .Weusespectrogramsas ourfeatures,so x ( i ) t;p denotesthepowerofthe p 'thfrequencybinintheaudioframeattime t .The goalofourRNNistoconvertaninputsequence x intoasequenceofcharacterprobabilitiesforthe transcription y ,with ^ y t = P ( c t j x ) ,where c t 2f a,b,c, :::; z ; space ; apostrophe ; blank g . OurRNNmodeliscomposedof5layersofhiddenunits.Foraninput x ,thehiddenunitsatlayer l aredenoted h ( l ) withtheconventionthat h (0) istheinput.Thethreelayersarenotrecurrent. Forthelayer,ateachtime t ,theoutputdependsonthespectrogramframe x t alongwitha contextof C framesoneachside. 1 Theremainingnon-recurrentlayersoperateonindependentdata foreachtimestep.Thus,foreachtime t ,the3layersarecomputedby: h ( l ) t = g ( W ( l ) h ( l  1) t + b ( l ) ) where g ( z )=min f max f 0 ;z g ; 20 g istheclipped(ReLu)activationfunctionand W ( l ) ;b ( l ) aretheweightmatrixandbiasparametersforlayer l . 2 Thefourthlayerisabi-directional recurrentlayer[38].Thislayerincludestwosetsofhiddenunits:asetwithforwardrecurrence, 1 Wetypicallyuse C 2f 5 ; 7 ; 9 g forourexperiments. 2 TheReLuunitsareclippedinordertokeeptheactivationsintherecurrentlayerfromexploding;inpractice theunitsrarelysaturateattheupperbound. 2 h ( f ) ,andasetwithbackwardrecurrence h ( b ) : h ( f ) t = g ( W (4) h (3) t + W ( f ) r h ( f ) t  1 + b (4) ) h ( b ) t = g ( W (4) h (3) t + W ( b ) r h ( b ) t +1 + b (4) ) Notethat h ( f ) mustbecomputedsequentiallyfrom t =1 to t = T ( i ) forthe i 'thutterance,while theunits h ( b ) mustbecomputedsequentiallyinreversefrom t = T ( i ) to t =1 . Theh(non-recurrent)layertakesboththeforwardandbackwardunitsasinputs h (5) t = g ( W (5) h (4) t + b (5) ) where h (4) t = h ( f ) t + h ( b ) t .Theoutputlayerisastandardsoftmaxfunction thatyieldsthepredictedcharacterprobabilitiesforeachtimeslice t andcharacter k inthealphabet: h (6) t;k =^ y t;k  P ( c t = k j x )= exp( W (6) k h (5) t + b (6) k ) P j exp( W (6) j h (5) t + b (6) j ) : Here W (6) k and b (6) k denotethe k 'thcolumnoftheweightmatrixand k 'thbias,respectively. Oncewehavecomputedapredictionfor P ( c t j x ) ,wecomputetheCTCloss[13] L (^ y;y ) tomeasure theerrorinprediction.Duringtraining,wecanevaluatethegradient r ^ y L (^ y;y ) withrespectto thenetworkoutputsgiventheground-truthcharactersequence y .Fromthispoint,computingthe gradientwithrespecttoallofthemodelparametersmaybedoneviaback-propagationthroughthe restofthenetwork.WeuseNesterov'sAcceleratedgradientmethodfortraining[41]. 3 Figure1:StructureofourRNNmodelandnotation. ThecompleteRNNmodelisillustratedinFigure1.Notethatitsstructureisconsiderablysimpler thanrelatedmodelsfromtheliterature[14]Šwehavelimitedourselvestoasinglerecurrentlayer (whichisthehardesttoparallelize)andwedonotuseLong-Short-Term-Memory(LSTM)circuits. OnedisadvantageofLSTMcellsisthattheyrequirecomputingandstoringmultiplegatingneu- ronresponsesateachstep.Sincetheforwardandbackwardrecurrencesaresequential,thissmall additionalcostcanbecomeacomputationalbottleneck.Byusingahomogeneousmodelwehave madethecomputationoftherecurrentactivationsasefaspossible:computingtheReLuout- putsinvolvesonlyafewhighlyoptimizedBLASoperationsontheGPUandasinglepoint-wise nonlinearity. 3 Weusemomentumof0.99andannealthelearningratebyaconstantfactor,chosentoyieldthefastest convergence,aftereachepochthroughthedata. 3 2.1Regularization Whilewehavegonetolengthstoexpandourdatasets(c.f.Section4),therecurrent networksweusearestilladeptatthetrainingdata.Inordertoreducevariancefurther,weuse severaltechniques. Duringtrainingweapplyadropout[19]ratebetween5%-10%.Weapplydropoutinthefeed- forwardlayersbutnottotherecurrenthiddenactivations. Acommonlyemployedtechniqueincomputervisionduringnetworkevaluationistorandomly jitterinputsbytranslationsorfeedeachjitteredversionthroughthenetwork,andvote oraveragetheresults[23].SuchjitteringisnotcommoninASR,howeverwefounditto translatetherawaudioby5ms(halfthebankstepsize)totheleftandright,thenforward propagatetherecomputedfeaturesandaveragetheoutputprobabilities.Attesttimewealsousean ensembleofseveralRNNs,averagingtheiroutputsinthesameway. 2.2LanguageModel Whentrainedfromlargequantitiesoflabeledspeechdata,theRNNmodelcanlearntoproduce readablecharacter-leveltranscriptions.Indeedformanyofthetranscriptions,themostlikelychar- actersequencepredictedbytheRNNisexactlycorrectwithoutexternallanguageconstraints.The errorsmadebytheRNNinthiscasetendtobephoneticallyplausiblerenderingsofEnglishwordsŠ Table1showssomeexamples.Manyoftheerrorsoccuronwordsthatrarelyorneverappearinour trainingset.Inpractice,thisishardtoavoid:trainingfromenoughspeechdatato hear allofthe wordsorlanguageconstructionswemightneedtoknowisimpractical.Therefore,weintegrateour systemwithanN-gramlanguagemodelsincethesemodelsareeasilytrainedfromhugeunlabeled textcorpora.Forcomparison,whileourspeechdatasetstypicallyincludeupto3millionutterances, theN-gramlanguagemodelusedfortheexperimentsinSection5.2istrainedfromacorpusof220 millionphrases,supportingavocabularyof495,000words. 4 RNNoutput DecodedTranscription whatistheweatherlikeinbostinrightnow whatistheweatherlikeinbostonrightnow primeminiternerenrmodi primeministernarendramodi arthernticketsforthegame arethereanyticketsforthegame Table1:ExamplesoftranscriptionsdirectlyfromtheRNN(left)witherrorsthatareedbyaddi- tionofalanguagemodel(right). Giventheoutput P ( c j x ) ofourRNNweperformasearchtothesequenceofcharacters c 1 ;c 2 ;::: thatismostprobableaccordingtoboththeRNNoutputandthelanguagemodel(wherethelanguage modelinterpretsthestringofcharactersaswords).,weaimtoasequence c that maximizesthecombinedobjective: Q ( c )=log( P ( c j x ))+  log( P lm ( c ))+  word count ( c ) where  and  aretunableparameters(setbycross-validation)thatcontrolthetrade-offbetween theRNN,thelanguagemodelconstraintandthelengthofthesentence.Theterm P lm denotesthe probabilityofthesequence c accordingtotheN-grammodel.Wemaximizethisobjectiveusinga highlyoptimizedbeamsearchalgorithm,withatypicalbeamsizeintherange1000-8000Šsimilar totheapproachdescribedbyHannunetal.[16]. 3Optimizations Asnotedabove,wehavemadeseveraldesigndecisionstomakeournetworksamenabletohigh- speedexecution(andthusfasttraining).Forexample,wehaveoptedforhomogeneous linearnetworksthataresimpletoimplementanddependonjustafewhighly-optimizedBLAS calls.Whenfullyunrolled,ournetworksincludealmost5billionconnectionsforatypicalutterance 4 WeusetheKenLMtoolkit[17]totraintheN-gramlanguagemodelsinourexperiments. 4 andthusefcomputationiscriticaltomakeourexperimentsfeasible.Weusemulti-GPU training[7,23]toaccelerateourexperiments,butdoingthiseffectivelyrequiressomeadditional work,asweexplain. 3.1Dataparallelism Inordertoprocessdataef,weusetwolevelsofdataparallelism.First,eachGPUprocesses manyexamplesinparallel.Thisisdoneintheusualwaybyconcatenatingmanyexamplesintoa singlematrix.Forinstance,ratherthanperformingasinglematrix-vectormultiplication W r h t inthe recurrentlayer,weprefertodomanyinparallelbycomputing W r H t where H t =[ h ( i ) t ;h ( i +1) t ;::: ] (where h ( i ) t correspondstothe i 'thexample x ( i ) attime t ).TheGPUismostefwhen H t is relativelywide(e.g.,1000examplesormore)andthusweprefertoprocessasmanyexampleson oneGPUaspossible(uptothelimitofGPUmemory). WhenwewishtouselargerminibatchesthanasingleGPUcansupportonitsownweusedata parallelismacrossmultipleGPUs,witheachGPUprocessingaseparateminibatchofexamplesand thencombiningitscomputedgradientwithitspeersduringeachiteration.Wetypicallyuse 2  or 4  dataparallelismacrossGPUs. Dataparallelismisnoteasilyimplemented,however,whenutteranceshavedifferentlengthssince theycannotbecombinedintoasinglematrixmultiplication.Weresolvetheproblembysorting ourtrainingexamplesbylengthandcombiningonlysimilarly-sizedutterancesintominibatches, paddingwithsilencewhennecessarysothatallutterancesinabatchhavethesamelength.This solutionisinspiredbytheITPACK/ELLPACKsparsematrixformat[21];asimilarsolutionwas usedbytheSutskeveretal.[42]toaccelerateRNNsfortext. 3.2Modelparallelism Dataparallelismyieldstrainingspeedupsformodestmultiplesoftheminibatchsize(e.g.,2to 4),butfacesdiminishingreturnsasbatchingmoreexamplesintoasinglegradientupdatefailsto improvethetrainingconvergencerate.Thatis,processing 2  asmanyexampleson 2  asmany GPUsfailstoyielda 2  speedupintraining.Itisalsoineftothetotalminibatchsizebut spreadouttheexamplesto 2  asmanyGPUs:astheminibatch within eachGPUshrinks,most operationsbecomememory-bandwidthlimited.Toscalefurther,weparallelizebypartitioningthe model(ﬁmodelparallelismﬂ[7,10]). Ourmodelischallengingtoparallelizeduetothesequentialnatureoftherecurrentlayers.Since thebidirectionallayeriscomprisedofaforwardcomputationandabackwardcomputationthatare independent,wecanperformthetwocomputationsinparallel.Unfortunately,naivelysplittingthe RNNtoplace h ( f ) and h ( b ) onseparateGPUscommitsustodatatransferswhenwegoto compute h (5) (whichdependsonboth h ( f ) and h ( b ) ).Thus,wehavechosenadifferentpartitioning ofworkthatrequireslesscommunicationforourmodels:wedividethemodelinhalfalongthe time dimension. Alllayersexcepttherecurrentlayercanbetriviallydecomposedalongthetimedimension,withthe halfofthetime-series,from t =1 to t = T ( i ) = 2 ,assignedtooneGPUandthesecondhalf toanotherGPU.Whencomputingtherecurrentlayeractivations,theGPUbeginscomputing theforwardactivations h ( f ) ,whilethesecondbeginscomputingthebackwardactivations h ( b ) .At themid-point( t = T ( i ) = 2 ),thetwoGPUsexchangetheintermediateactivations, h ( f ) T= 2 and h ( b ) T= 2 andswaproles.TheGPUthenthebackwardcomputationof h ( b ) andthesecondGPU theforwardcomputationof h ( f ) . 3.3Striding WehaveworkedtominimizetherunningtimeoftherecurrentlayersofourRNN,sincetheseare thehardesttoparallelize.Asaoptimization,weshortentherecurrentlayersbytakingﬁstepsﬂ (orstrides)ofsize2intheoriginalinputsothattheunrolledRNNhashalfasmanysteps.Thisis similartoaconvolutionalnetwork[25]withastep-sizeof2inthelayer.WeusethecuDNN library[2]toimplementthislayerofconvolutionef. 5 DatasetTypeHoursSpeakers WSJread80280 Switchboardconversational3004000 Fisherconversational200023000 Baiduread50009600 Table2:AsummaryofthedatasetsusedtotrainDeepSpeech.TheWallStreetJournal,Switchboard andFisher[3]corporaareallpublishedbytheLinguisticDataConsortium. 4TrainingData Large-scaledeeplearningsystemsrequireanabundanceoflabeleddata.Foroursystemweneed manyrecordedutterancesandcorrespondingEnglishtranscriptions,buttherearefewpublicdatasets ofsufscale.Totrainourlargestmodelswehavethuscollectedanextensivedatasetconsisting of5000hoursofreadspeechfrom9600speakers.Forcomparison,wehavesummarizedthelabeled datasetsavailabletousinTable2. 4.1Synthesisbysuperposition Toexpandourpotentialtrainingdataevenfurtherweusedatasynthesis,whichhasbeensuccessfully appliedinothercontextstoamplifytheeffectivenumberoftrainingsamples[37,26,6].Inourwork, thegoalisprimarilytoimproveperformanceinnoisyenvironmentswhereexistingsystemsbreak down.Capturinglabeleddata(e.g.,readspeech)fromnoisyenvironmentsisnotpractical,however, andthuswemustotherwaystogeneratesuchdata. Toaorder,audiosignalsaregeneratedthroughaprocessofsuperpositionofsourcesignals. Wecanusethisfacttosynthesizenoisytrainingdata.Forexample,ifwehaveaspeechaudiotrack x ( i ) andaﬁnoiseﬂaudiotrack ˘ ( i ) ,thenwecanformtheﬁnoisyspeechﬂtrack ^ x ( i ) = x ( i ) + ˘ ( i ) to simulateaudiocapturedinanoisyenvironment.Ifnecessary,wecanaddreverberations,echoesor otherformsofdampingtothepowerspectrumof ˘ ( i ) or x ( i ) andthensimplyaddthemtogetherto makefairlyrealisticaudioscenes. Thereare,however,somerisksinthisapproach.Forexample,inordertotake1000hoursofclean speechandcreate1000hoursofnoisyspeech,wewillneeduniquenoisetracksspanningroughly 1000hours.Wecannotsettlefor,say,10hoursofrepeatingnoise,sinceitmaybecomepossible fortherecurrentnetworktomemorizethenoisetrackandﬁsubtractﬂitoutofthesynthesizeddata. Thus,insteadofusingasinglenoisesource ˘ ( i ) withalengthof1000hours,weusealargenumber ofshorterclips(whichareeasiertocollectfrompublicvideosources)andtreatthemasseparate sourcesofnoisebeforesuperimposingallofthem: ^ x ( i ) = x ( i ) + ˘ ( i ) 1 + ˘ ( i ) 2 + ::: . Whensuperimposingmanysignalscollectedfromvideoclips,wecanendupwithﬁnoiseﬂsounds thataredifferentfromthekindsofnoiserecordedinrealenvironments.Toensureagoodmatch betweenoursyntheticdataandrealdata,werejectedanycandidatenoiseclipswheretheaverage powerineachfrequencybanddifferedfromtheaveragepowerobservedinrealnoisy recordings. 4.2CapturingLombardEffect Onechallengingeffectencounteredbyspeechrecognitionsystemsinnoisyenvironmentsisthe ﬁLombardEffectﬂ[20]:speakersactivelychangethepitchoroftheirvoicetoovercome noisearoundthem.This(involuntary)effectdoesnotshowupinrecordedspeechdatasetssince theyarecollectedinquietenvironments.Toensurethattheeffectisrepresentedinourtrainingdata weinducetheLombardeffectintentionallyduringdatacollectionbyplayingloudbackgroundnoise 6 throughheadphoneswornbyapersonastheyrecordanutterance.Thenoiseinducesthemto theirvoice,thusallowingustocapturetheLombardeffectinourtrainingdata. 5 5Experiments Weperformedtwosetsofexperimentstoevaluateoursystem.Inbothcasesweusethemodel describedinSection2trainedfromaselectionofthedatasetsinTable2topredictcharacter-level transcriptions.Thepredictedprobabilityvectorsandlanguagemodelarethenfedintoourdecoder toyieldaword-leveltranscription,whichiscomparedwiththegroundtruthtranscriptiontoyield theworderrorrate(WER). 5.1Conversationalspeech:SwitchboardHub5'00(full) Tocompareoursystemtopriorresearchweuseanacceptedbuthighlychallengingtestset,Hub5'00 (LDC2002S23).Someresearcherssplitthissetintoﬁeasyﬂ(Switchboard)andﬁhardﬂ(CallHome) instances,oftenreportingnewresultsontheeasierportionalone.Weusethefullset,whichisthe mostchallengingcaseandreporttheoverallworderrorrate. Weevaluateoursystemtrainedononlythe300hourSwitchboardconversationaltelephonespeech datasetandtrainedonbothSwitchboard(SWB)andFisher(FSH)[3],a2000hourcorpuscollected inasimilarmannerasSwitchboard.Manyresearchersevaluatemodelstrainedonlywith300hours fromSwitchboardconversationaltelephonespeechwhentestingonHub5'00.Inpartthisisbecause trainingonthefull2000hourFishercorpusiscomputationallydifUsingthetechniquesmen- tionedinSection3oursystemisableperformafullpassoverthe2300hoursofdatainjustafew hours. SincetheSwitchboardandFishercorporaaredistributedatasamplerateof8kHz,wecompute spectrogramsof80linearlyspacedlogbanksandanenergyterm.Thebanksarecomputed overwindowsof20msstridedby10ms.Wedidnotevaluatemoresophisticatedfeaturessuchasthe mel-scalelogbanksorthemel-frequencycepstralcoef SpeakeradaptationiscriticaltothesuccessofcurrentASRsystems[44,36],particularlywhen trainedon300hourSwitchboard.ForthemodelswetestonHub5'00,weapplyasimpleformof speakeradaptationbynormalizingthespectralfeaturesonaperspeakerbasis.Otherthanthis,we donotmodifytheinputfeaturesinanyway. Fordecoding,weusea4-gramlanguagemodelwitha30,000wordvocabularytrainedontheFisher andSwitchboardtranscriptions.Again,hyperparametersforthedecodingobjectivearechosenvia cross-validationonaheld-outdevelopmentset. TheDeepSpeechSWBmodelisanetworkof5hiddenlayerseachwith2048neuronstrainedon only300hourswitchboard.TheDeepSpeechSWB+FSHmodelisanensembleof4RNNseach with5hiddenlayersof2304neuronstrainedonthefull2300hourcombinedcorpus.Allnetworks aretrainedoninputsof+/-9framesofcontext. WereportresultsinTable3.ThemodelfromVeselyetal.(DNN-GMMsMBR)[44]usesase- quencebasedlossfunctionontopofaDNNafterusingatypicalhybridDNN-HMMsystemto realignthetrainingset.TheperformanceofthismodelonthecombinedHub5'00testsetisthebest previouslypublishedresult.Whentrainedonthecombined2300hoursofdatatheDeepSpeechsys- temimprovesuponthisbaselineby2.4%absoluteWERand13.0%relative.ThemodelfromMaas etal.(DNN-HMMFSH)[28]achieves19.9%WERwhentrainedontheFisher2000hourcorpus. ThatsystemwasbuiltusingKaldi[32],state-of-the-artopensourcespeechrecognitionsoftware. WeincludethisresulttodemonstratethatDeepSpeech,whentrainedonacomparableamountof dataiscompetitivewiththebestexistingASRsystems. 5 Wehaveexperimentedwithnoiseplayedthroughheadphonesaswellasthroughcomputerspeakers.Using headphoneshastheadvantagethatweobtainﬁcleanﬂrecordingswithoutthebackgroundnoiseincludedand canaddourownsyntheticnoiseafterward. 7 ModelSWBCHFull Veselyetal.(GMM-HMMBMMI)[44]18.633.025.8 Veselyetal.(DNN-HMMsMBR)[44]12.624.118.4 Maasetal.(DNN-HMMSWB)[28]14.626.320.5 Maasetal.(DNN-HMMFSH)[28]16.023.719.9 Seideetal.(CD-DNN)[39]16.1n/an/a Kingsburyetal.(DNN-HMMsMBRHF)[22]13.3n/an/a Sainathetal.(CNN-HMM)[36]11.5n/an/a Soltauetal.(MLP/CNN+I-Vector)[40] 10.4 n/an/a DeepSpeechSWB 20.031.825.9 DeepSpeechSWB+FSH 12.6 19.316.0 Table3:Publishederrorrates(%WER)onSwitchboarddatasetsplits.ThecolumnslabeledﬁSWBﬂ andﬁCHﬂarerespectivelytheeasyandhardsubsetsofHub5'00. 5.2Noisyspeech Fewstandardsexistfortestingnoisyspeechperformance,soweconstructedourownevaluationset of100noisyand100noise-freeutterancesfrom10speakers.Thenoiseenvironmentsincludeda backgroundradioorTV;washingdishesinasink;acrowdedcafeteria;arestaurant;andinsideacar drivingintherain.Theutterancetextcameprimarilyfromwebsearchqueriesandtextmessages,as wellasnewsclippings,phoneconversations,Internetcomments,publicspeeches,andmoviescripts. Wedidnothaveprecisecontroloverthesignal-to-noiseratio(SNR)ofthenoisysamples,butwe aimedforanSNRbetween2and6dB. Forthefollowingexperiments,wetrainourRNNsonallthedatasets(morethan7000hours)listed inTable2.Sincewetrainfor15to20epochswithnewlysynthesizednoiseineachpass,ourmodel learnsfromover100,000hoursofnoveldata.Weuseanensembleof6networkseachwith5hidden layersof2560neurons.Noformofspeakeradaptationisappliedtothetrainingorevaluationsets. Wenormalizetrainingexamplesonaperutterancebasisinordertomakethetotalpowerofeach exampleconsistent.Thefeaturesare160linearlyspacedlogbankscomputedoverwindows of20msstridedby10msandanenergyterm.Audioareresampledto16kHzpriortothe featurization.Finally,fromeachfrequencybinweremovetheglobalmeanoverthetrainingset anddividebytheglobalstandarddeviation,primarilysotheinputsarewellscaledduringtheearly stagesoftraining. AsdescribedinSection2.2,weusea5-gramlanguagemodelforthedecoding.Wetrainthelan- guagemodelon220millionphrasesoftheCommonCrawl 6 ,selectedsuchthatatleast95%ofthe charactersofeachphraseareinthealphabet.Onlythemostcommon495,000wordsarekept,the restremappedtoan UNKNOWN token. WecomparedtheDeepSpeechsystemtoseveralcommercialspeechsystems:(1)wit.ai,(2)Google SpeechAPI,(3)BingSpeechand(4)AppleDictation. 7 Ourtestisdesignedtobenchmarkperformanceinnoisyenvironments.Thissituationcreateschal- lengesforevaluatingthewebspeechAPIs:thesesystemswillgivenoresultatallwhentheSNRis tooloworinsomecaseswhentheutteranceistoolong.Thereforewerestrictourcomparisontothe subsetofutterancesforwhichallsystemsreturnedanon-emptyresult. 8 Theresultsofevaluating eachsystemonourtestappearinTable4. ToevaluatetheefyofthenoisesynthesistechniquesdescribedinSection4.1,wetrainedtwo RNNs,oneon5000hoursofrawdataandtheothertrainedonthesame5000hoursplusnoise.On the100cleanutterancesbothmodelsperformaboutthesame,9.2%WERand9.0%WERforthe 6 commoncrawl.org 7 wit.aiandGoogleSpeecheachhaveHTTP-basedAPIs.TotestAppleDictationandBingSpeech,weused akernelextensiontoloopaudiooutputbacktoaudioinputinconjunctionwiththeOSXDictationserviceand theWindows8BingspeechrecognitionAPI. 8 Thisleadstomuchhigheraccuraciesthanwouldbereportedifweattributed100%errorincaseswherean APIfailedtorespond. 8 cleantrainedmodelandthenoisetrainedmodelrespectively.However,onthe100noisyutterances thenoisymodelachieves22.6%WERoverthecleanmodel's28.7%WER,a6.1%absoluteand 21.3%relativeimprovement. SystemClean(94)Noisy(82)Combined(176) AppleDictation14.2443.7626.73 BingSpeech11.7336.1222.05 GoogleAPI6.6430.4716.72 wit.ai7.9435.0619.41 DeepSpeech6.5619.0611.85 Table4:Results(%WER)for5systemsevaluatedontheoriginalaudio.Scoresarereported only forutteranceswithpredictionsgivenbyallsystems.Thenumberinparenthesesnexttoeachdataset, e.g.Clean(94),isthenumberofutterancesscored. 6RelatedWork Severalpartsofourworkareinspiredbypreviousresults.Neuralnetworkacousticmodelsandother connectionistapproacheswereintroducedtospeechpipelinesintheearly1990s[1,34,11]. Thesesystems,similartoDNNacousticmodels[30,18,9],replaceonlyonestageofthespeech recognitionpipeline.Mechanically,oursystemissimilartoothereffortstobuildend-to-endspeech systemsfromdeeplearningalgorithms.Forexample,Gravesetal.[13]havepreviouslyintroduced theﬁConnectionistTemporal(CTC)lossfunctionforscoringtranscriptionsproduced byRNNsand,withLSTMnetworks,havepreviouslyappliedthisapproachtospeech[14].Wesim- ilarlyadopttheCTClossforpartofourtrainingprocedurebutusemuchsimplerrecurrentnetworks withactivations[12,29,31].Ourrecurrentnetworkissimilartothebidirectional RNNusedbyHannunetal.[16],butwithmultiplechangestoenhanceitsscalability.Byfocusing onscalability,wehaveshownthatthesesimplernetworkscanbeeffectiveevenwithoutthemore complexLSTMmachinery. OurworkiscertainlynotthetoexploitscalabilitytoimproveperformanceofDLalgorithms. Thevalueofscalabilityindeeplearningiswell-studied[8,24]andtheuseofparallelprocessors (includingGPUs)hasbeeninstrumentaltorecentlarge-scaleDLresults[43,24].EarlyportsofDL algorithmstoGPUsrevealedspeedgains[33].Researchershavealsobegunchoosing designsthatmapwelltoGPUhardwaretogainevenmoreefy,includingconvolutional[23, 4,35]andlocallyconnected[7,5]networks,especiallywhenoptimizedlibrarieslikecuDNN[2] andBLASareavailable.Indeed,usinghigh-performancecomputinginfrastructure,itispossible todaytotrainneuralnetworkswithmorethan10billionconnections[7]usingclustersofGPUs. Theseresultsinspiredustofocusonmakingscalabledesignchoicestoefutilizemany GPUsbeforetryingtoengineerthealgorithmsandmodelsthemselves. Withthepotentialtotrainlargemodels,thereisaneedforlargetrainingsetsaswell.Inother suchascomputervision,largelabeledtrainingsetshaveenabledleapsinperformance astheyareusedtofeedlargerandlargerDLsystems[43,23].Inspeechrecognition,however, suchlargetrainingsetsarelesscommon,withtypicalbenchmarkshavingtrainingsetsranging fromtensofhours(e.g.theWallStreetJournalcorpuswith80hours)toseveralhundredsofhours (e.g.SwitchboardandBroadcastNews).Largerbenchmarkdatasets,suchastheFishercorpus[3] with2000hoursoftranscribedspeech,arerareandonlyrecentlybeingstudied.Tofullyutilize theexpressivepoweroftherecurrentnetworksavailabletous,werelynotonlyonlargesetsof labeledutterances,butalsoonsynthesistechniquestogeneratenovelexamples.Thisapproachis wellknownincomputervision[37,26,6]butwehavefoundthisespeciallyconvenientandeffective forspeechwhendoneproperly. 7Conclusion Wehavepresentedanend-to-enddeeplearning-basedspeechsystemcapableofoutperformingexist- ingstate-of-the-artrecognitionpipelinesintwochallengingscenarios:clear,conversationalspeech 9 andspeechinnoisyenvironments.Ourapproachisenabledparticularlybymulti-GPUtrainingand bydatacollectionandsynthesisstrategiestobuildlargetrainingsetsexhibitingthedistortionsour systemmusthandle(suchasbackgroundnoiseandLombardeffect).Combined,thesesolutionsen- ableustobuildadata-drivenspeechsystemthatisatoncebetterperformingthanexistingmethods whilenolongerrelyingonthecomplexprocessingstagesthathadstymiedfurtherprogress.We believethisapproachwillcontinuetoimproveaswecapitalizeonincreasedcomputingpowerand datasetsizesinthefuture. Acknowledgments WearegratefultoJiaLei,whoseworkonDLforspeechatBaiduhasspurredusforward,forhis adviceandsupportthroughoutthisproject.WealsothankIanLane,DanPovey,DanJurafsky,Dario Amodei,AndrewMaas,CalisaColeandLiWeiforhelpfulconversations. References [1] H.BourlardandN.Morgan. ConnectionistSpeechRecognition:AHybridApproach .Kluwer AcademicPublishers,Norwell,MA,1993. [2] S.Chetlur,C.Woolley,P.Vandermersch,J.Cohen,J.Tran,B.Catanzaro,andE.Shelhamer. cuDNN:Efprimitivesfordeeplearning. [3] C.Cieri,D.Miller,andK.Walker.TheFishercorpus:aresourceforthenextgenerationsof speech-to-text.In LREC ,volume4,pages69Œ71,2004. [4] D.C.Ciresan,U.Meier,J.Masci,L.M.Gambardella,andJ.Schmidhuber.Flexible,high performanceconvolutionalneuralnetworksforimageIn InternationalJoint ConferenceonIntelligence ,pages1237Œ1242,2011. [5] D.C.Ciresan,U.Meier,andJ.Schmidhuber.Multi-columndeepneuralnetworksforimage In ComputerVisionandPatternRecognition ,pages3642Œ3649,2012. [6] A.Coates,B.Carpenter,C.Case,S.Satheesh,B.Suresh,T.Wang,D.J.Wu,andA.Y.Ng. Textdetectionandcharacterrecognitioninsceneimageswithunsupervisedfeaturelearning. In InternationalConferenceonDocumentAnalysisandRecognition ,2011. [7] A.Coates,B.Huval,T.Wang,D.J.Wu,A.Y.Ng,andB.Catanzaro.Deeplearningwith COTSHPC.In InternationalConferenceonMachineLearning ,2013. [8] A.Coates,H.Lee,andA.Y.Ng.Ananalysisofsingle-layernetworksinunsupervisedfeature learning.In 14thInternationalConferenceonAIandStatistics ,pages215Œ223,2011. [9] G.Dahl,D.Yu,L.Deng,andA.Acero.Context-dependentpre-traineddeepneuralnetworks forlargevocabularyspeechrecognition. IEEETransactionsonAudio,Speech,andLanguage Processing ,2011. [10] J.Dean,G.S.Corrado,R.Monga,K.Chen,M.Devin,Q.V.Le,M.Z.Mao,M.Ranzato, A.Senior,P.Tucker,K.Yang,andA.Y.Ng.Largescaledistributeddeepnetworks.In AdvancesinNeuralInformationProcessingSystems25 ,2012. [11] D.EllisandN.Morgan.Sizematters:Anempiricalstudyofneuralnetworktrainingforlarge vocabularycontinuousspeechrecognition.In ICASSP ,volume2,pages1013Œ1016.IEEE, 1999. [12] X.Glorot,A.Bordes,andY.Bengio.Deepsparseneuralnetworks.In 14thInterna- tionalConferenceonIntelligenceandStatistics ,pages315Œ323,2011. [13] A.Graves,S.Fern ´ andez,F.Gomez,andJ.Schmidhuber.Connectionisttemporal Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In ICML ,pages369Œ 376.ACM,2006. [14] A.GravesandN.Jaitly.Towardsend-to-endspeechrecognitionwithrecurrentneuralnet- works.In ICML ,2014. [15] R.Grosse,R.Raina,H.Kwong,andA.Y.Ng.Shift-invariancesparsecodingforaudioclassi-  arXivpreprintarXiv:1206.5241 ,2012. 10 [16] A.Y.Hannun,A.L.Maas,D.Jurafsky,andA.Y.Ng.First-passlargevocabularycon- tinuousspeechrecognitionusingbi-directionalrecurrentDNNs.abs/1408.2873,2014. http://arxiv.org/abs/1408.2873. [17] K.I.Pouzyrevsky,J.H.Clark,andP.Koehn.ScalableKneser-Neylan- guagemodelestimation.In Proceedingsofthe51stAnnualMeetingoftheAssociationfor ComputationalLinguistics ,Bulgaria,82013. [18] G.Hinton,L.Deng,D.Yu,G.Dahl,A.Mohamed,N.Jaitly,A.Senior,V.Vanhoucke, P.Nguyen,T.Sainath,andB.Kingsbury.Deepneuralnetworksforacousticmodelingin speechrecognition. IEEESignalProcessingMagazine ,29(November):82Œ97,2012. [19] G.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,andR.R.Salakhutdinov.Improv- ingneuralnetworksbypreventingco-adaptationoffeaturedetectors.abs/1406.7806,2014. http://arxiv.org/abs/1406.7806. [20] J.-C.Junqua.TheLombardxanditsroleonhumanlistenersandautomaticspeechrecog- nizers. JournaloftheAcousticalSocietyofAmerica ,1:510Œ524,1993. [21] D.R.Kincaid,T.C.Oppe,andD.M.Young.Itpackv2dusersguide.1989. [22] B.Kingsbury,T.Sainath,andH.Soltau.ScalableminimumBayesrisktrainingofdeepneural networkacousticmodelsusingdistributedhessian-freeoptimization.In Interspeech ,2012. [23] A.Krizhevsky,I.Sutskever,andG.Hinton.Imagenetwithdeepconvolutional neuralnetworks.In AdvancesinNeuralInformationProcessingSystems25 ,pages1106Œ1114, 2012. [24] Q.Le,M.Ranzato,R.Monga,M.Devin,K.Chen,G.Corrado,J.Dean,andA.Ng.Building high-levelfeaturesusinglargescaleunsupervisedlearning.In InternationalConferenceon MachineLearning ,2012. [25] Y.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,W.Hubbard,andL.D.Jackel. Backpropagationappliedtohandwrittenzipcoderecognition. NeuralComputation ,1:541Œ 551,1989. [26] Y.LeCun,F.J.Huang,andL.Bottou.Learningmethodsforgenericobjectrecognitionwith invariancetoposeandlighting.In ComputerVisionandPatternRecognition ,volume2,pages 97Œ104,2004. [27] H.Lee,P.Pham,Y.Largman,andA.Y.Ng.Unsupervisedfeaturelearningforaudio tionusingconvolutionaldeepbeliefnetworks.In AdvancesinNeuralInformationProcessing Systems ,pages1096Œ1104,2009. [28] A.L.Maas,A.Y.Hannun,C.T.Lengerich,P.Qi,D.Jurafsky,andA.Y.Ng.Increasing deepneuralnetworkacousticmodelsizeforlargevocabularycontinuousspeechrecognition. abs/1406.7806,2014.http://arxiv.org/abs/1406.7806. [29] A.L.Maas,A.Y.Hannun,andA.Y.Ng.nonlinearitiesimproveneuralnetwork acousticmodels.In ICMLWorkshoponDeepLearningforAudio,Speech,andLanguage Processing ,2013. [30] A.Mohamed,G.Dahl,andG.Hinton.Acousticmodelingusingdeepbeliefnetworks. IEEE TransactionsonAudio,Speech,andLanguageProcessing ,(99),2011. [31] V.NairandG.E.Hinton.Rectilinearunitsimproverestrictedboltzmannmachines.In 27thInternationalConferenceonMachineLearning ,pages807Œ814,2010. [32] D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,K.Vesel ´ y,N.Goel,M.Han- nemann,P.Motlicek,Y.Qian,P.Schwarz,J.Silovsky,andG.Stemmer.TheKaldispeech recognitiontoolkit.In ASRU ,2011. [33] R.Raina,A.Madhavan,andA.Ng.Large-scaledeepunsupervisedlearningusinggraphics processors.In 26thInternationalConferenceonMachineLearning ,2009. [34] S.Renals,N.Morgan,H.Bourlard,M.Cohen,andH.Franco.Connectionistprobability estimatorsinHMMspeechrecognition. IEEETransactionsonSpeechandAudioProcessing , 2(1):161Œ174,1994. [35] T.Sainath,B.Kingsbury,A.Mohamed,G.Dahl,G.Saon,H.Soltau,T.Beran,A.Aravkin, andB.Ramabhadran.ImprovementstodeepconvolutionalneuralnetworksforLVCSR.In ASRU ,2013. 11 [36] T.N.Sainath,A.rahmanMohamed,B.Kingsbury,andB.Ramabhadran.Deepconvolutional neuralnetworksforLVCSR.In ICASSP ,2013. [37] B.Sapp,A.Saxena,andA.Y.Ng.Afastdatacollectionandaugmentationprocedurefor objectrecognition.In AAAITwenty-ThirdConferenceonIntelligence ,2008. [38] M.SchusterandK.K.Paliwal.Bidirectionalrecurrentneuralnetworks. IEEETransactions onSignalProcessing ,45(11):2673Œ2681,1997. [39] F.Seide,G.Li,X.Chen,andD.Yu.Featureengineeringincontext-dependentdeepneural networksforconversationalspeechtranscription.In ASRU ,2011. [40] H.Soltau,G.Saon,andT.N.Sainath.Jointtrainingofconvolutionalandnon-convolutional neuralnetworks.In ICASSP ,2014. [41] I.Sutskever,J.Martens,G.Dahl,andG.Hinton.Ontheimportanceofmomentumandinitial- izationindeeplearning.In 30thInternationalConferenceonMachineLearning ,2013. [42] I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequencelearningwithneuralnetworks. 2014.http://arxiv.org/abs/1409.3215. [43] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,and A.Rabinovich.Goingdeeperwithconvolutions.2014. [44] K.Vesely,A.Ghoshal,L.Burget,andD.Povey.Sequence-discriminativetrainingofdeep neuralnetworks.In Interspeech ,2013. 12  
DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin DarioAmodei,SundaramAnanthanarayanan,RishitaAnubhai,JingliangBai,EricBattenberg,CarlCase, JaredCasper,BryanCatanzaro,QiangCheng,GuoliangChen,JieChen,JingdongChen,ZhijieChen, MikeChrzanowski,AdamCoates,GregDiamos,KeDing,NiandongDu,ErichElsen,JesseEngel,WeiweiFang, LinxiFan,ChristopherFougner,LiangGao,CaixiaGong,AwniHannun,TonyHan,LappiVainoJohannes, BingJiang,CaiJu,BillyJun,PatrickLeGresley,LibbyLin,JunjieLiu,YangLiu,WeigaoLi,XiangangLi, DongpengMa,SharanNarang,AndrewNg,SherjilOzair,YipingPeng,RyanPrenger,ShengQian, ZongfengQuan,JonathanRaiman,VinayRao,SanjeevSatheesh,DavidSeetapun,ShubhoSengupta, KavyaSrinet,AnuroopSriram,HaiyuanTang,LiliangTang,ChongWang,JidongWang,KaifuWang,YiWang, ZhijianWang,ZhiqianWang,ShuangWu,LikaiWei,BoXiao,WenXie,YanXie,DaniYogatama,BinYuan, JunZhan,ZhenyaoZhu BaiduSiliconValleyAILab 1 ,1195BordeauxAvenue,SunnyvaleCA94086USA BaiduSpeechTechnologyGroup,No.10XibeiwangEastStreet,KeJiYuan,HaidianDistrict,Beijing100193CHINA Abstract Weshowthatanend-to-enddeeplearningap- proachcanbeusedtorecognizeeitherEnglish orMandarinChinesespeechŒtwovastlydifferent languages.Becauseitreplacesentirepipelines ofhand-engineeredcomponentswithneuralnet- works,end-to-endlearningallowsustohan- dleadiversevarietyofspeechincludingnoisy environments,accentsanddifferentlanguages. KeytoourapproachisourapplicationofHPC techniques,enablingexperimentsthatpreviously tookweekstonowrunindays.Thisallowsus toiteratemorequicklytoidentifysuperiorarchi- tecturesandalgorithms.Asaresult,inseveral cases,oursystemiscompetitivewiththetran- scriptionofhumanworkerswhenbenchmarked onstandarddatasets.Finally,usingatechnique calledBatchDispatchwithGPUsinthedatacen- ter,weshowthatoursystemcanbeinexpen- sivelydeployedinanonlinesetting,delivering lowlatencywhenservingusersatscale. 1.Introduction Decadesworthofhand-engineereddomainknowledgehas goneintocurrentstate-of-the-artautomaticspeechrecogni- tion(ASR)pipelines.Asimplebutpowerfulalternativeso- lutionistotrainsuchASRmodelsend-to-end,usingdeep 1 Contactauthor:sanjeevsatheesh@baidu.com Proceedingsofthe 33 rd InternationalConferenceonMachine Learning ,NewYork,NY,USA,2016.JMLR:W&CPvolume 48.Copyright2016bytheauthor(s). learningtoreplacemostmoduleswithasinglemodelas in(Hannunetal.,2014a)and(Graves&Jaitly,2014b). This"endtoend"visionoftrainingthetrain- ingprocessbyremovingtheengineeringrequiredforthe bootstrapping/alignment/clustering/HMMmachineryoften usedtobuildstate-of-the-artASRmodels.Onsuchasys- tem,builtonend-to-enddeeplearning,wecanemploya rangeofdeeplearningtechniques:capturinglargetraining sets,traininglargermodelswithhighperformancecom- puting,andmethodicallyexploringthespaceofneuralnet- workarchitectures. Thispaperdetailsourcontributiontothemodelarchi- tecture,largelabeledtrainingdatasets,andcomputational scaleforspeechrecognition.Thisincludesanextensivein- vestigationofmodelarchitectures,andourdatacapturing pipelinethathasenabledustocreatelargerdatasetsthan whatistypicallyusedtotrainspeechrecognitionsystems. Webenchmarkoursystemonseveralpubliclyavailabletest setswithagoalofeventuallyattaininghuman-levelperfor- mance.Tothatend,wehavealsomeasuredtheperfor- manceofcrowdworkersoneachbenchmarkforcompari- son.WethatourbestMandarinChinesespeechsystem transcribesshortvoice-querylikeutterancesbetterthana typicalMandarinChinesespeaker. Theremainderofthepaperisasfollows.Webeginwitha reviewofrelatedworkindeeplearning,end-to-endspeech recognition,andscalabilityinSection2.Section3de- scribesthearchitecturalandalgorithmicimprovementsto themodelandSection4explainshowtoefcom- putethem.Wediscussthetrainingdataandstepstakento furtheraugmentthetrainingsetinSection5.Ananalysis ofresultsforoursysteminEnglishandMandarinispre- sentedinSection6.Weendwithadescriptionofthesteps neededtodeployoursystemtorealusersinSection7. DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin 2.RelatedWork Thisworkisinspiredbypreviousworkinbothdeeplearn- ingandspeechrecognition.Feed-forwardneuralnet- workacousticmodelswereexploredmorethan20years ago(Bourlard&Morgan,1993;Renalsetal.,1994).Re- currentneuralnetworksandnetworkswithconvolution werealsousedinspeechrecognitionaroundthesame time(Robinsonetal.,1996;Waibeletal.,1989).More recentlyDNNshavebecomeaintheASRpipeline withalmostallstateoftheartspeechworkcontainingsome formofdeepneuralnetwork(Mohamedetal.,2011;Hin- tonetal.,2012;Dahletal.,2011;N.Jaitly&Vanhoucke, 2012;Seideetal.,2011).Convolutionalnetworkshavealso beenfoundforacousticmodels(Abdel-Hamid etal.,2012;Sainathetal.,2013).Recurrentneuralnet- worksarebeginningtobedeployedinstate-of-theartrec- ognizers(Gravesetal.,2013;H.Saketal.,2014)and workwellwithconvolutionallayersforthefeatureextrac- tion(Sainathetal.,2015). End-to-endspeechrecognitionisanactiveareaofre- search,showingcompellingresultswhenusedtore- scoretheoutputsofaDNN-HMM(Graves&Jaitly, 2014a)andstandalone(Hannunetal.,2014a).TheRNN encoder-decoderwithattentionperformswellinpredict- ingphonemes(Chorowskietal.,2015)orgraphemes(Bah- danauetal.,2015;Chanetal.,2015).TheCTCloss function(Gravesetal.,2006)coupledwithanRNNto modeltemporalinformationalsoperformswellinend- to-endspeechrecognitionwithcharacteroutputs(Graves &Jaitly,2014a;Hannunetal.,2014b;a;Maasetal., 2015).TheCTC-RNNmodelalsoworkswellinpredicting phonemes(Miaoetal.,2015;Saketal.,2015),thougha lexiconisstillneededinthiscase. Exploitingscaleindeeplearninghasbeencentraltothe successofthethusfar(Krizhevskyetal.,2012;Le etal.,2012).TrainingonasingleGPUresultedinsubstan- tialperformancegains(Rainaetal.,2009),whichweresub- sequentlyscaledlinearlytotwo(Krizhevskyetal.,2012) ormoreGPUs(Coatesetal.,2013).Wetakeadvantageof workinincreasingindividualGPUefyforlow-level deeplearningprimitives(Chetluretal.).Webuiltonthe pastworkinusingmodel-parallelism(Coatesetal.,2013), data-parallelism(Deanetal.,2012)oracombinationofthe two(Szegedyetal.,2014;Hannunetal.,2014a)tocreatea fastandhighlyscalablesystemfortrainingdeepRNNsin speechrecognition. Datahasalsobeencentraltothesuccessofend-to-end speechrecognition,withover7000hoursoflabeledspeech usedin(Hannunetal.,2014a).Dataaugmentationhas beenhighlyeffectiveinimprovingtheperformanceofdeep learningincomputervision(LeCunetal.,2004;Sappetal., 2008;Coatesetal.,2011)andspeechrecognition(Gales Figure1:ArchitectureofthedeepRNNusedinbothEn- glishandMandarinspeech. etal.,2009;Hannunetal.,2014a).Existingspeechsystems canalsobeusedtobootstrapnewdatacollection.Forex- ample,anexistingspeechenginecanbeusedtoalignand thousandsofhoursofaudiobooks(Panayotovetal., 2015).Wedrawinspirationfromthesepastapproachesin bootstrappinglargerdatasetsanddataaugmentationtoin- creasetheeffectiveamountoflabeleddataforoursystem. 3.ModelArchitecture Figure1showsthewireframeofourarchitecture,andlays outtheswappablecomponentswhichweexploreinde- tailinthispaper.Oursystem(similaratitscoretothe onein(Hannunetal.,2014a)),isarecurrentneuralnet- work(RNN)withoneormoreconvolutionalinputlayers, followedbymultiplerecurrent(uniorbidirectional)lay- ersandonefullyconnectedlayerbeforeasoftmaxlayer. Thenetworkistrainedend-to-endusingtheCTClossfunc- tion(Gravesetal.,2006),whichallowsustodirectlypre- dictthesequencesofcharactersfrominputaudio. 2 Theinputstothenetworkareasequenceoflog- spectrogramsofpowernormalizedaudioclips,calculated on20mswindows.Theoutputsarethealphabetofeach language.Ateachoutputtime-step t ,theRNNmakesa prediction, p ( ` t j x ) ,where ` t iseitheracharacterinthe alphabetortheblanksymbol.InEnglishwehave ` t 2 f a,b,c, ::: , z , space , apostrophe , blank g ,wherewehave addedthe space symboltodenotewordboundaries.For theMandarinsystemthenetworkoutputsChi- 2 Mostofourexperimentsusebidirectionalrecurrentlay- erswithclippedunits(ReLU) ˙ ( x )= min f max f x ,0 g ,20 g astheactivationfunction. DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin Architecture BaselineBatchNorm GRU 5-layer,1RNN 13.5514.40 10.53 5-layer,3RNN 11.6110.56 8.00 7-layer,5RNN 10.779.78 7.79 9-layer,7RNN 10.839.52 8.19 9-layer,7RNN noSortaGrad 11.969.78 Table1:ComparisonofWERonadevelopmentsetaswe varydepthofRNN,applicationofBatchNormandSorta- Grad,andtypeofrecurrenthiddenunit.Allnetworkshave 38MparametersŠasdepthincreases,thenumberofhidden unitsperlayerdecreases.Thelasttwocolumnscompare theperformanceofthemodelonthedevsetaswechange thetypeoftherecurrenthiddenunit. nesecharacters. Atinferencetime,CTCmodelsarepairedawithlanguage modeltrainedonabiggercorpusoftext.Weuseaspecial- izedbeamsearch(Hannunetal.,2014b)tothetran- scription y thatmaximizes Q ( y )=log( p RNN ( y j x ))+  log( p LM ( y ))+  wc ( y ) (1) wherewc ( y ) isthenumberofwords(English)orcharac- ters(Chinese)inthetranscription y .Theweight  con- trolstherelativecontributionsofthelanguagemodeland theCTCnetwork.Theweight  encouragesmorewordsin thetranscription.Theseparametersaretunedonaheldout developmentset. 3.1.BatchNormalizationforDeepRNNs Toefabsorbdataaswescalethetrainingset,we increasethedepthofthenetworksbyaddingmorerecur- rentlayers.However,itbecomesmorechallengingtotrain networksusinggradientdescentasthesizeanddepthin- creases.WehaveexperimentedwiththeBatchNormaliza- tion(BatchNorm)methodtotraindeepernetsfaster(Ioffe &Szegedy,2015).RecentresearchhasshownthatBatch- NormcanspeedconvergenceofRNNstraining,though notalwaysimprovinggeneralizationerror(Laurentetal., 2015).Incontrast,wethatwhenappliedtoverydeep networksofRNNsonlargedatasets,thevariantofBatch- Normweusesubstantiallyimprovesgeneralizationer- rorinadditiontoacceleratingtraining. Arecurrentlayerisimplementedas h l t = f ( W l h l  1 t + U l h l t  1 + b ). (2) wheretheactivationsoflayer l attimestep t arecomputed bycombiningtheactivationsfromthepreviouslayer h l  1 t atthesametimestep t andtheactivationsfromthecurrent layerattheprevioustimestep h l t  1 . Figure2:Trainingcurvesoftwomodelstrainedwithand withoutBatchNorm(BN).Weseeawidergapinperfor- manceonthedeeper9-7network(whichhas9layersin total,7ofwhicharevanillabidirectionalRNNs)thanthe shallower5-1network(inwhichonly1ofthe5layersis abidirectionalRNN).Westarttheplotaftertheepoch oftrainingasthecurveismorediftointerpretdueto theSortaGradcurriculummethodmentionedinSection3.2 Asin(Laurentetal.,2015),therearetwowaysofapplying BatchNormtotherecurrentoperation.Anaturalextension istoinsertaBatchNormtransformation, B (  ) ,immediately beforeeverynon-linearityasfollows: h l t = f ( B ( W l h l  1 t + U l h l t  1 )). (3) Inthiscasethemeanandvariancestatisticsareaccumu- latedoverasingletime-stepoftheminibatch.Wedidnot thistobeeffective. Analternative( sequence-wise normalization)istobatch normalizeonlytheverticalconnections.Therecurrent computationisgivenby h l t = f ( B ( W l h l  1 t )+ U l h l t  1 ). (4) Foreachhiddenunitwecomputethemeanandvariance statisticsoverallitemsintheminibatchoverthelength ofthesequence.Figure2showsthatdeepnetworkscon- vergefasterwithsequence-wisenormalization.Table1 showsthattheperformanceimprovementfromsequence- wisenormalizationincreaseswiththedepthofthenetwork, witha12%performancedifferenceforthedeepestnet- work.Westorearunningaverageofthemeanandvariance fortheneuroncollectedduringtraining,andusethesefor evaluation(Ioffe&Szegedy,2015). 3.2.SortaGrad EvenwithBatchNormalization,wetrainingwithCTC tobeoccasionallyunstable,particularlyintheearlystages. Inordertomaketrainingmorestable,weexperimentwith atrainingcurriculum(Bengioetal.,2009;Zaremba& Sutskever,2014),whichacceleratestrainingandresultsin bettergeneralizationaswell. DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin Trainingverydeepnetworks(orRNNswithmanysteps) fromscratchcanfailearlyintrainingsinceoutputsand gradientsmustbepropagatedthroughmanypoorlytuned layersofweights.Inadditiontoexplodinggradients(Pas- canuetal.,2012),CTCoftenendsupassigningnear-zero probabilitytoverylongtranscriptionsmakinggradientde- scentquitevolatile.Thisobservationmotivatesacurricu- lumlearningstrategywetitleSortaGrad:weusethelength oftheutteranceasaheuristicfordifandtrainonthe shorter(easier)utterances ,inthetrainingepochweiteratethrough minibatchesinthetrainingsetinincreasingorderofthe lengthofthelongestutteranceintheminibatch.Afterthe epochtrainingrevertsbacktoarandomorderover minibatches.Table1showsacomparisonoftrainingcost withandwithoutSortaGradonthe9layermodelwith7 recurrentlayers.SortaGradimprovesthestabilityoftrain- ing,andthiseffectisparticularlypronouncedinnetworks withoutBatchNorm,sincetheseareevenlessnumerically stable. 3.3.ComparisonofvanillaRNNsandGRUs Themodelswehaveshownsofarare vanilla RNNswhich aremodeledbyEquation3withReLUactivations.More sophisticatedhiddenunitssuchastheLongShort-Term Memory(LSTM)units(Hochreiter&Schmidhuber,1997) andtheGatedRecurrentUnits(GRU)(Choetal.,2014), havebeenshowntobeveryeffectiveonsimilartasks(Bah- danauetal.,2015).WeexamineGRUsbecauseexperi- mentsonsmallerdatasetsshowtheGRUandLSTMreach similaraccuracyforthesamenumberofparameters,but theGRUsarefastertotrainandlesslikelytodiverge. BothGRUandvanillaRNNarchitecturesfrom BatchNormandshowstrongresultswithdeepnetworks. Thelasttwocolumnsintable1showthatforaednum- berofparameterstheGRUarchitectureachievesbetter WERforallnetworkdepths. 3.4.FrequencyConvolutions Temporalconvolutioniscommonlyusedinspeechrecog- nitiontoefmodeltemporaltranslationinvariance forvariablelengthutterances.Convolutioninfrequencyat- temptstomodelspectralvarianceduetospeakervariability moreconciselythanwhatispossiblewithlargefullycon- nectednetworks. Weexperimentwithaddingbetweenoneandthreelayersof convolution.Thesearebothinthetime-and-frequencydo- main(2D)andinthetime-onlydomain(1D).Inallcases weuseaﬁ same ﬂconvolution.Insomecaseswespecify astride(subsampling)acrosseitherdimensionwhichre- ducesthesizeoftheoutput. Figure3:Lookaheadconvolutionarchitecturewithfuture contextsizeof2. WereportresultsontwodatasetsŠadevelopmentset of2048utterances(ﬁRegularDevﬂ)andamuchnoisier datasetof2048utterances(ﬁNoisyDevﬂ)randomlysam- pledfromtheCHiME2015developmentdatasets(Barker etal.,2015).Wethatmultiplelayersof1Dconvolu- tionprovidesaverysmallThe2Dconvolutions improveresultssubstantiallyonnoisydata,whileprovid- ingasmalloncleandata.Thechangefromone layerof1Dconvolutiontothreelayersof2Dconvolution improvesWERby23.9%onthenoisydevelopmentset. 3.5.LookaheadConvolutionandUnidirectional Models BidirectionalRNNmodelsarechallengingtodeployinan online,low-latencysettingbecausetheycannotstreamthe transcriptionprocessastheutterancearrivesfromtheuser. However,modelswithonlyforwardrecurrencesroutinely performworsethansimilarbidirectionalmodels,imply- ingsomeamountoffuturecontextisvitaltogoodperfor- mance.Onepossiblesolutionistodelaythesystemfrom emittingalabeluntilithasmorecontextasin(Saketal., 2015),butwefounditdiftoinducethisbehaviorin ourmodels.Inordertobuildaunidirectionalmodelwith- outanylossinaccuracy,wedevelopaspeciallayerthatwe calllookaheadconvolution,showninFigure3.Thelayer learnsweightstolinearlycombineeachneuron'sactiva- tions ˝ timestepsintothefuture,andthusallowsustocon- troltheamountoffuturecontextneeded.Thelookahead layerisbyaparametermatrix W 2 R ( d , ˝ ) ,where d matchesthenumberofneuronsinthepreviouslayer.The activations r t forthenewlayerattime-step t are r t , i = ˝ +1 X j =1 W i , j h t + j  1, i ,for1  i  d . (5) Weplacethelookaheadconvolutionaboveallrecurrent layers.Thisallowsustostreamallcomputationbelowthe lookaheadconvolutiononagranularity. DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin ArchitectureChannelsFilterdimensionStrideRegularDevNoisyDev 1-layer1D12801129.5219.36 2-layer1D640,6405,51,29.6719.21 3-layer1D512,512,5125,5,51,1,29.2020.22 1-layer2D3241x112x28.9416.22 2-layer2D32,3241x11,21x112x2,2x19.0615.71 3-layer2D32,32,9641x11,21x11,21x112x2,2x1,2x18.6114.74 Table2:ComparisonofWERfordifferentofconvolutionallayers.Inallcases,theconvolutionsare followedby7recurrentlayersand1fullyconnectedlayer.For2Dconvolutionsthedimensionisfrequencyandthe seconddimensionistime.EachmodelistrainedwithBatchNorm,SortaGrad,andhas35Mparameters. 3.6.AdaptationtoMandarin Toportatraditionalspeechrecognitionpipelinetoanother languagetypicallyrequiresaamountofnew development.Forexample,oneoften needstohand-engineerapronunciationmodel(Shanetal., 2010).Wemayalsoneedtoexplicitlymodellanguage- pronunciationfeatures,suchastonesinMan- darin(Shanetal.,2010;Niuetal.,2013).Sinceourend- to-endsystemdirectlypredictscharacters,thesetimecon- sumingeffortsarenolongerneeded.Thishasenabledus toquicklycreateanend-to-endMandarinspeechrecogni- tionsystem(thatoutputsChinesecharacters)usingtheap- proachdescribedabovewithonlyafewchanges. Theonlyarchitecturalchangeswemaketoournetworks areduetothecharacteristicsoftheChinesecharacterset. Thenetworkoutputsprobabilitiesforabout6000char- acters,whichincludestheRomanalphabet,sincehybrid Chinese-Englishtranscriptsarecommon.Weincuranout ofvocabularyerroratevaluationtimeifacharacterisnot containedinthisset.Thisisnotamajorconcern,asour testsethasonly0.74%outofvocabcharacters. WeuseacharacterlevellanguagemodelinMandarinas wordsarenotusuallysegmentedintext.InSection6.2we showthatourMandarinspeechmodelsshowroughlythe sameimprovementstoarchitecturalchangesasourEnglish speechmodels,suggestingthatmodelingknowledgefrom developmentinonelanguagetransferswelltoothers. 4.SystemOptimizations Ournetworkshavetensofmillionsofparameters,anda trainingexperimentinvolvestensofsingle-precisionex- aFLOPs.Sinceourabilitytoevaluatehypothesesabout ourdataandmodelsdependsontrainingspeed,wecreated ahighlyoptimizedtrainingsystembasedonhighperfor- mancecomputing(HPC)infrastructure. 3 Althoughmany frameworksexistfortrainingdeepnetworksonparallel 3 Oursoftwarerunsondensecomputenodeswith8NVIDIA TitanXGPUspernodewithatheoreticalpeakthroughputof48 single-precisionTFLOP/s. machines,wehavefoundthatourabilitytoscalewellisof- tenbottleneckedbyunoptimizedroutinesthataretakenfor granted.Therefore,wefocusoncarefuloptimizationofthe mostimportantroutinesusedfortraining.,we createdcustomizedAll-ReducecodeforOpenMPItosum gradientsacrossGPUsonmultiplenodes,developedafast implementationofCTCforGPUs,andusecustommem- oryallocators.Takentogether,thesetechniquesenableus tosustain overall 45%oftheoreticalpeakperformanceon eachnode. OurtrainingdistributesworkovermultipleGPUsinadata- parallelfashionwithsynchronousSGD,whereeachGPU usesalocalcopyofthemodeltoworkonaportionof thecurrentminibatchandthenexchangescomputedgra- dientswithallotherGPUs.WeprefersynchronousSGD becauseitisreproducible,whichfacilitatesdiscovering andregressions.Inthissetup,however,theGPUs mustcommunicatequickly(usingan"All-Reduce"opera- tion)ateachiterationinordertoavoidwastingcomputa- tionalcycles.Priorworkhasusedasynchronousupdatesto mitigatethisissue(Deanetal.,2012;Rechtetal.,2011). WeinsteadfocusedonoptimizingtheAll-Reduceopera- tionitself,achievinga4x-21xspeedupusingtechniques toreduceCPU-GPUcommunicationforourwork- loads.Similarly,toenhanceoverallcomputation,wehave usedhighly-optimizedkernelsfromNervanaSystemsand NVIDIAthataretunedforourdeeplearningapplications. Wesimilarlydiscoveredthatcustommemoryallocation routineswerecrucialtomaximizingperformanceasthey reducethenumberofsynchronizationsbetweenGPUand CPU. WealsofoundthattheCTCcostcomputationaccounted forafractionofrunningtime.Sincenopublic well-optimizedcodeforCTCexisted,wedevelopedafast GPUimplementationthatreducedoveralltrainingtimeby 10-20%. 4 4 DetailsofourCTCimplementationwillbemadeavailable alongwithopensourcecode. DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin 5.TrainingData Large-scaledeeplearningsystemsrequireanabundanceof labeledtrainingdata.FortrainingourEnglishmodel,we use11,940hoursoflabeledspeechcontaining8million utterances,andtheMandarinsystemuses9,400hoursof labeledspeechcontaining11millionutterances. 5.1.DatasetConstruction PartsoftheEnglishandMandarindatasetswerecreated fromrawdatacapturedaslongaudioclipswithnoisytran- scriptions.Inordertosegmenttheaudiointoseveralsec- ondlongclips,wealignthespeechwiththetranscript.For agivenaudio-transcriptpair ( x , y ) ,themostlikelyalign- mentiscalculatedas `  =argmax ` 2 Align ( x , y ) T Y t p ctc ( ` t j x ;  ). (6) ThisisessentiallyaViterbialignmentfoundusingaRNN modeltrainedwithCTC.SincetheCTClossfunctioninte- gratesoverallalignments,thisisnotguaranteedtoproduce anaccuratealignment.However,wefoundthatthisap- proachproducesanaccuratealignmentwhenusingabidi- rectionalRNN. Inordertooutclipswithpoortranscriptions,webuild asimplewiththefollowingfeatures:therawCTC cost,theCTCcostnormalizedbythesequencelength,the CTCcostnormalizedbythetranscriptlength,theratioof thesequencelengthtothetranscriptlength,thenumberof wordsinthetranscriptionandthenumberofcharactersin thetranscription.Wecrowdsourcethelabelsforbuilding thisdataset.FortheEnglishdataset,wethatthe teringpipelinereducestheWERfrom17%to5%while retainingmorethan50%oftheexamples. Additionally,wedynamicallyaugmentthedatasetby addinguniquenoiseeveryepochwithanSNRbetween 0dBand30dB,justasin(Hannunetal.,2014a;Sainath etal.,2015). 5.2.ScalingData Weshowtheeffectofincreasingtheamountoflabeled trainingdataonWERinTable3.Thisisdonebyran- domlysamplingthefulldatasetbeforetraining.Foreach dataset,themodelwastrainedforupto20epochswith early-stoppingbasedontheerroronaheldoutdevelop- mentsettopreventovTheWERdecreasesby ˘ 40%relativeforeachfactorof10increaseintrainingset size.WealsoobserveaconsistentgapinWER( ˘ 60%rel- ative)betweentheregularandnoisydatasets,implyingthat moredatabothcasesequally. FractionofDataHoursRegularDevNoisyDev 1%12029.2350.97 10%120013.8022.99 20%240011.6520.41 50%60009.5115.90 100%120008.4613.59 Table3:ComparisonofEnglishWERforRegularand Noisydevelopmentsetsonincreasingtrainingdatasetsize. Themodelhas9-layers(2layersof2Dconvolutionand7 recurrentlayers)with68Mparameters. 6.Results Tobetterassessthereal-worldapplicabilityofourspeech system,weevaluateonawiderangeoftestsets.Weuse severalpubliclyavailablebenchmarksandseveraltestsets collectedinternally.Allmodelsaretrainedfor20epochs oneitherthefullEnglishdataset,orthefullMandarin datasetdescribedinSection5.Weusestochasticgradient descentwithNesterovmomentum(Sutskeveretal.,2013) alongwithaminibatchof512utterances.Ifthenormof thegradientexceedsathresholdof400,itisrescaledto 400(Pascanuetal.,2012).Themodelwhichperforms thebestonaheld-outdevelopmentsetduringtrainingis chosenforevaluation.Thelearningrateischosenfrom [1  10  4 ,6  10  4 ] toyieldfastestconvergenceandan- nealedbyaconstantfactorof1.2aftereachepoch.Weuse amomentumof0.99forallmodels. 6.1.English ThebestEnglishmodelhas2layersof2Dconvolution, followedby3layersofunidirectionalrecurrentlayerswith 2560GRUcellseach,followedbyalookaheadconvolution layerwith ˝ =80 ,trainedwithBatchNormandSortaGrad. Wedonotadaptthemodeltoanyofthespeechconditions inthetestsets.Languagemodeldecodingparametersare setonceonaheld-outdevelopmentset. Wereportresultsonseveraltestsetsforbothoursystem andanestimateofhumanaccuracy.Weobtainameasure ofhumanlevelperformancebyaskingworkersfromAma- zonMechanicalTurktohand-transcribeallofourtestsets. Crowdsourcedworkersarenotasaccurateasdedicated, trainedtranscriptionists.Forexample,(Lippmann,1997) thathumantranscribersachievecloseto1%WERon theWSJ-Eval92set,whentheyaremotivatedwithextra rewardforgettingalowerWER,andautomatictypoand spellcorrections,andfurtherreductionsinerrorratesby usingacommitteeoftranscribers. Weemploythefollowingmechanismwithoutrewardsand autocorrectasavalidcompeting"ASRwizard-of-Oz"that westrivetooutperform.Tworandomworkerstranscribe DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin TestsetOursHuman Read WSJeval'923.105.03 WSJeval'934.428.08 LibriSpeechtest-clean5.155.83 LibriSpeechtest-other12.7312.69 Accented VoxForgeAmerican-Canadian7.944.85 VoxForgeCommonwealth14.858.15 VoxForgeEuropean18.4412.76 VoxForgeIndian22.8922.15 Noisy CHiMEevalreal21.5911.84 CHiMEevalsim42.5531.33 Table4:ComparisonofWERforourspeechsystemand crowd-sourcedhumanlevelperformance. everyaudioclip,onaverageabout5secondslongeach. Wethentakethebetterofthetwotranscriptionsforthe WERcalculation.Mostworkersarebasedinthe UnitedStates,areallowedtolistentotheaudioclipmul- tipletimesandonaveragespend27secondspertranscrip- tion.Thehand-transcribedresultsarecomparedtotheex- istinggroundtruthtoproduceaWERestimate.Whilethe existinggroundtruthtranscriptionsdohavesomelabeler- ror,onmostsetsitislessthan1%. 6.1.1.B ENCHMARKRESULTS Readspeechwithhighsignal-to-noiseratioisarguablythe easiesttaskinlargevocabularycontinuousspeechrecog- nition.Webenchmarkoursystemontwotestsetsfrom theWallStreetJournal(WSJ)corpusofreadnewsarti- clesandtheLibriSpeechcorpusconstructedfromaudio books(Panayotovetal.,2015).Table4showsthatoursys- temoutperformscrowd-sourcedhumanworkerson3out of4testsets. Wealsotestedoursystemforrobustnesstocommonac- centsusingtheVoxForge( http://www.voxforge.org ) dataset.Thesetcontainsspeechreadbyspeakerswith manydifferentaccents.Wegrouptheseaccentsintofour categories:American-Canadian,Indian,Commonwealth 5 andEuropean 6 .WeconstructatestsetfromtheVoxForge datawith1024examplesfromeachaccentgroupforatotal of4096examples.Humanlevelperformanceisstillno- tablybetterthanthatofoursystemforallbuttheIndian accent. Finally,wetestedourperformanceonnoisyspeechus- ingthetestsetsfromtherecentlycompletedthirdCHiME challenge(Barkeretal.,2015).Thisdatasethasutterances 5 ﬁCommonwealthﬂincludesBritish,Irish,SouthAfrican, AustralianandNewZealandaccents. 6 ﬁEuropeanﬂincludescountriesinEuropewithoutEnglishas alanguage. fromtheWSJtestsetcollectedinrealnoisyenvironments andwithaddednoise.Usingall6channelsof theCHiMEaudiocanprovidesubstantialperformanceim- provements(Yoshiokaetal.,2015).Weusea single chan- nelforallourmodels,sinceaccesstomulti-channelaudio isnotyetpervasive.Thegapbetweenoursystemandhu- manlevelperformanceislargerwhenthedatacomesfrom arealnoisyenvironmentinsteadofsyntheticallyadding noisetocleanspeech. 6.2.Mandarin InTable5wecompareseveralarchitecturestrainedon MandarinChinesespeechonadevelopmentsetof2000 utterancesaswellasatestsetof1882examplesofnoisy speech.Thisdevelopmentsetwasalsousedtotunethede- codingparameters.Weseethatthedeepestmodelwith2D convolutionandBatchNormoutperformstheshallowRNN by48%relative. ArchitectureDevTest 5-layer,1RNN7.1315.41 5-layer,3RNN6.4911.85 5-layer,3RNN+BatchNorm6.229.39 9-layer,7RNN+BatchNorm +frequencyConvolution5.817.93 Table5:ComparisonofthedifferentRNNarchitectures. Thedevelopmentandtestsetsareinternalcorpora.Each modelinthetablehasabout80millionparameters. TestHumanRNN 100utterances/committee4.03.7 250utterances/individual9.75.7 Table6:WebenchmarkthebestMandarinsystemagainst humansontworandomlyselectedtestsets.Thesethas 100examplesandislabelledbyacommitteeof5Chinese speakers.Thesecondhas250examplesandislabelledby asinglehumantranscriber. Table6showsthatourbestMandarinChinesespeechsys- temtranscribesshortvoice-querylikeutterancesbetterthan atypicalMandarinChinesespeakerandacommitteeof5 Chinesespeakersworkingtogether. 7.Deployment Bidirectionalmodelsarenotwell-designedforrealtime transcription:sincetheRNNhasseveralbidirectionallay- ers,transcribinganutterancerequirestheentireutterance tobepresentedtotheRNN;andsinceweuseawidebeam searchfordecoding,beamsearchcanbeexpensive. Toincreasedeploymentscalability,whilestillproviding DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin lowlatencytranscription,webuiltabatchingscheduler calledBatchDispatchthatassemblesstreamsofdatafrom userrequestsintobatchesbeforeperformingRNNforward propagationonthesebatches.Withthisscheduler,wecan tradeincreasedbatchsize,andconsequentlyimprovedef- y,withincreasedlatency. Weuseaneagerbatchingschemethatprocesseseachbatch assoonasthepreviousbatchiscompleted,regardlessof howmuchworkisreadybythatpoint.Thisscheduling algorithmbalancesefyandlatency,achievingrel- ativelysmalldynamicbatchsizesupto10samplesper batch,withmedianbatchsizeproportionaltoserverload. LoadMedian98%ile 10streams4470 20streams4886 30streams67114 Table7:Latencydistribution(ms)versusload WeseeinTable7thatoursystemachievesamedianla- tencyof44ms,anda98thpercentilelatencyof70ms whenloadedwith10concurrentstreams.Thisserveruses oneNVIDIAQuadroK1200GPUforRNNevaluation.As designed,BatchDispatchshiftsworktolargerbatchesas serverloadgrows,keepinglatencylow. OurdeploymentsystemevaluatesRNNsinhalf-precision arithmetic,whichhasnomeasurableaccuracyimpact,but improvesefy.Wewroteourown16-bit matrix-matrixmultiplyroutinesforthistask,substantially improvingthroughputforourrelativelysmallbatches. Performingthebeamsearchinvolvesrepeatedlookupsin the n -gramlanguagemodel,mostofwhichtranslatetoun- cachedreadsfrommemory.Toreducethecostofthese lookups,weemployaheuristic:onlyconsiderthefewest numberofcharacterswhosecumulativeprobabilityisat least p .Inpractice,wethat p =0.99 workswell, andadditionallywelimitthesearchto40characters.This speedsupthecumulativeMandarinlanguagemodellookup timebyafactorof150x,andhasanegligibleeffectonCER (0.1-0.3%relative). 7.1.DeepSpeechinproductionenvironment DeepSpeechhasbeenintegratedwithastate-of-the-art productionspeechpipelineforuserapplications.Wehave foundseveralkeychallengesthataffectthedeploymentof end-to-enddeeplearningmethodslikeours.First,wehave foundthatevenmodestamountsof trainingdataisinvaluabledespitethelargequantitiesof generalspeechdatausedfortraining.Forexample,while weareabletotrainonmorethan10,000hoursofMan- darinspeech,wethattheadditionofjust500hoursof datacanenhanceperfor- mancefortheapplication.Similarly, languagemodelsareimportantforachievingtopaccuracy andweleveragestrongexistingn-grammodelswithour DeepSpeechsystem.Finally,wenotethatsinceoursys- temistrainedfromawiderangeoflabeledtrainingdata tooutputcharactersdirectly,thereareidiosyncraticcon- ventionsfortranscriptionsineachapplicationthatmustbe handledinpost-processing(suchastheformattingofdig- its).Thus,whileourmodelhasremovedmanycomplexi- ties,morexibilityandapplication-awarenessforend-to- enddeeplearningmethodsareopenareasforfurtherre- search. 8.Conclusion End-to-enddeeplearningpresentstheexcitingopportunity toimprovespeechrecognitionsystemscontinuallywithin- creasesindataandcomputation.Sincetheapproachis highlygeneric,wehaveshownthatitcanquicklybeap- pliedtonewlanguages.Creatinghigh-performingrecog- nizersfortwoverydifferentlanguages,EnglishandMan- darin,requiredessentiallynoexpertknowledgeofthelan- guages.Finally,wehavealsoshownthatthisapproach canbeefdeployedbybatchinguserrequeststo- getheronaGPUserver,pavingthewaytodeliverend-to- endDeepLearningtechnologiestousers. Toachievetheseresults,wehaveexploredvariousnet- workarchitectures,severaleffectivetechniques: enhancementstonumericaloptimizationthroughSorta- GradandBatchNormalization,andlookaheadconvolution forunidirectionalmodels.Thisexplorationwaspowered byawelloptimized,highperformancecomputinginspired trainingsystemthatallowsustotrainfull-scalemodelson ourlargedatasetsinjustafewdays. Overall,webelieveourresultsandexemplify thevalueofend-to-enddeeplearningmethodsforspeech recognitioninseveralsettings.Webelievethesetechniques willcontinuetoscale. References Abdel-Hamid,Ossama,Mohamed,Abdel-rahman,Jang,Hui,and Penn,Gerald.Applyingconvolutionalneuralnetworkscon- ceptstohybridnn-hmmmodelforspeechrecognition.In ICASSP ,2012. Bahdanau,Dzmitry,Chorowski,Jan,Serdyuk,Dmitriy,Brakel, Philemon,andBengio,Yoshua.End-to-endattention-based largevocabularyspeechrecognition.abs/1508.04395,2015. http://arxiv.org/abs/1508.04395. Barker,Jon,Marxer,RicardVincent,Emmanuel,andWatanabe, Shinji.Thethird'CHiME'speechseparationandrecognition challenge:Dataset,taskandbaselines.2015.Submittedto IEEE2015AutomaticSpeechRecognitionandUnderstanding Workshop(ASRU). DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin Bengio,Yoshua,Louradour,JérEome,Collobert,Ronan,andWe- ston,Jason.Curriculumlearning.In InternationalConference onMachineLearning ,2009. Bourlard,H.andMorgan,N. ConnectionistSpeechRecognition: AHybridApproach .KluwerAcademicPublishers,Norwell, MA,1993. Chan,William,Jaitly,Navdeep,Le,Quoc,andVinyals, Oriol.Listen,attend,andspell.abs/1508.01211,2015. http://arxiv.org/abs/1508.01211. Chetlur,Sharan,Woolley,Cliff,Vandermersch,Philippe,Cohen, Jonathan,Tran,John,Catanzaro,Bryan,andShelhamer,Evan. cuDNN:Efprimitivesfordeeplearning.URL http: //arxiv.org/abs/1410.0759 . Cho,Kyunghyun,VanMerrienboer,Bart,Gulcehre,Caglar, Bahdanau,Dzmitry,Bougares,Fethi,Schwenk,Holger,and Bengio,Yoshua.Learningphraserepresentationsusing rnnencoder-decoderforstatisticalmachinetranslation.In EMNLP ,2014. Chorowski,Jan,Bahdanau,Dzmitry,Cho,Kyunghyun,andBen- gio,Yoshua.End-to-endcontinuousspeechrecognitionus- ingattention-basedrecurrentnn:Firstresults.abs/1412.1602, 2015.http://arxiv.org/abs/1412.1602. Coates,Adam,Carpenter,Blake,Case,Carl,Satheesh,Sanjeev, Suresh,Bipin,Wang,Tao,Wu,DavidJ.,andNg,AndrewY. Textdetectionandcharacterrecognitioninsceneimageswith unsupervisedfeaturelearning.In InternationalConferenceon DocumentAnalysisandRecognition ,2011. Coates,Adam,Huval,Brody,Wang,Tao,Wu,DavidJ.,Ng,An- drewY.,andCatanzaro,Bryan.DeeplearningwithCOTS HPC.In InternationalConferenceonMachineLearning ,2013. Dahl,G.E.,Yu,D.,Deng,L.,andAcero,A.Context-dependent pre-traineddeepneuralnetworksforlargevocabularyspeech recognition. IEEETransactionsonAudio,Speech,andLan- guageProcessing ,2011. Dean,Jeffrey,Corrado,GregS.,Monga,Rajat,Chen,Kai,Devin, Matthieu,Le,Quoc,Mao,Mark,Ranzato,Marcâ  A ´ ZAurelio, Senior,Andrew,Tucker,Paul,Yang,Ke,andNg,Andrew. Largescaledistributeddeepnetworks.In AdvancesinNeu- ralInformationProcessingSystems25 ,2012. Gales,M.J.F.,Ragni,A.,Aldamarki,H.,andGautier,C.Support vectormachinesfornoiserobustASR.In ASRU ,pp.205Œ2010, 2009. Graves,A.andJaitly,N.Towardsend-to-endspeechrecognition withrecurrentneuralnetworks.In ICML ,2014a. Graves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J.Con- nectionisttemporalLabellingunsegmentedse- quencedatawithrecurrentneuralnetworks.In ICML ,pp.369Œ 376.ACM,2006. Graves,AlexandJaitly,Navdeep.Towardsend-to-endspeech recognitionwithrecurrentneuralnetworks.In Proceedings ofthe31stInternationalConferenceonMachineLearning (ICML-14) ,pp.1764Œ1772,2014b. Graves,Alex,Mohamed,Abdel-rahman,andHinton,Geoffrey. Speechrecognitionwithdeeprecurrentneuralnetworks.In ICASSP ,2013. H.Sak,Hasim,Senior,Andrew,andBeaufays,Francoise.Long short-termmemoryrecurrentneuralnetworkarchitecturesfor largescaleacousticmodeling.In Interspeech ,2014. Hannun,Awni,Case,Carl,Casper,Jared,Catanzaro,Bryan,Di- amos,Greg,Elsen,Erich,Prenger,Ryan,Satheesh,Sanjeev, Sengupta,Shubho,Coates,Adam,andNg,AndrewY.Deep speech:Scalingupend-to-endspeechrecognition.1412.5567, 2014a.http://arxiv.org/abs/1412.5567. Hannun,AwniY.,Maas,AndrewL.,Jurafsky,Daniel,and Ng,AndrewY.First-passlargevocabularycontinu- ousspeechrecognitionusingbi-directionalrecurrentDNNs. abs/1408.2873,2014b.http://arxiv.org/abs/1408.2873. Hinton,G.E.,Deng,L.,Yu,D.,Dahl,G.E.,Mohamed,A.,Jaitly, N.,Senior,A.,Vanhoucke,V.,Nguyen,P.,Sainath,T.,and Kingsbury,B.Deepneuralnetworksforacousticmodeling inspeechrecognition. IEEESignalProcessingMagazine ,29 (November):82Œ97,2012. Hochreiter,SeppandSchmidhuber,Jürgen.Longshort-term memory. NeuralComputation ,9(8):1735Š1780,1997. Ioffe,SergeyandSzegedy,Christian.Batchnormalization:Ac- celeratingdeepnetworktrainingbyreducinginternalcovariate shift.abs/1502.03167,2015.http://arxiv.org/abs/1502.03167. Krizhevsky,Alex,Sutskever,Ilya,andHinton,Geoff.Imagenet withdeepconvolutionalneuralnetworks.In AdvancesinNeuralInformationProcessingSystems25 ,pp. 1106Œ1114,2012. Laurent,Cesar,Pereyra,Gabriel,Brakel,Philemon, Zhang,Ying,andBengio,Yoshua.Batchnormal- izedrecurrentneuralnetworks.abs/1510.01378,2015. http://arxiv.org/abs/1510.01378. Le,Q.V.,Ranzato,M.A.,Monga,R.,Devin,M.,Chen,K.,Cor- rado,G.S.,Dean,J.,andNg.,A.Y.Buildinghigh-levelfeatures usinglargescaleunsupervisedlearning.In InternationalCon- ferenceonMachineLearning ,2012. LeCun,Yann,Huang,FuJie,andBottou,Léon.Learningmeth- odsforgenericobjectrecognitionwithinvariancetoposeand lighting.In ComputerVisionandPatternRecognition ,vol- ume2,pp.97Œ104,2004. Lippmann,RichardP.Speechrecognitionbymachinesandhu- mans. Speechcommunication ,22(1):1Œ15,1997. Maas,Andrew,Xie,Ziang,Jurafsky,Daniel,andNg,Andrew. Lexicon-freeconversationalspeechrecognitionwithneural networks.In NAACL ,2015. Miao,Yajie,Gowayyed,Mohammad,andMetz,Florian.EESEN: End-to-endspeechrecognitionusingdeeprnnmodelsand wfst-baseddecoding.In ASRU ,2015. Mohamed,A.,Dahl,G.E.,andHinton,G.E.Acoustic modelingusingdeepbeliefnetworks. IEEETransac- tionsonAudio,Speech,andLanguageProcessing ,(99), 2011.URL http://ieeexplore.ieee.org/xpls/abs_all. jsp?arnumber=5704567 . N.Jaitly,P.Nguyen,A.SeniorandVanhoucke,V.Application ofpretraineddeepneuralnetworkstolargevocabularyspeech recognition.In Interspeech ,2012. DeepSpeech2:End-to-EndSpeechRecognitioninEnglishandMandarin Niu,Jianwei,Xie,Lei,Jia,Lei,andHu,Na.Context-dependent deepneuralnetworksforcommercialmandarinspeechrecog- nitionapplications.In APSIPA ,2013. Panayotov,Vassil,Chen,Guoguo,Povey,Daniel,andKhudanpur, Sanjeev.Librispeech:anasrcorpusbasedonpublicdomain audiobooks.In ICASSP ,2015. Pascanu,Razvan,Mikolov,Tomas,andBengio,Yoshua.Onthe difoftrainingrecurrentneuralnetworks.abs/1211.5063, 2012.http://arxiv.org/abs/1211.5063. Raina,R.,Madhavan,A.,andNg,A.Y.Large-scaledeepunsuper- visedlearningusinggraphicsprocessors.In 26thInternational ConferenceonMachineLearning ,2009. Recht,Benjamin,Re,Christopher,Wright,Stephen,andNiu, Feng.Hogwild:Alock-freeapproachtoparallelizingstochas- ticgradientdescent.In AdvancesinNeuralInformationPro- cessingSystems ,pp.693Œ701,2011. Renals,S.,Morgan,N.,Bourlard,H.,Cohen,M.,andFranco,H. ConnectionistprobabilityestimatorsinHMMspeechrecogni- tion. IEEETransactionsonSpeechandAudioProcessing ,2 (1):161Œ174,1994. Robinson,Tony,Hochberg,Mike,andRenals,Steve.Theuse ofrecurrentneuralnetworksincontinuousspeechrecognition. pp.253Œ258,1996. Sainath,Tara,Vinyals,Oriol,Senior,Andrew,andSak,Hasim. Convolutional,longshort-termmemory,fullyconnecteddeep neuralnetworks.In ICASSP ,2015. Sainath,TaraN.,rahmanMohamed,Abdel,Kingsbury,Brian, andRamabhadran,Bhuvana.Deepconvolutionalneuralnet- worksforLVCSR.In ICASSP ,2013. Sak,Hasim,Senior,Andrew,Rao,Kanishka,andBeaufays,Fran- coise.Fastandaccuraterecurrentneuralnetworkacous- ticmodelsforspeechrecognition.abs/1507.06947,2015. http://arxiv.org/abs/1507.06947. Sapp,Benjaminn,Saxena,Ashutosh,andNg,Andrew.Afast datacollectionandaugmentationprocedureforobjectrecog- nition.In AAAITwenty-ThirdConferenceonIntelli- gence ,2008. Seide,Frank,Li,Gang,andYu,Dong.Conversationalspeech transcriptionusingcontext-dependentdeepneuralnetworks.In Interspeech ,pp.437Œ440,2011. Shan,Jiulong,Wu,Genqing,Hu,Zhihong,Tang,Xiliu,Jansche, Martin,andMoreno,Pedro.Searchbyvoiceinmandarinchi- nese.In Interspeech ,2010. Sutskever,I.,Martens,J.,Dahl,G.,andHinton,G.Ontheimpor- tanceofmomentumandinitializationindeeplearning.In 30th InternationalConferenceonMachineLearning ,2013. Szegedy,Christian,Liu,Wei,Jia,Yangqing,Sermanet,Pierre, Reed,Scott,Anguelov,Dragomir,Erhan,Dumitru,Van- houcke,Vincent,andRabinovich,Andrew.Goingdeeperwith convolutions.2014. Waibel,Alexander,Hanazawa,Toshiyuki,Hinton,Geoffrey, Shikano,Kiyohiro,andLang,Kevin.Phonemerecognitionus- ingtime-delayneuralnetworks,â  A  Iacousticsspeechandsig- nalprocessing. IEEETransactionsonAcoustics,Speechand SignalProcessing ,37(3):328Œ339,1989. Yoshioka,T.,Ito,N.,Delcroix,M.,Ogawa,A.,Kinoshita,K., Yu,M.F.C.,Fabian,W.J.,Espi,M.,Higuchi,T.,Araki,S., andNakatani,T.Thenttchime-3system:Advancesinspeech enhancementandrecognitionformobilemulti-microphonede- vices.In IEEEASRU ,2015. Zaremba,WojciechandSutskever,Ilya.Learningtoexecute. abs/1410.4615,2014.http://arxiv.org/abs/1410.4615.  
InternationalJointConferenceonNaturalLanguageProcessing ,pages293Œ301, Nagoya,Japan,14-18October2013. 293 294 295 296 297 298 299 300 301  
Workshoptrack-ICLR2016 L OOKAHEAD C ONVOLUTION L AYERFOR U NIDIREC - TIONAL R ECURRENT N EURAL N ETWORKS ChongWang  ,DaniYogatama  ,AdamCoates,TonyHan,AwniHannun,BoXiao BaiduResearch,SiliconValleyIntelligenceLab Sunnyvale,CA94089,USA Contact: dyogatama@baidu.com A BSTRACT Recurrentneuralnetworks(RNNs)havebeenshowntobeveryeffectiveformany sequentialpredictionproblemssuchasspeechrecognition,machinetranslation, part-of-speechtagging,andothers.Thebestvariantistypicallyabidirectional RNNthatlearnsrepresentationforasequencebyperformingaforwardanda backwardpassthroughtheentiresequence.However,unlikeunidirectionalRNNs, bidirectionalRNNsarechallengingtodeployinanonlineandlow-latencysetting (e.g.,inaspeechrecognitionsystem),becausetheyneedtoseeanentiresequence beforemakingaprediction.Weintroducealookaheadconvolutionlayerthat incorporatesinformationfromfuturesubsequencesinacomputationallyef mannertoimproveunidirectionalrecurrentneuralnetworks.Weevaluateour methodonspeechrecognitiontasksfortwolanguagesŠEnglishandChinese.Our experimentsshowthattheproposedmethodoutperformsvanillaunidirectional RNNsandiscompetitivewithbidirectionalRNNsintermsofcharacterandword errorrates. 1I NTRODUCTION Weareinterestedinsequentialpredictionproblems,wheregivenaninput x 1: T = f x 1 ; x 2 ;:::; x T g , thegoalistomakeaprediction y 1: T = f y 1 ;y 2 ;:::;y T g . 1 Inthispaper,wewillreferto t =1 ;:::;T astimesteps.Manyreal-worldtaskscanbeformulatedassequentialpredictionproblems.For example,inspeechrecognition(languagemodeling),wearegivenaspectrogramofpowernormalized audioclips(aword)ateverytimestepandpredictthecharacterorphoneme(thenextword)associated withthisinput. Recurrentneuralnetworks(RNNs)areapowerfulclassofmodelsforsequentialpredictionproblems (Mikolovetal.,2010;Sutskeveretal.,2014;Amodeietal.,2015; interalia ).Therearetwogeneral typesofRNNs:unidirectionalandbidirectionalRNNs.BidirectionalRNNstendtoperformbetter sincetheyincorporateinformationfromfuturetimestepswhenmakingapredictionattimestep t .ForbidirectionalRNNs,intheforwardpasswecompute p t = f ( b p + U p p t  1 + V p x t ) , where b p , U p ,and V p aremodelparameters.Similarly,inthebackwardpass,wecompute q t = f ( b q + U q q t +1 + V q x t ) .Theoutputattimestep t isthencomputedas y t = g ( W [ p t ; q t ]) ,where [  ] denotesthevectorconcatenationoperator.ForunidirectionalRNNs,onlytheforwardpassis performed,sotheoutputattimestep t is y t = g ( Wp t ) .Weonlyconsidervanillarecurrentlayersin thiswork,butourtechniqueiscompatiblewithmoresophisticatedrecurrentlayerssuchasLSTMs (Hochreiter&Schmidhuber,1997)andGRUs(Choetal.,2014)aswell. BidirectionalRNNsgenerallyachievebetterperformancesincetheycanincorporatefuturecontext, buttheycomewithadditionalcomputationalcosts,bothfortraininganddecoding.Whileanincrease intrainingtimeisnotalwaysanissue(sincethetrainingprocedurecanbecarriedoutofine),an increaseindecodingtimeisaissueforaproductionsystemthatneedstooperateinan online,low-latencysetting,Ascanbeseenfromtheequationsabove,bidirectionalRNNsneedtowait  Equalcontribution. 1 Weuselowercaseletterstodenotevariables,boldlowercaseletterstodenotevectors,andbolduppercase letterstodenotematrices. 1 Workshoptrack-ICLR2016 foranentiresequencetobeseenbeforemakingapredictionfortimestep t .UnidirectionalRNNs,on theotherhand,allowdecodinginastreamingfashionsincetheyonlyincorporatepreviouscontext. Inthispaper,weinvestigateacomputationallyefwaytoincorporateinformationfromfuture timesteps(context)usinganewconvolutionlayer.Ourgoalistodesignamethodthatachieves comparableperformancetobidirectionalRNNsandstillsupportsonlinedecoding.Weshowhowwe canmodifyaconvolutionallayertoachievethispurposeinthefollowings.Ourexperimentsshowthat ourproposedmethodoutperformsvanillaunidirectionalRNNsandiscompetitivewithbidirectional RNNsintermsofcharacterandworderrorrates.Thisworkincorporatesnewcomparisonsand discussionnotreportedinAmodeietal.(2015). Figure1:Lookaheadconvolutionarchitecturewithfuturecontextsize ˝ =2 . 2L OOKAHEAD C ONVOLUTION Weproposeaconvolutionlayerwhichwecalllookaheadconvolution,showninFigure1.The intuitionbehindthislayeristhatweonlyneedasmallportionoffutureinformationtomakean accuratepredictionatthecurrenttimestep.Supposeattimestep t ,weuseafuturecontextof ˝ steps. Wenowhaveafeaturematrix X t : t + ˝ =[ x t ; x t +1 ;:::; x t + ˝ ] 2 R d  ( ˝ +1) .Weaparameter matrix W 2 R d  ( ˝ +1) .Theactivations h t forthenewlayerattime-step t are h t = ˝ +1 X j =1 w j  x t + j  1 ; where  denotesanelement-wiseproduct.Theoutputattimestep t isthencomputedas g ( h t ) ,fora non-linearfunction g .Wenotethattheconvolution-likeoperationisroworientedforboth W and X t : t + ˝ . 3E XPERIMENTS Weevaluateourmethodonspeechrecognitiontasksfortwolanguages:EnglishandChinese. Model OurspeechrecognitionsystemisbasedontheDeepSpeechsystem(Amodeietal.,2015). Itisacharacter-leveldeeprecurrentneuralnetworkmodelthattakesspeechspectrogramsasan inputandpredictscharactersateverytimestep.Ourneuralnetworkarchitectureintheseexperiments consistsofeightlayers.Thelayerisaregularconvolutionlayer.Thenextvelayersareeitherall unidirectional(forward)orallbidirectionalrecurrentlayers.Thesecond-to-lastlayeristhelookahead convolutionlayer.Wealsocomparewithtwobaselinesconstructedbyreplacingthesecond-to-last layerwitheitheraunidirectionalrecurrentlayerorabidirectionalrecurrentlayer.Thelastlayerisa softmaxlayerovercharacteroutputs.WetrainthemodelusingtheCTClossfunction(Gravesetal., 2006).SeeAmodeietal.(2015)fordetailsofthearchitectureandtrainingprocedure. Datasets WeusetheWallStreetJournalcorpus 2 forourEnglishexperimentandaninternalBaidu speechcorpusforourChineseexperiment.TheWSJ(Baidu)speechcorpusconsistsofapproximately 80(800)hoursoftrainingdataand503(2000)testutterances. 2 https://catalog.ldc.upenn.edu/LDC93S6A 2 Workshoptrack-ICLR2016 Table1:Worderrorrates(English)andcharactererrorrates(Chinese)forcompetingmodels.We usefuturecontextsize ˝ =20 inallourexperiments. Model English Chinese NoLM SmallLM NoLM SmallLM ForwardRNN 23.13 18.79 25.86 15.71 ForwardRNN+lookahead-conv 22.66 16.77 21.32 13.45 BidirectionalRNN 19.47 15.42 20.46 12.76 Results Table1showstheresultsforEnglishandChinesespeechrecognition.Sinceourfocus isonevaluatingtheperformanceofthelookaheadconvolutionlayer,wereportresultswithoutany languagemodelandwithasmalllanguagemodel.Wenotethatmuchbetterperformancecanbe obtainedforbothdatasetsbyusingamorepowerfullanguagemodelormoretrainingdata.Wehave observedthatinbothcasestheimprovementsfromthelookaheadconvolutionlayerareconsistent withthesmallerscaleexperimentsshownhere. 4D ISCUSSION WeshowedthatthelookaheadconvolutionlayerimprovesunidirectionalRNNsforspeechrecognition onEnglishandChineseintermsofwordandcharactererrorrates.Weplacethelookaheadconvolution layeraboveall(unidirectional)recurrentlayers.Theadvantagesaretwofold.First,thisallowsus tostreamallcomputationsbelowthelookaheadconvolutionlayer.Forthelookaheadconvolution layer,togetanoutputattimestep t ,weonlyneedtheinputupto t + ˝ .Second,thisresultsinbetter performanceinourexperiments.Weconjecturethattherecurrentlayershavelearnedgoodfeature representations,sothelookaheadconvolutionlayersimplygatherstheappropriatefutureinformation tofeedtothe. WenotethatthereisstillasmallperformancegapbetweenbidirectionalRNNsandunidirectional RNNswithlookaheadconvolution.Inourpreliminaryexperiments,wefoundthatincreasingfuture contextsizedidnotclosethisgap.Wealsofoundthatincorporatingfuturecontextusingaregular convolutionlayerwithmultipleresultedinpoorperformance.Weobeservedthattheresulting modelovthetrainingdata,evenafteranextensivetuningofthelayerhyperparameters.A regularconvolutionlayeralsohashighercomputationalcomplexitythanthelookaheadconvolution layer(althoughthelatencyisstilllowerthanabidirectionalrecurrentlayer).Weplantorunmore experimentswithdifferentfuturecontextsizeandforothersequentialpredictiontaskstoevaluatethe effectivenessoftheproposedmethod. R EFERENCES D.Amodei,R.Anubhai,E.Battenberg,C.Case,J.Casper,B.Catanzaro,J.Chen,M.Chrzanowski, A.Coates,G.Diamos,E.Elsen,J.Engel,L.Fan,C.Fougner,T.Han,A.Hannun,B.Jun, P.LeGresley,L.Lin,S.Narang,A.Ng,S.Ozair,R.Prenger,J.Raiman,S.Satheesh,D.Seetapun, S.Sengupta,Y.Wang,Z.Wang,C.Wang,B.Xiao,D.Yogatama,J.Zhan,andZ.Zhu.Deep speech2:End-to-endspeechrecognitioninenglishandmandarin. ArXive-prints ,2015. KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,Holger Schwenk,andYoshuaBengio.LearningphraserepresentationsusingrnnencoderŒdecoderfor statisticalmachinetranslation.In Proc.ofEMNLP ,2014. AlexGraves,SantiagoFernandez,FaustinoGomez,andJurgenSchmidhuber.Connectionisttemporal Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In Proc.of ICML ,2006. SeppHochreiterandJurgenSchmidhuber.Longshort-termmemory. NeuralComputation ,9(8): 1735Œ1780,1997. TomasMikolov,MartinLukasBurget,JanﬂHonzaﬂCernocky,andSanjeevKhudanpur. Recurrentneuralnetworkbasedlanguagemodel.In Proc.ofInterspeech ,2010. 3 Workshoptrack-ICLR2016 IlyaSutskever,OriolVinyals,andQuocV.Le.Sequencetosequencelearningwithneuralnetworks. In Proc.ofNIPS ,2014. 4  
DynamicComputationalTimeforVisualAttention ZhichaoLi,YiYang,XiaoLiu,FengZhou,ShileiWen,WeiXu BaiduResearch f lizhichao01,yangyi05,liuxiao12,zhoufeng09,wenshilei,xuwei06g @baidu.comAbstractWeproposeadynamiccomputationaltimemodeltoac- celeratetheaverageprocessingtimeforrecurrentvisual  attention(RAM).Ratherthanattentionwithanum-  berofstepsforeachinputimage,themodellearnstode-  cidewhentostoponthe.Toachievethis,weaddan  additionalcontinue/stopactionpertimesteptoRAMand  usereinforcementlearningtolearnboththeoptimalatten-  tionpolicyandstoppingpolicy.Theissim-  plebutcoulddramaticallysavetheaveragecomputation-  altimewhilekeepingthesamerecognitionperformance  asRAM.ExperimentalresultsonCUB-200-2011andStan-  fordCarsdatasetdemonstratethedynamiccomputational  modelcanworkeffectivelyforainedimagerecog-  nition.Thesourcecodeofthispapercanbeobtainedfrom https://github.com/baidu-research/DT-RAM 1.Introduction Humanhavetheremarkableabilityofselectivevisualat- tention[ 1,2].CognitivescienceexplainsthisastheﬁBiased CompetitionTheoryﬂ[ 3,4]thathumanvisualcortexisen- hancedbytop-downguidanceduringfeedbackloops.The  feedbacksignalssuppressnon-relevantstimulipresentin  thevisualhelpinghumansearchingforﬂgoalsﬂ.With  visualattention,bothhumanrecognitionanddetectionper-  formancesincrease,especiallyinimageswith  clutteredbackground[ 5].Inspiredbyhumanattention,theRecurrentVisualAtten- tionModel(RAM)isproposedforimagerecognition[ 6].RAMisadeeprecurrentneuralarchitecturewithiterative  attentionselectionmechanism,thatmimicsthehumanvi-  sualsystemtosuppressnon-relevantimageregionsandex-  tractdiscriminativefeaturesinacomplicatedenvironment.  Thisimprovestherecognitionaccuracy[ 7],e- speciallyforobjectrecognition[ 8,9].RAM alsoallowsthenetworktoprocessahighresolutionimage  withonlylimitedcomputationalresources.Byiterative-  lyattendingtodifferentsub-regions(withaedresolu-  tion),RAMcouldefprocessimageswithvarious !"#$%&'#( !"#$%&'#( (a)Easy (b)Moderate !"#$%&'#( (c)Hard Figure1.Weshowtherecognitionprocessof3tawnyowlimages  withincreasinglevelofdifforrecognition.Whenrecogniz-  ingthesameobjectindifferentimages,humanmayspenddiffer-  entlengthoftime.  resolutionsandaspectratiosinaconstantcomputational  time[ 6,7].Besidesattention,humanalsotendtodynamicallyallo- catedifferentcomputationaltimewhenprocessingdifferent  images[ 5,10].Thelengthoftheprocessingtimeoftende- pendsonthetaskandthecontentoftheinputimages(e.g.  backgroundclutter,occlusion,objectscale).Forexample,  duringtherecognitionofabirdcategory,ifthe  birdappearsinalargeproportionwithcleanbackground  (Figure1a),humancanimmediatelyrecognizetheimage withouthesitation.However,whenthebirdisundercam-  (Figure 1b)orhidinginthescenewithbackground clutterandposevariation(Figure 1c),peoplemayspend muchmoretimeonlocatingthebirdandextractingdiscrim-  inativepartstoproduceaprediction. Inspiredbythis,weproposeanextensiontoRAMnamed asDynamicTimeRecurrentAttentionModel(DT-RAM),  byaddinganextrabinary(continue/stop)actionatevery  timestep.Duringeachstep,DT-RAMwillnotonlyupdate  thenextattention,butproduceadecisionwhetherstopthe  computationandoutputthescore.Themodel  isasimpleextensiontoRAM,butcanbeviewedasa  steptowardsdynamicmodelduringinference[ 11],where themodelstructurecanvarybasedoneachinputinstance.  ThiscouldbringDT-RAMmorexibilityandreducere- 11199 dundantcomputationtofurthersavecomputation,especial-  lywhentheinputexamplesareﬁeasyﬂtorecognize. AlthoughDT-RAMisanend-to-endrecurrentneuralar- chitecture,weithardtodirectlytrainthemodelparam-  etersfromscratch,particularlyforchallengingtaskslike  recognition.Whenthetotalnumberofstepsin-  creases,thedelayedrewardissuebecomesmoresevereand  thevarianceofgradientsbecomeslarger.Thismakespol-  icygradienttrainingalgorithmssuchasREINFORCE[ 12]hardertooptimize.Weaddressthisproblemwithcurricu-  lumlearning[ 13].DuringthetrainingofRAM,wegradual- lyincreasethetrainingdifbygraduallyincreasingthe  totalnumberoftimesteps.Wetheninitializetheparameters  inDT-RAMwiththepre-trainedRAManditwith  REINFORCE.Thisstrategyhelpsthemodeltoconvergeto  abetterlocaloptimumthantrainingfromscratch.Wealso  intermediatesupervisioniscrucialtotheperformance,  particularlywhentraininglongersequences. Wedemonstratetheeffectivenessofourmodelonpub- licbenchmarkdatasetsincludingMNIST[ 14]aswellast- wodatasets,CUB-200-2011[ 15]andStanford Cars[ 16].Wealsoconductanextensivestudytounderstand howdynamictimeworksinthesedatasets.Experimental  resultssuggestthatDT-RAMcanachievestate-of-the-art  performanceonimagerecognition.Compared  toRAM,themodelalsouseslessaveragecomputational  time,betterdeviceswithcomputationallimitations.  2.RelatedWork  2.1.VisualAttentionModels Visualattentionisalong-standingtopicincomputervi- sion[ 17,18,19].Withtherecentsuccessofdeepneural networks[ 20,21,22,23],Mnih etal .[ 6]developtheRe- currentVisualAttentionModel(RAM)forimagerecogni-  tion,wheretheattentionismodeledwithneuralnetworksto  capturelocalregionsintheimage.Ba etal .[ 7]followthe sameframeworkandapplyRAMtorecognizemultipleob-  jectsinimages.Sermanet etal .[ 8]furtherextendRAMto imagerecognition,sinceproblem-  susuallyrequirethecomparisonbetweenlocalparts.Be-  sidesrecognition,attentionmodelsalsowork  forvariousmachinelearningproblemsincludingmachine  translation[ 24],imagecaptioning[ 25],imagequestionan- swering[ 26,27,28,29]andvideoactivityrecognition[ 30].Basedonthedifferentiablepropertyofattentionmodels, mostoftheexistingworkcanbedividedintotwogroup-  s:softattentionandhardattention[ 25].Thesoftatten- tionmodelsattentionasasetofcontinuousvariables  representingtherelativeimportanceofspatialortemporal  cues.Themodelisdifferentiablehencecanbetrainedwith  backpropogation.Thehardattentionmodelsatten-  tionasactionsandmodelthewholeproblemasaPartially ObservedMarkovDecisionProcess(POMDP)[ 31].Such modelsareusuallynondifferentiabletotherewardfunction  henceusepolicygradientsuchasREINFORCE[ 12]toop- timizethemodelparameters.Ourmodelbelongstothehard  attentionsinceitsstoppingactionisdiscrete.  2.2.FeedbackNeuralNetworks Thevisualattentionmodelscanbealsoviewedasa specialtypeoffeedbackneuralnetworks[ 32,33,34,35].Afeedbackneuralnetworkisaspecialrecurrentarchitec-  turethatusespreviouslycomputedhighlevelfeaturesto  backlowlevelfeatures.Itusesbothtop-downand  bottom-upinformationtocomputetheintermediatelayer-  s.Besidesattentionmodels,feedbackneuralnetworksal-  sohaveothervariants.Forexample,Carreira etal .[ 36]performshumanposeestimationwithiterativeerrorfeed-  back.Newell etal .[ 37]buildastackedhourglassnetwork forhumanposeestimation.HuandRamanan[ 38]show thatnetworkfeedbackscanhelpbetterlocatinghumanface  landmarks.Allthesemodelsdemonstratetop-downinfor-  mationcouldpotentiallyimprovethemodeldiscriminative  ability[ 32].However,thesemodelseitherthenumberof recurrentstepsorusesimplerulestodecideearlystopping.  2.3.DynamicComputationalTime Graves[ 11]recentlyintroduce adaptivecomputational timeinrecurrentneuralnetworks.Themodelaugmentsthe networkwitha sigmoidalhaltingunit ateachtimestep, whoseactivationdeterminestheprobabilitywhetherthe  computationshouldstop.Figurnov etal .[ 39]extend[ 11]to spatiallyadaptivecomputationaltimeforresidualnetwork-  s.Theirapproachissimilarbutthe haltingunits over spatialpositions.Neumann etal .[ 40]extendthesimilar ideatotemporallydependentreasoning.Theyachieveas-  mallperformanceontopofasimilarmodelwithout  anadaptivecomponent.Jernite etal .[ 41]learnascheduler todeterminewhatportionofthehiddenstatetocompute  basedonthecurrenthiddenandinputvectors.Allthese  modelscanvarythecomputationtimeduringinference,but  thestoppingpolicyisbasedonthecumulativeprobability  ofhaltingunits ,whichcanbeviewedasaedpolicy. Asfarasweknow,Odena etal .[ 42]istheattempt thatlearntochangemodelbehaviorattesttimewithre-  inforcementlearning.Theirmodeladaptivelyconstructs  computationalgraphsfromsub-modulesonaper-inputba-  sis.However,theyonlyverifyonsmalldatasetsuchasM-  NIST[ 14]andCIFAR-10[ 43].Ba etal .[ 7]augmentRAM withtheﬂend-of-sequenceﬂsymboltodealwithvariable  numberofobjectsinanimage,whichinspiresourworkon  DT-RAM.However,theystillthenumberofattentions  foreachtarget.Thereisalsoalackofdiagnosticexperi-  mentsonunderstandinghowﬂend-of-sequenceﬂsymbolaf-  fectsthedynamics.Inthiswork,weconductextensiveex- 1200 perimentalcomparisonsonlargerscalenaturalimagesfrom  recognition.  2.4.Recognition Fine-grainedimagerecognitionhasbeenextensivelys- tudiedinrecentyears[ 44,45,46,47,48,16,49,50,51].Basedontheresearchfocus,recognitionap-  proachescanbedividedintorepresentationlearning,part  alignmentmodelsoremphasisondata.Thegroupat-  temptstobuildimplicitlypowerfulfeaturerepresentations  suchasbilinearpoolingorcompactbilinearpooling[ 52,53,54],whichturntobeveryeffectiveforprob- lems.Thesecondgroupattemptstolocalizediscriminative  partstoeffectivelydealwithlargeintra-classvariationas  wellassubtleinter-classvariation[ 55,56,57,47,9].The thirdgroupstudiestheimportanceofthescaleoftraining  data[ 58].Theyachievebetterperformanceon multipledatasetbyusinganextralargesetof  trainingimages. WiththefastdevelopmentofdeepmodelssuchasBilin- earCNN[ 54]andSpatialTransformerNetworks[ 59],itis unclearwhetherattentionmodelsarestilleffectivefor  grainedrecognition.Inthispaper,weshowthatthevisual  attentionmodel,iftrainedcarefully,canstillachievecom-  parableperformanceasstate-of-the-artmethods.  3.Model  3.1.LearningwithDynamicStructure Thedifferencebetweenadynamicstructuremodeland aedstructuremodelisthatduringinferencethemodel  structureS dependsonboththeinput x andparameter  .Givenaninput x ,theprobabilityofchoosingacompu- tationalstructure S isP( Sj x; ) .Whenthemodelspaceof S isthisprobabilitycanbemodeledwithaneu- ralnetwork.Duringtraining,withagivenmodelstructure,  thelossis L S ( x; ) .Hencetheoverallexpectedlossforan inputx isL = E S [ L S ( x; )]= X S P( Sj x; ) L S ( x; ) (1)Thegradientof L withrespecttoparameter  is:@ L @ = X S  @P ( S ) @ L S + P( S ) @L S @  = X S  P( S ) @ log P( S ) @ L S + P( S ) @L S @  = E S  @ log P( Sj x; ) @ L S ( x; )+ @L S ( x; ) @  ThetermintheaboveexpectationisthesameasRE-  INFORCEalgorithm[ 12],itmakesthestructureleadingto !"#"$"!"%& #"%& $"%& !"'& #"'& Figure2.Anillustrationofthearchitectureofrecurrentattention  model.Byiterativelyattendingtomorediscriminativearea l t ,the modelcouldoutputmorepredictions y t .smallerlossmoreprobable.Thesecondtermisthestandard  gradientforneuralnetswithaedstructure. Duringexperiments,itisdiftodirectlycomputethe gradientofthe L over  becauseitrequirestoevaluateexpo- nentiallymanypossiblestructuresduringtraining.Henceto  trainthemodel,wesampleasetofstructures,thenap-  proximatethegradientwithMonteCarloSimulation[ 12]:@ L @ ˇ 1M M X i =1  @ log P( S i j x; ) @ L S i ( x; )+ @L S i ( x; ) @  (2)3.2.RecurrentAttentionModel(RAM) TherecurrentattentionmodelisformulatedasaPartial- lyObservedMarkovDecisionProcess(POMDP).Ateach  timestep,themodelworksasanagentthatexecutesanac-  tionbasedontheobservationandreceivesareward.The  agentactivelycontrolhowtoact,anditmayaffectthestate  oftheenvironment.InRAM,theactioncorrespondstothe  locationoftheattentionregion.Theobservationisalocal  (partiallyobserved)regioncroppedfromtheimage.The  rewardmeasuresthequalityofthepredictionusingallthe  croppedregionsandcanbedelayed.Thetargetoflearning  istotheoptimaldecisionpolicytogenerateattentions  fromobservationsthatmaximizestheexpectedcumulative  rewardacrossalltimesteps. Moreformally,RAMtheinputimageas x andthetotalnumberofattentionsas T .Ateachtimestep t 2f 1;:::;T g,themodelcropsalocalregion ˚ ( x;l t  1 ) aroundlocation l t  1 whichiscomputedfromtheprevious timestep.Itthenupdatestheinternalstate h t witharecur- rentneuralnetwork h t = f h ( h t  1 ;˚ ( x;l t  1 ) ; h ) (3)whichisparameterizedby  h .Themodelthencomputes twobranches.Oneisthelocationnetwork f l ( h t ; l ) which1201 !"#"$"!"%& #"%& $"%& !"'& #"'& ("%& ("("'& Figure3.Anillustrationofthearchitectureofdynamictimerecur-  rentattentionmodel.Anextrabinarystoppingaction a t isadded toeachtimestep. a t =0 representsﬂcontinueﬂ(greensolidcircle) anda t =1 representsﬂstopﬂ(redsolidcircle). modelstheattentionpolicy,parameterizedby  l .Theoth- eristhenetwork f c ( h t ; c ) whichcomputes thescore,parameterizedby  c .Duringinfer- ence,itsamplestheattentionlocationbasedonthepoli-  cy ˇ( l t j f l ( h t ; l )) .Figure 2illustratestheinferenceproce- dure. 3.3.DynamicComputationalTimeforRecurrent Attention(DT WhenthedynamicstructurecomestoRAM,wesimply augmentitwithanadditionalsetofactions fa t gthatde- cideswhenitwillstoptakingfurtherattentionandoutput  results.a t 2f 0; 1gisabinaryvariablewith0representing ﬁcontinueﬂand1representingﬁstopﬂ.Itssamplingpolicyis  modeledviaastoppingnetwork f a ( h t ; a ) .Duringinfer- ence,wesampleboththeattention l t andstopping a t witheachpolicyindependently. l t ˘ ˇ( l t j f l ( h t ; l )) ;a t ˘ ˇ( a t j f a ( h t ; a )) (4)Figure3showshowthemodelworks.ComparedtoFig- ure2,thechangeissimplybyadding a t toeachtimestep. Figure4illustrateshowDT-RAMadaptsitsmodelstruc- tureandcomputationaltimetodifferentinputimagesfor  imagerecognition.Whentheinputimageisﬁeasyﬂtorec-  ognize(Figure 4left),weexpectDT-RAMstopatthe fewsteps.Whentheinputimageisﬁhardﬂ(Figure 4right),weexpectthemodellearntocontinuesearchingforinfor-  mativeregions.  3.4.Training Givenasetoftrainingimageswithgroundtruthlabels ( x n ;y n ) n =1  N ,wejointlyoptimizethemodelparameters bycomputingthefollowinggradient: @ L @ ˇ X n X S   @ log P( Sj x n ; ) @ R n + @L S ( x n ;y n ; ) @  (5)Figure4.AnillustrationofhowDT-RAMadaptsitsmodelstruc-  tureandcomputationaltimetodifferentinputimages.  where = f f ; l ; a ; c garetheparametersoftherecur- rentnetwork,theattentionnetwork,thestoppingnetwork  andthenetworkrespectively. ComparedtoEquation 2,Equation 5isanapproximation whereweuseanegativeofrewardfunction R toreplacethe lossofagivenstructure L S intheterm.Thistraining lossissimilarto[ 6,7].AlthoughthelossinEquation 2canbeoptimizeddirectly,using R canreducethevariancein theestimator[ 7].P( S j x n ; )= T ( n ) Y t =1 ˇ( l t j f l ( h t ; l )) ˇ( a t j f a ( h t ; a )) (6)isthesamplingpolicyforstructure S .R n = T ( n ) X t =1  t r nt (7)isthecumulativediscountedrewardover T ( n ) timestep- sforthe n -thtrainingexample.Thediscountfactor  controlsthetrade-offbetweenmakingcorrect  tionandtakingmoreattentions. r nt istherewardat t -thstep.Duringexperiments,weuseadelayedreward.  Weset r nt =0 ift 6= T ( n ) andr nT =1 onlyif y =argmax y P( y j f c ( h T ; c )) .IntermediateSupervision: UnlikeoriginalRAM[ 6],DT-RAMhasintermediatesupervisionforthe  networkateverytimestep,sinceitsunderlyingdynamic  structurecouldrequirethemodeltooutputs-  coresatanytimestep.Thelossof L S ( x n ;y n ; )= T ( n ) X t =1 L t ( x n ;y n ; h ; c ) (8)istheaveragecross-entropylossover N train-ingsamplesand T ( n ) timesteps.Notethat T dependson n ,indicatingthateachinstancemayhavedifferentstopping times.Duringexperiments,weintermediatesupervi-  sionisalsoeffectiveforthebaselineRAM. 1202 Dataset#Classes#Train#TestBBox MNIST[ 14]106000010000-CUB-200-2011[ 15]20059945794 p StanfordCars[ 16]19681448041p Table1.Statisticsofthethreedataset.CUB-200-2011andStan-  fordCarsarebothbenchmarkdatasetsinrecognition. CurriculumLearning: Duringexperiments,weadopt agradualtrainingapproachforthesakeofaccuracy.First,  westartwithabaseconvolutionalnetwork(e.g.Residu-  alNetworks[ 23])pre-trainedonImageNet[ 60].Wethen thebasenetworkontheneddataset.This  givesusaveryhighbaseline.Second,wetraintheRAM  modelbygraduallyincreasethetotalnumberoftimesteps.  Finally,weinitializeDT-RAMwiththetrainedRAMand  furtherthewholenetworkwithREINFORCEal-  gorithm. 4.Experiments  4.1.Dataset Weconductexperimentsonthreepopularbenchmark datasets:MNIST[ 14],CUB-200-2011[ 15]andStanford Cars[ 16].Table 1summarizesthedetailsofeachdataset. MNISTcontains70,000imageswith10digitalnumber-  s.Thisisthedatasetwheretheoriginalvisualattention  modeltestsitsperformance.However,imagesinMNIST  datasetareoftentoosimpletogenerateconclusionstonatu-  ralimages.Therefore,wealsocompareontwochallenging  recognitiondataset.CUB-200-2011[ 15]con- sistsof11,778imageswith200birdcategories.Stanford  Cars[ 16]includes16,185imagesof196carclasses.Both datasetscontainaboundingboxannotationineachimage.  CUB-200-2011alsocontainspartannotation,whichwedo  notuseinouralgorithm.Mostoftheimagesinthesetwo  datasetshaveclutteredbackground,hencevisualattention  couldbeeffectiveforthem.Allmodelsaretrainedandtest-  edwithoutgroundtruthboundingboxannotations.  4.2.ImplementationDetails MNIST: Weusetheoriginaldigitalimageswith28  28pixelresolution.Thedigitsaregenerallycenteredinthe  image.WetrainmultipleRAMmodelswithupto7  steps.Ateachtimestep,wecropa8  8patchfromthe imagebasedonthesampledattentionlocation.The8  8patchonlycapturesapartofadigit,hencethemodelusually  requiresmultiplestepstoproduceanaccurateprediction. Theattentionnetwork,ionnetworkandstop- pingnetworkalloutputactionsateverytimestep.Theout-  putdimensionsofthethreenetworksare2,10and1re-  spectively.Allthreenetworksarelinearlayersontopofthe recurrentnetwork.Thenetworkandstopping  networkthenhavesoftmaxlayers,computingthediscrete  probability.TheattentionnetworkaGaussianpol-  icywithaedvariance,representingthecontinuousdis-  tributionofthetwolocationvariables.Therecurrentstate  vectorhas256dimensions.Allmethodsaretrainedusing  stochasticgradientdescentwithbatchsizeof20andmo-  mentumof0.9.Therewardatthelasttimestepis1ifthe  agentcorrectlyand0otherwise.Therewardsfor  allothertimestepsare0.Onecanreferto[ 6]formore trainingdetails. CUB-200-2011andStanfordCars: Weusethesame settingforbothdataset.Allimagesarenormalizedby  resizingto512  512.Wethencropa224  224imagepatch ateverytimestepexceptthestep,whichisakeydif-  ferencefromMNIST.Atthestep,weusea448  448crop.Thisguaranteesthe1-stepRAMand1-stepDT-RAM  tohavethesameperformanceasthebaselineconvolution-  alnetwork.WeuseResidualNetworks[ 23]pre-trainedon ImageNet[ 60]asthebaselinenetwork.Weusetheﬁpool-5ﬂ featureasinputtotherecurrenthiddenlayer.Therecurrent  layerisafullyconnectedlayerwithReLUactivations.The  attentionnetwork,networkandstoppingnet-  workarealllinearlayersontopoftherecurrentnetwork.  Thedimensionalityofthehiddenlayerissetto512. Allmodelsaretrainedusingstochasticgradientdescent (SGD)withmomentumof0.9for90epochs.Thelearning  rateissetto0.001atthebeginningandmultipliedby0.1  every30epochs.Thebatchsizeis28whichisthemaxi-  mumwecanusefor512  512resolution.(Fordiagnostic experimentswithsmallerimages,weusebatchsizeof96.)  TherewardstrategyisthesameasMNIST.Duringtesting,  theactionsarechosentobethemaximalprobabilityoutput  fromeachnetwork.Notethatalthoughboundingboxsor  part-levelannotationsareavailablewiththesedatasets,we  donotutilizeanyofthemthroughouttheexperiments. ComputationalTime: Ourimplementationisbasedon Torch[ 61].Thecomputationaltimeheavilydependson theresolutionoftheinputimageandthebaselinenetwork  structure.WerunallourexperimentsonasingleTeslaK-  40GPU.TheaveragerunningtimeforaResNet-50ona  512 512resolutionimageis42ms.A3-stepRAMis77ms sinceitrunsResNet-50on2extra224  224images. 4.3.Comparisonwith MNIST: WetraintwoDT-RAMmodelswithdifferent discountfactors.WetrainDT-RAM-1withasmallerdis-  countfactor(0.98)andDT-RAM-2withalargerdiscount  factor(0.99).Thesmallerdiscountfactorwillencourage  themodeltostopearlyinordertoobtainalargereward,  henceonecanexpectDT-RAM-1stopswithlessnumberof  stepsthanDT-RAM-2. Table 2summarizestheperformanceofdifferentmodel- 1203 MNIST#StepsError(%) FC,2layers(256hiddenseach) -1.69Convolutional,2layers-1.21 RAM2steps 23.79RAM4steps41.54 RAM5steps 51.34RAM7steps7 1.07DT-RAM-13.6steps 3.61.46DT-RAM-25.2steps5.21.12 Table2.ComparisontorelatedworkonMNIST.AlltheRAM  resultsarefrom[ 6].!!"!# !"$ !"$# !"% !"%# !"& !"&# !"' !"'# !"# $%&'#()*+,-./.-01.2 34567-8.98:07/: ;<=>?@ =$A8$"'(B87--.- !;<=>?@ =%A8$"$%B87--.- !Figure5.Thedistributionofnumberofstepsfromtwodifferent  DT-RAMmodelsonMNISTdataset.Amodelwithlongeraverage  stepstendstohaveabetteraccuracy.  sonMNIST.ComparingtoRAMwithsimilarnumberof  steps,DT-RAMachieveabettererrorrate.Forexample,  DT-RAM-1gets1.46%recognitionerrorwithanaverage  of3.6stepswhileRAMwith4stepsgets1.54%error.Sim-  ilarly,DT-RAM-2gets1.12%errorwithanaverageof5.2  stepswhileRAMwith5stepshas1.34%error. Figure5showsthedistributionofnumberofstepsfor thetwoDT-RAMmodelsacrossalltestingexamples.We  acleartrade-offbetweenefyandaccuracy.DT-  RAM-1whichhaslesscomputationaltime,achieveshigher  errorthanDT-RAM-2. CUB-200-2011:Wecompareourmodelwithallpre- viouslypublishedmethods.Table 3summarizesthere- sults.WeobservethatmethodsincludingBilinearCN-  N[ 54,53](84.1%-84.2%),SpatialTransformerNetwork- s[ 59](84.1%)andFullyConvolutionalAttentionNetwork- s[ 62](84.3%)allachievesimilarperformances.[ 9]further improvethetestingaccuracyto85.4%byutilizingattribute  labelsannotatedinthedataset. Surprisingly,wethatacarefullyunedResidual Networkwith50layersalreadyhit84.5%,surpassingmost  oftheexistingworks.Addingfurtherrecurrentvisualatten-  tion(RAM)reaches86.0%,improvingthebaselineResid- CUB-200-2011Accuracy(%)Accw.Box(%) Zhangetal .[ 63]73.976.4Bransonetal .[ 56]75.785.4  Simonetal .[ 64]81.0-Krauseetal .[ 48]82.082.8 Linetal .[ 54]84.185.1Jaderberg etal .[ 59]84.1- Kong etal .[ 53]84.2-Liuetal .[ 62]84.384.7 Liuetal .[ 9]85.485.5ResNet-50[ 23]84.5- RAM3steps 86.0-DT-RAM1.9steps 86.0-Table3.ComparisontorelatedworkonCUB-200-2011dataset.  Testingwithbothgroundtruthboxandparts. StanfordCars Accuracy(%)Accw.Box(%) Chaietal .[ 65]78.0-Gosselinetal .[ 66]82.787.9 Girshicketal .[ 67]88.4-Linetal .[ 54]91.3- Wang etal .[ 68]-92.5Liuetal .[ 62]91.593.1 Krauseetal .[ 48]92.692.8ResNet-50[ 23]92.3- RAM3steps 93.1-DT-RAM1.9steps 93.1-Table4.ComparisontorelatedworkonStanfordCarsdataset.  ualNetby1.5%,leadingtoanewstate-of-the-artonCUB-  200-2011.DT-RAMfurtherimprovesRAM,byachieving  thesamestate-of-the-artperformance,withlessnumberof  computationaltimeonaverage(1.9stepsv.s.3steps). StanfordCars: WealsocompareextensivelyonStan- fordCarsdataset.Table 4showstheresults.Surprisingly,a 50-layerResidualNetworkagainachieves92.3%  accuracyonthetestingset,surpassingmostoftheexisting  work.Thissuggeststhatasingledeepnetwork(withoutany  furtherorextraboundingboxsupervision)can  bethechoiceforrecognition. A3-stepRAMontopoftheResidualNetfurtherim- provesto93.1%accuracy,whichissofarthenewstate-of-  the-artperformanceonStanfordCarsdataset.Compared  to[ 62],thisisachievedwithoutusingboundingboxan- notationduringtesting.DT-RAMagainachievesthesame  accuracyasRAMbutusing1.9stepsonaverage.Wealso  observethattherelativeimprovementofRAMtothebase-  linemodelisnolongerlarge(0.8%). 1204 ResolutionResNet-34RAM-34Resnet-50RAM-50 224 22479.981.881.582.8448 448--84.586.0 Table5.Theeffectofinputresolutionandnetworkdepthon  ResNetanditsRAMextension.  4.4.AblationStudy Weconductasetofablationstudiestounderstandhow eachcomponentaffectsRAMandDT-RAMon  recognition.WeworkwithCUB-200-2011datasetsinceits  imagesarerealandchallenging.However,duetothelarge  resolutionoftheimages,wearenotabletorunmanysteps  withaverydeepResidualNet.Therefore,insteadofusing  Resnet-50with512  512imageresolutionwhichproduces state-of-the-artresults,weuseResnet-34with256  256as thebaselinemodel(79.9%).Thisallowsustotrainmore  stepsonthebirdimagestodeeplyunderstandthemodel. InputResolutionandNetworkDepth: Table 5com-parestheeffectondifferentimageresolutionswithdiffer-  entnetworkdepths.Ingeneral,ahigherimageresolu-  tionhelpsrecognition.Forex-  ample,giventhesameResNet-50model,512  512resolu- tionwith448  448cropsgets84.5%accuracy,betterthan 81.5%from256  256with224  224crops.Thisisproba- blybecausehigherresolutionimagescontainmoredetailed  informationforrecognition.AdeeperResid-  ualNetworkalsoimprovesperformance.For  example,ResNet-50obtains81.5%accuracycomparedto  ResNet-34withonly79.9%.Duringexperiments,wealso  trainaResNet-101on224  224cropsandget82.8%recog- nitionaccuracy. RAMv.s.DT-RAM: Table 6showshowthenumber ofstepsaffectsRAMonCUB-200-2011dataset.Starting  fromResNet-34whichisalsothe1stepRAM,themodel  graduallyincreasesitsaccuracywithmoresteps(79.9% !81.8%).After5steps,RAMnolongerimproves.DT-RAM  alsoreachesthesameaccuracy.However,itonlyuses3.5  stepsonaveragethan6steps,whichisapromisingtrade-off  betweencomputationandperformance. Figure6plotsthedistributionofnumberofstepsfrom theDT-RAMonthetestingimages.Surprisinglydifferent  fromMNIST,themodelpreferstostopeitheratthebegin-  ningofthetime(1-2steps),orintheendofthetime(5-6  steps).Thereareveryfewimagesthatthemodelchooseto  stopat3-4steps. LearnedPolicyv.s.FixedPolicy: Onemaysuspectthat insteadoflearningtheoptimalstoppingpolicy,whetherwe  canjustuseaxedstoppingpolicyonRAMtodetermine  whentostoptherecurrentiteration.Forexample,onecan  simplylearnaRAMmodelwithintermediatesupervision  ateverytimestepanduseathresholdoverthe Model#StepsAccuracy(%) ResNet-34179.9RAM2steps280.7 RAM3steps 381.1RAM4steps481.5 RAM5steps 581.8RAM6steps681.8 DT-RAM(6maxsteps) 3.681.8Table6.ComparisontoRAMonCUB-200-2011.Notethatthe  1-stepRAMisthesameastheResNet. !!"!# !"$ !"$# !"% !"%# !"& !"&# $%&'#()*+,+*-.+/ 01234*5+657-4,7 Figure6.ThedistributionofnumberofstepsfromaDT-RAM  modelonCUB-200-2011dataset.UnlikeMNIST,thedistribution  suggeststhemodelprefereitherstopatthebeginningorintheend. Threshold#StepsAccuracy(%) 0179.90.41.480.7 0.51.681.00.61.981.2 0.93.681.31.0681.8 DT-RAM(6maxsteps) 3.681.8Table7.ComparisontoaedstoppingpolicyonCUB-200-2011.  TheedstoppingpolicyrunsonRAM(6steps)suchthatthe  recurrentattentionstopsifoneoftheclasssoftmaxprobabilitiesis  abovethethreshold.  network f c ( h t ; c ) todeterminewhentostop.Wecom- paretheresultsbetweenthedescribededpolicyandDT-  RAM.Table 7showsthecomparison.Wethatalthough theedstoppingpolicygivesreasonablygoodresults(i.e.  3.6stepswith81.3%accuracy),DT-RAMstillworksslight-  lybetter(i.e.3.6stepswith81.8%accuracy). CurriculumLearning: Table 8comparestheresults onwhetherusingcurriculumlearningtotrainRAM.Ifwe  learnthemodelparameterscompletelyfromscratchwith-  outcurriculum,theperformancestarttodecreasewithmore  timesteps(79.9% !80.9%!80.0%).Thisisbecausesim- 1205 (a)1step (b)2steps (c)3steps (d)4steps (e)5steps (f)6steps Figure7.QualitativeresultsofDT-RAMonCUB-200-2011 testingset.Weshowimageswithdifferentendingstepsfrom1to6.Each boundingboxindicatesanattentionregion.Boundingboxcolorsaredisplayedinorder.Thestepusesthefullimageasinputhence  thereisnoboundingbox.Fromstep1tostep6,weobserveagradualincreaseofbackgroundclutterandrecognitiondif,matching  ourhypothesisforusingdynamiccomputationtimefordifferenttypesofimages. (a)1step(b)2steps(c)3steps Figure8.QualitativeresultsofDT-RAMonStanfordCar testingset.Weonlymanagetotraina3-stepmodelwith512  512resolution. #Steps123456 w.oC.L. 79.980.780.580.980.380.0w.C.L.79.980.781.181.581.881.8 Table8.TheeffectofCurriculumLearningonRAM. plepolicygradientmethodbecomeshardertotrainwith  longersequences.Curriculumlearningmakestrainingmore  stable,sinceitguaranteestheaccuracyonaddingmore  stepswillnothurttheperformance.Thetestingperfor-  mancehencegraduallyincreasesovernumberofsteps,  from79.9%to81.8%. IntermediateSupervision: Table 9comparesthetest- ingresultsforusingintermediatesupervision.Notethatthe  originalRAMmodel[ 6]onlycomputesoutputatthelast timestep.AlthoughthisworksforsmalldatasetlikeM-  NIST.whentheinputimagesbecomemorechallengingand  timestepincreases,theRAMmodellearnedwithoutinter-  mediatesupervisionstartstogetworse.Oncontrary,adding  anintermediatelossateachstepmakesRAMmodelswith  morestepssteadilyimprovetheperformance. QualitativeResults: Wevisualizethequalitativeresults ofDT-RAMonCUB-200-2011andStanfordCarstesting #Steps123456 w.oI.S. 79.978.876.174.874.974.7w.I.S.79.980.781.181.581.881.8 Table9.TheeffectofIntermediateSupervisiononRAM. setinFigure 7andFigure 8respectively.Fromstep1to step6,weobserveagradualincreaseofbackgroundclutter  andrecognitiondif,matchingourhypothesisofusing  dynamiccomputationtimefordifferenttypesofimages.  5.ConclusionandFutureWork Inthisworkwepresentasimplebutnovelmethodfor learningtodynamicallyadjustcomputationaltimeduring  inferencewithreinforcementlearning.Weapplyitonthe  recurrentvisualattentionmodelandshowitseffectiveness  forrecognition.Webelievethatsuchmethods  willbeimportantfordevelopingdynamicreasoningindeep  learningandcomputervision.Futureworkondeveloping  moresophisticateddynamicmodelsforreasoningandapply  ittomorecomplextaskssuchasvisualquestionanswering  willbeconducted. 1206 References [1]MaryHayhoeandDanaBallard.Eyemovementsinnatural behavior. Trendsincognitivesciences ,9(4):188Œ194,2005. 1[2]RobertDesimoneandJohnDuncan.Neuralmechanismsof selectivevisualattention. Annualreviewofneuroscience ,18(1):193Œ222,1995. 1[3]DianeMBeckandSabineKastner.Top-downandbottom- upmechanismsinbiasingcompetitioninthehumanbrain.  Visionresearch ,49(10):1154Œ1165,2009. 1[4]RobertDesimone.Visualattentionmediatedbybiased competitioninextrastriatevisualcortex. PhilosophicalTransactionsoftheRoyalSocietyB:BiologicalSciences ,353(1373):1245,1998. 1[5]RadoslawMartinCichy,DimitriosPantazis,andAudeOli- va.Resolvinghumanobjectrecognitioninspaceandtime.  Natureneuroscience ,17(3):455Œ462,2014. 1[6]VolodymyrMnih,NicolasHeess,AlexGraves,etal.Re- currentmodelsofvisualattention.In Advancesinneural informationprocessingsystems ,pages2204Œ2212,2014. 1,2,4,5,6,8[7]JimmyBa,VolodymyrMnih,andKorayKavukcuoglu.Mul- tipleobjectrecognitionwithvisualattention. arXivpreprint arXiv:1412.7755,2014. 1,2,4[8]PierreSermanet,AndreaFrome,andEstebanReal.At- tentionforcategorization. arXivpreprintarX- iv:1412.7054,2014. 1,2[9]XiaoLiu,JiangWang,ShileiWen,ErruiDing,andYuan- qingLin.Localizingbydescribing:Attribute-guidedatten-  tionlocalizationforrecognition. arXivpreprint arXiv:1605.06217,2016. 1,3,6[10]GustavoDecoandEdmundTRolls.Aneurodynamicalcorti- calmodelofvisualattentionandinvariantobjectrecognition.  Visionresearch ,44(6):621Œ642,2004. 1[11]AlexGraves.Adaptivecomputationtimeforrecurrentneural networks. arXivpreprintarXiv:1603.08983 ,2016. 1,2[12]RichardSSutton,DavidAMcAllester,SatinderPSingh, YishayMansour,etal.Policygradientmethodsforrein-  forcementlearningwithfunctionapproximation.In NIPS,volume99,pages1057Œ1063,1999. 2,3[13]YoshuaBengio,J ´er‹omeLouradour,RonanCollobert,andJa- sonWeston.Curriculumlearning.In Proceedingsofthe26th annualinternationalconferenceonmachinelearning ,pages 41Œ48.ACM,2009. 2[14]YannLeCun,L ´eonBottou,YoshuaBengio,andPatrick Haffner.Gradient-basedlearningappliedtodocumentrecog-  nition.ProceedingsoftheIEEE ,86(11):2278Œ2324,1998. 2,5[15]CatherineWah,SteveBranson,PeterWelinder,PietroPer- ona,andSergeBelongie.Thecaltech-ucsdbirds-200-2011  dataset.2011. 2,5[16]JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3dobjectrepresentationsforcategorization.In  ProceedingsoftheIEEEInternationalConferenceonCom-  puterVisionWorkshops ,pages554Œ561,2013. 2,3,5[17]LaurentItti,ChristofKoch,andErnstNiebur.Amodel ofsaliency-basedvisualattentionforrapidsceneanalysis.  IEEETransactionsonpatternanalysisandmachineintelli-  gence ,20(11):1254Œ1259,1998. 2[18]LaurentIttiandChristofKoch.Computationalmodellingof visualattention. Naturereviewsneuroscience ,2(3):194Œ203, 2001.2[19]JohnKTsotsos,ScanMCulhane,WinkyYanKeiWai, YuzhongLai,NealDavis,andFernandoModeling  visualattentionviaselectivetuning. intelligence ,78(1-2):507Œ545,1995. 2[20]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetwithdeepconvolutionalneuralnet-  works.In Advancesinneuralinformationprocessingsys- tems,pages1097Œ1105,2012. 2[21]KarenSimonyanandAndrewZisserman.Verydeepconvo- lutionalnetworksforlarge-scaleimagerecognition. arXivpreprintarXiv:1409.1556 ,2014. 2[22]ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet, ScottReed,DragomirAnguelov,DumitruErhan,Vincen-  tVanhoucke,andAndrewRabinovich.Goingdeeperwith  convolutions.In ProceedingsoftheIEEEConferenceon ComputerVisionandPatternRecognition ,pages1Œ9,2015. 2[23]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.In Proceed- ingsoftheIEEEConferenceonComputerVisionandPattern  Recognition ,pages770Œ778,2016. 2,5,6[24]DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearningtoalignand  translate.arXivpreprintarXiv:1409.0473 ,2014. 2[25]KelvinXu,JimmyBa,RyanKiros,KyunghyunCho, AaronCCourville,RuslanSalakhutdinov,RichardSZemel,  andYoshuaBengio.Show,attendandtell:Neuralim-  agecaptiongenerationwithvisualattention.In ICML,vol- ume14,pages77Œ81,2015. 2[26]HuijuanXuandKateSaenko.Ask,attendandanswer:Ex- ploringquestion-guidedspatialattentionforvisualquestion  answering.In EuropeanConferenceonComputerVision ,pages451Œ466.Springer,2016. 2[27]KanChen,JiangWang,Liang-ChiehChen,HaoyuanGao, WeiXu,andRamNevatia.Abc-cnn:Anattentionbased  convolutionalneuralnetworkforvisualquestionanswering.  arXivpreprintarXiv:1511.05960 ,2015. 2[28]AkiraFukui,DongHukPark,DaylenYang,AnnaRohrbach, TrevorDarrell,andMarcusRohrbach.Multimodalcom-  pactbilinearpoolingforvisualquestionansweringandvi-  sualgrounding. arXivpreprintarXiv:1606.01847 ,2016. 2[29]ZichaoYang,XiaodongHe,JianfengGao,LiDeng,and AlexSmola.Stackedattentionnetworksforimagequestion 1207 answering.In ProceedingsoftheIEEEConferenceonCom- puterVisionandPatternRecognition ,pages21Œ29,2016. 2[30]SerenaYeung,OlgaRussakovsky,GregMori,andLiFei- Fei.End-to-endlearningofactiondetectionfromframe  glimpsesinvideos.In ProceedingsoftheIEEEConference onComputerVisionandPatternRecognition ,pages2678Œ 2687,2016. 2[31]RichardSSuttonandAndrewGBarto. Reinforcementlearn- ing:Anintroduction ,volume1.MITpressCambridge,1998. 2[32]AmirRZamir,Te-LinWu,LinSun,WilliamShen,Jiten- draMalik,andSilvioSavarese.Feedbacknetworks. arXivpreprintarXiv:1612.09508 ,2016. 2[33]MarijnFStollenga,JonathanMasci,FaustinoGomez,and J¨urgenSchmidhuber.Deepnetworkswithinternalselec- tiveattentionthroughfeedbackconnections.In Advancesin NeuralInformationProcessingSystems ,pages3545Œ3553, 2014.2[34]ChunshuiCao,XianmingLiu,YiYang,YinanYu,Jiang Wang,ZileiWang,YongzhenHuang,LiangWang,Chang  Huang,WeiXu,etal.Lookandthinktwice:Capturing  top-downvisualattentionwithfeedbackconvolutionalneu-  ralnetworks.In ProceedingsoftheIEEEInternationalCon- ferenceonComputerVision ,pages2956Œ2964,2015. 2[35]QianWang,JiaxingZhang,SenSong,andZhengZhang.At- tentionalneuralnetwork:Featureselectionusingcognitive  feedback.In AdvancesinNeuralInformationProcessing Systems,pages2033Œ2041,2014. 2[36]JoaoCarreira,PulkitAgrawal,KaterinaFragkiadaki,andJi- tendraMalik.Humanposeestimationwithiterativeerror  feedback.In ProceedingsoftheIEEEConferenceonCom- puterVisionandPatternRecognition ,pages4733Œ4742, 2016.2[37]AlejandroNewell,KaiyuYang,andJiaDeng.Stackedhour- glassnetworksforhumanposeestimation.In EuropeanCon- ferenceonComputerVision ,pages483Œ499.Springer,2016. 2[38]PeiyunHuandDevaRamanan.Bottom-upandtop-down reasoningwithhierarchicalgaussians.In Proceed- ingsoftheIEEEConferenceonComputerVisionandPattern  Recognition ,pages5600Œ5609,2016. 2[39]MichaelFigurnov,MaxwellDCollins,YukunZhu, LiZhang,JonathanHuang,DmitryVetrov,andRuslan  Salakhutdinov.Spatiallyadaptivecomputationtimefor  residualnetworks. arXivpreprintarXiv:1612.02297 ,2016. 2[40]MarkNeumann,PontusStenetorp,andSebastianRiedel. Learningtoreasonwithadaptivecomputation. arXivpreprintarXiv:1610.07647 ,2016. 2[41]YacineJernite,EdouardGrave,ArmandJoulin,andTomas Mikolov.Variablecomputationinrecurrentneuralnetworks.  arXivpreprintarXiv:1611.06188 ,2016. 2[42]AugustusOdena,DieterichLawson,andChristopherOlah. Changingmodelbehaviorattest-timeusingreinforcement  learning.arXivpreprintarXiv:1702.07780 ,2017. 2[43]AlexKrizhevskyandGeoffreyHinton.Learningmultiple layersoffeaturesfromtinyimages.2009. 2[44]LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101Œminingdiscriminativecomponentswithrandom  forests.In EuropeanConferenceonComputerVision ,pages 446Œ461.Springer,2014. 3[45]ThomasBerg,JiongxinLiu,SeungWooLee,MichelleL Alexander,DavidWJacobs,andPeterNBelhumeur.Bird-  snap:Large-scalevisualcategorizationofbirds.  InProceedingsoftheIEEEConferenceonComputerVision andPatternRecognition ,pages2011Œ2018,2014. 3[46]YinCui,FengZhou,YuanqingLin,andSergeBelongie. Fine-grainedcategorizationanddatasetbootstrappingusing  deepmetriclearningwithhumansintheloop.In Proceed- ingsoftheIEEEConferenceonComputerVisionandPattern  Recognition ,pages1153Œ1162,2016. 3[47]ShaoliHuang,ZheXu,DachengTao,andYaZhang.Part- stackedcnnforvisualcategorization.In Pro- ceedingsoftheIEEEConferenceonComputerVisionand  PatternRecognition ,pages1173Œ1182,2016. 3[48]JonathanKrause,HailinJin,JianchaoYang,andLiFei-Fei. Fine-grainedrecognitionwithoutpartannotations.In Pro- ceedingsoftheIEEEConferenceonComputerVisionand  PatternRecognition ,pages5546Œ5555,2015. 3,6[49]AdityaKhosla,NityanandaJayadevaprakash,Bangpeng Yao,andFei-FeiLi.Noveldatasetforimage  categorization:Stanforddogs.In Proc.CVPRWorkshop onFine-GrainedVisualCategorization(FGVC) ,volume2, 2011.3[50]JiongxinLiu,AngjooKanazawa,DavidJacobs,andPeter Belhumeur.Dogbreedusingpartlocalization.  InEuropeanConferenceonComputerVision ,pages172Œ 185.Springer,2012. 3[51]Maria-ElenaNilsbackandAndrewZisserman.Automat- edweroveralargenumberofclasses.  InComputerVision,Graphics&ImageProcessing,2008. ICVGIP'08.SixthIndianConferenceon ,pages722Œ729. IEEE,2008. 3[52]YangGao,OscarBeijbom,NingZhang,andTrevorDarrell. Compactbilinearpooling.In ProceedingsoftheIEEECon- ferenceonComputerVisionandPatternRecognition ,pages 317Œ326,2016. 3[53]ShuKongandCharlessFowlkes.Low-rankbilinearpool- ingfor arXivpreprintarX- iv:1611.05109,2016. 3,6[54]Tsung-YuLin,AruniRoyChowdhury,andSubhransuMaji. Bilinearcnnmodelsforvisualrecognition.In  ProceedingsoftheIEEEInternationalConferenceonCom-  puterVision ,pages1449Œ1457,2015. 3,6[55]ThomasBergandPeterBelhumeur.Poof:Part-basedone- vs.-onefeaturesforcategorization,facev  cation,andattributeestimation.In ProceedingsoftheIEEE ConferenceonComputerVisionandPatternRecognition ,pages955Œ962,2013. 31208 [56]SteveBranson,GrantVanHorn,SergeBelongie,andPietro Perona.Birdspeciescategorizationusingposenormalized  deepconvolutionalnets. arXivpreprintarXiv:1406.2952 ,2014.3,6[57]EfstratiosGavves,BasuraFernando,CeesGMSnoek, ArnoldWMSmeulders,andTinneTuytelaars.Fine-grained  categorizationbyalignments.In ProceedingsoftheIEEE InternationalConferenceonComputerVision ,pages1713Œ 1720,2013. 3[58]JonathanKrause,BenjaminSapp,AndrewHoward,Howard Zhou,AlexanderToshev,TomDuerig,JamesPhilbin,and  LiFei-Fei.Theunreasonableeffectivenessofnoisydatafor  recognition.In EuropeanConferenceonCom- puterVision ,pages301Œ320.Springer,2016. 3[59]MaxJaderberg,KarenSimonyan,AndrewZisserman,etal. Spatialtransformernetworks.In AdvancesinNeuralInfor- mationProcessingSystems ,pages2017Œ2025,2015. 3,6[60]JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi, andLiFei-Fei.Imagenet:Alarge-scalehierarchicalim-  agedatabase.In ComputerVisionandPatternRecognition, 2009.CVPR2009.IEEEConferenceon ,pages248Œ255. IEEE,2009. 5[61]RonanCollobert,KorayKavukcuoglu,andCl ´ementFarabet. Torch7:Amatlab-likeenvironmentformachinelearning.In  BigLearn,NIPSWorkshop ,numberEPFL-CONF-192376, 2011.5[62]XiaoLiu,TianXia,JiangWang,YiYang,FengZhou,and YuanqingLin.Fine-grainedrecognitionwithautomaticand  efpartattention. arXivpreprintarXiv:1603.06765 ,2016.6[63]NingZhang,JeffDonahue,RossGirshick,andTrevorDar- rell.Part-basedr-cnnsforcategorydetection.  InEuropeanconferenceoncomputervision ,pages834Œ849. Springer,2014. 6[64]MarcelSimonandErikRodner.Neuralactivationconstella- tions:Unsupervisedpartmodeldiscoverywithconvolutional  networks.In ProceedingsoftheIEEEInternationalConfer- enceonComputerVision ,pages1143Œ1151,2015. 6[65]YuningChai,VictorLempitsky,andAndrewZisserman. Symbioticsegmentationandpartlocalizationfor  grainedcategorization.In ProceedingsoftheIEEEInter- nationalConferenceonComputerVision ,pages321Œ328, 2013.6[66]Philippe-HenriGosselin,NailaMurray,Herv ´eJ ´egou,and FlorentPerronnin.Revisitingthevectorfor  grained PatternRecognitionLetters ,49:92Œ 98,2014. 6[67]RossGirshick,JeffDonahue,TrevorDarrell,andJitendra Malik.Richfeaturehierarchiesforaccurateobjectdetection  andsemanticsegmentation.In ProceedingsoftheIEEEcon- ferenceoncomputervisionandpatternrecognition ,pages 580Œ587,2014. 6[68]YamingWang,JonghyunChoi,VladMorariu,andLarryS Davis.Miningdiscriminativetripletsofpatchesfor grainedIn ProceedingsoftheIEEEConfer- enceonComputerVisionandPatternRecognition ,pages 1163Œ1172,2016. 61209  
Agreement-basedJointTrainingfor BidirectionalAttention-basedNeuralMachineTranslation YongCheng # ,ShiqiShen y ,ZhongjunHe + ,WeiHe + ,HuaWu + ,MaosongSun y ,YangLiu y # InstituteforInterdisciplinaryInformationSciences,TsinghuaUniversity,Beijing,China y StateKeyLaboratoryofIntelligentTechnologyandSystems TsinghuaNationalLaboratoryforInformationScienceandTechnology DepartmentofComputerScienceandTechnology,TsinghuaUniversity,Beijing,China + BaiduInc.,Beijing,China Abstract Theattentionalmechanismhasproventobeef- fectiveinimprovingend-to-endneuralmachine translation.However,duetotheintricatestruc- turaldivergencebetweennaturallanguages,unidi- rectionalattention-basedmodelsmightonlycap- turepartialaspectsofattentionalregularities.We proposeagreement-basedjointtrainingforbidirec- tionalattention-basedend-to-endneuralmachine translation.Insteadoftrainingsource-to-targetand target-to-sourcetranslationmodelsindependently, ourapproachencouragesthetwocomplementary modelstoagreeonwordalignmentmatriceson thesametrainingdata.ExperimentsonChinese- EnglishandEnglish-Frenchtranslationtasksshow thatagreement-basedjointtraining improvesbothalignmentandtranslationquality overindependenttraining. 1Introduction End-to-endneuralmachinetranslation(NMT)isanewlypro- posedparadigmformachinetranslation [ Kalchbrennerand Blunsom,2013;Cho etal. ,2014;Sutskever etal. ,2014; Bahdanau etal. ,2015 ] .Withoutexplicitlymodelinglatent structuresthatarevitalforconventionalstatisticalmachine translation(SMT) [ Brown etal. ,1993;Koehn etal. ,2003; Chiang,2005 ] ,NMTbuildsonan encoder-decoder frame- work:theencodertransformsasource-languagesentence intoacontinuous-spacerepresentation,fromwhichthede- codergeneratesatarget-languagesentence. WhileearlyNMTmodelsencodeasourcesentenceasa ed-lengthvector,Bahadanauetal. [ 2015 ] advocatethe useof attention inNMT.Theyindicatethatonlypartsof thesourcesentencehaveaneffectonthetargetwordbe- inggenerated.Inaddition,therelevantpartsoftenvary withdifferenttargetwords.Suchanattentionalmechanism hasproventobeaneffectivetechniqueintextgeneration taskssuchasmachinetranslation [ Bahdanau etal. ,2015; Luong etal. ,2015b ] andimagecaptiongeneration [ Xu etal. , 2015 ] .  YangLiuisthecorrespondingauthor:liuyang2011@tsinghua. edu.cn. However,duetothestructuraldivergencebetweennatural languages,modelingthecorrespondencebetweenwordsin twolanguagesstillremainsamajorchallengeforNMT,espe- ciallyfordistantly-relatedlanguages.Forexample,Luonget al. [ 2015b ] reportthatattention-basedNMTlagsbehindthe Berkeleyaligner [ Liang etal. ,2006 ] intermsofalignment errorrate(AER)ontheEnglish-Germandata.Onepossible reasonisthatunidirectionalattention-basedNMTcanonly capturepartialaspectsofattentionalregularitiesduetothe non-isomorphismofnaturallanguages. Inthiswork,weproposetointroduceagreement-based learning [ Liang etal. ,2006;Liang etal. ,2007 ] intoattention- basedneuralmachinetranslation.Thebasicideaistoencour- agesource-to-targetandtarget-to-sourcetranslationmodels toagreeonwordalignmentonthesametrainingdata.This canbedonebyanewtrainingobjectivethatcom- bineslikelihoodsintwodirectionsaswellasanagreement termthatmeasurestheconsensusbetweenwordalignment matricesintwodirections.ExperimentsonChinese-English andEnglish-Frenchdatasetsshowthatourapproachiscapa- bleofbetteraccountingforattentionalregularitiesandsignif- icantlyimprovesalignmentandtranslationqualityoverinde- pendenttraining. 2Background Givenasource-languagesentence x = x 1 ;:::; x m ;:::; x M thatcontains M wordsandatarget-languagesentence y = y 1 ;:::; y n ;:::; y N thatcontains N words,end-to-endneu- ralmachinetranslationdirectlymodelsthetranslationproba- bilityasasingle,largeneuralnetwork: P ( y j x ;  )= N Y n =1 P ( y n j x ; y <n ;  ) (1) where  isasetofmodelparametersand y <n = y 1 ;:::; y n  1 isapartialtranslation. Theencoder-decoderframework [ KalchbrennerandBlun- som,2013;Cho etal. ,2014;Sutskever etal. ,2014;Bahdanau etal. ,2015 ] usuallyusesarecurrentneuralnetwork(RNN) toencodethesourcesentenceintoasequenceofhiddenstates h = h 1 ;:::; h m ;:::; h M : h m = f ( x m ; h m  1 ;  ) (2) where h m isthehiddenstateofthe m -thsourcewordand f (  ) isanon-linearfunction.Notethattherearemanyways arXiv:1512.04650v2  [cs.CL]  22 Apr 2016Figure1:Theillustrationofattention-basedNMT.Thede- codergeneratesatargethiddenstate s n anditscorresponding targetword y n givenasourcesentence x .Abidirectional RNNisusedtoconcatenatetheforwardandbackwardstates asthehiddenstatesofsourcewords. toobtainthehiddenstates.Forexample,Bahdanauetal. [ 2015 ] useabidirectionalRNNandconcatenatetheforward andbackwardstatesasthehiddenstateofasourcewordto capturebothforwardandbackwardcontexts(seeFigure1). Bahdanauetal. [ 2015 ] theconditionalprobability inEq.(1)as P ( y n j x ; y <n ;  )= g ( y n  1 ; s n ; c n ;  ) (3) where g (  ) isanon-linearfunction, s n isthehiddenstatecor- respondingtothe n -thtargetwordcomputedby s n = f ( s n  1 ; y n  1 ; c n ;  ) (4) and c n isacontextvectorforgeneratingthe n -thtargetword: c n = M X m =1 A (  ) n;m h m (5) Wereferto A (  ) 2 R N  M as alignmentmatrix ,inwhich anelement A (  ) n;m thecontributionofthe m -th sourceword x m togeneratingthe n -thtargetword y n : 1 A (  ) n;m = exp( a ( s n  1 ; h m ;  )) P M m 0 =1 exp( a ( s n  1 ; h m 0 ;  )) (6) where a ( s n  1 ; h m ;  ) measureshowwell x m and y n are aligned.Notethatwordalignmentistreatedasafunction parameterizedby  insteadofalatentvariableinattention- basedNMT. Givenasetoftrainingexamples fh x ( s ) ; y ( s ) ig S s =1 ,the trainingalgorithmaimstothemodelparametersthat maximizethelikelihoodofthetrainingdata:   =argmax  ( S X s =1 log P ( y ( s ) j x ( s ) ;  ) ) (7) 1 Wedenotethealignmentmatrixas A (  ) insteadof  in [ Bah- danau etal. ,2015 ] toemphasizethatitisafunctionparameterized by  anddifferentiable.Although s n and c n alsodependon  ,we omitthedependenciesforsimplicity. Althoughtheintroductionofattentionhasadvancedthe state-of-the-artofNMT,itisstillchallengingforattention- basedNMTtocapturetheintricatestructuraldivergencebe- tweennaturallanguages.Figure2(a)showstheChinese- to-English(upper)andEnglish-to-Chinese(bottom)align- mentmatricesforthesamesentencepair.Boththetwo independentlytrainedmodelsfailtocorrectlycapturethe gold-standardcorrespondence:whiletheChinese-to-English alignmentassignswrongprobabilitiestoﬁusﬂandﬁbushﬂ,the English-to-Chinesealignmentmakesincorrectpredictionson ﬁcondemnsﬂandﬁbombingﬂ. Fortunately,althougheachmodelonlycapturespartialas- pectsofthemappingbetweenwordsinnaturallanguages, thetwomodelsseemtobecomplementary:theChinese-to- EnglishalignmentdoeswellonﬁcondemnsﬂandtheEnglish- to-Chinesealignmentassignscorrectprobabilitiestoﬁusﬂand ﬁbushﬂ.Therefore,combiningthetwomodelscanhopefully improvealignmentandtranslationqualityinbothdirections. 3Agreement-basedJointTraining Inthiswork,weproposetointroduceagreement-basedlearn- ing [ Liang etal. ,2006;Liang etal. ,2007 ] intoattention- basedneuralmachinetranslation.Thecentralideaistoen- couragethesource-to-targetandtarget-to-sourcemodelsto agreeonalignmentmatricesonthesametrainingdata.As showninFigure2(b),agreement-basedjointtrainingisca- pableofremovingunlikelyattentionandresultinginmore concentratedandaccuratealignmentmatricesinbothdirec- tions. Moreformally,wetrainboththesource-to-targetattention- basedneuraltranslationmodel P ( y j x ; !  ) andthetarget- to-sourcemodel P ( x j y ;    ) onasetoftrainingexamples fh x ( s ) ; y ( s ) ig S s =1 ,where !  and    aremodelparametersin twodirections,respectively.Thenewtrainingobjectiveis givenby J ( !  ;    )= S X s =1 log P ( y ( s ) j x ( s ) ; !  ) + S X s =1 log P ( x ( s ) j y ( s ) ;    )   S X s =1   x ( s ) ; y ( s ) ; ! A ( s ) ( !  ) ;   A ( s ) (    )  (8) where ! A ( s ) ( !  ) isthesource-to-targetalignmentmatrixfor the s -thsentencepair,   A ( s ) (    ) isthetarget-to-sourcealign- mentmatrixforthesamesentencepair,   ) isalossfunction thatmeasuresthedisagreementbetweentwomatrices,and  isahyper-parameterthatbalancesthepreferencebetween likelihoodandagreement. Forsimplicity,weomitthedependencyonthe sentencepairandsimplywritethelossfunctionas   ! A ( s ) ( !  ) ;   A ( s ) (    )  .Whiletherearemanyalterna- tivesforquantifyingdisagreement,weusethefollowing threetypesoflossfunctionsinourexperiments: (a)independenttraining (b)jointtraining Figure2:Examplealignmentsof(a)independenttrainingand(b)jointtrainingonaChinese-Englishsentencepair.The rowshowsChinese-to-EnglishalignmentsandthesecondrowshowsEnglish-to-Chinesealignments.Wethatthetwo unidirectionalmodelsarecomplementaryandencouragingagreementleadstoimprovedalignmentaccuracy. 1. Squareofaddition (SOA):thesquareoftheelement- wiseadditionofcorrespondingmatrixcells  SOA  ! A ( s ) ( !  ) ;   A ( s ) (    )  =  N X n =1 M X m =1  ! A ( s ) ( !  ) n;m +   A ( s ) (    ) m;n  2 (9) Intuitively,thislossfunctionencouragestoincreasethe sumofthealignmentprobabilitiesintwocorresponding matrixcells. 2. Squareofsubtraction (SOS):thesquareoftheelement- wisesubtractionofcorrespondingmatrixcells  SOS  ! A ( s ) ( !  ) ;   A ( s ) (    )  = N X n =1 M X m =1  ! A ( s ) ( !  ) n;m    A ( s ) (    ) m;n  2 (10) Derivedfromthesymmetryconstraintproposedby Ganchevetal. [ 2010 ] ,thislossfunctionencouragesthat analignedpairofwordssharecloseorevenequalalign- mentprobabilitiesinbothdirections. 3. Multiplication (MUL):theelement-wisemultiplication ofcorrespondingmatrixcells  MUL  ! A ( s ) ( !  ) ;   A ( s ) (    )  =  log N X n =1 M X m =1 ! A ( s ) ( !  ) n;m    A ( s ) (    ) m;n (11) Thislossfunctionisinspiredbytheagreementterm [ Liang etal. ,2006 ] andmodelinvertibilityregulariza- tion [ Levinboim etal. ,2015 ] . Thedecisionrulesforthetwodirectionsaregivenby !   =argmax !  ( S X s =1 log P ( y ( s ) j x ( s ) ; !  )   S X s =1   ! A ( s ) ( !  ) ;   A ( s ) (    )  ) (12)     =argmax    ( S X s =1 log P ( x ( s ) j y ( s ) ;    )   S X s =1   ! A ( s ) ( !  ) ;   A ( s ) (    )  ) (13) Notethatallthelossfunctionsaredifferentiablewithre- specttomodelparameters.Itiseasytoextendtheoriginal trainingalgorithmforattention-basedNMT [ Bahdanau etal. , 2015 ] toimplementagreement-basedjointtrainingsincethe twotranslationmodelsintwodirectionssharethesametrain- ingdata. 4Experiments 4.1Setup WeevaluatedourapproachonChinese-EnglishandEnglish- Frenchmachinetranslationtasks. ForChinese-English,thetrainingcorpusfromLDCcon- sistsof2.56Msentencepairswith67.53MChinesewordsand 74.81MEnglishwords.WeusedtheNIST2006datasetasthe validationsetforhyper-parameteroptimizationandmodelse- lection.TheNIST2002,2003,2004,2005,and2008datasets wereusedastestsets.IntheNISTChinese-Englishdatasets, eachChinesesentencehasfourreferenceEnglishtransla- tions.TobuildEnglish-Chinesevalidationandtestsets,we simplyﬁreverseﬂtheChinese-Englishdatasets:theEn- glishsentenceinthefourreferencesasthesourcesentence andtheChinesesentenceasthesinglereferencetranslation. ForEnglish-French,thetrainingcorpusfromWMT2014 consistsof12.07Msentencepairswith303.88MEnglish wordsand348.24MFrenchwords.Theconcatenationof news-test-2012andnews-test-2013wasusedasthevalida- tionsetandnews-test-2014asthetestset.EachEnglishsen- tencehasasinglereferenceFrenchtranslation.TheFrench- Englishevaluationsetscanbeeasilyobtainedbyreversing theEnglish-Frenchdatasets. Wecomparedourapproachwithtwostate-of-the-artSMT andNMTsystems: 1.M OSES [ KoehnandHoang,2007 ] :aphrase-basedSMT system; 2.RNN SEARCH [ Bahdanau etal. ,2015 ] :anattention- basedNMTsystem. ForM OSES ,weusedtheparallelcorpustotrainthephrase- basedtranslationmodelandthetarget-sidepartoftheparallel corpustotraina4-gramlanguagemodelusingtheSRILM Loss BLEU  SOA :squareofaddition 31.26  SOS :squareofsubtraction 31.65  MUL :multiplication 32.65 Table1:Comparisonoflossfunctionsintermsofcase- insensitiveBLEUscoresonthevalidationsetforChinese- to-Englishtranslation. [ Stolcke,2002 ] .Weusedthedefaultsystemsettingforboth traininganddecoding. ForRNN SEARCH ,weusedtheparallelcorpustotrainthe attention-basedNMTmodels.Thevocabularysizeissetto 30Kforalllanguages.WefollowJeanetal. [ 2015 ] toad- dresstheunknownwordproblembasedonalignmentmatri- ces.Givenanalignmentmatrix,itispossibletocalculate thepositionofthesourcewordtowhichismostlikelytobe alignedforeachtargetword.Afterasourcesentenceistrans- lated,eachunknownwordistranslatedfromitscorrespond- ingsourceword.WhileJeanetal. [ 2015 ] useabilingualdic- tionarygeneratedbyanoff-the-shelfwordalignertotranslate unknownwords,weuseunigramphrasesinstead. OursystemsimplyextendsRNN SEARCH byreplacingin- dependenttrainingwithagreement-basedjointtraining.The encoder-decoderframeworkandtheattentionalmechanism remainunchanged.Thehyper-parameter  thatbalances thepreferencebetweenlikelihoodandagreementissetto 1.0forChinese-Englishand2.0forEnglish-French.The trainingtimeofjointtrainingisabout1.2timeslongerthan thatofindependenttrainingfortwodirectionalmodels.We usedthesameunknownwordpost-processingtechniqueas RNN SEARCH foroursystem. 4.2ComparisonofLossFunctions Wecomparedthethreelossfunctionsasdescribedin Section3onthevalidationsetforChinese-to-Englishtrans- lation.Theevaluationmetriciscase-insensitiveBLEU. AsshowninTable1,thesquareofadditionlossfunction (i.e.,  SOA )achievesthelowestBLEUamongthethreeloss functions.Thiscanbepossiblyattributedtothefactthata largersumdoesnotnecessarilyleadtoincreasedagreement. Forexample,while0.9+0.1hardlyagree,0.2+0.2perfectly does.Therefore,  SOA seemstobeaninaccuratemeasureof agreement. Thesquareofsubtractionlossfunction(i.e,  SOS )isca- pableofaddressingtheaboveproblembyencouragingthe trainingalgorithmtominimizethedifferencebetweentwo probabilities:(0.2-0.2) 2 =0.However,thelossfunction failstodistinguishbetween(0.9-0.9) 2 and(0.2-0.2) 2 .Ap- parently,theformershouldbepreferredbecausebothmodels havehighinthematrixcell.Itisunfavorablefor twomodelsagreeonamatrixcellbutbothhaveverylowcon- Therefore,  SOS isperfectformeasuringagreement butignores Asthemultiplicationlossfunction(i.e.,  MUL )isableto takebothagreementandintoaccount(e.g.,0.9  0.9 > 0.2  0.2),itachievesimprovementsover  SOA and  SOS .Asaresult,weuse  MUL inthefollowing experiments. System Training Direction NIST06 NIST02NIST03NIST04NIST05NIST08 M OSES indep. C ! E 32.48 32.6932.3933.6230.2325.17 E ! C 14.27 18.2815.3613.9614.1110.84 RNN SEARCH indep. C ! E 30.74 35.1633.7534.6331.7423.63 E ! C 15.71 20.7616.5616.8515.1412.70 joint C ! E 32.65 ++ 35.68  + 34.79  ++ 35.72  ++ 32.98  ++ 25.62  ++ E ! C 16.25  ++ 21.70  ++ 17.45  ++ 16.98  15.70  + 13.80  ++ Table2:ResultsontheChinese-Englishtranslationtask.M OSES isaphrase-basedstatisticalmachinetranslationsystem. RNN SEARCH isanattention-basedneuralmachinetranslationsystem.Weintroduceagreement-basedjointtrainingforbidirec- tionalattention-basedNMT.NIST06isthevalidationsetandNIST02-05,08aretestsets.TheBLEUscoresarecase-insensitive. ﬁ*ﬂ:betterthanM OSES ( p< 0 : 05 );ﬁ**ﬂ:sibetterthanM OSES ( p< 0 : 01 );ﬁ+ﬂ:bet- terthanRNN SEARCH withindependenttraining( p< 0 : 05 );ﬁ++ﬂ:betterthanRNN SEARCH withindependent training( p< 0 : 01 ).Weusethestatisticaltestwithpairedbootstrapresampling [ Koehn,2004 ] . Training C ! EE ! C indep. 54.6452.49 joint 47.49  46.70  Table3:ResultsontheChinese-Englishwordalignmenttask. Theevaluationmetricisalignmenterrorrate.ﬁ**ﬂ:signif- icantlybetterthanRNN SEARCH withindependenttraining ( p< 0 : 01 ). 4.3ResultsonChinese-EnglishTranslation Table2showstheresultsontheChinese-to-English(C ! E) andEnglish-to-Chinese(E ! C)translationtasks. 2 We thatRNN SEARCH generallyoutperformsM OSES exceptfor theC ! EdirectionontheNIST08testset,which theeffectivenessofattention-basedNMTondistantly-related languagepairssuchasChineseandEnglish. Agreement-basedjointtrainingfurthersystematicallyim- provesthetranslationqualityinbothdirectionsoverindepen- dentlytrainingexceptfortheE ! CdirectionontheNIST04 testset. 4.4ResultsonChinese-EnglishAlignment Table3showstheresultsontheChinese-Englishwordalign- menttask.WeusedtheT SINGHUA A LIGNER evaluation dataset [ LiuandSun,2015 ] inwhichboththevalidation andtestsetscontain450manually-alignedChinese-English sentencepairs.WefollowLuongetal. [ 2015b ] toﬁforce- decodeﬂourjointlytrainedmodelstoproducetranslations thatmatchthereferences.Then,weextractonlyone-to- onealignmentsbyselectingthesourcewordwiththehighest alignmentweightforeachtargetword. Wethatagreement-basedjointtraining reducesalignmenterrorsforbothdirectionsascomparedwith independenttraining.Thissuggeststhatintroducingagree- mentdoesenableNMTtocaptureattentionmoreaccurately andthusleadtobettertranslations.Figure2(b)showsexam- plealignmentmatricesresultedfromagreement-basedjoint training. However,theerrorratesinTable3arestillhigherthancon- ventionalalignersthatcanachieveanAERaround30onthe 2 ThescoresforE ! CismuchlowerthanC ! EbecauseBLEU iscalculatedatthewordlevelratherthancharacterlevel. WordTypeFreq. Indep.Joint toprepositionhigh 2.211.80 andconjunctionhigh 2.211.60 thearticlehigh 1.961.56 yesterdaynounmedium 2.041.55 activelyadverbmedium 1.901.32 festivalnounmedium 1.550.85 inspectsverblow 0.290.02 rebelliousadjectivelow 0.290.02 noticingverblow 0.190.01 Table4:Comparisonofindependentandjointtrainingin termsofaverageattentionentropy(seeEq.(15))onChinese- to-Englishtranslation. samedataset.Thereisstillroomforimprovementinattention accuracy. 4.5AnalysisofAlignmentMatrices Weobservethatatargetwordispronetoconnecttotoomany sourcewordsinthealignmentmatricesproducedbyindepen- denttraining.Forexample,intheloweralignmentmatrixof Figure2(a),thethirdChinesewordﬁbuxiﬂisalignedtothree Englishwords:ﬁpresidentﬂ,ﬁbushﬂ,andﬁcondemnsﬂ.Inad- dition,allthethreealignmentprobabilitiesarerelativelylow. Similarly,fourEnglishwordscontributetogeneratingthelast Chinesewordﬁgongjiﬂ:ﬁcondemnsﬂ,ﬁsuicideﬂ,ﬁbomingﬂ, andﬁattackﬂ. Incontrast,agreement-basedjointtrainingleadstomore concentratedalignmentdistributions.Forexample,inthe loweralignmentmatrixofFigure2(b),thethirdChineseword ﬁbuxiﬂismostlikelytobealignedtoﬁbushﬂ.Likewise,the attentiontothelastChinesewordﬁgongjiﬂnowmainlyfo- cusesonﬁattackﬂ. Tomeasurethedegreeofconcentrationofattention,we the attentionentropy ofatargetwordinasentence pairasfollows: H y n =  M X m =1 A (  ) n;m log A (  ) n;m (14) Givenaparallelcorpus D = fh x ( s ) ; y ( s ) ig S s =1 ,the aver- System Training Direction Dev. Test M OSES Indep. E ! F 28.38 32.31 F ! E 28.52 30.93 RNN SEARCH Indep. E ! F 29.06 32.69 F ! E 28.32 29.99 Joint E ! F 29.86  ++ 33.45  ++ F ! E 29.01  ++ 31.51  ++ Table5:ResultsontheEnglish-Frenchtranslationtask.TheBLEUscoresarecase-insensitive.ﬁ**ﬂ:betterthan M OSES ( p< 0 : 01 );ﬁ++ﬂ:betterthanRNN SEARCH withindependenttraining( p< 0 : 01 ). ageattentionentropy isas ~ H y = 1 c ( y;D ) S X s =1 N X n =1  ( y ( s ) n ;y )H y ( s ) n (15) where c ( y;D ) istheoccurrenceofatargetword y onthe trainingcorpus D : c ( y;D )= S X s =1 N X n =1  ( y ( s ) n ;y ) (16) Table4givestheaverageattentionentropyofexample wordsontheChinese-to-Englishtranslationtask.We thattheentropygenerallygoesdownswiththedecreaseof wordfrequencies,whichsuggeststhatfrequenttargetwords tendtogainattentionfrommultiplesourcewords.Appar- ently,jointtrainingleadstomoreconcentratedattentionthan independenttraining.Thegapseemstoincreasewiththede- creaseofwordfrequencies. 4.6ResultsonEnglish-to-FrenchTranslation Table5givestheresultsontheEnglish-Frenchtransla- tiontask.WhileRNN SEARCH withindependenttrain- ingachievestranslationperformanceonparwithM OSES , agreement-basedjointlearningleadstoimprove- mentsoverbothbaselines.Thissuggeststhatourapproachis generalandcanbeappliedtomorelanguagepairs. 5RelatedWork Ourworkisinspiredbytwolinesofresearch:(1)attention- basedNMTand(2)agreement-basedlearning. 5.1Attention-basedNeuralMachineTranslation Bahdanauetal. [ 2015 ] introducetheattentionalmechan- ismintoneuralmachinetranslationtoenablethedecoderto focusonrelevantpartsofthesourcesentenceduringdecod- ing.Theattentionmechanismallowsaneuralmodeltocope betterwithlongsentencesbecauseitdoesnotneedtoencode alltheinformationofasourcesentenceintoaed-length vectorregardlessofitslength.Inaddition,theattentional mechanismallowsustolookintotheﬁblackboxﬂtogainin- sightsonhowNMTworksfromalinguisticperspective. Luongetal. [ 2015a ] proposetwosimpleandeffec- tiveattentionalmechanismsforneuralmachinetranslation andcomparevariousalignmentfunctions.Theyshowthat attention-basedNMTaresuperiortonon-attentionalmodels intranslatingnamesandlongsentences. Afteranalyzingthealignmentmatricesgeneratedby RNN SEARCH [ Bahdanau etal. ,2015 ] ,wethatmodel- ingthestructuraldivergenceofnaturallanguagesissochal- lengingthatunidirectionalmodelscanonlycapturepartof alignmentregularities.Thisinspiresustoimprove attention-basedNMTbycombiningtwounidirectionalmod- els.Inthiswork,weonlyapplyagreement-basedjointlearn- ingtoRNN SEARCH .Asourapproachdoesnotassumespe- networkarchitectures,itispossibletoapplyittothe modelsproposedbyLuongetal. [ 2015a ] . 5.2Agreement-basedLearning Liangetal. [ 2006 ] introduceagreement-basedlearning intowordalignment:encouragingasymmetricIBMmod- elstoagreeonwordalignment,whichisalatentstruc- tureinword-basedtranslationmodels [ Brown etal. ,1993 ] . Thisstrategyimprovesalignmentqualityacross manylanguages.Theyextendthisideatodealwithmore latent-variablemodelsingrammarinductionandpredicting missingnucleotidesinDNAsequences [ Liang etal. ,2007 ] . Liuetal. [ 2015 ] proposegeneralizedagreementforword alignment.Thenewgeneralframeworkallowsforarbitrary lossfunctionsthatmeasurethedisagreementbetweenasym- metricalignments.Thelossfunctionscannotonlybe betweenasymmetricalignmentsbutalsobetweenalignments andotherlatentstructuressuchasphrasesegmentations. Inattention-basedNMT,wordalignmentistreatedasa parametrizedfunctioninsteadofalatentvariable.Thismakes wordalignmentdifferentiable,whichisimportantfortraining attention-basedNMTmodels.Althoughalignmentmatrices inattention-basedNMTareinprincipleﬁsymmetricﬂasthey allowformany-to-manysoftalignments,wethatunidi- rectionalmodelingcanonlycapturepartialaspectsofstruc- turemapping.Ourcontributionistoadaptagreement-based learningintoattentionalNMT,whichimproves bothalignmentandtranslation. 6Conclusion Wehavepresentedagreement-basedjointtrainingforbidirec- tionalattention-basedneuralmachinetranslation.Byencour- agingbidirectionalmodelstoagreeonparametrizedalign- mentmatrices,jointlearningachievesimprove- mentsintermsofalignmentandtranslationqualityoverin- dependenttraining.Inthefuture,weplantofurthervalidate theeffectivenessofourapproachonmorelanguagepairs. Acknowledgements ThisworkwasdonewhileYongChengandShiqiShenwere visitingBaidu.Thisresearchissupportedbythe973Pro- gram(2014CB340501,2014CB340505),theNationalNatu- ralScienceFoundationofChina(No.61522204,61331013, 61361136003),1000TalentPlangrant,TsinghuaInitiative ResearchProgramgrants20151080475andaGoogleFaculty ResearchAward. References [ Bahdanau etal. ,2015 ] DzmitryBahdanau,KyungHyun Cho,andYoshuaBengio.Neuralmachinetranslationby jointlylearningtoalignandtranslate.In Proceedingsof ICLR ,2015. [ Brown etal. ,1993 ] PeterF.Brown,StephenA. DellaPietra,VincentJ.DellaPietra,andRobertL. Mercer.Themathematicsofstatisticalmachinetransla- tion:Parameterestimation. ComputationalLinguisitics , 1993. [ Chiang,2005 ] DavidChiang.Ahierarchicalphrase-based modelforstatisticalmachinetranslation.In Proceedings ofACL ,2005. [ Cho etal. ,2014 ] KyunghyunCho,BartvanMerrienboer, CaglarGulcehre,DzmitryBahdanau,FethiBougares, HolgerSchwenk,andYoshuaBengio.Learningphrase representationsusingrnnencoder-decoderforstatistical machinetranslation.In ProceedingsofEMNLP ,2014. [ Ganchev etal. ,2010 ] KuzmanGanchev,JoaoGrac¸a,Jen- niferGillenwater,andBenTaskar.Posteriorregulariza- tionforstructuredlatentvariablemodels. TheJournalof MachineLearningResearch ,11:2001Œ2049,2010. [ Jean etal. ,2015 ] SebastienJean,KyunghyunCho,Roland Memisevic,andYoshuaBengio.Onusingverylargetarget vocabularyforneuralmachinetranslation.In Proceedings ofACL ,2015. [ KalchbrennerandBlunsom,2013 ] NalKalchbrennerand PhilBlunsom.Recurrentcontinuoustranslationmodels. In ProceedingsofEMNLP ,2013. [ KoehnandHoang,2007 ] PhilippKoehnandHieuHoang. Factoredtranslationmodels.In ProceedingsofEMNLP , 2007. [ Koehn etal. ,2003 ] PhilippKoehn,FranzJ.Och,and DanielMarcu.Statisticalphrase-basedtranslation.In Pro- ceedingsofHLT-NAACL ,2003. [ Koehn,2004 ] PhilippKoehn.Statisticaltests formachinetranslationevaluation.In Proceedingsof EMNLP ,2004. [ Levinboim etal. ,2015 ] TomerLevinboim,Ashish Vaswani,andDavidChiang.Modelinvertibility regularization:Sequencealignmentwithorwithout paralleldata.In ProceedingsofNAACL ,2015. [ Liang etal. ,2006 ] PercyLiang,BenTaskar,andDanKlein. Alignmentbyagreement.In ProceedingsofNAACL ,2006. [ Liang etal. ,2007 ] PercyLiang,DanKlein,andMichaelI. Jordan.Agreement-basedlearning.In Proceedingsof NIPS ,2007. [ LiuandSun,2015 ] YangLiuandMaosongSun.Con- trastiveunsupervisedwordalignmentwithnon-localfea- tures.In ProceedingsofAAAI ,2015. [ Liu etal. ,2015 ] ChunyangLiu,YangLiu,HuanboLuan, MaosongSun,andHengYu.Generalizedagreementfor bidirectionalwordalignment.In ProceedingsofEMNLP , 2015. [ Luong etal. ,2015a ] Minh-ThangLuong,HieuPham,and ChristopherD.Manning.Effectiveapproachesto attention-basedneuralmachinetranslation.In Proceed- ingsofEMNLP ,2015. [ Luong etal. ,2015b ] Minh-ThangLuong,IlyaSutskever, QuocV.Le,OriolVinyals,andWojciechZaremba.Ad- dressingtherarewordprobleminneuralmachinetransla- tion.In ProceedingsofACL ,2015. [ Stolcke,2002 ] AndreasStolcke.Srilm-anextensiblelan- guagemodelingtoolkit.In ProceedingsofICSLP ,2002. [ Sutskever etal. ,2014 ] IlyaSutskever,OriolVinyals,and QuocV.Le.Sequencetosequencelearningwithneural networks.In ProceedingsofNIPS ,2014. [ Xu etal. ,2015 ] KelvinXu,JimmyLeiBa,RyanKiros, KyungHyunCho,AaronCourville,RuslanSalakhutdinov, RichardS.Zemel,andYoshuaBengio.Show,attendand tell:Neuralimagecaptiongenerationwithvisualattention. In ProceedingsofICML ,2015.  
DeepMetricLearningwithAngularLoss JianWang,FengZhou,ShileiWen,XiaoLiuandYuanqingLin BaiduResearch {wangjian33,zhoufeng09,wenshilei,liuxiao12,linyuanqing}@baidu.comAbstractThemodernimagesearchsystemrequiressemanticun- derstandingofimage,andakeyyetunder-addressedprob-  lemistolearnagoodmetricformeasuringthesimilarity  betweenimages.Whiledeepmetriclearninghasyielded  impressiveperformancegainsbyextractinghighlevelab-  stractionsfromimagedata,aproperobjectivelossfunction  becomesthecentralissuetoboosttheperformance.Inthis  paper,weproposeanovelangularloss,whichtakesan-  glerelationshipintoaccount,forlearningbettersimilarity  metric.Whereaspreviousmetriclearningmethodsfocuson  optimizingthesimilarity(contrastiveloss)orrelativesim-  ilarity(tripletloss)ofimagepairs,ourproposedmethod  aimsatconstrainingtheangleatthenegativepointoftriplet  triangles.Severalfavorablepropertiesareobservedwhen  comparedwithconventionalmethods.First,scaleinvari-  anceisintroduced,improvingtherobustnessofobjective  againstfeaturevariance.Second,athird-ordergeometric  constraintisinherentlyimposed,capturingadditionallocal  structureoftriplettrianglesthancontrastivelossortriplet  loss.Third,betterconvergencehasbeendemonstratedby  experimentsonthreepubliclyavailabledatasets.  1.Introduction MetriclearningforcomputervisionaimsatÞndingap- propriatesimilaritymeasurementsbetweenpairsofimages  thatpreservedesireddistancestructure.Agoodsimilarity  canimprovetheperformanceofimagesearch,particularly  whenthenumberofcategoriesisverylarge[2]orunknown.  ClassicalmetriclearningmethodsstudiedthecaseofÞnd-  ingabetterMahalanobisdistanceinlinearspace.However,  lineartransformationhasalimitednumberofparameters  andcannotmodelhigh-ordercorrelationsbetweentheorig-  inaldatadimensions.Withtheabilityofdirectlylearning  non-linearfeaturerepresentation,deepmetriclearninghas  achievedpromisingresultsonvarioustasks,suchasvisual  productsearch[1,20,17],facerecognition[6,30,24],fea-  turematching[7],Þne-grainedimageclassiÞcation[33,38],  zero-shotlearning[11,35]andcollaborativeÞltering[13]. Figure1.Exampleoffeatureembeddingcomputedbyt-SNE[32]  fortheStanfordcardataset[18],wheretheimagesofFordRanger  SuperCab(right)haveamorediversedistributionthanVolvoC30  Hatchback(left).ConventionaltripletlosshasdifÞcultyindealing  withsuchunbalancedintra-classvariation.Theproposedangular  lossaddressesthisissuebyminimizingthescale-invariantangleat  thenegativepoint. Despitethevariousforms,themajorworkofdeepmet- riclearningcanbecategorizedasminimizingeitherthe  contrastiveloss( a.k.a.,Siamesenetwork)[6]orthetriplet loss[34,5].However,ithasbeenwidelynoticedthatdi-  rectlyoptimizingdistance-basedobjectivesindeeplearn-  ingframeworkisdifÞcult,requiringmanypracticaltricks,  suchasmulti-tasklearning[1,38]orhardnegativemin-  ing[33,8].Recentworkincludingtheliftedstructure[26]  andtheN-pairloss[25]proposedtomoreeffectivelymine  relationsamongsampleswithinamini-batch.Neverthe-  less,alloftheseworksrelyoncertaindistancemeasure-  mentbetweenpairsofsimilaranddis-similarimages.We  hypothesizethatthedifÞcultyoftrainingdeepmetriclearn-  ingalsocomesfromthelimitationbydeÞningtheobjective  onlyindistance.First,distancemetricissensitivetoscale  change.Traditionaltripletlossconstrainsthedistancegap  betweendis-similarclusters.However,itisinappropriate  tochoosethesameabsolutemarginforclustersindifferent  scalesofintra-classvariation.Forinstance,Fig.1showsthe  t-SNE[32]featureembeddingofStanfordcardataset[18],  wherethesampledistributionofFordRangerSuperCabs 43212593 ismuchmorediversethanVolvoC30Hatchback.Second,  distanceonlyconsiderssecond-orderinformationbetween  samples.Optimizingdistance-basedobjectivesinstochas-  tictrainingleadstosub-optimalconvergenceinhigh-order  solutionspace. Tocircumventtheseissues,weproposeanovelangu- larlosstoaugmentconventionaldistancemetriclearning.  Themainideaistoencodethethird-orderrelationinside  tripletintermsoftheangleatthenegativepoint.Bycon-  strainingtheupperboundoftheangle,ourmethodpushes  thenegativepointawayfromthecenterofpositiveclus-  ter,anddragsthepositivepointsclosertoeachother.Our  ideaisanalogoustotheusageofhigh-orderinformationfor  augmentingpair-wiseconstraintsinthedomainofgraph  matching[9]andMarkovrandomÞelds[10].Tothebest  ofourknowledge,thisistheÞrstworktoexploreangular  constraintsindeepmetriclearning.Inparticular,thepro-  posedangularlossimprovestraditionaldistance-basedloss  intwoaspects.First,comparedtodistance-basedmetric,  angleisnotonlyrotation-invariantbutalsoscale-invariant  bynature.Thisrenderstheobjectivemorerobustagainst  thevariationoflocalfeaturemap.Forinstance,thetwo  tripletsshowninFig.1arequitedifferentintheirscales.  Itismorereasonabletoconstraintheanglethatispro-  portionaltotherelativeratiobetweenEuclideandistances.  Second,angledeÞnesthethird-ordertriangulationamong  threepoints.Giventhesametriplet,angularlossdescribes  itslocalstructuremorepreciselythandistance-basedtriplet  loss.Ourideaisgeneralandcanbepotentiallycombined  withexistingmetriclearningframeworks.Theexperimen-  talstudyshowsitachievessubstantialimprovementover  state-of-the-artsmethodsonseveralbenchmarkdatasets.  2.Relatedwork Metriclearninghasbeenalong-standingprobleminma- chinelearningandcomputervision.Thesimplestformof  metriclearningmaybeconsideredaslearningtheMaha-  lanobisdistancebetweenpairsofpoints.Ithasadeepcon-  nectionwithclassicaldimensionreductionmethodssuchas  PCA,LLEandclusteringproblemsbutinadiscriminative  setting.Anexhaustivereviewofpreviousworkisbeyond  thescopeofthispaper.WerefertothesurveyofKulis etal.[19]onearlyworksofmetriclearning.Herewefocus onthetwomainstreamsindeepmetriclearning,contrastive  embeddingandtripletembedding,andtheirrecentvariants  usedincomputervision. TheseminalworkofSiamesenetwork[4]consistsof twoidenticalsub-networksthatlearncontrastiveembed-  dingfromapairofsamples.Thedistancebetweenapos-  itivepairisminimizedandsmalldistancebetweenanega-  tivepairispenalized,suchthatthederiveddistancemetric  shouldbesmallerforpairsfromthesameclass,andlarger  forpairsfromdifferentclasses.Itwasoriginallydesigned forsignatureveriÞcation[4],butgainedalotofattention  recentlyduetoitssuperiorperformanceinfaceveriÞca-  tion[6,30,28,36]. Despiteitsgreatsuccess,contrastiveembeddingrequires thattrainingdatacontainsreal-valuedprecisepair-wise  similaritiesordistances,whichisusuallynotavailablein  practice.Toaddressthisissue,tripletembedding[23]is  proposedtoexploretherelativesimilarityofdifferentpairs  andithasbeenwidelyusedinimageretrieval[33,5]and  facerecognition[24].Atripletismadeupofthreesamples  fromtwodifferentclasses,thatjointlyconstituteapositive  pairandanegativepair.Thepositivepairdistanceisen-  couragedtobesmallerthanthenegativepairdistance,and  asoftnearestneighborclassiÞcationmarginismaximized  byoptimizingahingeloss. Comparedtosoftmaxloss,ithasbeenshownthat SiamesenetworkortripletlossismuchmoredifÞcultto  traininpractice.TomakelearningmoreeffectiveandefÞ-  cient,hardsampleminingwhichonlyfocusesonasubset  ofsamplesthatareconsideredhardisusuallyemployed.  Forinstance,FaceNet[24]suggestedanonlinestrategyby  associatingeachpositivepairintheminibatchwithasemi-  hardnegativeexample.Wang etal .[33]designedamore effectivesamplingstrategytodrawout-classandin-class  negativeimagestoavoidoverÞttingfortrainingtripletloss.  Tomoreeffectivelybootstrapalargeßowerdataset,Cui  etal .[8]utilizedthehardnegativeimageslabeledbyhu- mans,whichareoftenneglectedintraditionaldatasetcon-  struction.Huang etal .[14]introducedaposition-dependent deepmetricunit,whichcanbeusedtoselecthardsamples  toguidethedeepembeddinglearninginanonlineandro-  bustmanner.Morerecently,Yuan etal .[37]proposeda cascadeframeworkthatcanminehardexampleswithin-  creasingcomplexities. Recently,therearealsosomeworksondesigningnew lossfunctionsfordeepmetricembedding.Asimpleyet  effectivewayistojointlytrainembeddinglosswithclas-  siÞcationloss.Withadditionalsupervision,theimprove-  mentoftripletlosshasbeenevidencedinfaceveriÞca-  tion[28],Þne-grainedobjectrecognition[38]andproduct  searchproblems[1].However,thesemethodsstillsuffer  fromthelimitationoftheconventionalsamplingthatfo-  cusesonlyontherelationwithineachtriplet.ToÞxthisis-  sue,Song etal .[26]proposedtheliftedstructuretoenable updatingdensepaircombinationsinthemini-batch.Sohn  [25]furtherextendedthetripletlossintoN-pairloss,which  signiÞcantlyimprovesuponthetripletlossbypushingaway  multiplenegativeexamplesjointlyateachupdate.Inaddi-  tiontotheseeffortsthatonlyexplorelocalrelationinside  eachmini-batch,anotherdirectionofworkisdesignedto  optimizeclustering-likemetricthatisawareoftheglobal  structureofalltrainingdata.Earlymethodssuchasneigh-  borhoodcomponentsanalysis(NCA)[12,23]candirectly 43222594 optimizeleave-one-outnearest-neighborclassiÞcationloss.  Whenappliedtomini-batchtraining,however,NCAislim-  itedasitrequirestoseetheentiretrainingdataineachiter-  ation.Rippel etal .[21]improvedNCAbymaintainingan modelofthedistributionsofthedifferentclassesinfeature  space.Theclassdistributionoverlapisthenpenalizedto  achievediscrimination.Morerecently,Song etal .[27]pro- posedanewmetriclearningframeworkwhichencourages  thenetworktolearnanembeddingfunctionthatdirectlyop-  timizesaclusteringqualitymetric.Nevertheless,allabove-  mentionedlossesaredeÞnedintermofdistancesofpoints,  andveryfew[31]hasconsideredotherpossibleformsof  loss.Ourworkre-deÞnesthecorecomponentofmetric  learninglossusingangleinsteadofdistance,andweshow  itcanbeeasilyadaptedintoexistingarchitecturessuchas  N-pairlosstofurtherimprovetheirperformance.  3.Proposedmethod Inthissection,wepresentanovelangularlosstoaug- mentconventionaldeepmetriclearning.WeÞrstreviewthe  conventionaltripletlossinitsmathematicalform.Wethen  derivetheangularlossbyconstructingastabletriplettrian-  gle.Finally,wedetailtheoptimizationoftheangularloss  onamini-batch.  3.1.Reviewoftripletloss Supposethatwearegivenasetoftrainingimages {(x,y),ááá} ofKclasses,where x!RDdenotesthefea- tureembeddingofeachsampleextractedbyCNNand y!{1,ááá ,K}itslabel.Ateachtrainingiteration,wesample amini-batchoftriplets,eachofwhich T=(xa,xp,xn)consistsofananchorpoint xa,associatedwithapairof positive xpandnegative xnsamples,whoselabelssatisfy ya=yp"=yn.Thegoaloftripletlossistopushawaythe negativepoint xnfromtheanchor xabyadistancemargin m>0comparedtothepositive xp:#xa$xp#2+m%#xa$xn#2.(1)Forinstance,asshowninFig.2,weexpecttheanchor xatostayclosertothepositive xpcomparedtothenegative xn.Toenforcethisconstraint,acommonrelaxationofEq.1is  theminimizationofthefollowinghingeloss, ltri (T)=!#xa$xp#2$#xa$xn#2+m"+,(2)wheretheoperator [á]+=max(0 ,á)denotesthehingefunc- tion.Itisworthmentioningthatthefeaturemapoftenneeds  tobenormalizedtohaveunitlength, i.e.,#x#=1,inor- dertoberobusttothevariationinimageilluminationand  contrast.Figure2.Illustrationofthetripletlossanditsgradientonasyn-  theticexample. TooptimizeEq.2,wecancalculateitsgradientwithre- specttothethreesamplesoftripletrespectivelyas !ltri (T)!xn=2( xa$xn),(3)!ltri (T)!xp=2( xp$xa),!ltri (T)!xa=2( xn$xp),iftheconstraint(Eq.1)isviolated,orzerootherwise. Itiswidelyobservedthatstochasticgradientdescent convergespoorlyonoptimizingthetripletloss.Thereare  afewreasonscontributingtothisdifÞculty:First,itisim-  practicaltoenumerateallpossibletripletsduetothecubic  samplingsize.Therefore,itcallsforaneffectivesampling  strategytoensurethetripletqualityandlearningefÞciency.  Second,thegoaloftheobjective(Eq.2)istoseparateclus-  tersbyadistancemargin m.However,itisinappropriateto applythesingleglobalmargin montheinter-classgapas theintra-classdistancecanvarydramaticallyinreal-world  tasks.Third,thegradient(Eq.3)derivedforeachpointonly  takesitspair-wiserelationwiththesecondpoint,butfails  toconsidertheinteractionwiththethirdpoint.Consider  thenegativepoint xninFig.2foranexample.Itsgradient 2(xa$xn)maynotbeoptimalwithouttheguaranteeof movingawayfromtheclasswhichboththeanchor xaandpositivesample xpbelongto. 3.2.Angularloss Toalleviatetheproblemselaboratedabove,avarietyof techniques[1,38,33,8,26,25]havebeenproposedinthe  lastfewyears.However,thefundamentalcomponentinthe  lossdeÞnition, i.e.,thepair-wisedistancebetweenpoints, hasrarelybeenchanged.Instead,thissectionintroducesan  angularlossthatleadstoanovelsolutiontoimprovedeep  metriclearning. LetÕsÞrstconsiderthetripletexampleshowninFig.3a, wherethetriplet T=(xa,xp,xn)formsthetriangle 43232595 &apn ,whoseedgesaredenotedas ean=xa$xn,epn=xp$xnandeap=xa$xprespectively.Theoriginal tripletconstraint(Eq.1)penalizesalongeredge eancom-paredtotheone eaponthebottom.Becausetheanchorand positivesamplessharethesamelabel,wecanderiveasym-  metricaltripletconstraintthatenforces #eap#+m%#epn#.Accordingtothecosinerule,itcanbeprovedthattheangle  !nsurroundedbythelongeredges eanandepnhastobe thesmallestone, i.e.,!n%min( !a,!p).Furthermore, because!n+!a+!p=180 !,!nhastobelessthan 60!.Thisfactmotivatesustoconstraintheupperboundof !nforeachtriplettriangle, !n%",(4)where">0isapre-deÞnedparameter.Intuitively,this constraintselectsthetripletthatformsaskinnytriangle  whoseshortestedge eapconnectsnodesofthesameclass. Comparedtothetraditionalconstraint(Eq.1)thatisde-  Þnedontheabsolutedistancebetweenpoints,thepro-  posedangularconstraintoffersthreeadvantages:1)An-  gleisasimilarity-transform-invariantmetric,proportional  totherelativecomparisonoftriangleedges.WithaÞxed  margin ",Eq.4alwaysholdsforanyre-scalingofthelocal featuremap.2)Thecosineruledeterminesthecalculation  of!ninvolvesallthethreeedgesofthetriangle.Incon- trast,theoriginaltripletonlytakestwoedgesintoaccount.  Theadditionalconstraintimprovestherobustnessandef-  fectivenessoftheoptimization.3)Intheoriginaltriplet  constraint(Eq.1),itisdifÞculttochooseaproperdistance  margin mwithoutmeaningfulreference.Bycomparison, setting"intheangularconstraintisaneasiertaskbecause ithasconcreteandinterpretablemeaningingeometry. However,astraightforwardimplementationofEq.4be- comesunstableinsomespecialcase.Considerthetriangle  showninFig.3a,where !a>90!.ByenforcingEq.4 toreduce !n,thenegativepoint xnwouldbepotentially draggedtowards x" n,whichisclosertotheanchorpoint xa.Thisresultcontradictsouroriginalgoalofenlargingthe distancebetweenpointsofdifferentclasses.ToÞxthisis-  sue,were-constructthetriplettriangletomakeEq.4more  stable.Ourintuitionistomodeltherelationbetweenthe  negative xnwiththelocalsampledistributiondeÞnedby theanchor xaandthepositive xp,showninFig.3b.A naturalapproximationtothisdistributionisthecircumcir-  cleCpassingthrough xaandxp,centeredatthemiddle xc=(xa+xp)/2.Wethenintroduceahyper-plane P,whichisperpendiculartotheedge enc=xn$xcatxc.The hyper-plane Pintersectsthecircumcircle Cattwonodes, oneofwhichisdenotedas xm.Basedontheseauxiliary structures,wedeÞnethenewtriangle &mcnbyshiftingthe anchorxaandpositive xptoxcandxmrespectively.Given thenewtriangle,were-formulateEq.4toconstrainthean-  gle!n"closedbytheedgeof encandenmtobelessthana !"#!$#Figure3.Illustrationoftheangularconstraintonasynthetictriplet  where!a>90!.(a)Directlyminimizing !nisunstableasit woulddrag xncloserto xa.(b)Themorestable !n"deÞnedby re-constructingthetriangle !mcn .pre-deÞneupperbound ",i.e.,tan !n"=#xm$xc##xn$xc#=#xa$xp#2#xn$xc#%tan ",(5)where#xm$xc#istheradiusofthecircumcircle C,which equalsto #xa$xp#/2.Inspiredbythetripletloss(Eq.2),weseekfortheopti- mumembeddingsuchthatthesamplesofdifferentclasses  canbeseparatedwellastheangularconstraint(Eq.5)de-  scribes.Inanutshell,ourangularlossconsistsofminimiz-  ingthefollowinghingeloss, lang(T)=!#xa$xp#2$4tan 2"#xn$xc#2"+.(6)Tobetterunderstandtheeffectofoptimizingtheangular  loss,wecaninvestigatethegradientof langwithrespectto xa,xpandxn,whichare !lang(T)!xa=2( xa$xp)$2tan 2"(xa+xp$2xn),!lang(T)!xp=2( xp$xa)$2tan 2"(xa+xp$2xn),!lang(T)!xn=4tan 2"!(xa+xp)$2xn",(7)if!n"islargerthan ",orzerootherwise.Asillustrated inFig.3b,thegradientpushesthenegativepoint xnaway 43242596 !"#!$# Figure4.Comparisonbetweendifferentsamplingmethods.For  eachnode,weusecolortoindicatetheclasslabelandshapefor  itsrole( i.e.,anchor,positiveornegative)intriplet.(a)Traditional tripletsampling.(b)N-pairsampling.Tokeepplotclean,weonly  showtheconnectioninsideonetuplet.  fromxc,thecenteroflocalclusterdeÞnedby xaandxp.Inaddition,theanchor xaandthepositive xparedragged towardseachother.Comparedtotheoriginaltripletloss  whosegradients(Eq.3)onlydependontwopoints,thegra-  dientsinEq.7aremuchmorerobustastheyconsiderallthe  threepointssimultaneously.  3.3.Implementationdetails Eq.6deÞnestheangularlossonatriplet.Whenopti- mizingamini-batchcontainingmultipletriplets,wefound  ourmethodcanbefurtherimprovedintwoways. First,weenhancethemini-batchoptimizationbymaking thefulluseofthebatch.AsillustratedinFig.4a,thecon-  ventionalsamplestrategyconstructsamini-batchasmulti-  pledisjointtripletswithoutinteractionamongthem.This  posesalargebottleneckinoptimizationasitcanonlyen-  codealimitedamountofinformation.Toallowjointcom-  parisonamongallsamplesinthebatch,wefollowthesam-  plingstrategyproposedinN-pairloss[25]toconstructtu-  pletswithmultiplenegativepoints.Moreconcretely,we  Þrstdraw N/2differentclasses,fromeachofwhichwe thenrandomlysampletwotrainingimages.Themainbene-  ÞtbehindN-pairsamplingisthatitcanavoidthequadratic  possiblecombinationsoftuplets.Forinstance,asshownin  Fig.4b,givenabatchwith NsamplesB={xi,yi}N i=1,thereareintotal Ntuplets,eachofwhichiscomposedby apairofanchor xa!Bandpositive xp!Bofthesame class,and N$2negativefromotherclasses. Second,adirectextensionofEq.6toconsidermorethan onenegativepointwouldresultinaverynon-smoothob-  jectivefunction.Inspiredbyrecentwork[26,25,27],we  replacetheoriginalhingelosswithitssmoothupperbound,  i.e.,log(exp( y1)+exp( y2))'max( y1,y2).Byassuming featureisofunitlength( i.e.,#x#=1)inEq.6,wederive theangularlossforabatch Busingthefollowing log-sum- exp formulation:lang(B)=1N#xa#B$log %1+#xn#Byn$=ya,ypexp&fa,p,n'(),(8)wherein fa,p,n,wedroptheconstanttermsdependingon thevalueof #x#inasimilarspirittoN-pairloss[25], i.e.,fa,p,n=4tan 2"(xa+xp)Txn$2(1+tan 2")xT axp.Ourworkonangularlossexploresthethird-orderrela- tionsbeyondthescopeofthewell-studiedpair-wisedis-  tance.Duetoitsßexibilityandgenerality,wecaneas-  ilycombinetheangularconstraintwithtraditionaldistance  metriclosstoboosttheoverallperformance.Asanexam-  ple,wemainlyinvestigatethecombinationwiththeN-pair  loss[25],oneofthelatestworkfordeepmetriclearning, lnpair &ang(B)=lnpair (B)+#lang(B),(9)wherelnpair (B)denotestheoriginalN-pairlossas, lnpair (B)=1N#xa#B$log %1+#xn#Byn$=ya,ypexp&xT axn$xT axp'(),(10)and#isatrade-offweightbetweenN-pairandtheangular loss.Inallexperiments,wealwaysset #=2asitconsis- tentlyyieldspromisingresult.  4.Experiments Inthissection,weevaluatedeepmetriclearningalgo- rithmsonbothimageretrievalandclusteringtasks.Our  methodhasbeenshowntoachievestate-of-the-artperfor-  manceonthreepublicbenchmarkdatasets.  4.1.Benchmarkdatasets Weconductourexperimentsonthreepublicbenchmark datasets.Foralldatasets,wefollowtheconventionalproto-  colofsplittingtrainingandtesting: CUB-200-2011[3]datasethas200speciesofbirdswith 11,788imagesincluded,wheretheÞrst100species(5,864  images)areusedfortrainingandtheremaining100species  (5,924images)areusedfortesting. StanfordCar [18]datasetiscomposedby16,185cars imagesof196classes.WeusetheÞrst98classes(8,054  images)fortrainingandtheother98classes(8,131images)  fortesting. OnlineProducts [26]datasetcontains22,634classes with120,053productimagesintotal,wheretheÞrst11,318  classes(59,551images)areusedfortrainingandtherest  classes(60,502images)areusedfortesting. 43252597 4.2.Baselines Inordertoevaluatethesuperiorityoftheproposed method,wecomparewiththreebaselines: TripletLoss :Weimplementthestandardtripletem- beddingbyoptimizingEq.2.Tobefairincomparison,  weapplytripletlossembeddingwithtwosamplingstrate-  gies.Followingthemoststandardsetting,themini-batchof  Triplet-I(T-I) wasconstructedbysamplingdisjointtriplets asillustratedinFig.4a.Inthesecondcaseof Triplet-II(T- II),weoptimizeEq.2usingtheN-pairsamplingasshown inFig.4btokeepconsistentwiththeangularloss. LiftedStructure(LS) [26]:Weadopttheopen-source codefromtheauthorsÕwebsitewiththedefaultparameters  usedinthepaper. N-pairLoss(NL) [25]:WeimplementN-pairloss (Eq.10)closelyfollowingtheillustrationofthepaper.We  foundourimplementationachievedsimilarresultsasre-  portedinthepaper. Forourmethod,weimplementtwoversions, AngularLoss(AL) andN-pair&AngularLoss(NL&AL) ,that optimizeEq.8andEq.9respectively.Tobecomparable  withpriorwork,weemploytheN-pairsampling(Fig.4b)  sharedbythebaselinesof Triplet-II andN-pairLoss .Asthefocusofthisworkisthesimilaritymeasure,we didnotemployanyhardnegativeminingstrategiestocom-  plicatethecomparison.Butitisworthmentioningthatour  workcanbeeasilycombinedwithanyhardnegativemining  method. 4.3.Evaluationmetrics Followingthestandardprotocolusedin[26,25],we evaluatetheperformanceofdifferentmethodsinbothre-  trievalandclusteringtasks.Wespliteachdatasetintotwo  setsofdisjointclasses,onefortrainingandtheotherfor  testingtheretrievalandclusteringperformanceoftheun-  seenclasses.Forretrievaltask,wecalculatethepercentage  ofthetestingexampleswhose Rnearestneighborscontain atleastoneexampleofthesameclass.Thisquantityisalso  knownasRecall@R,thedefactometric[15]forimagere-  trievingevaluation.Forclusteringevaluation,weadoptthe  codefrom[26]byclusteringtestingexamplesusingthek-  meansalgorithm.Thequalityofclusteringisreportedin  termsofthestandardF 1andNMImetrics.See[26]for theirdetaileddeÞnition.  4.4.Trainingsetup TheCaffepackage[16]isusedthroughouttheexperi- ments.Allimagesarenormalizedto256-by-256beforefur-  therprocessing.Theembeddingsizeissetto D=512 forallembeddingvectors,andnonormalizationisconducted  beforecomputingloss.Weomitthecomparisonondifferent  embeddingsizesastheperformancechangeisminor.This  factisalsoevidencedin[26].GoogLeNet[29]pretrained onImageNetILSVRCdataset[22]isusedforinitialization  andarandomlyinitializedfullyconnectedlayerisadded.  Thenewlayerisoptimizedwith10timeslargerlearning  ratethantheotherlayers.WeÞxthebaselearningrateto  10%4foralldatasetsexceptfortheCUB-200-2011dataset, forwhichweuseasmallerrate 10%5asithasfewerimages andismorelikelytomeettheoverÞttingproblem.Weuse  SGDwith20ktrainingiterationsand128mini-batchsize.  Standardrandomcropandrandomhorizontalmirroringare  usedfordataaugmentation.Noticethatourmethodincurs  negligiblecomputationalcostcomparedtotraditionaltriplet  loss.Therefore,thetrainingtimeisalmostsameasother  baselines. 4.5.Resultanalysis Tables1,2and3compareourmethodwithallbaselines inbothclusteringandretrievaltasks.Thesetablesshowthat  thetworecentbaselines,liftedstructure(LS)[26]andN-  pairloss(NL)[25],canalwaysimprovethestandardtriplet  loss(T-IandT-II).Inparticular,N-pairachievesalarger  margininimprovementbecauseoftheadvanceinitsloss  designandbatchconstruction.Comparedtopreviouswork,  theproposedangularloss(AL)consistentlyachievesbet-  terresultsonallthreebenchmarkdatasets.Itisimportant  tonoticethattheproposedangularloss(AL)employsthe  samesamplingstrategiesastripletloss(T-II)andN-pair  loss(NL).Thisclearlyindicatesthesuperiorityofthenew  lossforsolvingdeepmetriclearningproblem.Byintegrat-  ingwiththeoriginalN-pairloss,thejointoptimizationof  angularlossinNL&ALcanleadtothebestperformance  amongallthemethodsinallmetrics. Fig.5comparesNL&ALwithN-pairlossonthetask ofimageretrieval.Asitcanbeobserved,theproposed  NL&ALlearnsamorediscriminativefeaturethathelpsin  identifyingthecorrectimagesespeciallywhentheintra-  classvarianceislarge.Forexample,givenaqueryimage  ofFIAT500Convertible2012atthefourthrowofFig.5on  therightside,thetop-5imagesretrievedbyNL&ALcon-  tainfoursuccessfulmatchesthatbelongtothesameclass  asthequery,whileN-pairmethodfailstoidentifythem.  Inaddition,Fig.6visualizesthefeatureembeddingcom-  putedbyourmethod(NL&AL)in2-Dusingt-SNE[32].  Wehighlightseveralrepresentativeclassesbyenlargingthe  correspondingregionsinthecorners.Despitethelargepose  andappearancevariation,ourmethodeffectivelygenerates  acompactfeaturemappingthatpreservessemanticsimilar-  ity. Akeyparameterofourmethodisthemargin ",thatde- terminestowhatdegreetheconstraint(Eq.5)wouldbeac-  tivated.Table4andTable5studytheimpactofchoosing  different "fortheretrievaltaskontheStanfordcarandon- lineproductdatasets,respectively.Choosing "=45 !forStanfordcarand "=36 !foronlineproductleadtothe 43262598 MethodClustering(%)Recall@R(%) NMIF 1R=1R=2R=4R=8 T-I 53.719 .742 .254 .466 .276 .7T-II 54.120 .042 .854 .966 .277 .6LS56.222 .746 .558 .169 .880 .2NL60.228 .251 .964 .374 .983 .2AL61.030.253.665 .075 .383 .7NL&AL61.129.454.766 .376 .083 .9Table1.ComparisonofclusteringandretrievalontheCUB-200-  2011[3]dataset. MethodClustering(%)Recall@R(%) NMIF 1R=1R=2R=4R=8 T-I 53.818 .745 .559 .071 .080 .8T-II 54.319 .646 .359 .971 .481 .3LS55.121 .548 .361 .171 .881 .1NL62.731 .868 .978 .985 .890 .9AL62.431 .871 .380 .787 .091 .8NL&AL63.232 .271 .481 .487 .592 .1Table2.ComparisonofclusteringandretrievalontheStanford  car[18]dataset. MethodClustering(%)Recall@R(%) NMIF 1R=1R=10R=100R=1000 T-I 86.219 .956 .574 .788 .396 .2T-II 86.421 .058 .176 .089 .196 .4LS87.424 .763 .080 .591 .797 .5NL87.726 .366 .983 .092 .397 .7AL87.826 .567 .983 .292 .297 .7NL&AL88.629 .970 .985 .093 .598 .0Table3.Comparisonofclusteringandretrievalontheonlineprod-  ucts[26]dataset.  bestperformanceforthemethodofNL&AL.Wefoundthat  ourmethodperformsconsistentlywellinallthreedataset  for36!%"%55!.Itdeservestobementionedthat,with- outintegratingwithNL,ALpreformscomparablywithNL,  andevenbetterwhenminingapropervalueof ",whichis showninTable5.  5.Conclusion Inthispaper,weproposeanovelangularlossfordeep metriclearning.Unlikemostmethodsthatformulateobjec-  tivebasedondistance,weresorttoconstraintheangleofthe  triplettriangleintheloss.Comparedtopair-wisedistance, NL&AL(")Recall@R(%) R=1R=2R=4R=8 "=36 !69.979 .786 .891 .8"=42 !70.780 .587 .291 .9"=45 !71.481 .487 .592 .1"=48 !71.380 .487 .091 .9"=55 !69.078 .185 .390 .8Table4.Comparisonofdifferentvaluesfor !forourmethodon Stanfordcardataset. MethodNLNL&AL( "=45 !)NL&AL( "=36 !)Recall@1(%) 66.969 .270.9MethodNLAL( "=45 !)AL( "=36 !)Recall@1(%) 66.966 .467.9Table5.Comparisonofdifferentvaluesfor !forourmethodon theonlineproductdataset.  angleisarotationandscaleinvariantmetric,renderingthe  objectivemorerobustagainstthelargevariationoffeature  mapinrealdata.Inaddition,thevalueofangleencodesthe  triangulargeometryofthreepointssimultaneously.Given  thesametriplet,itoffersadditionalsourceofconstraints  toensurethatdis-similarpointscanbeseparated.Further-  more,weshowhowtheangularlosscanbeeasilyintegrated  intootherframeworkssuchasN-pairloss[25].Thesupe-  riorityofourmethodoverexistingstate-of-the-artworkis  veriÞedonseveralbenchmarkdatasets. Inthefuture,wehopetoextendourworkintwodirec- tions.First,ourmethodoriginsfromthetripletlossand  leveragesthethird-orderrelationamongthreepoints.It  isinterestingtoconsidermoregeneralcasewithfouror  moresamples.Previouswork[38,14]studiedthecaseof  quadrupletbutstillemployedcertaindistance-basedobjec-  tives.Onepossibleextensionofourideaonquadrupletisto  constructatriangularpyramidandconstraintheanglebe-  tweenthesideedgeandtheplaneonthebottom.Second,  itisbeneÞcialtocombineourmethodwithotherpractical  trickssuchashardnegativemining[37]ornewclustering-  likeframeworks[21,27].  References [1]S.BellandK.Bala.Learningvisualsimilarityforprod- uctdesignwithconvolutionalneuralnetworks. ACMTrans. Graph. ,34(4):98:1Ð98:10,2015. [2]K.Bhatia,H.Jain,P.Kar,M.Varma,andP.Jain.Sparse localembeddingsforextrememulti-labelclassiÞcation.In  NIPS,pages730Ð738,2015. [3]S.Branson,G.V.Horn,C.Wah,P.Perona,andS.Belongie. Theignorantledbytheblind:Ahybridhuman-machinevi- 43272599 !"#$%&#'$(#)*+!"#$%&#'$(#)*+,-./-,-,-,-./-,-./-,-,-./-,-,- ,-./-,-./-,- Figure5.Comparisonofqueriesandtop-5retrievalsbetweenN-pair(NP)andourmethod(NP&AL).Fromtoptobottom,weplottwo  examplesfortheCUB-200-2011,Stanfordcarandonlineproductsdatasetrespectively.Theretrievedimagespointedbyanarrowarethe  onesthatbelongtothesameclassasthequery. Figure6.Visualizationoffeatureembeddingcomputedbyourmethod(NP&AL)usingt-SNEontheCUB-200-2011dataset. sionsystemforÞne-grainedcategorization. Int.J.Comput. Vis. ,108(1-2):3Ð29,2014. [4]J.Bromley,I.Guyon,Y.LeCun,E.S ¬ackinger,andR.Shah. SignatureveriÞcationusingaSiamesetimedelayneuralnet- work.In NIPS,1993. [5]G.Chechik,V.Sharma,U.Shalit,andS.Bengio.Largescale onlinelearningofimagesimilaritythroughranking. Journal ofMachineLearningResearch ,11:1109Ð1135,2010. 43282600 [6]S.Chopra,R.Hadsell,andY.LeCun.Learningasimilarity metricdiscriminatively,withapplicationtofaceveriÞcation.  InCVPR,2005. [7]C.B.Choy,J.Gwak,S.Savarese,andM.K.Chandraker. Universalcorrespondencenetwork.In NIPS,2016. [8]Y.Cui,F.Zhou,Y.Lin,andS.J.Belongie.Fine-grained categorizationanddatasetbootstrappingusingdeepmetric  learningwithhumansintheloop.In CVPR,2016. [9]O.Duchenne,F.R.Bach,I.Kweon,andJ.Ponce.Atensor- basedalgorithmforhigh-ordergraphmatching. IEEETrans. PatternAnal.Mach.Intell. ,33(12):2383Ð2395,2011. [10]A.Fix,A.Gruber,E.Boros,andR.Zabih.Agraphcut algorithmforhigher-ordermarkovrandomÞelds.In ICCV,pages1020Ð1027,2011. [11]A.Frome,G.S.Corrado,J.Shlens,S.Bengio,J.Dean, M.Ranzato,andT.Mikolov.DeViSE:Adeepvisual-  semanticembeddingmodel.In NIPS,2013. [12]J.Goldberger,S.Roweis,G.Hinton,andR.Salakhutdinov. Neighborhoodcomponentsanalysis.In NIPS,2004. [13]C.-K.Hsieh,L.Yang,Y.Cui,T.-Y.Lin,S.Belongie,and D.Estrin.Collaborativemetriclearning.In WWW,2017. [14]C.Huang,C.C.Loy,andX.Tang.Localsimilarity-aware deepfeatureembedding.In NIPS,pages1262Ð1270,2016. [15]H.J «egou,M.Douze,andC.Schmid.Productquantiza- tionfornearestneighborsearch. IEEETrans.PatternAnal. Mach.Intell. ,33(1):117Ð128,2011. [16]Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.B. Girshick,S.Guadarrama,andT.Darrell.Caffe:Convolu-  tionalarchitectureforfastfeatureembedding.In ACMMM ,pages675Ð678,2014. [17]M.H.Kiapour,X.Han,S.Lazebnik,A.C.Berg,andT.L. Berg.Wheretobuyit:Matchingstreetclothingphotosin  onlineshops.In ICCV,2015. [18]J.Krause,M.Stark,J.Deng,andL.Fei-Fei.3Dobjectrep- resentationsforÞne-grainedcategorization.In ICCVWork- shopon3DRepresentationandRecognition ,2013. [19]B.Kulis.Metriclearning:Asurvey. FoundationsandTrends inMachineLearning ,5(4):287Ð364,2013. [20]Y.Li,H.Su,C.R.Qi,N.Fish,D.Cohen-Or,andL.J. Guibas.JointembeddingsofshapesandimagesviaCNNim-  agepuriÞcation. ACMTrans.Graph. ,34(6):234:1Ð234:12, 2015.[21]O.Rippel,M.Paluri,P.Dollar,andL.Bourdev.Metriclearn- ingwithadaptivedensitydiscrimination.In CVPR,2015. [22]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh, S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.S.Bernstein,  A.C.Berg,andF.Li.ImageNetlargescalevisualrecog-  nitionchallenge. InternationalJournalofComputerVision ,115(3):211Ð252,2015. [23]R.SalakhutdinovandG.Hinton.Learninganonlinearem- beddingbypreservingclassneighbourhoodstructure.In  AISTATS ,2007. [24]F.Schroff,D.Kalenichenko,andJ.Philbin.FaceNet:A uniÞedembeddingforfacerecognitionandclustering.In  CVPR,2015. [25]K.Sohn.Improveddeepmetriclearningwithmulti-class N-pairlossobjective.In NIPS,2016. [26]H.Song,Y.Xiang,S.Jegelka,andS.Savarese.Deepmetric learningvialiftedstructuredfeatureembedding.In CVPR,2016.[27]H.O.Song,S.Jegelka,V.Rathod,andK.Murphy.Learn- ablestructuredclusteringframeworkfordeepmetriclearn-  ing.CoRR,abs/1612.01213,2016. [28]Y.Sun,Y.Chen,X.Wang,andX.Tang.Deeplearningface representationbyjointidentiÞcation-veriÞcation.In NIPS,2014.[29]C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.E.Reed, D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.  Goingdeeperwithconvolutions.In CVPR,2015. [30]Y.Taigman,M.Yang,M.Ranzato,andL.Wolf.DeepFace: Closingthegaptohuman-levelperformanceinfaceveriÞca-  tion.In CVPR,2014. [31]E.UstinovaandV.S.Lempitsky.Learningdeepembeddings withhistogramloss.In NIPS,pages4170Ð4178,2016. [32]L.vanderMaaten.Acceleratingt-SNEusingtree- basedalgorithms. JournalofMachineLearningResearch ,15(1):3221Ð3245,2014. [33]J.Wang,Y.Song,T.Leung,C.Rosenberg,J.Wang, J.Philbin,B.Chen,andY.Wu.LearningÞne-grainedim-  agesimilaritywithdeepranking.In CVPR,2014. [34]K.WeinbergerandL.Saul.Distancemetriclearningfor largemarginnearestneighborclassiÞcation. JournalofMa- chineLearningResearch ,10:207Ð244,2009. [35]J.Weston,S.Bengio,andN.Usunier.WSABIE:scalingup tolargevocabularyimageannotation.In IJCAI,pages2764Ð 2770,2011. [36]D.Yi,Z.Lei,andS.Z.Li.Deepmetriclearningforpractical personre-identiÞcation. CoRR,abs/1407.4979,2014. [37]Y.Yuan,K.Yang,andC.Zhang.Hard-awaredeeplycas- cadedembedding. CoRR,abs/1611.05720,2016. [38]X.Zhang,F.Zhou,Y.Lin,andS.Zhang.Embeddinglabel structuresforÞne-grainedfeaturerepresentation.In CVPR,2016.43292601  
TemporalModelingApproachesforLarge-scale Youtube-8MVideoUnderstanding FuLi,ChuangGan,XiaoLiu,YunlongBian,XiangLong,YandongLi,ZhichaoLi,JieZhou,ShileiWen BaiduIDL&TsinghuaUniversity Abstract Thispaperdescribesoursolutionforthevideorecogni- tiontaskoftheGoogleCloud&YouTube-8MVideoUnder- standingChallengethatrankedthe3rdplace.Becausethe challengeprovidespre-extractedvisualandaudiofeatures insteadoftherawvideos,wemainlyinvestigatevarious temporalmodelingapproachestoaggregatetheframe-level featuresformulti-labelvideorecognition.Oursystemcon- tainsthreemajorcomponents:two-streamsequencemodel, fast-forwardsequencemodelandtemporalresidualneural networks.ExperimentresultsonthechallengingYoutube- 8Mdatasetdemonstratethatourproposedtemporalmodel- ingapproachescanimproveexistingtemporal modelingapproachesinthelarge-scalevideorecognition tasks.Tobenoted,ourfast-forwardLSTMwithadepthof7 layersachieves82.75%intermofGAP@20ontheKaggle Publictestset. 1.Introduction Videounderstandingisachallengingtaskwhichhas receivedresearchattentionincomputervision andmachinelearning.Theubiquitousvideocapturede- viceshavecreatedvideosfarsurpassingwhatwecanwatch. Therefore,ithasbeenapressingneedtodevelopautomatic videounderstandingalgorithmsforvariousapplications. Torecognizeactionsandeventsinvideos,existing approachesbasedondeepconvolutionalneuralnetworks (CNNs)[11,15,6,20]and/orrecurrentnetworks[9,18, 10,3]haveachievedstate-of-the-artresults.However,due tothelackofpubliclyavailabledatasets,existingvideo recognitionapproachesarerestrictedtosmall-scaledata, whilelarge-scalevideounderstandingremainsanunder- addressedproblem.Toremedythisissue,Googlere- leasesanewwebcrawledlarge-scalevideodataset,named asYouTube-8M,whichcontainsover7millionYouTube videoswithavocabularyof4716classes.Avideomayhave multipletagclassesandtheaveragenumberoftagclasses pervideois1.8.Priortothis,Gan et.al [5,7]alsoinves- tigatedtolearnvideorecognitionmodelsusingWebvideos andimages. AnotherappealingpointoftheYoutube-8Mdatasetis thatthiscompetitiononlyprovidesthepre-extractedvisual andaudiofeaturesfromeverysecondofvideoinsteadof rawvideos.WecanneithertraindifferentCNNsarchitec- turesnorlearnasopticalwfeaturesfromtherawvideos. Therefore,wefocusontemporalmodelingapproachesto aggregatetheframe-levelfeaturesthatyieldrobustand discriminativevideorepresentationforfurthermulti-label recognition.Particularly,weproposethreenoveltemporal modelingapproaches,namelytwo-streamsequencemodel, fast-forwardsequencemodelandtemporalresidualneural networks.Experimentresultsveritytheeffectivenessof thethreemodelsoverthetraditionaltemporalmodelingap- proaches.Wealsothatthesethreetemporalmodeling approachesarecomplementarywitheachothersandleadto thestate-of-the-artsperformancesafterensemble. Theremainingsectionsareorganizedasfollows.Section 2presentsourtemporalmodelingapproachtolearnrobust anddiscriminativevideofeaturerepresentationforrecogni- tion.Section3reportsempiricalresults,followedbydis- cussionandconclusioninSection4. 2.Approach Inthissection,wedescribeourthreefamiliesoftemporal approachesrespectively. 2.1.TweamSequenceModels Ourtwostreamsequencemodelsbuilduponthebidi- rectionalLSTM[10]andGRU[3],sincetheyhaveshown strongtemporalmodelingabilitiesforvideorecognition. Thechallengehereishowtoincorporatethevisualandau- dioinformationcontainedinthevideos.Inordertobest taketheadvantageofmulti-modalclues,weproposeseveral sequencearchitecturestofusethesetwomodalityfeatures. Theoriginaltwo-streamCNN[15]frameworktrains CNNswithRGBandopticalwfeaturesseparately,and thenreliesonalatescorefusionstrategytoleveragethe complementarynatureofthetwomodalities.Recently,Ma et.al [14]hasproposedatemporalsegmentRNNnetwork 1 arXiv:1707.04555v1  [cs.CV]  14 Jul 2017Figure1.Thearchitectureofourproposedtwo-streamLSTMmodel. byconcatenatingthetwomodalityfeaturesandthen fedthemintooneLSTMtoachievevideorecognition. Differentfromthem,weproposetotraintwobidirec- tionalLSTMsorGRUsmodels(i.e.oneforRGBfeatures, andtheotherforaudiofeatures).Attentionlayersarein- sertedafterthesequencemodelsandattendedfeaturevec- torsfromtwomodalitiesarethenconcatenated.Finally,the concatenatedfeaturevectorisfedintotwofully-connected layerandasigmoidlayersequentiallyformulti-labelclas- WeoutlinetheframeworkinFigure1.Exper- imentsresultsveritytheeffectivenessoftheourproposed two-streamsequencemodelapproachesoverotheralterna- tivetwo-streamfusionapproaches. 2.2.ForwardSequenceModels Recently,wehavewitnessedthesuccessofdeepCNNs onlarge-scaleimage[16,19,8].Typically, modelswithdeeperconvolutionlayersoutperformshallow ones.However,thissuccesshasnotbeentransferredtothe sequencemodelsthatusedinvideorecognitiontasks.The bestsequencemodelsreportedinliteraturearestillshallow models.Thephenomenoniscausedbytworeasons.First, itisimpossibletoexploredeepersequencemodelsinthe pre-existingsmall-scalevideorecognitiondataset[17,12], whichonlycontainaround10thousandsvideos.Second, theoptimizationofdeepersequencemodelismuchmore challengingthantrainingdeeperCNNsbecausetheexis- tenceofmanymorenonlinearactivationsandtherecurrent computationresultsinsmallerandinstablegradient. ThenewYoutube8Mdatasetshedslightonopportunities toexploresequencemodelswithdeeparchitectures.Since large-scalevideorecognitionisaverydifandchal- lengingproblem,webelievethatdeepersequencemodels withmorecomplexarchitectureisnecessaryforcapturing thetemporalrelationshipbetweenframes.Inthecompeti- tion,wefocusonenhancingthecomplexityofthesequence modelbyincreasingthemodeldepth.However,weobserve thatnaivelyincreasingthedepthoftheLSTMandGRUstill entailstoovandoptimizationdifandthus alwayshavenegativeresultsforthevideorecognition.This phenomenonisconsistentwiththeresultsreportedbythe originalYoutube8Mtechniquereport[1]. Toaddressthesechallenges,weexploreanoveldeep LSTM/GRUarchitecturebyaddingthefast-forwardcon- 2 Figure2.Thearchitectureofourproposedfast-forwardsequencemodels. nections[22]tosequencemodels,whichplaysanessen- tialroleinbuildingasequencemodelwith7bidirectional LSTMs.Weoutlinetheframeworkinthe2.We concatenatetheRGBandaudiofeaturesofeachframe togetherandthenfedthemintothefast-forwardsequence model.Thefast-forwardconnectionsareaddedbetween twofeed-forwardcomputationblocksofadjacentrecurrent layers.Eachfast-forwardconnectiontakestheoutputsof previousfast-forwardandrecurrentlayerasinput,anduse afully-connectedlayertoembedthem.Thefast-forward connectprovidesafastpathforinformationtopropagate, sowecallthepathfast-forwardconnections.Wewillin- troducemoredetailofourproposedfast-forwardsequence modelandimplementationdetailsinafollowingtechnique report. 2.3.TemporalResidualNeuralNetworks Althoughthepowerofrecurrentmodels(LSTMsand GRUs)havebeenwidelyacknowledged,recentsequential convolutionarchitectures[13,14]showstrongpotentials forvarioustemporalmodelingtasks.Li et.al [13]pro- posedatemporalResCNNbasedneuralspeakerrecogni- tionsystemforspeakerandvMa et.al [14]proposedatemporal-inceptionarchitecturefor videorecognition,andachievedstate-of-the-artresultson UCF101andHMDB51datasets. Inthecompetition,weinvestigatetheusageoftempo- ralconvolutionneuralnetworksfortemporalmodelingon videorecognition.Incontrastwith[14]thatperformscon- volutionsonframe-levelfeaturestolearnglobalvideo-level representations,wecombineconvolutionandrecurrentneu- ralnetworkstotaketheadvantagesofbothmodels.The temporalconvolutionneuralnetworksareutilizedtotrans- formtheoriginalframe-levelfeaturesintoamorediscrimi- nativefeaturesequence,andLSTMsareusedforclas-  ThearchitectureoftheproposedTemporalCNNisillus- tratedinFigure3.RGBandaudiofeaturesineachframeare concatenatedandzero-valuedfeaturesarepaddedtomake edlengthdata.Thesizeoftheresultedinputdatais 4000  1152  300 ,where4000,1152,and300indicates mini-batchsize,channelnumber,andlengthofframes,rep- sectively.WethenpropagatethebatchdataintoaTempo- ralResnet,whichisastackof9TemporalResnetBlocks (TRB),andeachTRBconsistsoftwotemporalconvolu- tionallayers(followedbybatchnormandactivation)anda 3 Figure3.ThearchitectureofourproposedtemporalresidualCNNs. shortcutconnection.Weuse1024 3  1 forallthe temporalconvolutionlayers.Theoutputofthetemporal CNNisthenfedintoabidirectionalLSTMwithattention. 3.Experiment Inthissection,wepresentthedataset,experimentsetting andourexperimentalresults. 3.1.Dataset WeconductexperimentonthechallengingYoutube- 8Mdataset[1].Thisdatasetcontainsaround7million Youtubevideos.Eachvideoisannotatedwithoneormul- tipletags.Inthecompetition,visualandaudiofeaturesare pre-extractedandprovidedwiththedatasetforeachsecond ofthevideo.VisualfeaturesareobtainedbytheGoogleIn- ceptionCNNpre-trainedontheImageNet[4],followedby thePCA-compressionintoa1024dimensionalvector.The audiofeaturesareextractedfromapre-trainedVGG[16] network.Intheofsplit,thedatasetisdividedintothree parts:70%fortraining,20%forvalidation,and10%for testing.Inpractice,weonlymaintain60Kvideosfromthe ofvalidationsettocrossvalidatetheparameters.Other videosinthevalidationsetareincludedintothetraining set.Weobservethatthisstrategycanslightlyimprovethe performances.Resultsareevaluatedusingthe GlobalAveragePrecision(GAP)metricattop20asusedin theYoutube-8MKagglecompetition. Table1.ComparisonresultsonYoutube8Mtestset. Method GAP@20 Video-level 0.80824 VLAD 0.80423 TemporalCNN 0.80889 Two-streamLSTM 0.82172 Two-streamGRU 0.82366 Fast-forwardLSTM 0.81885 Fast-forwardGRU 0.81970 Fast-forwardLSTM(depth7) 0.82750 Ensemble 0.84542 3.2.ExperimentResults Table1reportstheperformanceofindividualmodelson theYoutube8Mtestset.Forthevideo-levelapproach,we usetheaveragepoolingtoaggregatetheframe-levelfeature vector.ForVLADencodingbasedapproaches,weuse256 clustercentersfollowedbysignedsquarerootandL2nor- malizationsassuggestedin[2,21].Wethenfedtheserep- resentationsintoaMLPtoobtainthevideo scores. FromTable1,wehavethreekeyobservations.(1)Our proposedtwo-streamsequencemodelsandfastforwardse- quencemodelsachievebetterresultscompared topreviousvideopoolingapproaches.(2)Thefast-forward LSTMmodelwithdepth7canboosttheshallowsequence 4 modelaround0.5%intermofGAP.(3)Differenttemporal modelingapproachesarecomplementarytoeachother.Our submissionensembles57modelswithdifferenthidden cellsanddepths. 4.Conclusions Inthiswork,wehaveproposedthreetemporalmod- elingapproachestoaddressthechallenginglarge-scale videorecognitiontask.Experimentresultsverifythat ourapproachesachievebetterresultsthan thetraditionaltemporalpoolingapproaches.Theen- sembleofourindividualmodelshasbeenshowntoim- provetheperformancefurther,enablingourmethodto rankthethirdplaceoutof650teamsinthechallenge competition.OurPaddlePaddlevideotoolboxisavailable fordownloadfrom https://github.com/baidu/ Youtube-8M andincludesimplementationsofthreetem- poralmodelingapproaches. References [1] S.Abu-El-Haija,N.Kothari,J.Lee,P.Natsev, G.Toderici,B.Varadarajan,andS.Vijayanarasimhan. Youtube-8m:Alarge-scalevideobench- mark. arXivpreprintarXiv:1609.08675 ,2016. [2] R.ArandjelovicandA.Zisserman.Allaboutvlad.In CVPR ,pages1578Œ1585,2013. [3] K.Cho,B.VanMerri ¨ enboer,D.Bahdanau,and Y.Bengio.Onthepropertiesofneuralmachinetrans- lation:Encoder-decoderapproaches. arXivpreprint arXiv:1409.1259 ,2014. [4] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,and L.Fei-Fei.Imagenet:Alarge-scalehierarchicalim- agedatabase.In CVPR ,2009. [5] C.Gan,C.Sun,L.Duan,andB.Gong.Webly- supervisedvideorecognitionbymutuallyvotingfor relevantwebimagesandwebvideoframes.In ECCV , pages849Œ866,2016. [6] C.Gan,N.Wang,Y.Yang,D.-Y.Yeung,andA.G. Hauptmann.Devnet:Adeepeventnetworkformul- timediaeventdetectionandevidencerecounting.In CVPR ,pages2568Œ2577,2015. [7] C.Gan,T.Yao,K.Yang,Y.Yang,andT.Mei.You lead,weexceed:Labor-freevideoconceptlearning byjointlyexploitingwebvideosandimages.CVPR, 2016. [8] K.He,X.Zhang,S.Ren,andJ.Sun.Deepresid- uallearningforimagerecognition.In Proceedingsof theIEEEconferenceoncomputervisionandpattern recognition ,pages770Œ778,2016. [9] S.HochreiterandJ.Schmidhuber.Longshort-term memory. Neuralcomputation ,9(8):1735Œ1780,1997. [10] S.HochreiterandJ.Schmidhuber.Longshort-term memory. Neuralcomputation ,9(8):1735Œ1780,1997. [11] A.Karpathy,G.Toderici,S.Shetty,T.Leung,R.Suk- thankar,andL.Fei-Fei.Large-scalevideo tionwithconvolutionalneuralnetworks.In CVPR , 2014. [12] H.Kuehne,H.Jhuang,E.Garrote,T.Poggio,and T.Serre.HMDB:alargevideodatabaseforhuman motionrecognition.In ICCV ,pages2556Œ2563,2011. [13] C.Li,X.Ma,B.Jiang,X.Li,X.Zhang,X.Liu, Y.Cao,A.Kannan,andZ.Zhu.Deepspeaker:an end-to-endneuralspeakerembeddingsystem. arXiv preprintarXiv:1705.02304 ,2017. [14] C.-Y.Ma,M.-H.Chen,Z.Kira,andG.AlRegib.TS- LSTMandtemporal-inception:Exploitingspatiotem- poraldynamicsforactivityrecognition. arXivpreprint arXiv:1703.10667 ,2017. [15] K.SimonyanandA.Zisserman.Two-streamconvo- lutionalnetworksforactionrecognitioninvideos.In NIPS ,2014. [16] K.SimonyanandA.Zisserman.Verydeepconvo- lutionalnetworksforlarge-scaleimagerecognition. ICLR ,2015. [17] K.Soomro,A.R.Zamir,andM.Shah.UCF101:A datasetof101humanactionsclassesfromvideosin thewild. arXivpreprintarXiv:1212.0402 ,2012. [18] N.Srivastava,E.Mansimov,andR.Salakhutdinov. Unsupervisedlearningofvideorepresentationsusing lstms. ICML ,2015. [19] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed, D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabi- novich.Goingdeeperwithconvolutions.In CVPR , pages1Œ9,2015. [20] D.Tran,L.Bourdev,R.Fergus,L.Torresani,and M.Paluri.C3D:Genericfeaturesforvideoanalysis. ICCV ,2015. [21] Z.Xu,Y.Yang,andA.G.Hauptmann.Adiscrimi- nativeCNNvideorepresentationforeventdetection. CVPR ,2015. [22] J.Zhou,Y.Cao,X.Wang,P.Li,andW.Xu. Deeprecurrentmodelswithfast-forwardconnec- tionsforneuralmachinetranslation. arXivpreprint arXiv:1606.04199 ,2016. 5  
SpeedingUpNeuralMachineTranslationDecodingbyCubePruning WenZhang 1 ; 2 LiangHuang 3 ; 4 YangFeng 1 ; 2 LeiShen 1 ; 2 QunLiu 5 ; 1 1 KeyLab.ofIntelligentInformationProcessing,CASInstituteofComputingTechnology 2 UniversityofChineseAcademyofSciences,Beijing,China f zhangwen , fengyang , shenlei17z g @ ict.ac.cn 3 OregonStateUniversity,Corvallis,OR,USA 4 BaiduResearch,Sunnyvale,CA,USA liang.huang.sh @ gmail.com 5 HuaweiNoah'sArkLab,HongKong,China qun.liu @ huawei.com Abstract Althoughneuralmachinetranslationhas achievedpromisingresults,itsuffersfrom slowtranslationspeed.Thedirectconse- quenceisthatatrade-offhastobemadebe- tweentranslationqualityandspeed,thusits performancecannotcomeintofullplay.We applycubepruning,apopulartechniqueto speedupdynamicprogramming,intoneural machinetranslationtospeedupthetransla- tion.Toconstructtheequivalenceclass,simi- lartargethiddenstatesarecombined,leading tolessRNNexpansionoperationsonthetarget sideandless softmax operationsoverthelarge targetvocabulary.Theexperimentsshowthat, atthesameorevenbettertranslationquality, ourmethodcantranslatefastercomparedwith naivebeamsearchby 3 : 3  onGPUsand 3 : 5  onCPUs. 1Introduction Neuralmachinetranslation(NMT)hasshown promisingresultsanddrawnmoreattentionre- cently( KalchbrennerandBlunsom , 2013 ; Cho etal. , 2014b ; Bahdanauetal. , 2015 ; Gehringetal. , 2017a , b ; Vaswanietal. , 2017 ).Awidelyusedar- chitectureistheattention-basedencoder-decoder framework( Choetal. , 2014b ; Bahdanauetal. , 2015 )whichassumesthereisacommonseman- ticspacebetweenthesourceandtargetlanguage pairs.Theencoderencodesthesourcesentence toarepresentationinthecommonspacewiththe recurrentneuralnetwork(RNN)( Hochreiterand Schmidhuber , 1997 )andthedecoderdecodesthis representationtogeneratethetargetsentenceword byword.Togenerateatargetword,aprobabil- itydistributionoverthetargetvocabularyisdrawn basedontheattentionovertheentiresourcese- quenceandthetargetinformationrolledbyan- otherRNN.Atthetrainingtime,thedecoderis forcedtogeneratethegroundtruthsentence,while atinference,itneedstoemploythebeamsearch algorithmtosearchthroughaconstrainedspace duetothehugesearchspace. Evenwithbeamsearch,NMTstillsuffersfrom slowtranslationspeed,especiallywhenitworks notonGPUs,butonCPUs,whicharemorecom- monpractice.Thereasonfortheinefy isthatthegenerationofeachtargetwordrequires extensivecomputationtogothroughallthesource wordstocalculatetheattention.Worsestill,due totherecurrenceofRNNs,targetwordscanonly begenerated sequentially ratherthaninparallel. Thesecondreasonisthatlargevocabularyontar- getsideisemployedtoavoidunknownwords (UNKs),whichleadstoalargenumberofnor- malizationfactorsforthe softmax operationwhen drawingtheprobabilitydistribution.Toaccelerate thetranslation,thewidelyusedmethodistotrade offbetweenthetranslationqualityandthedecod- ingspeedbyreducingthesizeofvocabulary( Mi etal. , 2016a )or/andthenumberofparameters, whichcannotrealizethefullpotentialofNMT. Inthispaper,weborrowideasfromphrase- basedandsyntax-basedmachinetranslationwhere cubepruninghasbeensuccessfullyappliedto speedupthedecoding( Chiang , 2007 ; Huang andChiang , 2007 ).Informally,cubepruning ﬁcoarsensﬂthesearchspacebyclusteringsimilar statesaccordingtosomeequivalencerelations.To applythisideatoNMT,however,ismuchmore involved.,intheprocessofbeam search,weclustersimilartargethiddenstatesto constructequivalenceclasses,thethreedimen- sionsofwhicharetargetwordsinthetargetvocab- ulary,parttranslationsretainedinthebeamsearch anddifferentcombinationsofsimilartargethid- denstates,respectively.Theclusteringoperation candirectlydecreasethenumberoftargethidden statesinthefollowingcalculations,togetherwith cubepruning,resultinginlessRNNexpansionop- erationstogeneratethenexthiddenstate(related tothereason)andless softmax operations overthetargetvocabulary(relatedtothesecond reason).Theexperimentresultsshowthat,when receivingthesameorevenbettertranslationqual- ity,ourmethodcanspeedupthedecodingspeed by 3 : 3  onGPUsand 3 : 5  onCPUs. 2Background Theproposedstrategycanbeadaptedtooptimize thebeamsearchalgorithminthedecoderofvari- ousNMTmodels.Withoutlossofgenerality,we taketheattention-basedNMT( Bahdanauetal. , 2015 )asanexampletointroduceourmethod.In thissection,weintroducetheattention-based NMTmodelandthenthecubepruningalgorithm. 2.1TheAttention-basedNMTModel Theattention-basedNMTmodelfollowsthe encoder-decoderframeworkwithanextraatten- tionmodule.Inthefollowingparts,wewillintro- duceeachofthethreecomponents.Assumethe sourcesequenceandtheobservedtranslationare x = f x 1 ;  ;x j x j g and y = f y  1 ;  ;y  j y j g . Encoder TheencoderusesabidirectionalGRU toobtaintwosequencesofhiddenstates.The nalhiddenstateofeachsourcewordisgotbycon- catenatingthecorrespondingpairofhiddenstates inthosesequences.Notethat e x i isemployedto representtheembeddingvectoroftheword x i . ! h i = ! GRU  e x i ; ! h i  1  (1)   h i =   GRU  e x i ;   h i +1  (2) h i = h ! h i ;   h i i (3) Attention Theattentionmoduleisdesignedto extractsourceinformation(calledcontextvector) whichishighlyrelatedtothegenerationofthe nexttargetword.Atthe j -thstep,togetthecon- textvector,therelevancebetweenthetargetword y  j andthe i -thsourcewordisevaluatedas r ij = v T a tanh( W a s j  1 + U a h i ) (4) Then,therelevanceisnormalizedoverthesource sequence,andallsourcehiddenstatesareadded weightedlytoproducethecontextvector.  ij = exp( r ij ) P j x j i 0 =1 exp  r i 0 j  ; c j = X j x j i =1  ij h i (5) Decoder ThedecoderalsoemploysaGRUto unrollthetargetinformation.Thedetailsarede- scribedin Bahdanauetal. ( 2015 ).Atthe j -thde- codingstep,thetargethiddenstate s j isgivenby s j = f  e y  j  1 ;s j  1 ;c j  (6) Theprobabilitydistribution D j overallthewords inthetargetvocabularyispredictedconditioned onthepreviousgroundtruthwords,thecontext vector c j andtheunrolledtargetinformation s j . t j = g  e y  j  1 ;c j ;s j  (7) o j = W o t j (8) D j =softmax( o j ) (9) where g standsforalineartransformation, W o is usedtomap t j to o j sothateachtargetwordhas onecorrespondingdimensionin o j . 2.2CubePruning Thecubepruningalgorithm,proposedby Chiang ( 2007 )basedonthe k -bestparsingalgorithmof HuangandChiang ( 2005 ),isactuallyanaccel- eratedextensionbasedonthenaivebeamsearch algorithm.Beamsearch,aheuristicdynamicpro- grammingsearchingalgorithm,exploresagraph byexpandingthemostpromisingnodesinalim- itedsetandsearchesapproximateoptimalresults fromcandidates.Forthesequence-to-sequence learningtask,givenapre-trainedmodel,the beamsearchalgorithmndsasequencethatap- proximatelymaximizestheconditionalprobabil- ity( Graves , 2012 ; Boulanger-Lewandowskietal. , 2013 ).Both Sutskeveretal. ( 2014 )and Bahdanau etal. ( 2015 )employedthebeamsearchalgorithm intotheNMTdecodingtoproducetranslations withrelativelylargerconditionalprobabilitywith respecttotheoptimizedmodelparameters.Re- markably, HuangandChiang ( 2007 )successfully appliedthecubepruningalgorithmtothedecod- ingofSMT.Theyfoundthatthebeamsearchal- gorithminSMTcanbeextended,andtheyuti- lizedthecubepruningandsomevariantstoop- timizethesearchprocessinthedecodingphase ofphrase-based( OchandNey , 2004 )andsyntax- based( Chiang , 2005 ; Galleyetal. , 2006 )systems, Figure1 :CubepruninginSMTdecoding.(a):thevaluesinthegriddenotethenegativelog-likelihood costoftheterminalcombinationsonbothdimensions,andeachdimensiondenotesatranslationcandi- dateinthisexample;(b)-(d):theprocessofpoppingthebestcandidateoftopthreeitems. GPU CPU CalculationUnits Time(s) Percentage Time(s) Percentage Eq.( 6 ): s j = f ( e y  j  1 ;s j  1 ;c j ) 551.07 75.73% 1370.92 19.42% Eq.( 7 ): t j = g ( e y  j  1 ;c j ;s j ) 88.25 12.13% 277.76 3.93% Eq.( 8 ): o j = W o t j 25.33 3.48% 2342.53 33.18% Eq.( 9 ): D j =softmax( o j ) 63.00 8.66% 3069.25 43.47% Table1 :TimecoststatisticsfordecodingthewholeMT03testsetonGPUsandCPUswithbeamsize 10 . whichdecreasedamassoftranslationcandidates andachievedaspeedimprovementby reducingthesizeofcomplicatedsearchspace, therebymakingitpossibletoactualizethethought ofimprovingthetranslationperformancethrough increasingthebeamsize. InthetraditionalSMTdecoding,thecubeprun- ingalgorithmaimstopruneagreatnumberof partialtranslationhypotheseswithoutcomputing andstoringthem.Foreachdecodingstep,those hypotheseswiththesametranslationruleare groupedtogether,thenthecubepruningalgorithm isconductedoverthehypotheses.Weillustratethe detailedprocessinFigure 1 . 3NMTDecoderwithCubePruning 3.1 Wetherelatedstorageunittupleofthe i -thcandidatewordinthe j -thbeamas n i j = ( c i j ;s i j ;y i j ;bp i j ) ,where c i j isthenegativelog- likelihood(NLL)accumulationinthe j -thbeam, s i j isthedecoderhiddenstateinthe j -thbeam, y i j istheindexofthe j -thtargetwordinlargevocab- ularyand bp i j isthebacktrackingpointerforthe j -thdecodingstep.Notethat,foreachsourcesen- tence,webeginwithcalculatingitsencodedrep- resentationandthehiddenstate s 0 0 indecoder, thensearchingfromtheinitialtuple (0 : 0 ;s 0 0 ; 0 ; 0) existinginthebeam 1 . ItisafactthatEquation( 9 )producestheprob- abilitydistributionofthepredictedtargetwords overthetargetvocabulary V . Choetal. ( 2014b ) indicatedthatwheneveratargetwordisgenerated, the softmax functionover V computesprobabil- itiesforallwordsin V ,sothecalculationisex- pensivewhenthetargetvocabularyislarge.As such, Bahdanauetal. ( 2015 )(andmanyothers) onlyusedthetop- 30 k frequentwordsastarget vocabulary,andreplacedotherswithUNK.How- ever,thenormalizationoperationstillbrought highcomputationcomplexityforforwardcalcula- tions. 3.2TimeCostinDecoding Weconductedanexperimenttoexplorehowlong eachcalculationunitinthedecoderwouldtake. WedecodedtheMT03testdatasetbyusingnaive beamsearchwithbeamsizeof 10 andrecorded thetimeconsumedinthecomputationofEquation ( 6 ),( 7 ),( 8 )and( 9 ),respectively.Thestatistical resultsinTable 1 showthattherecurrentcalcula- tionunitconsumedthemosttimeonGPUs,while 1 Theinitialtargetwordindex y 0 0 equalsto 0 ,whichactu- allycorrespondstotheBeginningOfSentence(BOS)token intargetvocabulary. the softmax computationalsotooklotsoftime. OnCPUs,themostexpensivecomputationaltime costwascausedbythe softmax operationoverthe entiretargetvocabulary 2 .Inordertoavoidthe time-consumingnormalizationoperationintest- ing,weintroduced self-normalization (denotedas SN )intothetraining. 3.3Self-normalization Self-normalization ( Devlinetal. , 2014 )wasde- signedtomakethemodelscoreswhicharepro- ducedbytheoutputlayerbeapproximatedby theprobabilitydistributionoverthetargetvocab- ularywithoutnormalizationoperation.According toEquation( 9 ),foranobservedtargetsentence y = f y  1 ;  ;y  j y j g ,theCross-Entropy(CE)loss couldbewrittenas L  =  j y j X j =1 log D j [ y  j ] =  j y j X j =1 log exp  o j [ y  j ]  P y 0 2 V exp( o j [ y 0 ]) = j y j X j =1 log X y 0 2 V exp  o j [ y 0 ]   o j [ y  j ] (10) where o j isthemodelscoregeneratedbyEqua- tion( 8 )atthe j -thstep,wemarkedthe softmax normalizer P y 0 2 V exp( o j [ y 0 ]) as Z . Followingtheworkof Devlinetal. ( 2014 ),we theCElossfunctioninto L  =  j y j X j =1  log D j [ y  j ]   (log Z  0) 2  =  j y j X j =1  log D j [ y  j ]   log 2 Z  (11) Theobjectivefunction,showninEquation( 11 ), isoptimizedtomakesure log Z isapproximated to 0 ,equally,make Z closeto 1 onceitconverges. Wechosethevalueof  empirically.Becausethe softmax normalizer Z isconvergedto 1 ininfer- ence,wejustneedtoignore Z andpredictthetar- getworddistributionatthe j -thsteponlywith o j : D j = o j (12) 3.4CubePruning Table 1 clearlyshowsthattheequationsinthe NMTforwardcalculationtakelotsoftime.Here, accordingtotheideabehindthecubepruning algorithm,wetriedtoreducethetimeoftime- consumingcalculations,e.g.,Equation( 6 ),and furtherdecreasethesearchspacebyintroducing thecubepruningalgorithm. 3.4.1IntegratingintoNMTDecoder ExtendedfromthenaivebeamsearchintheNMT decoder,cubepruning,treatedasapruningal- gorithm,attemptstoreducethesearchspaceand computationcomplexitybymergingsomesimi- laritemsinabeamtoacceleratethenaivebeam search,keepingthe 1 -bestsearchingresultalmost unchangedorevenbetterbyincreasingthebeam size.Thus,itisafastandeffectivealgorithmto generatecandidates. Assumethat T restoresthesetofthe translations.Foreachstepinnaivebeamsearch process, beamsize  T j timesforwardcalcula- tionsarerequiredtoacquire beamsize  T j prob- abilitydistributionscorrespondingtoeachitemin thepreviousbeam( Bahdanauetal. , 2015 ).while foreachstepincubepruning,intermsofsome constraints,wemergeallsimilaritemsinthepre- viousbeamintoone equivalenceclass (calleda sub-cube).Theconstraintweusedhereisthat itemsbeingmergedinthepreviousbeamshould havethesametargetwords.Then,forthesub- cube,onlyoneforwardcalculationisrequiredto obtaintheapproximatepredictionsbyusingthe loosehiddenstate.Elementsinthesub-cubeare sortedbypreviousaccumulatedNLLalongthe columns(thedimensionofbeamsize)and bytheapproximatepredictionsalongtherows (theseconddimensionofvocabularysize).Af- termerging,onebeammaycontainseveralsub- cubes(thethirddimension),westarttosearch fromitemintheupperleftcornerofeachsub- cube,whichisthebestoneinthesub-cube,and continuetospreadoutuntilenoughcandidatesare found.Onceaitemisselected,theexacthidden statewillbeusedtocalculateitsexactNLL. Throughallabovesteps,thefrequencyoffor- wardcomputationsdecreases.Wegiveanexam- pletodiveintothedetailsinFigure 2 . Assumethatthebeamsizeis 4 .Giventhe 10 th 2 Notethat,identicalto Bahdanauetal. ( 2015 ),weonly used 30 k asthevocabularysize. Figure2 :CubepruningdiagraminbeamsearchprocessduringNMTdecoding.Weonlydepictthe accumulatedNLLandtheword-levelcandidateforeachiteminthebeam(inthebracket).Assumethe beamsizeis 4 ,weinitializeaheapforthecurrentstep,elementsinthe 10 th beamaremergedintotwo sub-cubes C1 and C2 accordingtotheprevioustargetwords;(a)thetwoelementslocatedintheupper- leftcornerofthetwosub-cubesarepushedintotheheap;(b)minimalelement (6 : 2 ; 674) ispoppedout, meanwhile,itsneighbor (8 : 6 ; 8357) ispushedintotheheap;(c)minimalelement (7 : 3 ; 8) ispoppedout, itsright-neighbor (7 : 7 ; 880) andlower-neighbor (7 : 8 ; 8) arepushedintotheheap;(d)minimalelement (7 : 7 ; 880) ispoppedout,itsright-neighbor (9 : 8 ; 29) anddown-neighbor (8 : 2 ; 880) arepushedintothe heap;(e)minimalelement (7 : 8 ; 8) ispoppedout,thenitsdown-neighbor (8 : 1 ; 8) ispushedintotheheap. 4 elementshavebeenpoppedout,weusethemtoconstructthe 11 th beam.Yellowboxesindicatethe 4 -bestword-levelcandidatestobepushedintothe 11 th beam. beam,wegeneratethe 11 th beam.Differentfrom thenaivebeamsearch,wegroupitemsinthe previousbeamintotwosub-cubes C1 and C2 in termofthetargetword y j  1 .Asshowninpart ( a ) ofFigure 2 , (6 : 1 ; 433) constructsthesub-cube C1 ; (6 : 5 ; 35) , (7 : 0 ; 35) and (7 : 3 ; 35) areputto- gethertocomposeanothersub-cube C2 .Itemsin part ( a ) arerankedinascendingorderalongboth rowandcolumndimensionaccordingtotheac- cumulatedNLL.Foreachsub-cube,weusethe statevectorineachsub-cubeastheapprox- imateonetoproducethenextprobabilitydistribu- tionandthenextstate.Atbeginning,eachupper- leftcornerelementineachsub-cubeispushedinto aminimumheap,afterpoppingminimumelement fromtheheap,wecalculateandrestoretheexact NLLoftheelement,thenpushtherightandlower onesalongsidetheminimumelementintoheap. Atthisrate,thesearchingcontinuesjustlikethe ﬁdiffusionﬂinthesub-cubeuntil 4 elementsare popped,whicharerankedintermsoftheirexact NLLstoconstructthe 11 th beam.Notethatonce anelementispopped,wecalculateitsexactNLL. Fromthestep(e)inFigure 2 ,wecanseethat 4 elementshavebeenpoppedfrom C1 and C2 ,and thenrankedintermsoftheirexactNLLstobuild the 11 th beam. Wereferabovealgorithmasthenaivecube pruningalgorithm(called NCP ) 3.4.2AcceleratedCubePruning Ineachstepofthecubepruningalgorithm,after mergingtheitemsinthepreviousbeam,somesim- ilarcandidatesaregroupedtogetherintooneor moresub-cube(s).Wealsotrytopredicttheap- proximatedistributionforeachsub-cubeonlyac- cordingtothetop- 1 statevector(therowin thesub-cubeinFigure 2 ),andselectnextcandi- datesafterranking.Thepredictedprobabilitydis- tributionwillbeverysimilartothatofthenaive beamsearch.Besides,Eachsub-cubeonlyre- quiresoneforwardcalculation.Thus,itcouldsave moresearchspaceandfurtherreducethecomputa- tioncomplexityforthedecoder.Unlikethenaive cubepruningalgorithm,acceleratedcubepruning popseachitem,thenstillusetheapproximateNLL insteadoftheexactone.Wedenotethiskindof acceleratedcubepruningalgorithmas ACP . 4Experiments Wevtheeffectivenessofproposedcube pruningalgorithmontheChinese-to-English(Zh- En)translationtask. 4.1DataPreparation TheChinese-Englishtrainingdatasetconsistsof 1 : 25 Msentencepairs 3 .WeusedtheNIST2002 (MT02)datasetasthevalidationsetwith 878 sen- tences,andtheNIST2003(MT03)datasetasthe testdataset,whichcontains 919 sentences. Thelengthsofthesentencesonbothsideswere limitedupto 50 tokens,thenactually 1 : 11 Msen- tencepairswereleftwith 25 : 0 MChinesewords and 27 : 0 MEnglishwords.Weextracted 30 k most frequentwordsasthesourceandtargetvocabular- iesforbothsides. Inalltheexperiments,case-insensitive 4 -gram BLEU( Papinenietal. , 2002 )wasemployed fortheautomaticevaluation,weusedthescript mteval-v11b.pl 4 tocalculatetheBLEUscore. 4.2System Thesystemisanimprovedversionofattention- basedNMTsystemnamedRNNsearch( Bahdanau etal. , 2015 )wherethedecoderemploysacon- ditionalGRUlayerwithattention,consistingof twoGRUsandanattentionmoduleforeachstep 5 . ,Equation( 6 )isreplacedwiththefol- lowingtwoequations: ~ s j = GRU 1 ( e y  j  1 ;s j  1 ) (13) s j = GRU 2 ( c j ; ~ s j ) (14) Besides,forthecalculationofattentioninEqua- tion( 4 ), s j  1 isreplacedwith ~ s j  1 .Othercompo- nentsofthesystemkeepthesameasRNNsearch. Also,were-implementedthebeamsearchalgo- rithmasthenaivedecodingmethod,andnaive searchingontheGPUandCPUserverwerecon- ductedastwobaselines. 4.3TrainingDetails Specially,weemployedalittledifferentsettings from Bahdanauetal. ( 2015 ):Wordembedding sizesonbothsidesweresetto 512 ,allhiddensizes 3 Thesesentencepairsaremainlyextractedfrom LDC2002E18,LDC2003E07,LDC2003E14,Hansardspor- tionofLDC2004T07,LDC2004T08andLDC2005T06 4 https://github.com/moses-smt/mosesdecoder/blob/ master/scripts/generic/mteval-v11b.pl 5 https://github.com/nyu-dl/dl4mt-tutorial/blob/ master/docs/cgru.pdf intheGRUsofbothencoderanddecoderwere alsosetto 512 .Allparametermatrices,including biasmatrices,wereinitializedwiththeuniform distributionover [  0 : 1 ; 0 : 1] .Parameterswereup- datedbyusingmini-batchStochasticGradientDe- scent(SGD)withbatchsizeof 80 andthelearning ratewasadjustedbyAdaDelta( Zeiler , 2012 )with decayconstant ˆ = 0 : 95 anddenominatorconstant  = 1 e - 6 .Thegradientsofallvariableswhose L2 - normarelargerthanathreshold 1 : 0 werenormalizedtothethresholdtoavoidgradi- entexplosion( Pascanuetal. , 2013 ).Dropoutwas appliedtotheoutputlayerwithdropoutrateof 0 : 5 .Weexploitedlengthnormalization( Choetal. , 2014a )strategyoncandidatetranslationsinbeam searchdecoding. ThemodelwhoseBLEUscorewasthehigh- estonthevalidationsetwasusedtodotesting. Maximalepochnumberwassetto 20 .Training wasconductedonasingleTeslaK80GPU,ittook about 2 daystotrainasingleNMTmodelonthe Zh-Entrainingdata.For self-normalization ,we empiricallyset  as 0 : 5 inEquation( 11 ) 6 . 4.4SearchStrategies WeconductedexperimentstodecodetheMT03 testdatasetontheGPUandCPUserverrespec- tively,thencomparedsearchqualityandefy amongfollowingsixsearchstrategiesunderdiffer- entbeamsizes. NBS-SN: NaiveBeamSearchwithout SN NBS+SN: NaiveBeamSearchwith SN NCP-SN: CubePruningwithout SN NCP+SN: CubePruningwith SN ACP-SN: AcceleratedCubePruningwithout SN ACP+SN: AcceleratedCubePruningwith SN 4.5ComparisonofAverageMergingRate WegivetheoftheAverageMerging Rate(denotedasAMR).Givenatestdataset,we countedthetotalword-levelcandidates(notedas N w )andthetotalsub-cubes(notedas N c )during thewholedecodingprocess,thentheAMRcanbe simplycomputedas m =N w = N c (15) TheMT03testdatasetwasutilizedtocom- parethetrendsoftheAMRvaluesunderall 6 Following Devlinetal. ( 2014 ),wehadtried 0 : 01 , 0 : 1 , 0 : 51 : 0 and 10 : 0 forthevalueof  ,wefoundthat 0 : 5 pro- ducedthebestresult. Figure3 :AMRcomparisonontheMT03test dataset.DecodingtheMT03testdatasetonasin- gleGeForceGTXTITANXGPUserverunderthe differentsearchingsettings.y-axisrepresentsthe AMRonthetestdatasetinthewholesearching processandx-axisindicatesbeamsize.Unsurpris- ingly,wegotexactlythesameresultsontheCPU server,notshownhere. sixmethods.Weusedthepre-trainedmodel totranslatethetestdatasetonasingleGeForce GTXTITANXGPUserver.Beamsizevaries from 1 to 40 ,valuesareincludedintheset f 1 ; 2 ; 3 ; 4 ; 8 ; 10 ; 15 ; 18 ; 20 ; 30 ; 40 g .Foreach beamsize,sixdifferentsearchingsettingswereap- pliedtotranslatethetestdatasetrespectively.The curvesoftheAMRsduringthedecodingonthe MT03testdatasetundertheproposedmethodsare showninFigure 3 .NotethattheAMRvaluesof NBS arealways 1 whetherthereis SN ornot. ComparingthecurvesintheFigure 3 ,wecould observethatthenaivebeamsearchdoesnotcon- ductanymergingoperationinthewholesearching process,whiletheaveragemergingrateinthecube pruningalmostgrowsasthebeamsizeincreases. Comparingtheredcurvestotheblueones,wecan concludethat,inanycaseofbeamsize,theAMR oftheacceleratedcubepruningsurpassestheba- siccubepruningbyalargemargin.Besides,self- normalizationcouldproducesthehigheraverage mergingratecomparingtothecounterpartwithout self-normalization. 4.6ComparisonontheGPUServer Intuitively,asthevalueoftheAMRincreases,the searchspacewillbereducedandcomputationef- yimproves.Wecomparethetwoproposed searchingstrategiesandthenaivebeamsearchin twoconditions(withself-normalizationandwith- outself-normalization).Figure 4 demonstrates theresultsofcomparisonbetweentheproposed searchingmethodsandthenaivebeamsearch baselineintermsofsearchqualityandsearchef ciencyunderdifferentbeamsizes. Bythebeamsizeandthedataset,we comparedthechangingtrendofBLEUscoresfor thethreedistinctsearchingstrategiesundertwo conditions.Withoutself-normalization,Figure 4a showstheimprovementofthesearch speed,howevertheBLEUscoredropsabout 0 : 5 points.Wethenequippedthesearchalgorithm withself-normalization.Figure 4b showsthatthe acceleratedcubepruningsearchalgorithmonly spendaboutone-thirdofthetimeofthenaive beamsearchtoachievethebestBLEUscorewith beamsize 30 .Concretely,whenthebeamsizeis settobe 30 , ACP+SN is 3 : 3 timesfasterthanthe baselineontheMT03testdataset,andbothper- formancesarealmostthesame. 4.7ComparisonontheCPUServer SimilartotheexperimentsconductedonGPUs, wealsotranslatedthewholeMT03testdataset ontheCPUserverbyusingallsixsearchstrate- giesunderdifferentbeamsizes.Thetrendsofthe BLEUscoresoverthosestrategiesareshownin Figure 5 . Theproposedsearchmethodsgainthesimilar superiorityonCPUstothatonGPUs,andthe decodingspeedisobviouslyslowerthanthaton GPUs.FromtheFigure 5a ,wecanalsoclearly seethat,comparedwiththe NBS-SN , NCP-SN onlyspeedsupthedecoderalittle, ACP-SN pro- ducesmuchmoreacceleration.However,when wedidnotintroduceself-normalization,thepro- posedsearchmethodswillalsoresultinalossof about 0 : 5 BLEUscore.Theself-normalization madethe ACP strategyfasterthanthebaseline byabout 3 : 5  ,inwhichconditionthe NBS+SN gotthebestBLEUscore 38 : 05 withbeamsize 30 whilethe ACP+SN achievedthehighestscore 38 : 12 withbeamsize 30 .Theresultscouldbeob- servedinFigure 5b .Becauseourmethodisonthe algorithmiclevelandplatform-independent,itis reasonablethattheproposedmethodcannotonly performwellonGPUs,butalsoacceleratethede- codingonCPUs.Thus,theacceler- atedcubepruningwithself-normalizationcould improvethesearchqualityandefystably. 4.8DecodingTime Inthissection,weonlyfocusontheconsuming timeoftranslatingtheentireMT03testdataset. (a)BLEUvs.decodingspeed,withoutself-normalization (b)BLEUvs.decodingspeed,withself-normalization Figure4 :ComparisonamongthedecodingresultsoftheMT03testdatasetonthesingleGeForceGTX TITANXGPUserverunderthethreedifferentsearchingsettings.y-axisrepresentstheBLEUscoreof translations,x-axisindicatesthathowlongitwilltakefortranslatingonewordonaverage. (a)BLEUvs.decodingspeed,withoutself-normalization (b)BLEUvs.decodingspeed,withself-normalization Figure5 :ComparisonamongthedecodingresultsoftheMT03testdatasetonthesingleAMD Opteron(tm)Processorunderthethreedifferentsearchingsettings.y-axisrepresentstheBLEUscore oftranslations,x-axisindicatesthathowlongitwilltakefortranslatingonewordonaverage. Underthetwoconditions,wecalculatedthetimes spentontranslatingtheentiretestdatasetfordif- ferentbeamsizes,thendrawthecurvesinFigure 6 and 7 .FromtheFigure 6a and 6b ,wecould observethatacceleratedcubepruningalgorithm speedsupthedecodingbyabout 3 : 8  onGPUs whenthebeamsizeissetto 40 .Figure 7a and 7b showthattheacceleratedcubepruningalgo- rithmspeedsupthedecodingbyabout 4 : 2  on CPUserverwiththebeamsize 40 . 5RelatedWork Recently,lotsofworksdevotedtoimprovetheef- yoftheNMTdecoder.Someresearchers employedthewayofdecreasingthetargetvocabu- larysize. Jeanetal. ( 2015 )improvedthedecoding efyevenwiththemodelusingaverylarge targetvocabularybutselectingonlyasmallsub- setofthewholetargetvocabulary.Basedonthe workof Jeanetal. ( 2015 ), Mietal. ( 2016b )intro- ducedsentence-levelandbatch-levelvocabularies asaverysmallsubsetofthefulloutputvocabu- lary,thenpredictedtargetwordsonlyonthissmall vocabulary,inthisway,theyonlylost 0 : 1 BLEU points,butreducedtargetvocabularysubstantially. Someotherresearcherstriedtoraisetheef ciencyofdecodingfromotherperspectives. Wu etal. ( 2016 )introducedacoveragepenalty  and lengthnormalization  intobeamsearchdecoder toprunehypothesesandspedupthesearchpro- cessby 30% ˘ 40% whenrunningonCPUs. Hu etal. ( 2015 )usedapriorityqueuetochoosethe besthypothesisforthenextsearchstep,which drasticallyreducedsearchspace. Inspiredbytheworksof Mietal. ( 2016b ) and HuangandChiang ( 2007 ),weconsiderprun- inghypothesisinNMTdecodingbyusingcube pruningalgorithm,butunliketraditionalSMTde- codingwheredynamicprogrammingwasusedto mergeequivalentstates(e.g.,ifweusephrase- (a)TimespentontranslatingMT03testdatasetfordifferent beamsizeswithoutself-normalization (b)TimespentontranslatingMT03testdatasetfordifferent beamsizeswithself-normalization Figure6 :ComparisonamongthedecodingresultsoftheMT03testdatasetonthesingleGeForceGTX TITANXGPUserverunderthethreedifferentsearchingsettings.y-axisrepresentstheBLEUscoreof translations,x-axisindicatesthathowlongitwilltakefortranslatingonewordonaverage. (a)TimespentontranslatingMT03testdatasetfordifferent beamsizeswithoutself-normalization (b)TimespentontranslatingMT03testdatasetfordifferent beamsizeswithself-normalization Figure7 :ComparisonamongthedecodingresultsoftheMT03testdatasetonthesingleAMD Opteron(tm)Processorunderthethreedifferentsearchingsettings.y-axisrepresentstheBLEUscore oftranslations,x-axisindicatesthathowlongitwilltakefortranslatingonewordonaverage. baseddecodingwithtrigramlanguagemodel,we canmergestateswithsamesource-sidecoverage vectorandsameprevioustwotargetwords).How- ever,thisisnotappropriateforcurrentNMTde- coding,sincetheembeddingoftheprevioustarget wordisusedasoneinputofthecalculationunitof eachstepinthedecodingprocess,wecouldgroup equivalenceclassescontainingthesameprevious targetwordtogether. 6Conclusions Weextendedcubepruningalgorithmintothede- coderoftheattention-basedNMT.Foreachstep inbeamsearch,wegroupedsimilarcandidates inpreviousbeamintooneormoreequivalence class(es),andbadhypotheseswereprunedout. Westartedsearchingfromtheupper-leftcornerin eachequivalenceclassandspreadoutuntilenough candidatesweregenerated.Evaluationsshowthat, comparedwithnaivebeamsearch,ourmethod couldimprovethesearchqualityandefyto alargeextent,acceleratingtheNMTdecoderby 3 : 3  and 3 : 5  onGPUsandCPUs,respectively. Also,thetranslationprecisioncouldbethesame orevenbetterinbothsituations.Besides,self- normalizationisvtobehelpfultoaccelerate cubepruningevenfurther. Acknowledgements Wethankthethreeanonymousreviewersfortheir comments,KaiZhaoandHaitaoMiforugges- tions.ThisworkissupportedinpartbyNSFIIS- 1817231&IIS-1656051,andisalsosupportedin partbyNationalNaturalScienceFoundationof China(Nos.61472428&61662077). References DzmitryBahdanau,KyunghyunCho,andYoshuaBen- gio.2015.Neuralmachinetranslationbyjointly learningtoalignandtranslate. ICLR2015 . NicolasBoulanger-Lewandowski,YoshuaBengio,and PascalVincent.2013.Audiochordrecognitionwith recurrentneuralnetworks.In ISMIR ,pages335Œ 340.Citeseer. DavidChiang.2005.Ahierarchicalphrase-based modelforstatisticalmachinetranslation.In Pro- ceedingsofthe43rdAnnualMeetingonAssociation forComputationalLinguistics ,pages263Œ270.As- sociationforComputationalLinguistics. DavidChiang.2007.Hierarchicalphrase-basedtrans- lation. computationallinguistics ,33(2):201Œ228. KyunghyunCho,BartvanMerrienboer,DzmitryBah- danau,andYoshuaBengio.2014a.Ontheproper- tiesofneuralmachinetranslation:EncoderŒdecoder approaches.In ProceedingsofSSST-8,EighthWork- shoponSyntax,SemanticsandStructureinStatisti- calTranslation ,pages103Œ111,Doha,Qatar.Asso- ciationforComputationalLinguistics. KyunghyunCho,BartvanMerrienboer,CaglarGul- cehre,DzmitryBahdanau,FethiBougares,Holger Schwenk,andYoshuaBengio.2014b.Learning phraserepresentationsusingrnnencoderŒdecoder forstatisticalmachinetranslation.In Proceedingsof the2014ConferenceonEmpiricalMethodsinNat- uralLanguageProcessing(EMNLP) ,pages1724Œ 1734,Doha,Qatar.AssociationforComputational Linguistics. JacobDevlin,RabihZbib,ZhongqiangHuang,Thomas Lamar,RichardSchwartz,andJohnMakhoul.2014. Fastandrobustneuralnetworkjointmodelsforsta- tisticalmachinetranslation.In Proceedingsofthe 52ndAnnualMeetingoftheAssociationforCompu- tationalLinguistics(Volume1:LongPapers) ,pages 1370Œ1380,Baltimore,Maryland.Associationfor ComputationalLinguistics. MichelGalley,JonathanGraehl,KevinKnight,Daniel Marcu,SteveDeNeefe,WeiWang,andIgnacio Thayer.2006.Scalableinferenceandtrainingof context-richsyntactictranslationmodels.In Pro- ceedingsofthe21stInternationalConferenceon ComputationalLinguisticsand44thAnnualMeet- ingoftheAssociationforComputationalLinguis- tics ,pages961Œ968,Sydney,Australia.Association forComputationalLinguistics. JonasGehring,MichaelAuli,DavidGrangier,and YannDauphin.2017a.Aconvolutionalencoder modelforneuralmachinetranslation.In Proceed- ingsofthe55thAnnualMeetingoftheAssociation forComputationalLinguistics(Volume1:LongPa- pers) ,pages123Œ135,Vancouver,Canada.Associa- tionforComputationalLinguistics. JonasGehring,MichaelAuli,DavidGrangier,De- nisYarats,andYannN.Dauphin.2017b.Con- volutionalsequencetosequencelearning.In Pro- ceedingsofthe34thInternationalConferenceon MachineLearning ,volume70of Proceedingsof MachineLearningResearch ,pages1243Œ1252,In- ternationalConventionCentre,Sydney,Australia. PMLR. AlexGraves.2012.Sequencetransductionwith recurrentneuralnetworks. arXivpreprint arXiv:1211.3711 . SeppHochreiterandJ ¨ urgenSchmidhuber.1997. Longshort-termmemory. NeuralComputation , 9(8):1735Œ1780. XiaoguangHu,WeiLi,XiangLan,HuaWu,and HaifengWang.2015.Improvedbeamsearchwith constrainedsoftmaxfornmt. ProceedingsofMT SummitXV ,page297. LiangHuangandDavidChiang.2005.Betterk- bestparsing.In ProceedingsoftheNinthInter- nationalWorkshoponParsingTechnology ,Parsing '05,pages53Œ64,Stroudsburg,PA,USA.Associa- tionforComputationalLinguistics. LiangHuangandDavidChiang.2007.Forestrescor- ing:Fasterdecodingwithintegratedlanguagemod- els.In Proceedingsofthe45thAnnualMeetingof theAssociationofComputationalLinguistics ,pages 144Œ151,Prague,CzechRepublic.Associationfor ComputationalLinguistics. S ´ ebastienJean,KyunghyunCho,RolandMemisevic, andYoshuaBengio.2015.Onusingverylarge targetvocabularyforneuralmachinetranslation. In Proceedingsofthe53rdAnnualMeetingofthe AssociationforComputationalLinguisticsandthe 7thInternationalJointConferenceonNaturalLan- guageProcessing(Volume1:LongPapers) ,pages 1Œ10,Beijing,China.AssociationforComputa- tionalLinguistics. NalKalchbrennerandPhilBlunsom.2013.Recurrent convolutionalneuralnetworksfordiscoursecompo- sitionality.In ProceedingsoftheWorkshoponCon- tinuousVectorSpaceModelsandtheirComposition- ality ,pages119Œ126,Bulgaria.Association forComputationalLinguistics. HaitaoMi,BaskaranSankaran,ZhiguoWang,andAbe Ittycheriah.2016a.Coverageembeddingmodels forneuralmachinetranslation.In Proceedingsof the2016ConferenceonEmpiricalMethodsinNat- uralLanguageProcessing ,pages955Œ960,Austin, Texas.AssociationforComputationalLinguistics. HaitaoMi,ZhiguoWang,andAbeIttycheriah.2016b. Vocabularymanipulationforneuralmachinetransla- tion.In Proceedingsofthe54thAnnualMeetingof theAssociationforComputationalLinguistics(Vol- ume2:ShortPapers) ,pages124Œ129,Berlin,Ger- many.AssociationforComputationalLinguistics. FranzJosefOchandHermannNey.2004.Thealign- menttemplateapproachtostatisticalmachinetrans- lation. ComputationalLinguistics ,30(4):417Œ449. KishorePapineni,SalimRoukos,ToddWard,andWei- JingZhu.2002.Bleu:amethodforautomaticeval- uationofmachinetranslation.In Proceedingsof 40thAnnualMeetingoftheAssociationforCom- putationalLinguistics ,pages311Œ318,Philadelphia, Pennsylvania,USA.AssociationforComputational Linguistics. RazvanPascanu,TomasMikolov,andYoshuaBengio. 2013.Onthedifoftrainingrecurrentneural networks.In Proceedingsofthe30thInternational ConferenceonMachineLearning ,volume28of ProceedingsofMachineLearningResearch ,pages 1310Œ1318,Atlanta,Georgia,USA.PMLR. IlyaSutskever,OriolVinyals,andQuocVLe.2014. Sequencetosequencelearningwithneuralnet- works.InZ.Ghahramani,M.Welling,C.Cortes, N.D.Lawrence,andK.Q.Weinberger,editors, Ad- vancesinNeuralInformationProcessingSystems 27 ,pages3104Œ3112.CurranAssociates,Inc. AshishVaswani,NoamShazeer,NikiParmar,Jakob Uszkoreit,LlionJones,AidanNGomez,ukasz Kaiser,andIlliaPolosukhin.2017.Attentionisall youneed.InI.Guyon,U.V.Luxburg,S.Bengio, H.Wallach,R.Fergus,S.Vishwanathan,andR.Gar- nett,editors, AdvancesinNeuralInformationPro- cessingSystems30 ,pages5998Œ6008.CurranAs- sociates,Inc. YonghuiWu,MikeSchuster,ZhifengChen,QuocV Le,MohammadNorouzi,WolfgangMacherey, MaximKrikun,YuanCao,QinGao,Klaus Macherey,etal.2016.Google'sneuralma- chinetranslationsystem:Bridgingthegapbetween humanandmachinetranslation. arXivpreprint arXiv:1609.08144 . MatthewDZeiler.2012.Adadelta:anadaptivelearn- ingratemethod. arXivpreprintarXiv:1212.5701 .  
Proceedingsofthe50thAnnualMeetingoftheAssociationforComputationalLinguistics ,pages979Œ987, Jeju,RepublicofKorea,8-14July2012. c2012AssociationforComputationalLinguistics 979980981982983984985986987 
LearningtoRespondwithDeepNeuralNetworksfor Retrieval-BasedHuman-ComputerConversationSystem RuiYan BaiduInc. No.10,XibeiwangEastRoad, Beijing100193,China yanrui02@baidu.com YipingSong BaiduInc. No.10,XibeiwangEastRoad, Beijing100193,China songyiping01@baidu.comHuaWu BaiduInc. No.10,XibeiwangEastRoad, Beijing100193,China wu_hua@baidu.comABSTRACT Toestablishanautomaticconversationsystembetweenhumans andcomputersisregardedasoneofthemosthardcoreproblems  incomputerscience,whichinvolvesinterdisciplinarytechniquesin informationretrieval,naturallanguageprocessing,intelli- gence,etc.Thechallengeslieinhowtorespondsoastomaintain  arelevantandcontinuousconversationwithhumans.Alongwith  theprosperityofWeb2.0,wearenowabletocollectextremely  massiveconversationaldata,whicharepubliclyavailable.Itcasts  agreatopportunitytolaunchautomaticconversationsystems.Ow-  ingtothediversityofWebresources,aretrieval-basedconversa-  tionsystemwillbeabletoatleastsomeresponsesfromthe  massiverepositoryforanyuserinputs.Givenahumanissuedmes-  sage,i.e.,query,oursystemwouldprovideareplyafteradequate  trainingandlearningofhowtorespond.Inthispaper,wepropose  aretrieval-basedconversationsystemwiththe deeplearning-to- respond schemathroughadeepneuralnetworkframeworkdriven bywebdata.Theproposedmodelisgeneralandfordif-  ferentconversationscenariosinopendomain.Weincorporatethe  impactofmultipledatainputs,andformulatevariousfeaturesand  factorswithoptimizationintothedeeplearningframework.Inthe  experiments,weinvestigatetheeffectivenessoftheproposeddeep  neuralnetworkstructureswithbettercombinationsofalldifferen-  tevidence.Wedemonstrateperformanceimprovement  againstaseriesofstandardandstate-of-artbaselinesintermsof  p@1,MAP,nDCG,andMRRforconversationalpurposes. CategoriesandSubjectDescriptors H.3.3[ InformationSystems ]:InformationSearchandRetrieval; H.3.1[ InformationStorageandRetrieval ]:ContentAnalysis andIndexing;I.5.1[ PatternRecognition ]:ModelsŠ Deeplearn- ingKeywords Learning-to-respond;conversationsystem;contextualmodeling; deepneuralnetworks Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed  fororcommercialadvantageandthatcopiesbearthisnoticeandthefullcita-  tiononthepage.Copyrightsforcomponentsofthisworkownedbyothersthan  ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orre-  publish,topostonserversortoredistributetolists,requirespriorpermission and/orafee.Requestpermissionsfrompermissions@acm.org. SIGIR'16,July17-21,2016,Pisa,Italy c2016ACM.ISBN978-1-4503-4069-4/16/07...$15.00 DOI:http://dx.doi.org/10.1145/2911451.2911542 1.INTRODUCTION Tohaveavirtualassistantand/orchatcompanionsysteminopen domainswithadequateintelligencehasseemedillusive, andmightonlyexistinSci-Fimoviesforalongtime.Recently,the  goalofcreatinganautomatichuman-computerconversationsys-  tem,asourpersonalassistantorchatcompanion,isnolongeran  illusionfaraway.Duetoeasilyaccessibleﬁbigdataﬂforconver- sationsontheWeb,wemightbeabletolearnhowtorespondand whattorespondgiven(almost)anyinputs.Itislikelytobeagreat  timingtobuilddata-driven,open-domainconversationsystemsbe-  tweenhumansandcomputers. Buildingconversationsystems,infact,hasattractedmuchatten- tionoverthepastdecades.Inearlyyears,researchershaveinves-  tigatedintotask-orientedconversationsystems[44,33,36],which  arebasicallyforverticaldomains.Theconversationalinputsarere-  strictedandpredictable;henceitwouldbeeasierŠcomparedwith  open-domainsystemsŠtodesignthelogic,createtherules,prepare  thedataandconstructthecandidaterepliestohandletheparticular  task[23].Forinstance,inaconversationsystemforbooking  orbusrouteinquiring,thecomputersideonlyneedstocapturethe origin,destinationandusinformation,andthenrespondac- cordinglywithtemplates[44].Oneofthemostobviouslimitation  ofserviceisthattheconversationcannotexceedthe  systemtopicscope.Illegibleinputswillnotbeaccepted,whichis  regardedasahardconstraint.Theunderlyingsystemdesignphi-  losophyisnearlyimpossibletogeneralizetotheopendomain. Itisonlyrecentlythatresearchersfocusonnon-task-oriented (i.e.,open-domain)conversationsystemsfortheirfunctional,so-  cial,andentertainmentrolesinreal-worldapplications[2,26,9,  35,17,31,6].Creatinganopen-domainconversationsystemto  interactwithhumansisaninterestingbutnotoriouslychallenging  problem.Sincepeoplearefreetosayanythingtothesystem,itis  impossibletopreparetheinteractionlogicanddomainknowledge, whichcanbe,incontrast,insystemsbefore hand.Besides,thenumberofpossiblecombinationsofconversa-  tionstatusareliterallysothatconventionalhand-crafted  rulesandtemplateswouldfailbeyondanydoubt[34]. AlongwiththematurityofWeb2.0,therehasbeenanexplo- sioninthenumberofpeoplehavingpublicconversationsonweb-  sitessuchasBulletinBoardSystem(BBS)forums,socialmedi-  a(e.g.,Facebook 1,Twitter 2)andcommunityquestionanswering (cQA)platforms(e.g.,BaiduZhidao 3,Yahoo!Answers 4).These resourcesprovideauniqueopportunitytobuildcollectionsofnat-  urallyoccurringconversationsthatareordersofmagnitudelarger 1http://www.facebook.com 2http://www.twitter.com 3http://www.zhidao.baidu.com 4https://answers.yahoo.comthanthosepreviouslyavailable.Theyalsopropelthedevelopment ofretrieval-basedtechniquesintheofopen-domainconversa- tionresearch.Themeritisthat,owingtothediversityontheWeb,  thesystemwillbeabletoretrieveatleastsomeresponsesforany  userinput,andreturnasensibleresponse. Thebigdataera,however,seemslikeadouble-edgedsword. Ononeside,itbringsthegreatopportunity,asmentionedabove,  tobuildpracticalhuman-computerconversationsystemsinopen  domain.Ontheotherside,therearealsochallenges.Givenauser-  issuedquery,weoughttoidentifyappropriatecandidatereplies  fromaverylargevolumeofdata.Besides,theproposedmodel  shouldalsobegeneralandfordifferentconversationsce-  narios.Inaconversationsystem,usuallythereisadditionalinfor-  mationtousesuchasﬁcontextsﬂ(a.k.a.previousutterancesen-  tencesinacontinuousconversationsession).Therefore,capturing  andintegratingasmuchinformationaspossibleinaproperwayis  importantforconversationsystems. Inthispaper,weproposeaﬁdeeplearning-to-respondﬂframe- workforopen-domainconversationsystems.Wecreateahuge conversationaldatasetfromWeb,andthecrawleddataarestored asanatomicunitofnaturalconversations:anutterance,namely  aposting,andits reply .Each hposting-reply icanberegardedasa single-turnconversation.Foragivenquery,weapplytradition-  alkeyword-basedretrievalmethodsandobtainalistofcandidate  replies;eachreplyisassociatedwithitsantecedentposting.We  thenenhancethecurrentquerybyaddingitscontexts,i.e.,oneor  morepreviousutterancesinthecurrentconversationsession.Thus,  weobtainasetofreformulatedqueriesaswellastheoriginalquery.  Adeepneuralnetwork(DNN)-basedrankerthereaftertellshow  eachcandidatereply/postisrelatedtoa(reformulated)query,and  yieldsarankinglistforeach(reformulated)query.Wemergethe  rankinglistscorrespondingtodifferentreformulations.Inthisway, weareabletoorganicallyincorporateintotheconversationsystem multi-dimensionofrankingevidencesincludingqueries,contexts,  candidatepostingsand/orreplies,whichisanovelinsight. Theproposedreformulationapproachandmergingstrategypro- videanewmeansofconversationmodeling,especiallymulti-turn  conversations.Byusingpreviousutterances,weareawareofback-  groundinformationofthequery,whichmightbeinformative.More-  over,differentreformulationscancapturedifferentaspectsofback-  groundinformation;theirresultingrankedlistsarefurthermerged  byanovelformula,inwhichweconsidertherelatednessbetween  thereformulatedqueries(withcontext)andtheoriginalone. TheDNNranker,servingasthecoreofﬁdeeplearning-to-rankﬂ schema,modelstherelationbetweentwosentences(queryversus context =posting=reply).Weuseabi-directionalrecurrentneural networktopropagateinformationacrosswords;aconvolutional neuralnetworklayerfurthercapturespatternsofadjacentwords.  Thenamatchinglayercombinestheinformationineachindividual  sentence.NotethatourDNNisagenericframeworkandappliesto  Query-Reply,Query-Posting andQuery-Context inaway. Weconductextensiveexperimentsinavarietyofconversation setupsbetweenhumansandcomputers.Inparticular,webuild  thesystemuponanextremelylargeconversationresource,i.e.,al-  most10millionpairsofhumanconversationresources.Werun  experimentsagainstseveralotherrivalalgorithmstoverifytheef-  fectivenessoftheproposedDNNmodel.Oursystemoutperforms  standardandstate-of-the-artbaselinesregardingavarietyofeval- uationmetricsintermsofp@1,MAP,nDCGandMRRmetrics. Theresultindicatesthatourconversationsystemisratherhelpful  tofacilitateconversationsbetweenhumanandcomputer. Tosumup,ourcontributionsaremainlyasfollows: Weproposeaﬁdeeplearning-to-respondﬂschemaforauto- matichuman-computerconversationsystemswithdeepneu-  ralnetworks(DNNs).Thedeeplearningmodelisgeneral  andwelltoadaptforvariousconversationscenarios. Weproposeanovelconcepttomodel context inacontinu- ousconversationsessioninmulti-turns.Theproposedquery  reformulationcancapturedifferentaspectsofbackgroundin-  formation.Wedesignaninsightfulframeworktomergethe  rankinglistsofcandidatestoall(reformulated)queries. Weproposeanewresponserankingparadigmgiveeach(re-  formulated)query,incorporatingmulti-dimensionofranking evidences: Query-Reply,Query-Posting andQuery-Context ,whichisanadaptionforconversationalscenarios. Therestofthepaperisorganizedasfollows.Westartbyreview- ingrelatedwork.InSection3,wedescribethetaskmodelingand proposedframeworkforconversationsystems.InSections4and  5,weintroducethedetailedmechanismsofcontextualqueryrefor-  mulationandthedeeplearning-to-respondarchitecture.Wedevise  experimentalsetupsandevaluationsagainstavarietyofbaselines  anddiscussresultsinSection6.Finallywedrawconclusionsin  Section7. 2.RELATEDWORK 2.1ConversationSystems Earlyworkonconversationsystemsisgenerallybasedonrules ortemplatesandisdesignedfordomains[33,36].These rule-basedapproachesrequiresnodataorlittledatafortraining,  whileinsteadrequiremuchmanualefforttobuildthemodel,or tohandcraftrules,whichisusuallyverycostly.Theconversation structureandstatustrackinginverticaldomainsaremorefeasible  tolearnandinfer[44].However,thecoverageofsuchsystemsare  alsofarfromsatisfaction.Later,peoplebegintopaymoreattention  toautomaticconversationsystemsinopendomains[31,6]. Fromdomainstoopendomain,theneedforahuge amountofdataisincreasingsubstantiallytobuildaconversation  system.Asinformationretrievaltechniquesaredevelopingfast,  researchersobtainpromisingachievementsin(deep)questionand  answeringsystems.Inthisway,analternativeapproachistobuild  aconversationsystemwithaknowledgebaseconsistingofanum-  berofquestion-answerpairs.Leuski etal. buildsystemstos- electthemostsuitableresponsetothecurrentmessagefromthe question-answerpairsusingastatisticallanguagemodelincross- lingualinformationretrieval[12],buthaveamajorbottleneckof  thecreationoftheknowledgebase(i.e.,question-answerpairs)  [13].Researchersproposetoaugmenttheknowledgebasewith  question-answerpairsderivedfromplaintexts[24,3].Thenum-  berofresourcepairscanbe,tosomeextent,expanded,butarestill  relativelysmallwhiletheperformanceisnotquitestableeither. Nowadays,withtheprosperityofsocialmediaandotherWeb 2.0resources,suchascommunityquestionandanswering(cQA)  ormicrobloggingservices,averylargeamountofconversationda-  tabecomeavailable[35].Aseriesofinformationretrieval-based  methodsareappliedtoshorttextconversationusingmicroblogda-  ta[9,14,17,16].Higashinaka etal. alsocombinetemplategen- erationwiththesearch-basedmethods[6].Ritter etal. havein- vestigatedthefeasibilityofconductingshorttextconversationby  usingstatisticalmachinetranslation(SMT)techniques,aswellas  millionsofnaturallyoccurringconversationdatainTwitter[26].In  theapproach,aresponseisgeneratedfromamodel,notretrieved  fromarepository,andthusitcannotbeguaranteedtobealegiti-  matenaturallanguagetext. Unlikepreviouswork,weconductanovelstudyofretrieval- basedautomaticconversationsystemswitha deeplearning-to- respond schemaviadeeplearningparadigm.Weformulatethe possiblefactorsintoadeepneuralnetworkarchitectureandfurther investigatethepotentialofcombiningdifferentrankingevidences  forthecandidateresponses.Deeplearningstructuresarewellfor- mulatedtodescribeinstinctsemanticrepresentations. 2.2DeepNeuralNetworks Inrecentyears,deepneuralnetworks(DNNs,alsoknownas deeplearning )havemadeimprovementinNLP[11]. DNNsarehighlyautomatedlearningmachines;theycanextrac- tunderlyingabstractfeaturesofdataautomaticallybyexploring  multiplelayersofnon-lineartransformation[1]. InNLPmodels,awordtypicallyactsasanatomicunit.Howev- er,wordsarediscretebynature;itseemsnonsensicaltofeedword  indexestoDNNs.Atypicalapproachistomapadiscretewordto  adense,low-dimensional,real-valuedvector,calledan embedding[19].Eachdimensioninthevectorcapturessome(anonymous)  aspectofunderlyingwordmeanings. PrevailingDNNsforsentence-levelmodelingincludeconvolu- tionalneuralnetworks(CNNs)andrecurrentneuralnetworks(RNNs).  InCNNs,wehaveaed-sizeslidingwindowtocapturelocalpat-  ternsofsuccessivewords[10],whereasRNNskeeponeorafew  hiddenstates,andcollectinformationalongthewordsequencein  aniterativefashion[38,37,39].Socher etal. leveragesentence parsetreesandbuildrecursivenetworks[30].Mouetal.[21,20] proposesyntax-awareconvolutionbasedonparsetrees.Howev- er,conversationalutterancesareusuallycasual,andhencerecur-  sivemodelsarelessapplicableinconversationsystems.Weprefer  structure-insensitivemodelslikeCNNsandRNNs. Beyondasinglesentence,somestudiesareaimedtocapture therelationshipbetweentwosentencesŠknownassentencepair  modelingŠwithapplicationslikeparaphrasedetection[5],discourse  unitrecognition[45],textualentailmentrecognition[27],etc.A  sentence-pairDNNmodelistypicallybuiltuponunderlyingsentence-  levelmodels(CNNs =RNNs).Thentwosentences'informationis  combinedbymatchingheuristicslikeconcatenation,cosinemea-  sure,orinner-product[5,28].Hu etal. developword-by-word matchingapproaches[7],andobtainasimilaritymatrixbetween twosentences.Veryrecently,Rocktäschel etal. proposecontext- awarematchingapproaches[27],wherethesentence'sinfor- mationisavailablewhenmodelingthesecondone.Suchcontext-  awarenessinterweavesindividualsentencemodelingandsentence  matching,prohibitingpre-calculatingthevectorrepresentationof  asentence;hencethesemethodsareconsiderablymorecomputa-  tionalintensive,especiallywithmultiplequeryreformulationsin  ourscenario.Forefyconsideration,weleveragevectorcon-  catenation,whichissimpleyeteffective. Althoughthestudiesofsentence-pairmodelingdescribedabove aresimilartoourDNNtosomeextent,theproposedlearning-to-  rankmodelismorethantraditionalrankingormatching.Wehave multiplequeryreformulationswithﬁcontextsﬂ.Aftercomputing thesimilaritybetweenthequeryandreply/post/context,ourDNN  furthermergestherankingresultscorrespondingtodifferentrefor-  mulatedqueries. 3.TASKMODELING Inthissection,weprovideabigpictureoftheproposed learning-to-respond schemaforconversationsystems.Weillustratethetask modelingforconversations,andestablishthepipelinewithsever- alprocessingproceduresincludingdatacollection,searchandre-  trieval,contextualqueryreformulation,DNN-basedrankingwith Table1:Anexampleoftheoriginalmicroblog postingandthe associatedreplies .Eachpostingmighthavemorethanonere- ply,e.g., Reply 1andReply 2.Tocreateourdatabaseofconver- sationdata,weseparatedifferentrepliestoasamepost,andob-  tainhpost-replyipairs.Westoretwo Posting-Reply pairsinthe conversationaldataset,i.e., hPosting -Reply 1iandhPosting -Reply 2i.Useraccountsareanonymized. Posting:...(Ineedapairofglassesbecauseofthemyopia...) Reply 1:(Iwilloffertheglassesforyou!) Reply 2:... (Youwillberecovered.Don'tworry.) Table2:Part(I)indicatesarealhuman(denotedby A)-com- puter(denotedby B)conversationscenario,whilePart(II)in- dicatesourproposedtaskmodelingandformulations. A2isthe currentuser-issuedquery.Wehavecontextsandreformulated  queriesaslisted.` 'istheliteralconcatenationaction.Note thattheselectedresponse Reply 1isassociatedwitha Posting intheconversationaldatabaseshowninTable1. (I)(II)Human-ComputerConversation TaskFormulation A1:Userquery:q0=A2(OMGIgotmyopiaatsuchanﬁoldﬂage) Contextinformation: B1:C=fc1=A1,c2=B1g(Really?)Reformulatedqueries: A2:q1=A2A1,q2=A2B1(Yeah.Wishapairofglassesasagift.) q3=A2A1B1,::: B2:Top-1rankedresponse: (Iwilloffertheglassesforyou!) r?=Reply 1scoring,andrankedlistfusion.Wegooverthroughthe pipelineandelaboratethedetailsinthenextsection. Datacollection. WiththeprosperityofWeb2.0,peopleinter- activelycommunicatewitheachotherontheWeb,whichprovides  ahugethesaurusforconversationdata.Wecollectalargeamount  ofdatasamplesfromsocialmediasuchasmicroblogwebsites,fo-  rums,cQAbases,etc.Userscanpublishapostingmessagevisible  tothepublic,andthenreceiveoneormorerepliesorcomments  inresponsetotheirposting.Thecommunicationcanhaveasingle turnaswellasmultipleturns.WeillustrateanexampleinTable1. Duetotheheterogeneityofthesources,wetreateachutterance,in  multi-turnconversations,withitssubsequentoneasaposting-reply  pair(i.e.,ourdatabaseiscontext-free).Forapostingwithmulti-  plyreplies,weseparatethemandconstructdifferent hp;r ipairs.Table1showsthepre-processedsamplesinourdataset,appliedto  arealhuman-computerconversationillustratedinTable2.Inthe  sampleshowninTable1,themessageofaconversationistypi-  callyunique.Therearemanyxibleformstoﬁrespondﬂtoagiven  message,whichisexactlythenatureofrealconversations:various  responsesareallpossiblyappropriate,withdifferentaspectsofin-  formationtoaconversation.Weseparatethepostingand  repliesasagroupof hposting-reply ipairs.Thedatarepositoryis demonstratedtobearichresourcetofacilitatehuman-computer  conversations. Searchandretrieval. Inthescenarioofconversations,theuser issuesaquery( q0inTable2),whichmaybeasentenceorafew terms.Weapplyastandardretrievalprocessviakeywordsearch  usingtraditional tf.idf weightingschema[18]ontheconversation data-base(formattedasaninvertedindexpreparedoff-line)based onthelight-weightsearchprovidedbyBaidu 5.Notethatwetreat eachpairofpostingandreplyasashort virtualdocument ,which isnotatraditionalprocess.Inthisway,theretrievedﬁvirtualdoc- umentﬂcomprisestwoparts:the candidatereply ,namely r,along withitsantecedent posting,namely p.Contextualqueryreformulation. Asinglequerymaynotfully conveyuserintentionsin(multi-turn)conversations,asillustrated  inTable2.Undersuchconditions,weusuallyhavecontextinfor-  mationtouse.Weproposeanovelinsighttomodeltheconver-  sationtask.Inparticular,weviewpreviousutterancesfromboth  sidesas contexts ,denotedas C=fcig.Oneormoreutterancesin Ccanbeusedtoenhance q0soastoprovidemoreinformation.We callthisa contextualqueryreformulation process.Moreover,the contextmaycompriseseveralsentences,andhencewehavesever-  alstrategiestoreformulatetheoriginalquery.Eachreformulated  queryisdenotedas qi.MoredetailswillbegiveninSection4. DNN-basedscoring,ranking,andrankedlistfusion. Weap- plyadeepneuralnetwork(DNN)-basedmodeltorankoptimiza-  tion.Inparticular,wedesignabi-directionallongshorttermmem- ory(LSTM)neuralnetworktocapturesentence-levelsemantic- sofaquery q0,candidatereplyandtheassociatedposting,i.e., hp;r i,aswellascontext C.Amatchinglayercombinesmulti- dimensionsofthesentenceinformation,sothatweknowhowcan-  didatereplieswithassociatedpostingsarerelatedtothequeryand  contexts.Analogoustothetraditional Query-Documentmatching,therelevancerankingcanbemeasuredvia Query-Replymatching,theadditional Query-Posting matching,aswellas Query-Context matching.Intuitively,whenaqueryandapostinglooksimilar,they  mightsharethesameresponse(Tables1Œ2).Foreach(reformu-  lated)query,DNNrankscandidatesreplieswithrelevancescores  fromQuery-QueryandQuery-Posting .Wefurthermergeallcan- didaterankedlistsofallcontextualreformulationsusingweighted fusion.Theweightiscontrolledbytherelevancebetweentheorig- inalqueryandthereformulatedonewithcontextualinformation,  i.e.,Query-Context .Forcontinuousconversations,contextscanbe usedtooptimizetheresponseselectionforthegivenquery. Table3summarizestheinputandoutputoftheproposedsystem withdeeplearning-to-respondschema.Givenaquerywithcontext,  theproposedmodelwouldreturnaresponseŠwhichhasthehigh-  estoverall(merged)rankingscore F(:)Šfromthepre-constructed repository.WeuseDNNtoassesstherelevancebetweencandidate  replies,postingsandreformulatedquerieswithdifferentcombina-  tionsofcontexts.TheDNNalsomergestherankingscorescorre-  spondingtodifferentcontextualqueryreformulations. Weapplyhingelosswithnegativesamplingtotrainthenetwork. Gradientscanbeback-propagatedallthewaybackfrommerging,  ranking,sentencepairing,toindividualsentencemodeling.There-  fore,alltheseheterogeneousrankingevidencesareintegratedto-  getherthroughtheproposedDeepLearning-to-Respondschema. 4.CONTEXTUALREFORMULATION Generally,contextinformationmaybeinformative(butsome- timesmaybenot)whenmodelingaquery.Itisnon-trivialtoex- ploredifferentstrategiestoutilizecontextinformationforconver- sations.Inthissection,wedescribetheproposedcontextualquery reformulationapproach. Thecontextualqueryreformulationstrategiesaremainlyinspired bythefollowingobservations: Somecontextutterancesareinformative,whileothersare  not.AstheexampleinTable2illustrates,thecontextut- 5BaiduisthelargestChinesesearchengineprovider.Someofthe servicesareavailableathttp://www.baidu.com. Table3:Symbolsandannotationsforproblemformulation. q0thecurrentquery r;p candidatereplywiththeassociatedantecedentposting C=fcigcontexts(utterancesbefore q0).ciisautterancein CQ=fqigreformulatedquery:q0concatenateswithsome cif(:)matchingmetricfor Query-Replyg(:)matchingmetricfor Query-Posting h(:)matchingmetricfor Query-Context Conversationrepository: hp;r iInputQuery:q0Context:COutputSelectedresponse:r=argmaxrFrjq0;C;fhp;r igteranceﬁReally?ﬂshouldnotbeconsideredasinformative asthecontextutteranceﬁ OMGIgotmyopiaatsuchan`old' age ﬂ.Therefore,weshallhavedifferentreformulations:we  canaddthecontextsasawhole,orwecanaddthecontexts  one-by-one.Theintuitionisthatcontextsentencesarenot  equallyinformative,andtheyshallbeuseddifferently. Givenacontextof Nsentences,thenumberofpossibleways toconcatenatetheoriginalqueryistheoretically 2N.Asa combinatoryproblem,thenumberofcombinationswillgrow  exponentiallyas Ngrowsup.Henceweneedtoimposecon- straintsonthecontextualqueryreformulationstrategies.An acceptablesolutionshouldbe(atmost)linearas Ngrows. Weoughttouseasubsetofreformulations,andcombine suchstrategiestoapproximateallpossiblereformulations.  TheresultsarethenmergedbyDNNwithdifferentweights,  analogoustoBayesianmodelaverageoverdifferentrefor-  mulations.Underthescenarioofacontinuousconversation,weobserve  thatcontexts,nomatterfromthe humansideorfromthe computerside,playasimilarroletothefutureconversation. Therefore,wedonotdistinguishutterancesfromtwosides inthecontext.Nevertheless,howrelevantisacontextwith thecurrentqueryisassessedbythe Query-Context match-ingfunction,andthedegreeofrelevancematterswhenDNN  mergesdifferentrankedlistsfromreformulatedqueries. Withoutlossofgenerosity,letusassumethereare Nsentencesinthecontext C,i.e., c1; ;c N2Cbeingpreviousutterances inthecurrentconversationsession.Weaddoneormorecontext  sentencestothequery q0andobtainasetofreformulatedqueries (eachisdenotedas qi2Q ).Toreducetheexplosivenumberofall possible2Ncombinations,werestrictthecontextualqueryrefor- mulationstrategiesinpracticeasfollows: NoContext :thesimplestreformulationstrategyisthatno contextinformationwillbeadded,i.e., QNoContext =fq0g.WholeContext :wedonotdistinguishdifferentcontextsen- tencesandhenceincorporatetheentirecontextsasawhole,  i.e.,QWholeContext =fq0;q 0Cg.Add-One:weconcatenate q0withonecontextsentence,one atatime,i.e., QAdd-One=fq0;q 0c1;:::;q 0cNg.Drop-Out :weconcatenate q0withthewholecontextwhile leave-one-outeachcontextsentence,oneatatime,i.e., QDrop-Out=fq0;q 0[Cnc1];:::;q 0[CncN]g.Combined:thecombinationofallstrategies,i.e., Q=QNoContext SQWholeContext SQAdd-OneSQDrop-Out.`'referstoliteralconcatenation,wheretheorderofcontextand thequeryispreserved.ﬁ Cnciﬂindicates ciisoutfrom C.Theforallthecontextualqueryreformulationstrategies arethatthecostised(for QNoContext andQWholeContext )orlinear (forQAdd-OneandQDrop-Out)alongas Ngrows.Theintuitionfor Add-OneandDrop-Out strategiesisbasedona-granularity: toincorporaterelevantcontextsentencesonly,ortoexemptone  irrelevantcontextsentences.Thelaststrategycombinesallthese  strategiestoapproximateallpossibilities. 5.DEEPLEARNINGTORESPOND Inthissection,wedescribethedeepmodelforquery-replyrank- ingandmerging.Ourmodelrstdeterminesthescoreofacan- didatereplygiventhe(reformulated)query,basedonthecandi-  datereplyanditsassociatedposting(Subsection5.1).Thenthe  modelmergesthescorebysummingoverallqueryreformulation-  sincludingtheoriginalquerywitha gating(product)mechanism (Subsection5.2).Inthisway,allheterogeneousrankingevidence  iscombinedorganicallyinadifferentiablemanner. 5.1SentencePairModeling Asmentioned,ourmodeldeterminestherelationshipfor Query-Reply,Query-Posting ,and Query-Context matchings.The threescoringfunctionsareas: f(q;r ):Thisscoringfunctiondirectlyjudgestherelatedness betweenareplyandthe(reformulated)query.Alargerscore achievedmeansmorerelevanceofthecandidatereply. g(q;p ):Iftheassociatedpostingofthereplyissimilarto thequery,itssubsequentreplyislikelytobeanappropriate  response.h(q;q 0):Thisscoringfunctiontellshowthereformulated queryiscorrelatedwiththeoriginal q0.Amorerelevantcon- text,i.e.,alarger h(q;q 0),shouldleadstoamore rankingresult,andthescoresfrom f(q;r )andg(q;p )willbecreditedwithmoreweightsforthe Sum fusion.Thescoringfunction f(q;r )outputsascalarin R(appropriate-nessorinappropriateness)inforaparticularcandidatereply,while  thelattertwofunctionsservingasanadjustmentorﬁ gateﬂ,which aresquashedbyalogisticfunctionintothe (0;1)range.Neverthe- less,alltheabovefunctionsarecomputedbythesamedeepneural  networkarchitecture(exceptforthelastactivationfunction),but theirparametersaredifferentsothatthethreescoringfunctions candepictdifferentmeanings.Inparticular,thedeepstructurefor  sentencepairmodelingincludesthefollowingcomponents. 5.1.1WordEmbeddings Traditionalmodelsusuallytreatawordasadiscretetoken;thus, theinternalrelationbetweensimilarwordswouldbelost.Word  embeddings[19]areastandardapparatusinneuralnetwork-based  textprocessing.Awordismappedtoalowdimensional,real-  valuedvector.Thisprocess,knownasvectorization,capturessome underlyingmeanings.Givenenoughdata,usage,andcontext,word embeddingscanmakehighlyaccurateguessesaboutthemeaning  ofaparticularword.Embeddingscanequivalentlybeviewedthat  awordisrepresentedasaone-hotvectorandmultipliedbya  look-uptable[19]. Inourmodel,wevectorizeallwordsusingtheirembed- dings,whichserveasthefoundationofourdeepneuralnetworks. Wordembeddingsareinitializedrandomly,andthentunedduring  trainingaspartofmodelparameters. 5.1.2Bi-DirectionalLSTM Weuseabi-directionallongshorttermmemory(Bi-LSTM)re- currentnetworktopropagateinformationalongthewordsequence. AsreviewedinSection2,arecurrentneuralnetwork(RNN) keepsahiddenstatevector,whichchangesaccordingtotheinput  ineachtimestep.AsRNNscaniterativelyaggregateinformation  alongasequence,theyarenaturallysuitableforsentencemodeling. LSTMisanadvancedtypeofRNNbyfurtherusingmemory cellsandgatestolearnlongtermdependencieswithinasequence [32,25].LSTMmodelsareasfollows:givenasequence ofinputs,anLSTMassociateseachpositionwith input,forget ,and outputgates ,denotedas it,ft,and otrespectively.Thevector ltisusedtoadditivelymodifythememorycontents.Givenaninput  sentenceS=fx0;x 1;:::;x Tg,where xtisthewordembedding atposition tinthesentence.LSTMoutputsarepresentation htforpositiont,givenby 2664itftotlt3775=2664˙˙ ˙tanh3775Wht1et~ht=ft~ht1+itlthst=ot~ht(1)where~hisanauxiliaryvariableandcanbeviewedastheinfor- mationstoredinmemorycell. ˙()= 11+eisaknownasa sigmoid/logisticfunction. AsingledirectionalLSTMtypicallypropagatesinformationfrom thewordtothelast;hencethehiddenstateatacertainstepis dependentonitspreviouswordsonlyandblindoffutureword- s.ThevariantBi-LSTM[4]isproposedtoutilizebothprevious andfuturewordsbytwoseparateRNNs,propagatingforwardand  backward,andgeneratingtwoindependenthiddenstatevectors ! htand  ht,respectively.Thetwostatevectorsareconcatenat- edtorepresentthemeaningofthe t-thwordinthesentence,i.e., ht=h! ht;  hti.5.1.3Convolution Wefurtherapplyaconvolutionalneuralnetwork(CNN)toex- tractlocalneighboringfeaturesofsuccessivewordsŠi.e.,discrim- inativewordsequencescanbedetectedŠyieldingamorecompos-  iterepresentationofthesentences.ThestructureofCNNinthis  workissimilarto[10],showninFigure1.UnlikeRNNs,CNNs  onlyimposelocalinteractionsbetweensuccessivewordswithina  (size m).Concretely,webuildaCNNupontheoutputofBi-LSTM.For everywindowwiththesizeof minBi-LSTMoutputvectors,i.e., (Ht)m=[ht;h t+1; ;h t+m1],where tisacertainposition, theconvolutional F=[F(0);:::;F (m1)]willgeneratea vectorsequenceusingtheconvolutionoperationﬁ ﬂbetweenthe twovectors.Moreformally,theconvolutionresultsinavector  whereeachcomponentisasfollows: oF=tanhm1Xi=0h(t+i)F(i)(2)Inpractice,wealsoaddascalarbias btotheresultofconvolu- tion.Inthisway,weobtainthevector oFisavector,eachdimen- sioncorrespondingtoeachwordinthesentence. Noticethattheaboveequationdescribesasingleﬁsliceﬂofcon- volution.Infact,wemayhavemultiplefeatureandthus Figure1:Thedeepneuralnetworkarchitectureformatchingsentencepairs. multiplefeaturemaps.Differentdonotshareparameters( Fandb),sothattheycancapturedifferentmeanings. 5.1.4Pooling,Concatenation,andMatching OnthebasisofsentencerepresentationsusingBi-LSTMwith CNN,wecanmodeltheinteractionsbetweentwosentences.We applypoolingtoaggregateinformationalongthewordsequence.  Inparticular,amaxpoolinglayerchoosesthemaximumvaluein eachdimensioninthefeaturemapsaftertheconvolution,indicating howmuchthefeatureismostalongthesequence. Weconcatenatetwoindividualsentences'vectorrepresentations (withpossibleadditionalfeatures),whicharethenfedtoanensuing  networkforfurtherinformationmixing.Vectorconcatenationfor  sentencematchingisalsoappliedinotherstudieslike[45,22],  whichiseffectiveyetoflowcomplexityorder,comparedwithother  word-by-wordmatching[7],orattentionmethods[27]. Thejointvectoristhenpassedthrougha3-layer,fully-connected, feed-forwardneuralnetwork,alsoknownas multi-layerperception (MLP)[1],whichallowsrichinteractionsbetweenasentencepair  fromoneofthethreecomponents.Thenetworkenablestoextract  featuresautomatically,startingfromlower-levelrepresentationsto higher-levelones. Finally,asingleneuronoutputsthematchingscoreoftwosen- tences.Asmentioned, f(q;r )isin R;hencethescoringneu- ronisessentiallyalinearregression.For g(q;p );h (q;q 0)2(0;1),weapplyasigmoid/logisticfunctiongivenby ˙()= 11+e.5.2Merging Intheprevioussubsection,wehavedescribedhowthemodel capturestherelationamongsentencepairs.Now,wemergethe scoresofaparticularcandidate rintermsofdifferentcontextu- alqueryreformulations.AsdiscussedinSection5.1,ifaposting  ismorerelatedtothe(reformulated)queryandthereformulated  queryismorerelatedtotheoriginalquery,then f(q;r )wouldbe morereliable.Inspiredbythisobservation, g(q;p )andh(q;q 0)asdesignedastwoadjustingﬁgatesﬂ.Inparticular,scoresfromd-  ifferentqueryreformulationsaresummed,weightedbythesetwo  gates.Theoverallrankingscoreofacandidatereply randq0isde- asfollows.Theequationandspiritalsoappearsimilartothe  integrationofa sum-product process,whichcombinesthe sumoperationsand product operations:F(q0;r )= jQjXi=0h(q0;q i)Xpf(qi;r )g(qi;p )(3)Hereweproposetosumoverallpostingsassociatedwiththe reply.Differentdatarepositorywillhavedifferentsettings:acan-  didatereplyispossibletoassociatewithmorethanonepostings.  Inourdatasettings,eachreplyisassociatedwithonlyoneposting.  However,Equation(3)isgeneralandextensible. Rankingproblemscanapplypairwiserankinglosssuchashinge lossorcross-entropyloss.Hereweapplyhingelosstotrainour  DNNnetwork.Givenatriple F(q0;r +)inthetrainingset,weran- domlysampleanegativeinstance r.Theobjectiveistomaximize thescoresofpositivesampleswhileminimizingthatofthenega-  tivesamples.Concretely,wewouldlike F(q0;r +)tobeasleast F(q0;r )plusamargin .Thus,thetrainingobjectiveisto minimizeXq0;r+max0;+ F(q0;r +)F (q0;r )+kk22(4)whereweaddan `2penaltywithcoef foralltheparameters = f;;˚gwhichareweightandbiasvaluesoptimizedbythe networkfrommulti-dimensionsofrankingevidences,i.e., Query-Reply,Query-Posting andQuery-Context ,correspondingly. 5.3Training Asourmodelis(almost)everywheredifferentiable,theparame- tersofthenetworksareoptimizedwithstochasticgradientdescen- tusingthebackprorogationalgorithmtocomputethegradients. Thegradientscanbepropagatedallthewaybackthroughmerging, gating,matching,andindividualsentencemodeling.Inthisway,  heterogeneousinformation(rankingevidences)canbeincorporat-  edorganicallywithourmodelunderthedeeparchitecture.  Accordinglytotheobjectivefunctiontooptimize O(:)inEquation (4),itissuftolearnthemodelbycomputingthegradients  withrespecttothemodelparameters ,and˚;thatis,ourgoals Table4:Datastatistics.Postingsandrepliesareallunique. Source#Posting#Reply#Vocabulary Zhidao8,915,6943,705,3021,499,691Douban10,618,9812,963,226483,846Tieba4,189,1603,730,2481,046,130Weibo186,963393,654119,163Misc.3,0561,5484,297Total9,023,8547,293,9782,857,378aretocompute @O@,@O@and@O@˚forQuery-Reply,Query-Posting andQuery-Context .6.EXPERIMENTSANDEVALUATION Inthissection,weevaluateourmodelforconversationtaska- gainstaseriesofbaselinesbasedonahugeconversationresource. Theobjectivesofourexperimentsareto1)evaluatetheeffective- nessofourproposeddeeplearning-to-respondschema,and2)eval-  uatecontextualreformulationstrategiesandcomponentsofmulti-  dimensionofrankingevidencesfortheconversationaltask. 6.1Dataset Asmentioned,wecollectedmassiveconversationresourcesfrom variousforums,microblogwebsites,andcQAplatformsincluding BaiduZhidao 6,Doubanforum 7,BaiduTieba 8,SinaWeibo 9,etc. Weconducteddataandcleaningproceduresbyremoving  extremelyshortrepliesandthoseoflowlinguisticqualitysuchas  meaninglessbabblingsaccordingtotheevaluationframeworkput  forwardin[40,42],soastomaintainameaningful,high-quality conversationrecord.Intotal,thedatabasecontains ˘10million hposting;reply ipairs.SomestatisticsaresummarizedinTable4. Weconstructedthedatasetof1,606,583samplestotrainthedeep neuralnetworks,357,018forvalidation,and11,097fortesting.It isimportantthatthedatasetforlearningdoesnotoverlapwiththe  databaseforretrieval,sothatwestrictlycomplywiththemachine  learningregime.Foreachtrainingandvalidationsample,weran-  domlychoseareplyasanegativesample.Validationwasbased  ontheaccuracyofpositive/negativeForthetestset,  wehiredworkersonacrowdsourcingplatformtojudgetheappro-  priatenessof30candidaterepliesretrievedforeachquery.Each  samplewasjudgedby7annotatorsviamajorityvotingbasedon  theappropriateness fortheresponsegiventhequeryandcontexts (ifany):ﬁ1ﬂdenotesanappropriateresponseandﬁ0ﬂindicatesan  inappropriateone. 6.2ExperimentalSetups 6.2.1Hyperparameters Inourproposedmodel,weused128-dimensionalwordembed- dings,andtheywereinitializedrandomlyandlearnedduringtrain- ing.AsourdatasetisinChinese,weperformedstandardChinese  wordsegmentation.Wemaintainedavocabularyof177,044phras-  esbychoosingthosewithmorethan2occurrences. Thebi-directionalLSTMhas128hiddenunitsforeachdimen- sion;CNNis256dimensionalwithawindowsizeof3.Weused  stochasticgradientdescent(withamini-batchsizeof100)foropti- mization,gradientcomputedbystandardback-propagation.Initial 6http://www.zhidao.baidu.com7http://www.douban.com 8http://www.tieba.baidu.com 9http://www.weibo.com learningratewassetto0.8,andamultiplicativelearningratedecay wasapplied.Theaboveparameterswerechosenempirically.We  usedthevalidationsetforearlystopping. 6.2.2EvaluationMetrics Giventherankinglists(annotatedbycrowdsourcedworkers)for testqueries,weevaluatedtheperformanceintermsofthefollow-  ingmetrics:precision@1(p@1),meanaverageprecision(MAP)  [31,43],andnormalizeddiscountedcumulativegain(nDCG)[8,  41].Sincethesystemoutputsthebestselectedreply,p@1isthe  precisionatthe1stposition,andshouldbethemostnaturalwayto  indicatethefractionofsuitableresponsesamongthetop- 1replyre- trieved.Besides,wealsoprovidedthetop- krankinglistforthetest queriesusingnDCGandMAP,whichtestthepotentialforasystem  toprovidemorethanoneappropriateresponsesascandidates.We aimedatselectingasmanyappropriateresponsesaspossibleinto thetop- klistandrewardingmethodsthatreturnsuitablereplieson thetop. Formally,themetricsarecomputedasfollows. nDCG@i=1jTjXq2T1ZkXi=12ri1log(1+i)whereTindicatesthetestingqueryset, kdenotesthetop- kposi-tionintherankinglist,and Zisanormalizationfactorobtained fromaperfectranking. riistherelevancescoreforthe i-thcandi- datereplyintherankinglist(i.e.,1:appropriate,0:inappropriate). MAPiscomputedby MAP=1jTjXq2T1NqkXi=1PiriHereNqisthenumberofappropriateresponsesselected,and Piistheprecisionat i-thpositionforthequery. Sinceweuserealconversationsfortesting,wealsohavethehu- manresponsetakenfromthehuman-humanconversationsession  asoneofcandidaterepliesorderedintherankedlist.Hencewein-  cludetheMeanReciprocalRank(MRR)evaluationcomputedas: MRR=1jTjXq2T1rank(q)whererank (q)isthepositionoftheoriginalresponseinthecan- didaterankinglist.UnlikeMAPandnDCG,whichexaminethe  ranksofallappropriateresponses,MRRfocusesonevaluatingthe  capabilityofretrievalsystemsto(perhaps)thebestresponse.  MRRisusefulbutdoesnottestthefullcapabilitybecausetherecan  bemorethanoneappropriateresponsestoaconversation. 6.2.3AlgorithmsforComparison Toillustratetheperformanceofourapproach,weincludeseveral alternativealgorithmsasbaselinesforcomparison.Thebaselines  canbedividedintotwocategories,i.e.,1)generation-basedmeth-  odsand2)retrieval-basedmethodsforconversationsystemsfrom  veryrecentstudies.Sinceourproposedapproachistechnicallya  retrieval-basedmethod,wemainlyfocusonthesecondcategory.  Forfairnessweconductedthesamepre-processingproceduresand  datacleaningforallalgorithms. Generation-basedConversation. Forthisgroupofalgorithms, theconversationsystemwillgeneratearesponsefromagivenin- put,i.e.,aqueryfromtheuserundertheconversationalscenario. StatisticalMachineTranslation(SMT) :SMTisamachinetrans- lationparadigmwhichtranslatesonesentenceinthesourcelan-  guagetoasentenceinthetargetlanguage.Ifwetreatqueriesand  repliesasseparatelanguages,wecantrainatranslationmodelto Table5:Retrievalperformanceagainstbaselineswithourproposedadaptionofcontextualreformulation.` ?'indicatesthatwe accepttheimprovementhypothesisofDL2RoverthebestbaselinebyWilcoxontestatalevelof0.01.Performanceof bothgenerativemethodsandretrievalmethods.Forgenerativemethods,theygenerateoneresponsegiveneachquery.Hencethe  p@1infactreferstoaccuracy.Othermetricsarenotapplicable. Modelp@1MAPnDCG@5nDCG@10nDCG@20MRRSMT(Ritteretal.,[26]) 0.363LSTM-RNN(Sutskeveretal.,[32]) 0.441NRM(Shangetal.,[29]) 0.465RandomMatch0.2660.2460.2470.2890.3530.083OkapiBM250.2720.2530.3370.3020.3680.169DeepMatch(LuandLi,[17]) 0.4570.3170.4190.4540.5080.275LSTM-RNN(Palangietal.,[25]) 0.3380.2830.3300.3710.4310.228ARC(Huetal.,[7]) 0.3940.2940.3970.4210.4770.232DeepMatchw/contextadaption 0.6030.3780.5550.5840.6280.349LSTM-RNNw/contextadaption 0.3620.2960.3540.3950.4530.237ARCw/contextadaption 0.4000.3090.3830.4220.4800.319DeepLearning-to-Respond(DL2R) 0.731?0.416?0.663?0.682?0.717?0.333Table6:Performanceevaluationsofdifferentcontextualqueryreformulationstrategies. p@1MAPnDCG@5nDCG@10nDCG@20MRRNoContext 0.5220.3400.4760.5090.5590.296WholeContext 0.6980.4040.6350.6570.6960.327Add-One0.7160.4110.6500.6700.7060.322Drop-Out0.7200.4130.6560.6750.7110.328Combined0.7310.4160.6630.6820.7170.333Table7:Performanceevaluationsofdifferentcomponentswithmulti-dimensionofrankingevidences. p@1MAPnDCG@5nDCG@10nDCG@20MRRQuery-Replyw/oQuery-Context 0.5220.3400.4760.5090.5590.296Query-Postingw/oQuery-Context 0.5100.3020.4040.4250.4890.285Query-Replyw/Query-Context 0.5960.3660.5280.5610.6030.327Query-Postingw/Query-Context 0.5630.3620.4830.5160.5680.316FullCombination0.7310.4160.6630.6820.7170.333ﬁtranslateﬂqueriesintoreplies.Weimplementedthephrase-based translationideaforconversationproposedin[26]. LSTM-RNN:LSTM-RNNisbasicallyaRecurrentNeuralNet- work(RNN)usingtheLongShortTermMemory(LSTM)archi-  tecture.TheRNNwithLSTMunitsconsistsofmemorycellsin  ordertostoreinformationforextendedperiodsoftime.Weuse  LSTM-RNNforbothgenerationandretrievalbaselines.Forgen-  eration,weuseanLSTM-RNNtoencodetheinputsequence (query)toavectorspace,andthenuseanotherLSTM-RNNtode- codethevectorintotheoutputsequence(reply)[32];forretrievals,  weadopttheLSTM-RNNtoconstructsentencerepresentationsand  usecosinesimilaritytooutputthematchingscore[25]. NeuralRespondingMachine. Weimplementtheneuralre- spondingmachine(NRM)proposedin[29],whichisanRNN-  basedgenerationapproachwithaglobal-localattentionschema. Retrieval-basedConversation. Theapproacheswithinthisgroup ofbaselinesarebasedonretrievalsystems,whichreturnthebest matchedcandidatereplyoutoftheconversationalrepositorygiven  aparticularquery.Sinceourapproachisretrieval-based,weselect  strongretrieval-basedmethodstomakeathoroughcomparison. RandomMatch. Themethodrandomlyselectsrepliesfrom theretrievedlistforeachquery.Beawareitisnot truerandombecauseitonlyrandomizestheorderoftheretrievedresults.The  truerandommatchistooweaktobeincludedasadecentbaseline. OkapiBM25. Weincludethestandardretrievaltechniqueto rankcandidatereplies.Foreachquery,weretrievethemostrele-  vantreplyusingBM25model[18]fromthecorpus. DeepMatch. TheDeepMatchmethodconsidersmultiplegran- ularityfromtheperspectiveoftopics,obtainedviaLDA[17]. ARC.TheARCapproachisaCNNbasedmethodwithconvo- lutionarylayerswhichconstructsentencerepresentationsandpro-  ducethematchingscoresviaaMLPlayer[7]. DeepLearning-to-Respond(DL2R) .WeproposetheDL2R systembasedonthreenovelinsights:1)theintegrationofmulti-  dimensionofrankingevidences,2)context-basedqueryreformu-  lationswithrankedlistsfusion,and3)deeplearningframework  fortheconversationaltask.Thereisactuallyaseriesofvariants  ofDL2Rmodelwithdifferentcomponentsanddifferentcontext  utilizationstrategies.Wewillreporttheperformancecompar-  isonsbetweenDL2Ragainstbaselinesandthenshowthedetailsof  componentsandstrategiesinSection6.4. 6.3OverallPerformance Wecomparetheperformanceofallmethodsincludingbaselines andourproposedDL2Rmodelmeasuredintermsofp@1,MAP, nDCGandMRR.InTable5welisttheoverallresultsagainstal- lbaselinemethods.OurproposedmethodDL2Rshowsclearly betterperformancethanthebaselinemethods.Onaverage,DL2R achievesanaverage+38.63%improvement(averagedonallmet-  rics)comparedwiththestrongestbaselinegroupwithcontextadap-  tion(inTable5).Wethendiscussthecomparisonsindetails. Weillustratetheresultfromgenerativemethods.Givenoneus- erquery,thegenerativemethodsgenerallyprovideonegeneration  astheresponsetooutput.HencewedonotcompareMAPornD-  CG@1forthisalgorithmgroup.Notethattheoriginalresponseis  notlikelytobegenerated;thusitisinfeasibletocalculatetheM-  RR.Ingeneral,thegenerativealgorithmshaverelativelyhighp@1  scores,whileLSTM-RNNandNRMperformbetterthantheSMT  method.Thereasonscanbeascribedtotwoaspects:,SMT  isnotinstinctivelytailoredforconversationsystemsandsecondly,  deepneuralnetworksforLSTM-RNNandNRMwillbemorelike-  lytolearnabetterrepresentationforthequeriesandthenreturn  abetterdecodedgenerationasresponse.Thegeneratedresponses  areingeneralquiteambiguousorbroadtoanswerawiderangeof  queries,butnotenough.Suchresponsesmightberelevan- tbutnotappropriateenoughtomakeameaningfulconversation, whichhavebeenraisedasaproblemin[15,29]. WecanseegreatimprovementforDL2Ragainstoriginalretrieval- basedbaselines. RandomMatch isalowerboundforallbase- lines.Aswementioned,itrandomizestheorderoftheretrieved  results.HencetheresultiscomparabletothatofBM25buts-  lightlyworse. OkapiBM25 representsthestandard(andsimple) retrievalsystem.TheperformanceforBM25isnotasgoodasthe  otherdeeplearning-basedretrievalsystems,whichisnotsurpris-  ing.Deeplearningsystemsareprovedtohavestrongcapabilities  tolearntheabstractiverepresentation[1,10,30],whileBM25only  utilizestheshallowrepresentationofterm-levelretrieval.Thedeep  learningalgorithmgroupsclearlyoverwhelmshallowlearningal- gorithms,yetitisinterestingtoseethatDL2Rstilloutperformsthe otherdeeplearningbaselinesinTable5.Themightbedue  tothecontextinformationwehavemanagedtouse,whiletheother  deeplearningbaselinesarematchingmetricsforsingleturnsonly.  Foramorefaircomparison,weadapttheoriginalbaselinesintoour  proposedcontextualreformulationframeworktoincorporatecon-  textinformation. InTable5,wecanseewiththeusageofcontextualreformula- tion,theperformancesforDeepMatch,ARC,andLSTM-RNNall  getboosted,whichgreatlyindicatestheeffectivenessofourpro-  posedcontextualqueryreformulationforconversationsystems.It  isausefulwaytoincorporatecontextinformationforconversa-  tionalscenarios.OurproposedDL2Rhasobviousimprovement againstthebaselinesystems.Themostprobablecredits comefromtheretrievalformulation:weframethevirtualdocu-  mentasapostingandreply,andweintegratemulti-dimensionsof  rankingevidencestofacilitateabetterrankinglist.Thecontextu-  alDeepMatchmethodveryslightlyoutperformsDL2RonMRR  evaluation.Asmentionedbefore,MRRisusefulwhentryingthe  thebestresponse.Sinceconversationsareopenwithmorethan  oneappropriateresponses,MAPandnDCGscoresindicatethefull  capacityoftheretrievalsystems. Tillnow,wehavevalidatedthatdeeplearningstructures,contex- tualreformulationsandintegrationsofmulti-dimensionsofranking  evidencesareeffective.Nextwewillcometoacloserlookatthese  strategiesandcomponentsforfurtheranalysisanddiscussions. 6.4AnalysisandDiscussions Weexaminetherelativecontributionsofdifferentstrategiesand individualcomponentsofourproposedmodel.Otherthanthepro- poseddeepneuralnetwork-basedlearningframework,wehavet- woothercontributions:1)contextualqueryreformulationtoutilize  contextinformation,and2)integrationofmulti-dimensionofrank-  ingevidences.Wenowanalyzethestrategiesandcomponents. 6.4.1ReformulationStrategies AsmentionedinSection4,wehavedifferentwaystousecon- textualinformationviacontextualqueryreformulationstrategies:  NoContext ,WholeContext ,Add-One,Drop-Out andCombined.TheWholeContext strategyincorporatescontextinformationina coarsegranularity,whilethe Add-OneandDrop-Out strategiesare ina-granularity.Weillustratethedifferentperformancewith  differentstrategiesusingtheDL2RframeworkinTable6. Wecanseethattheimprovementfrom NoContext toWholeContext isratherobvious,indicatingthatcontextinformationis quitetobetterresponsesundertheconversational  scenarios.Wealsohaveaninterestingobservationthat Add-OneandDrop-Out strategiesarebetterthanthe WholeContext strate-gy. WholeContext strategyisacoarse-grainedmethodwhichuses contextinformationwithoutdistinguishingirrelevantcontextsfrom  relevantones.Theresultsindicateaproperwaytousecontextin-  formationisimportant. Drop-Out strategyisslightlybetterthan theAdd-Onestrategy,whichtheexemptionofirrelevant contextinformationisnecessary,andinmostcases,therearemore  relevantcontextsentencesthanirrelevantones.Thecombinationof  allstrategiesperformsbestsinceitbalancesboththecoarse-grained andcontextmodeling.Wecombinethebestapproxi- mationsforallpossibleutilizationsofcontexts,avoidingexplosive  numberofcombinations. 6.4.2ComponentsAnalysis Sincewehavethreemajorcomponentsfromthemulti-dimensions ofrankingevidences,i.e., Query-Reply,Query-Posting andQuery-Context ,weexaminethecontributionsofsuchcomponents.Note  thatwecanretrieveresponsesfromthevirtualdocumentconsisting  ofa posting-reply pair,fromthe reply sideand/orthe postingside.Butwecannotdirectlyretrievetheresponseusing Query-Context .Itispointlesstorunsolelyonthe Query-Context part.Thegroupistorunonlybasedon Query-ReplyandQuery-Posting .Neitherofthetwocomponentsincorporates Query-Context information.Withoutcontextinformation,theproposedframework  mighthandlesingle-turnconversationwellenough,whilegeneral-  lymulti-turnconversationisbeyondthecapabilityofthesystem.  Withtheincorporationofthecontextualinformation,theperfor- manceofbothcomponentsgetboosted.Itisnotsurprisingthatthe combinationofallthreecomponentsyieldtothebestresults:each  componentcharacterizestheappropriatenessoftheresponsefrom  adifferentaspect,andallaspectsshouldbeintegratedforscoring. 7.CONCLUSIONS Inthispaper,weproposetoestablishanautomaticconversation systembetweenhumansandcomputers.Givenahuman-issued messageasthequery,ourproposedsystemwillreturnthecorre-  spondingresponsesbasedona deeplearning-to-respond schema.Thereare3majorcontributionsinthiswork:1)weproposeacon-  textualqueryreformulationframeworkwithrankingfusionsforthe  conversationtask;2)weintegratemulti-dimensionofrankingevi- dences,i.e., queries,postings,replies andcontexts ;3)weestablish thedeepneuralnetworkarchitecturefeaturedwithabovestrategies andcomponents.Welaunchtheconversationsystembasedona  massiverepository( ˘10million posting-reply pairs)andrunex- perimentstovalidatetheproposedparadigm. WeexaminetheeffectofourproposedDL2Rmodelwithseveral baselinesonaseriesofevaluationmetrics.Ourmethodconsistent- lyandoutperformsthealternativebaselinesinterms ofp@1,MAP,nDCG,andMRR.Furthermore,wehaveinvestigat- edfurtherexperimentsforcomponentcontributionsandstrategy  analysis.Ingeneral,contextinformationisdemonstratedtobeuse-  fulforconversations,especiallymulti-turnconversationsandalldi-  mensionsofrankingevidencesarehelpful.Thisworkopenstosev- eralinterestingdirectionsforfutureworkwithregardtoautomat- icconversationbetweenhumansandcomputers.Forinstance,we  canincorporatemoreadditionalfeaturesandmoreconversation-  orientedformulations,suchasdialogueacts,conversationallogics,  anddiscoursestructures,etc. 8.ACKNOWLEDGMENTS ThisworkissupportedbytheNationalBasicResearchProgram ofChina(No.2014CB340505).Wethankallthereviewersfortheir valuablecomments,andthankthesupportfromtheDeepLearning  PlatforminBaiduNaturalLanguageProcessingDepartment. 9.REFERENCES [1]Y.Bengio.LearningdeeparchitecturesforAI. Foundationsand TrendsinMachineLearning ,2(1):1Œ127,2009. [2]F.Bessho,T.Harada,andY.Kuniyoshi.Dialogsystemusing real-timecrowdsourcingandTwitterlarge-scalecorpus.In SIGDIAL,pages227Œ231,2012. [3]G.Cong,L.Wang,C.-Y.Lin,Y.-I.Song,andY.Sun.Finding question-answerpairsfromonlineforums.In SIGIR,pages467Œ474. [4]A.Graves,A.-r.Mohamed,andG.Hinton.Speechrecognitionwith deeprecurrentneuralnetworks.In Proc.Acoustics,Speechand SignalProcessing ,pages6645Œ6649,2013. [5]H.He,K.Gimpel,andJ.Lin.Multi-perspectivesentencesimilarity modelingwithconvolutionalneuralnetworks.In EMNLP,pages 1576Œ1586,2015. [6]R.Higashinaka,K.Imamura,T.Meguro,C.Miyazaki,N.Kobayashi, H.Sugiyama,T.Hirano,T.Makino,andY.Matsuo.Towardsanopen domainconversationalsystemfullybasedonnaturallanguage processing.In COLING,2014. [7]B.Hu,Z.Lu,H.Li,andQ.Chen.Convolutionalneuralnetwork architecturesformatchingnaturallanguagesentences.In NIPS,pages2042Œ2050,2014. [8]K.JärvelinandJ.Kekäläinen.Cumulatedgain-basedevaluationof IRtechniques. ACMTrans.Inf.Syst. ,20(4):422Œ446,2002. [9]Z.Ji,Z.Lu,andH.Li.Aninformationretrievalapproachtoshort textconversation. CoRR,abs/1408.6988,2014. [10]N.Kalchbrenner,E.Grefenstette,andP.Blunsom.Aconvolutional neuralnetworkformodellingsentences. arXivpreprint arXiv:1404.2188,2014. [11]C.-J.Lee,Q.Ai,W.B.Croft,andD.Sheldon.Anoptimization frameworkformergingmultipleresultlists.In CIKM'15 ,pages 303Œ312,2015. [12]A.Leuski,R.Patel,D.Traum,andB.Kennedy.Buildingeffective questionansweringcharacters.In SIGDIAL,pages18Œ27,2009. [13]A.LeuskiandD.Traum.NPCEditor:Creatingvirtualhuman dialogueusinginformationretrievaltechniques. AIMagazine ,32(2):42Œ56,2011. [14]H.LiandJ.Xu.Semanticmatchinginsearch. Foundationsand TrendsinInformationRetrieval ,8:89,2014. [15]J.Li,M.Galley,C.Brockett,J.Gao,andB.Dolan.A diversity-promotingobjectivefunctionforneuralconversation models.arXivpreprintarXiv:1510.03055 ,2015. [16]X.Li,L.Mou,R.Yan,andM.Zhang.Stalematebreaker:Aproactive content-introducingapproachtoautomatichuman-computer conversation.In IJCAI,2016. [17]Z.LuandH.Li.Adeeparchitectureformatchingshorttexts.In NIPS,pages1367Œ1375,2013. [18]C.D.Manning,P.Raghavan,andH.Schütze. Introductionto InformationRetrieval .CambridgeUniversityPress,2008. [19]T.Mikolov,K.Chen,G.Corrado,andJ.Dean.Efestimationof wordrepresentationsinvectorspace. arXiv:1301.3781,2013. [20]L.Mou,G.Li,L.Zhang,T.Wang,andZ.Jin.Convolutionalneural networksovertreestructuresforprogramminglanguageprocessing.  InAAAI,pages1287Œ1292,2016. [21]L.Mou,H.Peng,G.Li,Y.Xu,L.Zhang,andZ.Jin.Discriminative neuralsentencemodelingbytree-basedconvolution.In EMNLP,pages2315Œ2325,2015. [22]L.Mou,M.Rui,G.Li,Y.Xu,L.Zhang,R.Yan,andZ.Jin. Recognizingentailmentandcontradictionbytree-basedconvolution.  arXivpreprintarXiv:1512.08422 ,2015. [23]M.Nakano,N.Miyazaki,N.Yasuda,A.Sugiyama,J.-i.Hirasawa, K.Dohsaka,andK.Aikawa.WIT:Atoolkitforbuildingrobustand real-timespokendialoguesystems.In SIGDIAL,pages150Œ159. [24]E.Nouri,R.Artstein,A.Leuski,andD.R.Traum.Augmenting conversationalcharacterswithgeneratedquestion-answerpairs.In  AAAIFallSymposium:QuestionGeneration ,2011. [25]H.Palangi,L.Deng,Y.Shen,J.Gao,X.He,J.Chen,X.Song,and R.Ward.Deepsentenceembeddingusingthelongshortterm  memorynetwork:Analysisandapplicationtoinformationretrieval.  arXivpreprintarXiv:1502.06922 ,2015. [26]A.Ritter,C.Cherry,andW.B.Dolan.Data-drivenresponse generationinsocialmedia.In EMNLP,pages583Œ593,2011. [27]T.Rocktäschel,E.Grefenstette,K.M.Hermann,T.Ko cisk˚y,and P.Blunsom.Reasoningaboutentailmentwithneuralattention. arXivpreprintarXiv:1509.06664 ,2015. [28]A.SeverynandA.Moschitti.Learningtorankshorttextpairswith convolutionaldeepneuralnetworks.In SIGIR'15 ,pages373Œ382. [29]L.Shang,Z.Lu,andH.Li.Neuralrespondingmachineforshort-text conversation.In ACL-IJCNLP ,pages1577Œ1586,2015. [30]R.Socher,J.Pennington,E.H.Huang,A.Y.Ng,andC.D.Manning. Semi-supervisedrecursiveautoencodersforpredictingsentiment distributions.In EMNLP,pages151Œ161,2011. [31]H.Sugiyama,T.Meguro,R.Higashinaka,andY.Minami. Open-domainutterancegenerationforconversationaldialogue systemsusingWeb-scaledependencystructures.In SIGDIAL,pages 334Œ338,2013. [32]I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequencelearning withneuralnetworks.In NIPS,pages3104Œ3112,2014. [33]M.A.Walker,R.Passonneau,andJ.E.Boland.Quantitativeand qualitativeevaluationofdarpacommunicatorspokendialogue  systems.In ACL ,pages515Œ522,2001. [34]R.S.Wallace. TheAnatomyofALICE .Springer,2009. [35]H.Wang,Z.Lu,H.Li,andE.Chen.Adatasetforresearchon short-textconversations.In EMNLP,pages935Œ945,2013. [36]J.Williams,A.Raux,D.Ramachandran,andA.Black.Thedialog statetrackingchallenge.In SIGDIAL,pages404Œ413,2013. [37]Y.Xu,R.Jia,L.Mou,G.Li,Y.Chen,Y.Lu,andZ.Jin.Improved relationbydeeprecurrentneuralnetworkswithdata  augmentation.arXivpreprintarXiv:1601.03651 ,2016. [38]Y.Xu,L.Mou,G.Li,Y.Chen,H.Peng,andZ.Jin.Classifying relationsvialongshorttermmemorynetworksalongshortest dependencypaths.In EMNLP,2015. [39]R.Yan.i,poet:Automaticpoetrycompositionthroughrecurrent neuralnetworkswithiterativepolishingschema.In IJCAI,2016. [40]R.Yan,M.Lapata,andX.Li.Tweetrecommendationwithgraph co-ranking.In ACL ,pages516Œ525,2012. [41]R.Yan,C.-T.Li,H.-P.Hsieh,P.Hu,X.Hu,andT.He.Socialized languagemodelsmoothingviabi-directionalpropagation onsocialnetworks.In WWW'16 ,pages1395Œ1405,2016. [42]R.Yan,X.Wan,J.Otterbacher,L.Kong,X.Li,andY.Zhang. Evolutionarytimelinesummarization:Abalancedoptimization  frameworkviaiterativesubstitution.In SIGIR'11 ,pages745Œ754, 2011.[43]R.Yan,I.E.Yen,C.-T.Li,S.Zhao,andX.Hu.Tacklingtheachilles heelofsocialnetworks:propagationbasedlanguagemodel smoothing.In WWW'15 ,pages1318Œ1328,2015. [44]K.ZhaiandD.J.Williams.Discoveringlatentstructurein task-orienteddialogues.In ACL ,pages36Œ46,2014. [45]B.Zhang,J.Su,D.Xiong,Y.Lu,H.Duan,andJ.Yao.Shallow convolutionalneuralnetworkforimplicitdiscourserelation recognition.In EMNLP,pages2230Œ2235,2015.  
ProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers , pages2689Œ2698,Osaka,Japan,December11-172016. 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698  
ExploitingCollectiveHiddenStructuresinWebpageTitles forOpenDomainEntityExtraction WeiSong y ,ShiqiZhao z,ChaoZhang z,HuaWu z,HaifengWang z,LizhenLiu y,HanshiWang yyCollegeofInformationEngineering,CapitalNormalUniversity,Beijing,China zBaiduInc.,Beijing,China {wsong,lzliu,hswang}@cnu.edu.cn {zhaoshiqi,zhangchao01,wu_hua,wanghaifeng}@baidu.com ABSTRACT Wepresentanovelmethodforopendomainnamedentity extractionbyexploitingthecollectivehiddenstructuresin webpagetitles.Ourmethoduncoversthehiddentextual structuressharedbysetsofwebpagetitlesbasedongen-  eralizedURLpatternsandamultiplesequencealignment  technique.Thehighlightsofourmethodinclude:1)The  boundariesofentitiescanbeidentiedautomaticallyina  collectivewaywithoutanymanuallydesignedpattern,seed  orclassname.2)Theconnectionsbetweenentitiesarealso  discoverednaturallybasedonthehiddenstructures,which  makesiteasytoincorporatedistantorweaksupervision.  Theexperimentsshowthatourmethodcanharvestlarge  scaleofopendomainentitieswithhighprecision.Alarge  ratiooftheextractedentitiesarelong-tailedandcomplex  andcoverdiversetopics.Giventheextractedentitiesand theirconnections,wefurthershowthetivenessofour methodinaweaklysupervisedsetting.Ourmethodcan  producebetterdomainspcentitiesinbothprecisionand  recallcomparedwiththestate-of-the-artapproaches. CategoriesandSubjectDescriptors I.2.6[ ArtiialIntelligence ]:Learning| Knowledgeac- quisition ;I.2.7[ ArtiialIntelligence ]:Naturallanguage processing;H.3.1[ InformationStorageandRetrieval ]:ContentAnalysisandIndexing GeneralTerms Algorithms,Experimentation Keywords Opendomainentityextraction;Multiplesequencealignment;URL pattern;Templategeneration;Weaksupervision Thisworkwasdonewhenthetauthorwasvisiting Baidu. CopyrightisheldbytheInternationalWorldWideWebConferenceCom- mittee(IW3C2).IW3C2reservestherighttoprovideahyperlinktothe author'ssiteiftheMaterialisusedinelectronicmedia. WWW2015, May18Œ22,2015,Florence,Italy. ACM978-1-4503-3469-3/15/05. http://dx.doi.org/10.1145/2736277.2741107. 1.INTRODUCTION Anamedentity(entityforshort)isacontiguoussequence oftextualtokens,whichrepresentsthenameofanobjectof acertainclass,suchasapersonoranorganization.En-  tityextractionisakeysubtaskofInformationExtraction  (IE),andalsoafundamentalcomponentformanyNatural  LanguageProcessing(NLP)andInformationRetrieval(IR)  tasks.Forexample,relationextractionisbasedonidenti-  fyingentitiesinadvance.Wordsegmentationandparsing wouldbeimprovedifonesystemalreadyknowsthatapiece oftextisanentity.Besides,searchengineshavebeenpay-  ingmuchattentiononsemanticandknowledgedrivensearch  beyondtraditionalkeywordbasedsearchparadigm.Insuch  scenario,miningandintegratingnewandlong-tailedentities  wouldbtknowledgebaseconstructionandhelpunder-  standinguserintent. Themainchallengeofentityextractionistopreciselyde- terminetheboundariesofentities[14].Itischallenging,  especiallyforlongandcomplexentitieslikebooknames  andrestaurantnames.Theformsofthesenamesmight  bexible,containingmultiplewordsandevenotherenti- ties.Almostallexistingapproachesattempttosolvethis probleminerentways,amongwhichsupervisedmodels  arebasedonvarioussyntacticandcontextualfeaturesex-  tractedfrommanuallyannotatedtrainingdata[5,11,22],  whilesemi(weakly)-supervisedmethodsmakeuseofclass  names,seedsormanuallydesignedsyntacticpatternstoin-  ducetemplatesfordeterminingtheboundariesofentities  [12,15,26].Anotheralternativewayistomakeuseofex-  plicitstructureddatasuchasHTMLtags[1,19]andHTML  tablesofwebpages[13,17]. However,mostoftheaboveworkreliesondomainknowl- edgeorhumanlabor.Duetothenecessityoftrainingdata,  supervisedmethodsonlyworkwellonpredenedcategories [18].Semi-supervisedmethodssimplifythetrainingprocess, butdon'tgetridofpriorknowledgeeither.Themanualla-  borscaleswiththenumberofclassesofinterest.Because  HTMLpatternsareusuallywebsitespecic,itisimpossi-  bleforpeopletoenumerateHTMLpatternsorsetseeds  foreveryrelevantwebsite.Therefore,searchenginesand  seedsareusuallyneededforretrievingwebpagesandinduc-  ingwrappers[19,31].Besides,someopenentityextraction  methodsareproposed[14,21],whichattempttoextract  entitieswithouthumanintervention.Butexistingsystems  commonlyutilizelanguagedependentheuristics,whichare  diulttobeappliedtootherlanguages. Inthispaper,weproposeanopenentityextractionmethod, whichextractsentitiesandtheirconnectionsfromtheweb Figure1:Webpagetitlealignmentforinducingtem- plateswithsubstitutableslots. withoutrequiringpriorknowledge,manuallabororsophis- ticatedNLPtechniques,andislanguageindependent. Thekeypointisthatweinducetemplatesandextract entitiesbycollectivelyidentifyingandaligningwebpageti-  tleswiththesamehiddentextualstructure.Ourmethod  isbasedonthefollowingobservation: Parallelwebpage titlesthatsharethesamehiddentextualstructures maycontaincertainclassesofentities .Theterm hid- den heremeansthatthereisnoexplicitstructureinan isolatedwebpagetitle,butifwealignthesetitlestogether, thecommonstructuresamongtitlesemergenaturally,such  astheexampleshowninFigure1.Wecanseethatentities  usuallyplayaroleassubstitutableslotlerswithinthese  titles.Therefore,thetasktodeterminetheboundariesof  entitiesisactuallycastedtothetaskofinducingtemplates  withinparallelwebpagetitlesandidentifyingsubstitutable  slotsinthetemplates. Althoughtheobservationisobvious,themainchallenges aretwo-fold:howtoidentifyparallelwebpagetitleswith  thesamehiddenstructuresandhowtoinduceandutilize thehiddenstructuresforentityextraction. Forthetchallenge,weidentifywebpagetitleswiththe sameorsimilarhiddenstructuresbyexploiting generalized URLpatterns .Wenoticethatwebpagetitleswithsimilar URLmorphologytendtosharesimilartextualstructures. Basedonthisstrategy,wecancollectlargescaleofparallel  webpagetitles,whichareusedasthedataresourceforentity  extraction. Forthesecondchallenge,weapplytheideaof Multi- pleSequenceAlignment (MSA)toalignthesewebpage titlesforinducingtemplatesautomatically.Then,weuti-  lizesearchengineuserbehaviorsandheuristicconstraints  toteroutinvalidtemplateswhichfailtoproducevalid entities.Finally,foreachtargetwebpagetitle,werankcan- didatetemplatesbasedontheprobabilitythattheycould  correctlydeterminetheentityboundaryinthistitleand  choosethemostappropriateoneforentityextraction. Tothebestofourknowledge,wearethetoexploit webpagetitlesforentityextractioninanunsupervisedman-  ner.Figure2showstheworkingwchartoftheproposed  approach.Thesystemtakeswebpagesasinputandnoad-  ditionalmanuallaborisnecessary.Thesystemoutputin-  cludesextractedopendomainentitiesandtheirconnections.  Theconnectionsareformedbyrecordingfromwhichtem-  plateseachentityisextracted.Sincetheentitiesextracted  bythesametemplateareoftenofthesameclass,thecon- nectionsrevealtopicrelatednessamongentitiesandmakeit easytoincorporatedistantorweaksupervision. Insummary,themaincontributionsofthispaperinclude: Weexploitthehiddentextualstructuresinwebpageti-  tlesforopendomainentityextraction.Weproposeal- gorithmsforbothdiscoveringandutilizingthesestruc-  tures,whichmakeourmethodgetridofmanuallabor. Wedemonstrateexperimentallythat:1)Ourmethod  canextractlargescaleofopendomainentitieswith  highprecisionandtopiccoverage,manyofwhichare  long-tailedentities.2)Basedontheextractedentities  andtheirconnections,wecaneasilyincorporateweak  supervisionforminingmoreaccurateandcompletedo-  mainspecicentities.Inbothsettings,ourmethod outperformsthestate-of-the-artapproachesbasedon searchenginequerylogs. Therestofthispaperisorganizedasfollows.InSec- tion2,weproposeourmethod.Weshowourexperiments  andresultsonopendomainentityextractioninSection3,  anddemonstrateitsivenessondomainspcentity extractioninSection4.Weintroducetherelatedworkin Section5andconcludethispaperinSection6. 2.PROPOSEDMETHOD 2.1Motivation Ourideacomesfromthefollowingsimpleobservations: Thereareextensiveentitiesofvarioustopicsexisting inwebpagetitles,especiallyfromverticaldomainweb-  sites. Thewebpagetitles,whicharefromthesamewebsite  andcontainthesameclassofentities,havesimilar  hiddentextualstructures. Withinthetextualstructures,entitiesplayaroleas  substitutableslotlers. Numerouswebpagesareaboutentities.Becausewebpage titlessummarizethekeyconceptsofthecontent,ifaweb-  pageintroducesanentity,itisnaturalthatthetitlecontains  theentity.Consideringtheredundancyoftheweb,forone  entity,itisquitelikelythatthereisatleastonewebpage  titlecontainingit.Therefore, webpagetitlesarevaluablere- sourcesforentityextraction .Thesecondandthethirdobservationsarealsounder- standable.Thewebpagesintroducingthesameclassesof entitiesortheirattributesareoftenorganizedinthesame waywithinawebsite.AsshowninFigure1,itiseasytosee  thatthreetitlessharethesamehiddenstructure: <moviename> (<digit> )-PlotSummary-IMDB Thehiddenstructureformsa template .Thedlike <moviename> or <digit> issubstitutable,notedasa slot .The sloters arethetextfragmentswhichcouldbe placedattheslotpositionsandexpectedtobesomeclass  ofentities.Thisphenomenonbecomesobviouswhenthese  titlesarealignedtogether.Motivatedbythis,weaimto  learnanextractorbyaligningwebpagetitleswiththesame  hiddentextualstructurescollectively. 2.2GeneralizedURLPatterns AgeneralizedURLpatternisaregularexpressionwhich couldrecognizeasetofURLs.ThewebpagetitlesofURLs whichcouldberecognizedbythesamegeneralizedURL  patternarenamedasparallelwebpagetitles.Weexpect Figure2:Thewchartoftheproposedsystem. thatparallelwebpagetitlesshouldhavethesameorsimi- larhiddentextualstructures.Theexamplebelowshowsa  generalizedURLpatternand2URLinstances. Pattern :http:==www:imdb:com=title=tt nd+=$URL 1: http:==www:imdb:com=title=tt 0000001=URL 2: http:==www:imdb:com=title=tt 0000002=ExploitinggeneralizedURLpatternsforidentifyingparal- lelwebpagetitlesismotivatedbytheideathattopicrelated  webpagesoftenformclustersandsharethesameURLpat-  terninawebsite[6,32].WelearnthegeneralizedURL  patternsfromalargescaleURLdatabase.Theprocessis  asfollows: Step1:Segmentthenon-domainpartofeachURL  with\/".WewillgeneratecandidateURLpatternsby  replacingonesegmentwitharegularexpressioneach  time.Forexample,thecandidatepatternsforURL1  arehttp:==www:imdb:com=REG=tt 0000001=$and http:==www:imdb:com=title=REG= $,where REG is aspecialsymboltorepresentregularexpressions. Step2:Todeterminewhetherasegmentshouldbe generalized,weaccumulateallcandidatepatternsover  theURLdatabase.Theoneswithafrequencyabove  athresholdareretained,suchas  http:==www:imdb:com=title=REG= $.Step3:Foreachretainedcandidatepattern,theal  generalizedURLpatternisgotbyreplacingthesymbol  REGwitharegularexpressionbasedontheersof thissegment.TheresultingpatternforURL1becomes  http:==www:imdb:com=title=tt nd+=$.GivenonegeneralizedURLpattern,wecangetcorre- spondingparallelwebpagetitleswhichsharethesameor  similartextualstructures.Thesestructuresarediultto  bediscoveredifwedealwitheachtitleseparately.Wewill  showthestructurescouldbediscoveredautomaticallyby  aligningthemtogetherinacollectiveway. 2.3OpenDomainEntityExtraction GivenparallelwebpagetitlesofageneralizedURLpat- tern,weaimtogeneratetemplatestouncoverthehidden textualstructuresforentityextraction. 2.3.1MSAbasedTemplateGeneration Weviewparalleltitlesasasetofwordsequencesandcon- ductalignmenttoinducetemplates.Thetextualstructures  ofparalleltitlesinrealdataoftenhavevariations.There  mightbemultipletemplates,eachofwhichmatchesasub-  setoftheexaminedparalleltitles.Therefore,weadapt theNeedleman-Wunschalgorithm[23]forpair-wisealign- menttoinduceallpossibletemplates. PairwiseAlignment .Wenetheelementsetas= VSfg,where Vreferstothewordvocabulary,and\ "isusedtorepresentanindel(suppose\ "62V).Giventwo wordsequences A2V and B2V ,analignmentcanbe representedasa2-dimensionalarray Align A;B2Ithatevery wordinonesequenceisalignedtoonewordintheother sequenceortoanindelwhichiscausedby inserting aword intoonesequenceor deleting awordfromtheother. Iisthe numberofalignedelementpairs.A substitutionmatrix Risusedtoassignalignmentscores,inwhich R(a;b )represents thealignmentscorebetweenapairofelements( a;b )2Awellalignedpairwillberewardedwithahigherscore.We  neanoverallscorefunction AlignScore (A;B )asthesum ofthescoresoverallalignedelementpairsintwosequences: AlignScore (A;B )= IXi=1R(Align A;B0;i;Align A;B 1;i)(1) Therefore,thetask,asshowninEquation2,istodan optimalalignmentwiththebestoverallscore. Align =argmax Align A;B2IAlignScore (A;B )(2) Toachievethis,weshouldtneapropersubstitute matrix R.Indetail, R(a;b )isnedasfollows. ExactMatch: a2and b2areexactmatchedif a==b.e R(a;a )= ,> 0,sothatidentical matchisawarded.Iftwowordsarebothdelimiters, theyareconsideredasanexactmatchaswell. Mismatch:Includetwotypes:normalwordmismatch  andgappenalty.If a2V and b2V areerent normalwords,ne R(a;b )= ,< 0.If a2V and b==\ ",dene R(a;b )= d,d< 0.ForbidenMatch:Anormalword aisnotallowedto bealignedwithadelimiter b,forwhichwedene R(a;b )= .Weconstructadelimiterdictionarymanuallyincluding \[",\ j",\-"andsoon.Thedelimitersandnormalwords togetherformthevocabulary V.Wespeciallyconsiderde- limiters,becausedelimitersareoftenusedforsegmenting derentblocksintitles.Generally,delimitersshouldn'tbe  partofanentitysothattheyshouldn'tbealignedwithnor-  malwords.Normalwordmismatchesarepenalizedbyas-  signinganegativescore,whilegapmismatchesareactually topenalizelongdistancealignment. Giventhesubstitutematrix R,wecomputeamatrix FbydynamicprogrammingasdescribedinAlgorithm1.The entriesof Fgivethemaximumscoresamongallpossible alignments.Oncethematrix Fiscomputed,webacktrack thecomingpathstartingfromthebottomrightcornerof Fandgettheoptimalalignment. Algorithm1 ComputingScoringMatrixforAllPossible Alignments Input:Sequences Aand B;SubstitutionMatrix Randgappenalty d;Output: ScoreMatrix F;1:/*Initializematrix F(length (A)+1)(length (B)+1)witha lineargappenalty.*/ 2:fori=0to length (A)do3:Fi;0=id;4:endfor 5:forj=0to length (B)do6:F0;j=jd;7:endfor 8:/*Computeoptimalscores.*/ 9:fori=1to length (A)do10: for j=1to length (B)do11:MATCH   Fi1;j1+R(Ai;B j);12:DELETE   Fi1;j+d;13:INSERT   Fi;j1+d;14: Fi;j=max fMATCH,DELETE,INSERT g;15: endfor 16: endfor 17: return F;TemplateInduction .Givenapairwisesequencealign- ment,wecaninduceatemplateforeachsequenceaccord- ingly.Wereplaceconsecutivemismatchwordsineachse-  quencewitha <SLOT >symbolwhichrepresentsatemplate slot.Thematchedpartsareconsideredasthebackboneof  thetemplate.Ifonetemplatedoesnotcontain <SLOT>or thetemplateitselfis <SLOT >,itisdiscarded,becausethis indicatestwosequencesarethesameorcompletelydierent.  Anexampleisshownbelow. Giventwosequences A=\ w1[w2w3]",B=\ v1(w2w3)"respectively,thealignmentresultandcorrespondingtem- platesare: A:w1[w2w3]B:v1(w2w3)+Template1: <SLOT >[w2w3]Template2: <SLOT >(w2w3)Weaccumulatealltemplatesgeneratedbypairwisealign- mentandremovethetemplateswithlowfrequencies. Figure3:Userbehaviorbasedslotselection,in whichwecanseethattheslotformovienamesisse- lectedasthekeyslot,sinceitmatchesusers'search  interestwhenclickingonthecorrespondingtitles. UserBehaviorbasedSlotSelection .Someofthe retainedtemplatescontainmultipleslots.Suchastheex- ampleshowninFigure1,theslotlersformovienames arewhatwereallywant.Toselecttherightslotinatem-  plate,whichistermed keyslot inwhatfollows,wetake advantageofuserclick-throughbehaviorsinsearchengines.  Ourassumptionisthatthekeyslotofatemplateismore  likelytomatchusers'searchinterest.Inotherwords,for  titlesmatchedbythetemplate,theersofthekeyslotare  morelikelytomatchusers'searchqueriesthatclickonthese  titles.AmotivatingexampleisshowninFigure3. Weconsidertemplatesanduserqueriestogetherthrough clickedURLsinquerylogs.Supposeatemplate Thas mslots.Onetitle xismatchedby Tandthecorresponding slotersare ff1(x);:::;f m(x)g,where fi(x)istheslotter ofthe ithslot siofthetemplate T.Ifthetitle xisclicked byasearchersubmittingquery q,atitle-querypair( x;q )forms.Byaccumulatingalltitle-querypairs f(x;q )grelated totemplate T,theweightofthe ithslot sitouserqueries couldbemeasuredas: weight (si;T )= X(x;q)Jaccard (fi(x);q )(3) where Jaccard (:;: )isthejaccardsimilaritybetweentwo wordsets.Wechoose s=argmax iweight (si;T )asthekey slotoftemplate T,if weight (s;T )>0.Soifonetemplate hasmorethanoneslot,weusealltemplateslotsforgen-  eralization,butextractslotlersascandidateentitiesonly  fromthekeyslot.Thisconstraintaskshelpfromsearchers  toguaranteethequalityofentities. Templatering .Wefurtherteroutinvalidtem- platesbyassessingthequalityofextractedcandidateenti-  ties.Tothisend,wetidentifyhigh-condentvalidand  invalidentitysetsinaheuristicwayandthendesigntwo  indicatorsfortemplateering. Lexiconbasedindicator:Accordingtoourobservation,  invalidcandidateentitiesareoftenextractedfromthe  titlesofnewsorforumwebpages.Toteroutsuch  candidates,weconstructastopwordlistincludingin-  terrogativewordsandcolloquialwordslike\where", \who",\ah"andsomepunctuation.Ifacandidateen- titycontainsthesewords,itisconsideredasinvalid. Redundancybasedindicator:Weureoutasetof  validentitiesbasedonredundancy.Acandidateentity ismorelikelytobevalidifitappearsmanytimeson theweb.Inthiswork,acandidateentityisconsidered asvalidifitcanbeextractedmorethan5timesby  dittemplates. Finally,atemplateisconsideredasavalidtemplate,if itcouldproduceatleast Jvalidentitiesdeterminedbythe redundancybasedindicatorandlessthan Kinvalidentities determinedbythelexiconbasedindicator. 2.3.2TemplatebasedEntityExtraction Weapplythegeneratedvalidtemplatesforentityextrac- tion.Oneissueisthatonetitlecouldbematchedbymore  thanonetemplateatditgranularity.Forexample,the  titlebelowcanbematchedbythefollowingthreetemplates. Title :StirFriedBeefRecipe-Websitename Template1 :<SLOT >Recipe-Websitename Template2 :<SLOT >-Websitename Template3 :<SLOT >BeefRecipe-Websitename Tochoosethebesttemplateforeachtitle,weviewit asatemplaterankingproblem.Formally,giventhetitles  X=fx1;:::;x jXj gforoneURLpattern,andatemplate setT=ft1;:::;t jTj gassociatedwith X,wehavetoselect thebesttemplate t2T xforeachtitle x2X forentity extraction,where Txisasubsetof Tthatcanmatch x.Atglance,somesimpleheuristicscouldbeusedfor ranking: Common-First:Choosethetemplatewiththebest  coverageoveralltitles. Alignment-First:Choosethetemplatewiththebest  alignmentscorewiththetitle. Theheuristicwillworkformosttitles,butmaypro- duce super-entities insomecases.Forexample,applying template2intheaboveexamplewillextract\ StirFriedBeef Recipe "asacandidateentity.Incontrast,alignmentrst heuristicshouldbepreferredforextractingcleanentities.  Butsometimes,itmayresultin sub-entities suchas\ StirFried "whichisextractedbyapplyingtemplate3.Suchtem- platesaregeneratedwhentwoentitieswithcommonwords  happentoappearinpairedtitles. Tobalancebetweenthetwoextremes,weproposean adaptiveranking(AdaRank) functioncombiningboth alignmentpreferenceandpopularity.Giventitle x,theprob- abilityofchoosing t2T xastheperfecttemplatecanbe representedas: p(tjx)= p(xjt)p(t)p(x)(4)p(xjt)representsthelikelihoodthattitle xisgeneratedus- ingtemplate t.Here,weapproximateitusing 1alRank(t;x)ˆ,where alRank (t;x )istherankoftemplate tamong Txac- cordingtoalignmentscores.Thismeansthattemplateswith  largeralignmentscoreswiththetitlearemorelikelytogen-  eratethetitle. ˆisapositivevaluetocontrolthepunish- mentdegreeforthelowranktemplates.Alargervalueof ˆfavorstoprankedtemplatesmore. p(t)canbeseenasthe priorprobabilityoftemplate tbeingagoodtemplate.We estimate p(t)basedontheglobalalignmentinformation: p(t)= #(alRank (t;)==1) jXj(5) where#( alRank (t;)==1)representsthenumberoftitles inXthattemplate thasthelargestalignmentscorewith them.Accordingtotheprobabilityrankingprincipleand  removingconstantfactors,wecomputetherankingscoreof  thetemplate tforthetitle xas:rank (tjx)#(alRank (t;)==1) alRank (t;x )ˆ(6) Insummary,AdaRankintegratesboththelocalalignment scoreandtheglobalscoreofatemplateandachievesthebest  balancebetweenthem.Itprefersthetemplateswithboth goodalignmentscoreandacertainsize.BasedonEquation 6,weusethetop1rankedtemplateforentityextractionand  storethepairoftheentityandthetemplateforextracting  it.Thesameprocedureisappliedtoalltitlesassummarized  inAlgorithm2. Algorithm2 ExtractingEntitieswithTemplates Input:Asetofwebpagetitles XofcertainURLpattern; Asetoftemplates T;Output: Asetofentity-templatepairs, ET ;1:Initiate ET =;2: for each xinXdo3:Gettemplates Txwhichcanmatch x;4:Rank Txtogetthebesttemplate tfor x;5:Extracttheslotler efromxwhichsthekeyslot oft;6:Add( e;t )to ET ;7: endfor 8: return ET ;3.EVALUATINGOPENDOMAINENTITY EXTRACTION Inthissectionweevaluatethequalityofextractedenti- ties.Wetdescribethedata,thenintroducethebaseline  ourmethodiscomparedwith.Finally,wereporttheex-  perimentalresultsonsystemcomparisonsandtakeadeep  insightintotheextractionresults. 3.1ExperimentalSettings 3.1.1Data Thedataweusedforexperimentsincludequerylogsand awebpagedataset.WeusequerylogsfromBaidu,which isthelargestChinesesearchengineintheworld.Inall,  therearemorethan1.9billionqueriesandcorresponding  userclickedURLs.Inourwork,thequerylogsareusedfor selectingkeyslots.Whilethebaseline(seeSection3.1.2) extractsentitiesdirectlyfromthequeries. erentmethodsextractentitiesfromerentresources. Weconstructedawebpagedatasetaccordingtothescaleof  querylogs.First,generalizedURLpatternswerelearned  usingthestrategydescribedinSection2.2on30billionweb-  pages.Then,weretainedgeneralizedURLpatternswhich Table1:Statisticsofthedata. Webpagedataset Volume #generalizedURLpatterns 918,541 #webpages 11,056,478,017 querylogs Volume #queries 1,929,617,042 couldrecognizeatleastoneuserclickedURLinthequery logs.Finally,wecollectedallwebpageswhoseURLscan berecognizedbyoneoftheretainedgeneralizedURLpat-  terns.Weonlyusedthetitlesofthesewebpagesforentity  extractioninourmethod.Thestatisticsofquerylogsand webpagedatasetareshowninTable1. BecausewedealwithChinese,allwebpagetiltesareseg- mentedintowordsforfurtherprocessingsuchassequence alignment.NotethatsinceonlineencyclopediashaveURL  patternsaswell,ourmethodcouldexhaustentriesfrom  them.Toeliminatethistofthesewebsites,ourmethod  doesn'tuseURLpatternsofthreemainChineseonlineency-  clopedias:BaiduBaike 1,HudongBaike 2andChineseWikipedia 3.3.1.2Baseline Themostcommondataresourcesforentityextractionare webdocumentsandquerylogs.Extractingopendomainen-  titiesfromwebdocumentswithoutanysupervisionishard  andcomputationallyexpensive.Asreportedinboth[21]and  [25],entityextractionfromquerylogsoutperformssystems  basedondocuments.Inthiswork,weaimtoshowthat  byexploitingthehiddentextualstructures,entityextrac-  tionfromwebpagetitlescanachievesuperiorperformance  comparedwithapproachesbasedonquerylogs. Weimplementedamethodwhichextractsopenentities fromqueries[21],notedasOEQ.WeadaptOEQtoChi-  nese:1)Weextractalltextfragmentssegmentedbyblank  charactersfromqueries,sinceanentityshouldnotcross 2fragmentsseparatedbyblankcharactersinChinese.2) Weextractall k-gramsintextfragmentsandcounttheir frequencies.3)Weretainall k-grams,iftheysatisfythe representationandstand-aloneconstraints.Representation  constraintisthatthenumberofentityoccurrencemustex-  ceedathreshold.Stand-alonemeansthatoneentityformsa  querybyitself,andstand-aloneconstraintisthattheratio  ofanentityoccurringinastand-alonewaymustexceeda  threshold.Referto[21]formoredetails. 3.1.3Parameters Ourmethodhasseveralparameterstobeset.Duetothe largesizeofdataset,weselectasmallportionofdatatotune  theparameters.Forpairwisealignment,wenethesub-  stitutematrixbysetting =2, =1and d=2.The templatesmatchinglessthan20titleswereeliminatedafter  templateinduction.Fortemplateering,wesetparameter  Kto20,and Jto30.Foradaptivetemplateranking, ˆis setto2. Forthebaseline,wesetthethresholdforstand-alonescore to0.1whichisthesamein[21],andtriedseveralvaluesof  representationscorethreshold.Wewouldreporttheper- 1http://baike.baidu.com/ 2http://www.baike.com/ 3http://zh.wikipedia.org/ Table2:Performanceonopenentityextraction. Method #Entity Precision Coverage OEQ(10,0.1) 12419342 0.62 0.31 OEQ(100,0.1) 930133 0.67 0.025 Ours 12375492 0.81 0.43 formancewhenitissetto10,withwhichOEQproduces asimilarscaleofentitiestous.Thequantityofproduced  entitiesreducesastherepresentationscorethresholdgets  larger.Wealsoreporttheresultswhenthethresholdisset  to100asareference. 3.2ExperimentalResults 3.2.1OverallPerformance PrecisionandCoverage .Werandomlysampled500 entitiesproducedbyeachsystemrespectively,andasked twoannotatorstojudgewhethertheextractedonesarecor-  rect.Theannotationstandardisaccordingto[28].Enti-  tiesthatarenamesofspcobjectsorgeneralconcepts  suchas\ .ã4ÿ#~ (low-carboneconomy) "areallconsidered ascorrect.Inner-annotatoragreementmeasuredbyKappa  coent[7]is0.79.Weadopt Precision asanevaluation metrictomeasuretheratioofcorrectentities,whichisde-  edas #correctextractedentities #allextractedentities .Becauseitisimpossibleto evaluatetherecall,weevaluatethe Coverage bycompar- ingthesystemoutputsagainstentriesinBaiduBaike|the  largestChineseonlineencyclopedia,whichhasmorethan 6.5millionentries.Wedidn'tconsiderentityaliassothat the Coverage ismeasuredbasedonexactstringmatching. Table2showstheexperimentalresults.Wecanseewhen producingsimilarquantityofentities,ourmethodhashigher  precisioncomparedwithOEQ.Thisindicatesthatourmethod  couldproducelargescaleofentitieswithanacceptablepre-  cision.OEQachieveshigherprecisionwhensettingalarger  threshold(100)forrepresentationscoreatthepriceofa  sharpdecreaseofcoverage.Buttheprecisionisstillob-  viouslylowerthanourmethod.Oncoverage,ourmethod  outperformsOEQaswell.Becausebothmethodscouldbe  appliedtolargerdataset,thecoveragecouldbefurtherim-  proved. TopicAnalysis .Weexaminethetopicdistributionof extractedentitiestoseewhethertheycoverawiderange  oftopics.Tothisend,weextractallentitieswithcategory  tagsfromBaiduBaike.Indetail,BaiduBaikemaintains  anentitycategorytaxonomy,basedonwhichentitiescan  beassignedacategorytagbyusers.Thetaxonomycon-  tains23generaltopicalcategoriesand90sub-categories.In  thisexperiments,ifaBaiduBaikeentityalreadyhasasub-  categorytag,wewillautomaticallyassignitshigherlevel  generalcategorytoit.Inthisway,wecollectedabout1.1  millionentitiesinBaiduBaikewithgeneralcategorylabels.  InFigure4,theblackbarsshowthenumberofentitieseach  categoryhas.Wecanseethedistributionisunbalanced, person(PER),literaturework(LITERATURE)andorga- nization(ORG)arethemostpopularcategories. Amongthe1.1millionentities,morethan0.59million entitiescouldbeextractedbyourmethod.Thedistribution  overcategoriesofourextractedentitiesareshowninFigure  4aswell.Wecanseethatthedistributionisconsistentwith  thatoftheentitiesinBaiduBaike.Thecoverageranges Figure4:Entitycategorydistributions. Figure5:Numberofentitieswitherentfrequen- ciesinquerylogs. from41%(onLITERATURE)to74%(onSPORT)across allcategories.Thisindicatesthattheentitiesextractedby  ourmethodcoverdiversetopics. FrequencyAnalysis .Weanalysethefrequenciesofour extractedentitiesinquerylogs.Figure5illustratesthe numberofentitieswithdierentfrequencies.Wecansee thatabout9millionentitiesneveroccurinquerylogswhich  makeup73%ofallentities.Thisindicatesthatourmethod  isgoodatextractinglong-tailedentitiesthatareunableto  beextractedfromqueries. LengthAnalysis .Discoveringcomplexmulti-worden- titiesisimportantandchallenging.Thusweanalyzethe  length(thenumberofwords)oftheextractedentities.Fig-  ure6showsthedistributionofentitiesextractedbyOEQ  andbyoursacrossdintlengthintervals.Wecanseethat  OEQextractsmoresimpleentitieswithlengthnolonger  than3,whileourmethodextractsmorelongentities.From  theoutputsofourmethodandOEQ,wesampled100en- titieswhoselengthsarelongerthan10andevaluatedthe precision.Ourmethodachievesa Precision of77%,while OEQgetsa Precision of52%.ManyerrorsresultedbyOEQ arelongqueriesrevealingsomehotinformationneed.Al-  thoughtheyfrequentlyappearonweb,theyarenotentities. ErrorAnalysis .Wealsoanalyzetheincorrectentities extractedbyourmethod.Theerrorscouldbecategorized Figure6:Numberofentitiesacrossrentlength intervals. into3types:1)Toogeneralclasses.Theextractedentities playanavigationalfunctioninwebpagetitlesandusually arerelatedtogeneralinformationneeds,suchas\ (a ,´˙· (essaysaboutpatriotism) ".2)Containingentity attributeorintent.Theextractedcandidatesdescribeat-  tributesoraspectsofentities,suchas\ (Attractions inBeijing) ".3)Usergeneratederrors.Somewebpagetitles aregeneratedbyusers.Insomecases,althoughthetitles  sharethesamehiddenstructure,theslotlersarenoisy  duetoinformalityofusers'edit. Discussion .Basedontheaboveanalysis,weshowthat ourmethodcouldextractmassive,long-tailedentitieswith  highprecisionandtopiccoverage.Theseentitieshaveunique characteristicscomparedwiththeonesextractedfromsearch queriesandthosealreadyexistinginencyclopedias:1)The  entitiesextractedfromwebpagetitlesaremoreformal.For  example,wecanextractcompletefullnamesofobjectssuch  asbooks,researchpapersandorganizations.Incontrast,  manyentitiesextractedfromqueriesareshortenformsor  aliasoffullnames.2)Largescaleofentitiesarelong-tailed.  Lesspopularentities,whicharediulttobeextracted  basedonredundancy,canbeextractedbytemplatesin-  ducedbasedonhiddenstructures.3)Entitiesofsomee-  graineddomainscanbeextracted.Forexample,wecan  dnamesofrareplants,becauseourmethodcanautomat-  icallydiscoverandextractentitiesfromverticalwebsites. Theselong-tailede-grainedentitiesarepotentiallyuseful fordevelopmentofverticalapplications,forwhichweexpect  toexhaustallentitiesbelongingtotheapplicationdomain.  Ontheotherhand,althoughtheentitiesinencyclopedias  coverawiderangeofdomains,thecoverageonlong-tailed  entitiesofcertaindomainsisfarfromenoughforsatisfying  therequirementofpracticalapplications. Ontheotherside,wethatourmethoddoesnotwork wellonextractingcommonconceptssuchas climate .The reasonmaybethatfewwebsitecollectsandorganizescom-  monconceptstogether. 3.2.2EvaluatingGeneralizedURLPatterns Werandomlysampled100fromtheretainedgeneralized URLpatternsintroducedinSection3.1.1andjudgedwhether  thecorrespondingtitleshavehiddenstructures.Theresults  showthat78outof100generalizedURLpatternshavehid-  denstructures(thoughtheymightnotproducecorrectenti- Table3:Performanceontemplateranking. Method Precision@1 Common-rst 0.79Alignment-rst 0.84AdaRank 0.93ties).Theparallelwebpagetitlesthathavenohiddenstruc- turecanbedividedinto2categories:1)Alltitlesarethe same.2)Alltitlesaretotallydirent.Fortunately,ourex-  tractorcandealwithbothcasessothattheycannott  entityextraction.26amongthe78generalizedURLpat-  ternshaveasinglehiddenstructureeach,whiletheothers  havemorehiddenstructures.Thisanalysisindicatesthat  generalizedURLpatternsareusefulforidentifyingwebpage  titleswithhiddentextualstructures. Aftertemplateering,92,318generalizedURLpatterns remain,whichaccountfor10%ofall.Eachremainedgen-  eralizedURLpatternhas24entityextractiontemplateson average.However,themostfrequenttemplateforeachpat- terncancovernearly73%webpagetitles.Thenumberin-  creasesto93%,ifwetaketop-5templatesintoaccountfor  eachURLpattern.Thisfurtherrevealsthatwebpagetitles  withthesameURLpatterntrulyhavehighlysimilarhidden  textualstructures. 3.2.3EvaluatingEntityExtractor EvaluatingTemplateGeneration Dataandmetrics .Herewealsoevaluatetheentityextrac- tiontemplates.Spcally,weevaluatewhetherwehave correctlydetectedthekeyslot.Forthispurpose,weran-  domlysampled200templates,andaskedtwoannotatorsto  labeltheselectedkeyslotascorrectornot.Tofacilitatethe  annotation,wealsoprovide10lersforeachslotineach  template,sothattheannotatorscanjudgewhethereach  examinedslotcanbeusedtoextractcorrectentities.We  use Precision asanevaluationmetric,whichisnedas #correctlyselectedkeyslots #selectedkeyslots .Theexperimentalresultsshowthatourmethodachievesa Precision of81.5%.Thisindicatesthatuserbehaviorbased slotselectionistiveforselectinghighqualityslotsin  templatesforentityextraction. EvaluatingTemplateRanking Dataandmetrics .Werandomlysampled50generalized URLpatternswithmorethan10entityextractiontemplates  aftertemplategenerationphase(Section2.3.1).Fromeach  URLpattern,wesampled10titleswhichcouldbematched  byatleast2templates.Theresultingdatasetconsistsof500  titleswhichhave16templatesonaverage.Foreachtitle,  werankthetemplatesusingthreerankingstrategiesintro-  ducedinSection2.3.2:Commonrst,Alignment-rstand  AdaRank.Weuse Precision @1asametrictoseewhether thetoponetemplatecancorrectlydeterminetheboundary  oftheentityforeachtitle. Results. Table3showstheresults.Wecanseethat AdaRankobviouslyoutperformstheothertwostrategies.  Thisindicatesthatconsideringbothlocalalignmentand  globalpopularityistiveforrankingentityextraction  templates.Alignment-rstheuristicperformsbetterthan commonrst.Allerrorsofalignment-rstheuristiccome  fromincorrectwordalignmentinmulti-wordentities,in  whichsinglewordswithintwoentitiesarealignedsothat  entitiesareincorrectlysplitintofragments. 4.INCORPORATINGSUPERVISION Theextractedentity-templatepairscanbeviewedasa dataresourceformorespecictasks.Actually,theentity- templatepairsformabipartitegraph.Therefore,thedis-  tantsupervision(e.g.existingentriesinknowledgebases)  andweaksupervision(e.g.manuallyprovidedseedswith  labels)canbeeasilyincorporatedbypropagatinginforma-  tionthroughthisgraph.Inthispaper,weapplythisdata  resourceforsetexpansionofdomainentities. 4.1SetExpansionofDomainEntities Wefocusonevaluatingitinaweaklysupervisedsetting asin[26]: afewseeds Sofadomainisgiven,the systemshouldreturnmoreentitiesofthesamedo- main. Thissettingisusefulforknowledgebaseexpansion basedonavailableresources. Wegenerateasubsetofentitiesthatarepotentiallyrele- vanttotheseedsfromtheentity-templatepairswehaveex-  tractedandthenranktheseentities.Thisprocessinvolves  thefollowingsteps: Step1:Extractalltemplates Tthatcontainatleast oneoftheseeds. Step2:Buildabipartitegraph G=fT[E ;Lg,where Erepresentsthesetofentitiesthatcanbeextracted bytemplatesin T.Ifanentity e2E isextractedbya template t2T ,thereisanedge l2L betweenthem. Step3:Aniterativealgorithmisadoptedtoranken- titiesinthisgraph. Weuse TScore (t)torepresenttherelevanceofatemplate t,use EScore (e)torepresenttherelevanceofanentity e,anduse Pri (e)torepresentthepriorrelevanceof etotheex- amineddomain.Wesetthe Pri (e)forseednodesas 1jSjand0forallotherentities.Theiterativealgorithmismotivated  bythepersonalizedPageRank[20].Itstartsbyinitializing the TScore 0(t)foralltemplatesandthe EScore 0(e)forall entitiesas0.Thenineachiteration,therelevancescores ofentitiesandtemplatesinthisgraphareupdatedalterna-  tivelyasfollows. EScore i+1(e)= ri (e)+(1 )Xtp(ejt)TScore i(t)(7) TScore i+1(t)=(1 )Xep(tje)EScore i(e)(8) where p(ejt)istheprobabilitythatatemplate textractsan entity e.If textracts e,p(ejt)= 1#entitiesextractedbyt ,other- wisep(ejt)=0.Similarly, p(tje)issetto 1#templatesthatextracte ,iftextracts e,otherwise p(tje)=0. isarealvaluebetween 0and1foralinearcombinationofthepriorrelevanceofan  entityandthesumoftherelevancescoresofthetemplates  thatcanextractit.Thealgorithmwilliterateforaprede-  ednumberofiterations. Wecandfromthewholeprocessthatthehiddenstruc- turesrepresentedbytemplatesplayimportantrolesforprop-  agatinginformation. 4.2ExperimentalSettings 4.2.1Baseline Wecompareourdomainspecicentityexpansionmethod withthemethodproposedin[26],whichisthestate-of-the- artweaklysupervisedmethodfordomainspecicentityex-  tractionbasedonwebsearchlogs,notedasWSLOG.This  algorithmmakesuseofseedsofagivendomaintodpat-  ternsfromqueriesofasearchengineandfurtherminesand  ranksentitiesmatchingthepatterns. 4.2.2DataandMetrics Weconductedexperimentson10domainsthatarethe sameasthoseexaminedin[26].Foreachdomain,5seedsare  given.Were-implementedWSLOGandconductedexperi-  mentsusingthequerylogdataintroducedinSection3.1.1.  Ourmethodrunsontheentity-templatepairsextractedby  ouropenentityextractionmodule.Theparameter isset to0.5,andthealgorithmiterates10times. Foreachdomain,wechosethetopranked500expanded entitiesfromeachmethodandpooledthemtogether.We askedtwoannotatorstojudgethequalityofthesepooled entitiesofeachdomain.Oneentitythatcanbeannotated  ascorrectmustsatisfy:1)Itisacorrectentity.2)Itbelongs  totheexamineddomain.TheKappavalueofannotationis  0.82.Weuse Precision (P),Recall (R)and F1asevaluation metrics.Wene P=#correctentitiesextractedbythesystem #allextractedentitiesbythesystem ,R=#correctentitiesextractedbythesystem #allcorrectentitiesamongpooledentities and F1= 2PRP+R.4.3ExperimentalResults Table4showstheperformanceofourmethodandWSLOG. Inmostdomains,ourmethodoutperformsWSLOGinboth  precisionandrecall. ThecontextualtemplatesextractedbyWSLOGbasedon seedssometimescannotcorrectlydeterminetheboundaries  ofentitiesinqueries.Forexample,manytemplatesfordo-  main University canextracttextfragmentslike\ 2o14  (2014PekingUniversity) "ascandidateentities.Incon- trast,ourmethodextractsentitiescollectivelyfromparallel  webpagetitles.Withthesupportofweaksupervision,our  methodcanbetteravoidincorrectentityboundaries. Inaddition,WSLOGfavorspopularentitieswhichappear enoughtimesinquerylogs,butitcanhardlyminelong-  tailedentitieswell.Forthedomain Food ,WSLOGandour methodbothachievegoodprecisionbutlowrecall.Itindi-  catesthatthecorrectentitiesextractedbytwomethodshave  alowoverlap.WSLOGprovidesmoreshortfoodnames  suchas\ R¬h) "and\ )Z6¹ (pork) ",becausetheseappear moreofteninqueries.Evenweusecomplexseedslike\ (KungPaoChicken) ",thesimplefoodnamesstillrank higher.Incontrast,ourmethodcanprovidemorecomplex  entitieslike\ ?¯4Ò˚/&ÂTQ<û(ScrambledEggwithTomato) ". Finally,thetemplatesextractedbyWSLOGareambigu- ousinsomecases.Forexample,thetemplateslike\ <drug >(allergy) "and\ <drug >(instruction) "areimpor- tanttemplatesfordomain Drug .However,thesetemplates canextractentitiesofotherdomainssuchas Cosmetics aswell.Actually,ourmethodalsofacesthechallengeondis-  tinguishingentitiesfromhighlyrelateddomains.Thisisbe-  causetheentitiescanbegroupedtogetheraccordingtothe  purposeofthedesignersofthewebsites.Forexample,news- papers,TVprogramsandmagazinesareoftenorganizedto- Table4:ComparisonwithWSLOGonsetexpansion ofdomainentities. Domain WSLOG OursMetrics PRF 1PRF 1City 0.810.690.75 0.910.770.83 Country 0.10.310.15 0.300.940.46 Drug 0.490.520.5 0.750.810.78 Food 0.980.570.72 0.880.520.65 Location 0.840.660.74 0.990.780.87 Movie 0.640.540.59 0.920.770.84 Newspaper 0.110.460.18 0.170.720.27 Person 0.770.350.48 0.980.450.61 University 0.530.450.49 0.990.850.91 VideoGame 0.820.730.77 0.910.810.85 Average 0.610.530.54 0.780.740.71 getherinsomewebsites.Thatiswhyourperformanceon domain Newspaper isnotsogood.Extractingentitiesof e-graineddomainsisnecessarytobestudiedfurther. 5.RELATEDWORK Inthissection,wediscusstherelatedworkclosetoours andhighlightthemainerences. 5.1EntityExtraction Traditionalentityextractionistoidentifyandclassifyen- titieswithindomain-spctextsintoprednedcategories [5,11,12,18,22].Ourworkismorerelatedtoentitydiscov-  eryfrommassivewebandusergenerateddata.Theer-  encesamongthesestudieslieinusingdinttypesofdata  resourcesandrequiringerentdegreesofsupervision. 5.1.1EntityExtractionwithSupervision Extensiveworkextractsentitiesfromdintdatare- sourceswith(weak)supervision. ExtractionfromDocuments .Themainstreamworkisto extractentitiesfromwebdocuments[8,15].Forexam-  ple,Etzionietal.presentasystemthatcreateskeyword  queriesforagivenclassandextractsentitiesfromsearch  resultpagesusingprednedextractingrules.Althoughthe systemdoesn'thavetotrainaclasserusingtrainingdata, itstillneedsdomainknowledgeassysteminput. ExtractionfromStructuredData .Ourworkisalsorelated toapproachesthatextractrecordsfromstructuredwebdata  [1,13,17,19,24].In[30],asemi-supervisedmethodispro-  posedforcombininginformationfrombothfreetextsand  webtables.Thesemethodsidentifytablesforextraction  basedonexplicitHTMLtagsorvisuallystructuredfeatures.  Instead,weexploithiddencollectivetextualstructures.Be-  sides,theoutputsofrecordextractionusuallyaretuples  containingvarioustypesofdata,whereasourmethodfo-  cusesonextractingentities. ExtractionfromSearchQueries .Anotherbodyofresearch focusesonextractingentitiesfromusersearchqueries.For  example,in[26],aweaklysupervisedmethodisproposed.  Asetofseedsareprovidedforanexaminedclass.First,  contextpatternsarelearnedfromqueriesofasearchen-  ginebasedontheseeds.Theclassisrepresentedwiththese  patterns.Thenthesepatternsareusedtoextractmoreen-  titiesfromqueries.Eachentityisrepresentedwithasetof patternswhichextractit.Finally,theentitiesareranked accordingtothedistancebetweentheentityrepresentation andtheclassrepresentation. 5.1.2OpenEntityExtraction OpenInformationExtraction(OIE)attemptstoextract entitiesorrelationswithouthumanintervention[2,16]. ExtractionfromDocuments .Bankoetal.proposeanopen domaininformationextractionparadigmforentityextrac-  tion[2].Theyttrainaclassieronasmallamountofsam-  plestoclassifyacandidateastrustfulornot,andthenmake  adatadrivenpassoverthecorpustoextractcandidateenti-  ties.Downeyetal.alsopresentamethodtolocatecomplex  entitiesintexts[14].Theymakeuseofcapitalizationinfor-  mationandmeasuringcollocationtoextractmulti-wordex-  pressionsasentities.However,thestrategiesforgenerating  candidatesarebasedoneithersophisticatedNLPtechniques suchasdependencyparsingandnounphrasechunking,or languagedependentheuristics. ExtractionfromQueryLogs .Parameswaranetal.pro- poseaconceptextractionalgorithmbasedonstatisticalmea-  suresofsupportandcondenceon k-grams[25].Butthe complexityofthisalgorithmishighwhen kislarge.Soit couldonlyextractconceptswithlimitedlength.Jainetal.  presentamethodforopenentityextractionfromqueries  [21].Likepreviousopenentityextractionworkontexts,  thismethodalsomakesuseoflanguagedependentcapital-  izationinformationtoidentifycandidateentities.According  totheirexperimentalresults,querylogbasedmethodsout-  performdocumentbasedmethods. Ourmethodfollowstheopenentityextractionparadigm. Itdoesn'tneedanyhumanlaborordeepNLPtechiniques.  Unlikepreviousopenentityextractionwork,wefocusonthe  mostimportantdofadocument,thetitle,insteadofus-  ingdocumentcontents.Spcally,weidentifyandutilize  thehiddenstructuresinwebpagetitlesandprocessthem  collectively.Inthisway,wesigncantlysimplifythetask  ofdeterminingtheboundariesofentities.Inaddition,we  alsodiscovertheconnectionsbetweenentitiesbasedonthe  hiddenstructures,whichprovidenewresourcesforpotential  applications. 5.2MultipleSequenceAlignment(MSA) Multiplesequencealignmenttechniqueiscommonlyused incomputationalbiologyfordeterminingthecommonalities withinacollectionofbiologicalsequences,generallypro-  tein,DNA,orRNA[9,23].MSAisalsoappliedtonatural  languageprocessingforconstructingconceptmappingdic-  tionary[3],identifyingsentencelevelparaphrases[4]and  modelingtheorganizationofstudentessays[27].Motivated  bythesework,weadaptMSAforonekeycomponentin  ourframeworktoinferthehiddenstructureswithinparal-  lelwebpagetitles.Inthisway,texturaltemplatescanbe  learnedautomaticallyandunsupervisedlyforextractingen-  titiesfromlargescalewebdata. Somepreviousstudiessharesimilarideastoours.In[29], entitiesareminedfromcomparablenewsarticles.Wecan  viewitasanalignmentbetweenerentsources.Buttheir  approachiserentfromours,anddiulttoidentifycom-  plexmulti-wordentities.In[1],recordtuplesareextracted  fromstructureddatabyaligningHTMLlabels.Related  workontemplateinductionforIE(e.g.,[31])couldbeseen asakindofalignmentaswell.In[10],clusteringisadopted  tolearntemplatesautomaticallyforeventextraction. Ourmethoddirsfrompreviousworkinthefollowing aspects:1)Inpreviouswork,thedatatobealignedispro-  videdinadvanceorretrievedbasedonseeds,andsearch  enginesareoftenusedforsearchingforrelateddata,while weidentifydatatobealignedautomaticallyinwebscale.2) Insteadofusingexplicitstructureddata,wedealwithplain  textswithcollectivehiddenstructuresinwebpagetitles. 6.CONCLUSIONS Inthispaperweputforwardanovelmethodtoextracten- titiesbyexploitingthehiddenstructureswithinwebpageti- tlesandapplyingthemultiplesequencealignmenttechnique  forautomatictemplategenerationandentityextraction. Experimentalresultsshowthatourmethodcouldextract largescaleopendomainentitieswithhighprecision.The  extractedentitiescoverawiderangeoftopics,alargera-  tioofwhichislong-tailedandcomplex.Throughthewhole  process,manuallaborisunnecessary. Inaddition,thegeneratedtitletemplatesimplyrelated- nessconnectionsbetweenentities.Theextractedentity-  templatepairscanbeusedasadataresource,whichcan  producemoreprecisedomainspecicentitiesbyincorporat-  ingweaksupervision. Acknowledgments ThisresearchissupportedbytheNationalBasicResearch ProgramofChina(973Program,GrantNo.2014CB340505),  theNationalNaturalScienceFoundationofChina(Grant  No.61402304,GrantNo.61303105)andtheBeijingMunici- palNaturalScienceFoundation(GrantNo.4154065). 7.REFERENCES [1]A.ArasuandH.Garcia-Molina.Extractingstructured datafromwebpages.In SIGMOD2003 ,pages 337{348.ACM,2003. [2]M.Banko,M.J.Cafarella,S.Soderland, M.Broadhead,andO.Etzioni.Openinformation extractionfortheweb.In IJCAI ,volume7,pages 2670{2676,2007. [3]R.BarzilayandL.Lee.Bootstrappinglexicalchoice viamultiple-sequencealignment.In EMNLP2002 ,pages164{171.AssociationforComputational  Linguistics,2002. [4]R.BarzilayandL.Lee.Learningtoparaphrase:An unsupervisedapproachusingmultiple-sequence  alignment.In NAACL2003-Volume1 ,pages16{23. AssociationforComputationalLinguistics,2003. [5]D.M.Bikel,S.Miller,R.Schwartz,and R.Weischedel.Nymble:ahigh-performancelearning  namender.In Proceedingsoftheconferenceon Appliednaturallanguageprocessing ,pages194{201. AssociationforComputationalLinguistics,1997. [6]L.Blanco,N.Dalvi,andA.Machanavajjhala.Highly cientalgorithmsforstructuralclusteringoflarge  websites.In WWW2011 ,pages437{446.ACM,2011. [7]J.Carletta.Assessingagreementonclasscation tasks:thekappastatistic. Computationallinguistics ,22(2):249{254,1996. [8]A.Carlson,J.Betteridge,R.C.Wang,E.R. HruschkaJr,andT.M.Mitchell.Coupled semi-supervisedlearningforinformationextraction.In  WSDM2010 ,pages101{110.ACM,2010. [9]H.CarrilloandD.Lipman.Themultiplesequence alignmentprobleminbiology. SIAMJournalon AppliedMathematics ,48(5):1073{1082,1988. [10]N.ChambersandD.Jurafsky.Template-based informationextractionwithoutthetemplates.In ACL 2011:Volume1 ,pages976{986.Associationfor ComputationalLinguistics,2011. [11]H.L.ChieuandH.T.Ng.Namedentityrecognition: amaximumentropyapproachusingglobal  information.In COLING2002:Volume1 ,pages1{7. AssociationforComputationalLinguistics,2002. [12]M.CollinsandY.Singer.Unsupervisedmodelsfor namedentityclassication.In Proceedingsofthejoint SIGDATconferenceonempiricalmethodsinnatural  languageprocessingandverylargecorpora ,pages 100{110.Citeseer,1999. [13]B.B.Dalvi,W.W.Cohen,andJ.Callan.Websets: Extractingsetsofentitiesfromthewebusing  unsupervisedinformationextraction.In WSDM2012 ,pages243{252.ACM,2012. [14]D.Downey,M.Broadhead,andO.Etzioni.Locating complexnamedentitiesinwebtext.In IJCAI ,volume7,pages2733{2739,2007. [15]O.Etzioni,M.Cafarella,D.Downey,A.-M.Popescu, T.Shaked,S.Soderland,D.S.Weld,andA.Yates.  Unsupervisednamed-entityextractionfromtheweb:  Anexperimentalstudy. ArtiialIntelligence ,165(1):91{134,2005. [16]O.Etzioni,A.Fader,J.Christensen,S.Soderland,and M.Mausam.Openinformationextraction:Thesecond  generation.In IJCAI ,volume11,pages3{10,2011. [17]W.Gatterbauer,P.Bohunsky,M.Herzog,B.Kr upl, andB.Pollak.Towardsdomain-independent  informationextractionfromwebtables.In WWW2007 ,pages71{80.ACM,2007. [18]R.GrishmanandB.Sundheim.Message understandingconference-6:Abriefhistory.In COLING ,volume96,pages466{471,1996. [19]R.GuptaandS.Sarawagi.Answeringtable augmentationqueriesfromunstructuredlistsonthe web. ProceedingsoftheVLDBEndowment ,2(1):289{300,2009. [20]T.Haveliwala,S.Kamvar,andG.Jeh.Ananalytical comparisonofapproachestopersonalizingpagerank.  2003. [21]A.JainandM.Pennacchiotti.Openentityextraction fromwebsearchquerylogs.In COLING2010 ,pages 510{518,Stroudsburg,PA,USA,2010.Associationfor  ComputationalLinguistics. [22]A.McCallumandW.Li.Earlyresultsfornamed entityrecognitionwithconditionalrandomlds,  featureinductionandweb-enhancedlexicons.In  NAACL2003-Volume4 ,pages188{191.Association forComputationalLinguistics,2003. [23]S.B.NeedlemanandC.D.Wunsch.Ageneral methodapplicabletothesearchforsimilaritiesinthe  aminoacidsequenceoftwoproteins. Journalof molecularbiology ,48(3):443{453,1970. [24]Z.Nie,F.Wu,J.-R.Wen,andW.-Y.Ma.Extracting objectsfromtheweb.In ICDE2006. ,pages123{123. IEEE,2006. [25]A.Parameswaran,H.Garcia-Molina,and A.Rajaraman.Towardsthewebofconcepts:  Extractingconceptsfromlargedatasets. Proceedings oftheVLDBEndowment ,3(1-2):566{577,2010. [26]M.Pa˘sca.Weakly-superviseddiscoveryofnamed entitiesusingwebsearchqueries.In CIKM2007 ,pages683{690.ACM,2007. [27]I.Persing,A.Davis,andV.Ng.Modelingorganization instudentessays.In EMNLP2010 ,pages229{239. AssociationforComputationalLinguistics,2010. [28]S.Sekine,K.Sudo,andC.Nobata.Extendednamed entityhierarchy.In LREC ,2002. [29]Y.ShinyamaandS.Sekine.Namedentitydiscovery usingcomparablenewsarticles.In COLING2004 ,page848.AssociationforComputationalLinguistics,  2004. [30]P.P.Talukdar,J.Reisinger,M.Pa˘sca, D.Ravichandran,R.Bhagat,andF.Pereira.  Weakly-supervisedacquisitionoflabeledclass  instancesusinggraphrandomwalks.In EMNLP2008 ,pages582{590.AssociationforComputational  Linguistics,2008. [31]R.C.WangandW.W.Cohen.Character-level analysisofsemi-structureddocumentsforset  expansion.In EMNLP2009:Volume3 ,pages 1503{1512.AssociationforComputationalLinguistics,  2009. [32]C.Zhang,S.Zhao,andH.Wang.Bootstrapping large-scalenamedentitiesusingurl-texthybrid  patterns.In IJCNLP2013 ,pages293{301,2013.  
Convolutional Recurrent Neural Networks for Small -Footprint Keyword  Spotting  Sercan  –. Arõk 1,*, Markus Kliegl 1,*, Rewon Child 1, Joel Hestness 1, Andrew Gibiansky 1, Chris  Fougner 1, Ryan Prenger 1, Adam Coates 1 1Baidu Silicon Valley  Artificial Intelligence  Lab, 1195 Bordeaux Dr. Sunnyvale, CA 94089 , USA  *Equal contribution  sercanarik@baidu.com, klieglmarkus@baidu.com  Abstract  Keyword spotting (KWS) constitutes a major component of  human -technology interfaces. Maximizing the detection  accuracy at a low false alarm (FA) rate, while minimizing the  footprint size, latency and complexity are the goals for KWS.  Towards achieving them, we study Convolutional Recurrent  Neural Networks (CRNNs). Inspired by large -scale state -of-the -art speech recognition systems, we combine the strengths of  convolutional layers and recurrent layers to exploit local  structure and long -range context. We analyze the effect of  architecture parameters, and propose training strategies to  improve performance. With only ~230k parameters, our CRNN  model yields acceptably low latency, and achieves 97.71%  accuracy at 0.5 FA/hour for 5 dB signal -to-noise ratio.  Index Terms : Keyword spotting,  speech re cognition,  convolutional neural networks, recurrent neural networks . 1.!Introduction  Motivated by the most common way humans interact with  each other,  conversational  human -technology interfaces are  becoming increasingly popular in numerous applicati ons. High -performance speech -to-text conversion and text -to-speech  conversion  constitute  two  important  aspects  of such interfaces , as most  computational  algor ithms  are developed for  text input s and output s. Another  crucial aspect  of  conversational  interfaces  is keyword spotting (KWS)  Ð also known as wakeword  detection , to enable  transitioning between different  computational  states based on the voice  input  provided by the  users . KWS systems aim to detect a particular keyword from a  continuous stream  of audio . As their output determines different  states of the device, very high detection accuracy for a very low  false alarm (FA) rate is critical to enab le satisfactory  user  experience. Typical appl ications exist  in environments with  interference from background  audio , reverberation distortion,  and the sounds generated by the speaker of the device  in which  the  KWS  is embedded.  A KWS system  should demonstrate  robust performance in  this  wide range of situations.  Furthermore , the computational complexity and model size are  impo rtant concerns for KWS systems, a s they are typically  embedded  in  consumer  devices with limited memory and  comput ational resources, such as smartphones or smart -home  sensors.   There are already millions of devices with embedded KWS  systems . Traditional approaches for KWS  are  based on Hidden  Markov Models with sequence  search algorithms  [1].  With  the  advances  in deep learning and increase in the amount of  available data,  state -of-the -art KWS  has  been replaced  by deep  learning -based approaches  due to their  superior performance  [2].  Deep learning -based  KWS systems  commonly  use  Deep  Neural N etworks (DNNs)  combined  with  compression  techniques [ 3,4] or multi -style training  approaches  [5,6].  A potential  drawback of DNNs is that they ignore the structure  and context  of the input,  and an audio input  can have strong  dependencies  in  time or frequency dom ain s. With the goal of  exploiting such local connectivity patterns by shared weights,  Convolutional  Neural  Networks (CNNs)  were  explored for  KWS  [7,8 ]. A potential drawback of CNNs is that they cannot  model the context over the entire frame  without  wide filters or  great depth . Recurrent neural networks ( RNNs ) were also  studied  for KWS  with  connectionist temporal classification  (CTC)  loss [9,10 ], unlike  the aforementioned DNN and CNN  models [ 2-6] with  cross -entropy  (CE)  loss . Yet, a  high accuracy  at a low FA rate  could not be obtained , given the ambitious  targets of the applications of such systems . Similar to DNNs, a  potential limitation of RNNs is that the modeling is done  on the  input features, without learning the structure between  successive time and frequency steps. Recently, [ 11] proposed a  Convolutional  Recurrent  Neural N etwork (CRNN) architecture  with CTC loss . However,  despite the large model size,  similar  to RNNs,  a high  accuracy at a low FA rate  could not be  obtained.   In th is  paper , we focus on developing  a prod uction -quality  KWS  system  using  CRNNs  with  CE loss for a small -footprint  model , applied  for a single keyword . Our goal is to combine  the  strengths of CNNs and RNNs , with additional strategies  applied  during training  to improve  the  overall  performance, while  keeping a small -footprint size . The rest of the paper is  organized  as follows.  In Sec tion 2,  we describe the end -to-end architecture  and training methodologies for small -footprint KWS. In  Section 3, we explain the experiments and the corresponding  results. In Section  4, we present  our conclusions .  2.!Small -footprint  keyword spotting  2.1. !End -to-end architecture  We focus on a canonical CRNN architecture, inspired by  the successful large -scale speech recognition systems [ 12-14].  To adapt these architectures for small -footprint KWS, the  model size needs to be shrunk two  to three  orders of magnitude.  We will analyze the impact of different parameters on  performance while shrinking the size of the model.   Fig. 1 shows t he  CRNN  architecture  with the corresponding  parameters . The raw t ime -domain input s are  converted to per -channel energy normalized (PCEN) mel spectrogram s [8], for  succinct representation and efficient training. (Other input  representations we experimented  with  yielded worse   performance  for  model architectures of  comparable  size .) The  2-D PCEN features are given as inputs to the convolutional  layer, which employ 2 -D filtering  along  both the  time and  frequency dimensions . The outputs of the convolutional layer  are  fed  to bidirectional recurrent layers , which migh t i nclude  gated  recurrent units (GRUs)  [15] or long short -term memory  (LSTM) units  [16] and process the entire frame . Outputs of the  recurrent layers are given to the  fully connected  (FC)  layer . Lastly,  softmax decoding is applied  over two neurons , to obtain  a corresponding scalar  score . We use rectified linear units  as activation function  in all layers.    Figure  1: End -to-end CRNN architecture for KWS.  2.2. !End -to-end t raining   In speech recognition,  large -scale architectures with  recurrent layers  typically  use variants of CTC loss to decode the  most probable  output  label. Aside from the modeling  limitations  due to conditional independence assumptions of  targets, CTC loss has a  high computational complexity and  typically yields good performance  only  when the model  capacity  is sufficiently large  to  efficiently  learn from a large   data set. As  we focus on small -footprint arch itectures , the  loss  function that is optimized during the training is  chosen as the  CE loss  for the estimated and target binary labels, indicating  whether a frame corresponds to a keyword or not .  To train with a  CE loss, unlike CTC, precise alignment of  the training samples is important. We  use  Deep Speech 2 [ 14],  a large -scale speech recognition model, to obtain the estimated  probability distributions of keyword characters  !" (#$%$&) for each time instance. As the  CTC  decoding yields peaked  distributions, we further smooth the output over time and obtain  smoothed character occupancy scores  '!"(). We then obtain  the beginning and end times of the keywords using the  heuristic  algorithm shown  in Algorithm 1 . An extra short padding is  added while chopping the keywords to cover edge cases. The  accuracy of alignments obtained were significantly beyond  the  time scale  of human perception.   3.!Experiments  and Results  3.1. !Dat a and training  We develop our  KWS system for the keyword ÒTalkTypeÓ  (which can be pronounced as a single word or two words) . We  choose a fr ame  length  of  T = 1.5 seconds, which  is s ufficiently  long to capture  a reasonable pronunciation of  ÒTalkTypeÓ . Using a sampling rate of 16 k Hz,  each frame contains 24k  raw  time -domain samples.  Corresponding  PCEN  mel spectrograms  are obtained for  10 ms  stride  and 40 channels, yielding an input  dimensionality of 40  ! 151.  The  entire  dat a set consists  of ~16k  different  samples,  collected from more than  5k speakers.  The  dataset is split into training,  development  and test sets with  6-1-1 ratio.  Training  samples are  augmented by applying additive  noise,  with a power  determined by  a signal -to-noise ratio (SNR)  sampled from  [-5,15] dB  interval . The additive noise is  sampled  from  a data set of  representative background noise and speech , with a total length exceeding 300 hours.  To provide robustness  against al ignment errors,  training samples are also augmented  by introducing  random  timing jitter.  We use the  ADAM  optimization algorithm for traini ng [17],  with  a batch size of  64. The learning rate is initially chosen as 0.001, and  later  dropped  to 0.0003 .  Our evaluation considers a streaming scenario  such that  inference is performed  for  overlapping frames of  duration  T. The  shift  between the frames is chosen  as 100 ms, (which  should  ideally  be much longer than the spectrogram stride and  much shorter than the inference latency  - see Section 3.2 for  more  details ). The  metrics we focus on are the  false rejection  rate (FRR)  and  false alarms (FA) per hour , typically fixing the  latter at a  desired value such as 1 FA/hr  [7] . Noise is added to  the  development and test sets, with a magnitude depending on  the SNR value . We  note that the  collected  samples are already  noisy so the actual SNR is lower  if defined precisely as the ratio  of powers of  the information -bearing signal and the noise . Similar to  our augmentation of the  training sets,  negative  samples and  noise datasets are sampled from representative  background noise and speech . 3.2. !Impact of the model architecture  Table 1 shows the performance of various CRNN  architectures for the development s et with 5 dB SNR. We note  that all models were trained until convergence, even though it  requires very different number of epochs. We observe the  general trend that the larger model size typically yields better  performance. Increasing the number of convolu tion filters or  increasing the number of recurrent hidden units are the two  effective approaches to improve the performance. Increasing  the number of recurrent layers has a limited impact, and GRU  is preferred over LSTM as a better performance can be obtai ned  for a lower complexity.   Algorithm 1  Sequential alignment of keyword samples   require:  keyword characters  !" (#$%$&), smoothed  character occupancy scores  '!"(), decay rate  * (+$*$#) initialize: ,'-.!"()/,'!"(), '.-!"()/,'!"()  for:  01/#,to,2345.      for:  %1/+,to,&67 (right -to-left decoding)  ,,,,,,,,89:.-/;<=>;? 4'.-!"()        '.-!"@A()/*B'.-!"@A() for  )C89:.-     end      for:  %1/&6#,to,# (left -to-right decoding)  ,,,,,,,,89:-./;<=>;? 4'-.!"()        '-.!"DA()/*B'-.!"DA() for  )$89:-.     end  end  return : (>EF 89G-.(89G.-(>;? ,H89I-.(89I.-J)  Table 1 : Performance of different CRNN architectures (see Fig. 1 for the description of the parameters). The chosen set of  parameters for the rest of the paper is colored  and highlighted in bold . It is desired to  limit  the  model size  given the resource  constraints for inference latency, memory, and power  consumption . Following [ 7],  we choose the  size limit a s 250k (which is more than 6 times smaller than the architecture with  CTC loss in [11]) . For  the rest of the paper,  the  default  architecture is the  set o f parameters highlighted in bold , which  also corresponds to a fairly optimal point given  the model size  vs. performance trade -off .  We  compare the performance with  a CNN architecture  based on [ 7]. Given the discrepancy in input dimensionality and  training data, we reoptimize the  model  hyperparameters for  the  best performance while  upper -bounding  the number of  parameters to 250k for a fair comparison.  For the same  development set with 5 dB SNR,  the best CNN architecture  achieves 4.31% FRR at 1 FA/hour and 5.73% FRR at 0.5  FA/hour . Both metrics are  ~51% highe r compared to the  FRR  values of the  chosen CRNN model with 229k parameters . Interestingly, the performance gap is lower for higher SNR  values. We elaborate on this in Section 3.4.  Recall that  the  model is bidirectional and runs on  overlapping 1.5 second wi ndows at 100  ms stride.  However , thanks to the small model size and the large time stride of 8 in  the initial convolution layer, we are able to do inference  comfortably faster than real time.  The inference computational  complexity of the chosen CRNN -based  KWS model with 229k  parameters is roughly ~ 30M floating point operations   (FLOPs)  when implemented on  processors  of  modern  consumer  devices  (without special functions to implement  nonlinear operations).  Even w hen implemented  on modern  smartphones  without any approximations and special function  units , our KWS model  can achieve  an inference time much  faster than the time scale for reactive time for humans  with  auditory stimuli,  which is ~280 ms [ 18]. 3.3. !Impact of  the amount of  training data    Fig ure  2: FRR at 0.5 FA/hour vs. number of unique  training keywords for the test set with 5 dB  SNR.  Convolutional  Recurrent  FC Total number of  parameters  FRR (%) for the noise development  set with 5 dB SNR  NC (LT, LF) (ST, SF) R NR Recurrent  unit  NF at 1 FA/hour  at 0.5 FA/hour  32 (20,5)  (8,2)  2 8 GRU  32 45k 5.54  7.44  32 (20,5)  (8,2)  3 8 LSTM 64 68k 6.17  7.68  32 (5,1)  (4,1)  2 8 GRU  64 102k 6.04  7.31  32 (20,5)  (8,2)  2 16 GRU  64 110k 3.48  4.46  32 (20,5)  (20,5)  2 32 GRU  64 110k 5.70  7.99  32 (20,5)  (8,2)  3 16 GRU  64 115k 3.42  4.10  16 (20,5)  (8,2)  2 32 GRU  32 127k 3.53  5.55  32 (20,5)  (12,4)  2 32 GRU  64 143k 5.80  7.72  16 (20,5)  (8,2)  1 32 GRU  64 148k 4.20  6.27  128 (20,5)  (8,2)  3 8 GRU  32 159k 3.83  5.21  64 (10,3)  (8,2)  1 16 GRU  32 166k 3.21  4.31  128 (20,5)  (8,2)  1 32 LSTM 64 197k 3.37  4.56  32 (20,5)  (12,2)  2 32 GRU  64 205k 3.26  4.40  32 (20,5)  (8,2)  1 32 GRU  64 211k 3.00  3.84  32 (20,5)  (8,2)  2 32 GRU 64 229k 2.85  3.79  32 (40,10)  (8,2)  2 32 GRU  64 239k 3.57  5.03  32 (20,5)  (8,2)  3 32 GRU  64 248k 3.00  3.42  32 (20,5)  (8,2)  2 32 LSTM 64 279k 3.06  4.41  32 (20,5)  (8,1)  2 32 GRU  64 352k 2.23  3.31  64 (20,5)  (8,2)  2 32 GRU  64 355k 2.43  3.99  64 (20,5)  (8,2)  2 32 LSTM 32 407k 3.11 4.04  64 (10,3)  (4,1)  2 32 GRU  64 674k 3.37  4.35  128 (20,5)  (8,2)  2 32 GRU  128 686k 2.64 3.78  32 (20,5)  (8,2)  2 128 GRU  128 1513k 2.23  2.95  256 (20,5)  (8,2)  4 64 GRU  128 2551k 2.18  3.42  128 (20,5)  (4,1)  4 64 GRU  128 2850k 2.64  3.21   Given the repre sentation capacity limit imposed by the  architecture  size , increasing th e amount of  positive samples in  the  training data has a  limit ed effect  on the  performance.  Fig. 2  shows  the FRR  at 0.5 FA/hour (for the test set with 5 dB SNR)  vs. the number of unique ÒTalkTypeÓ samples used while  training.  Saturation of performance occurs faster  than  applications  with similar type of data but  with large -scale  models , e.g.  [14]. Besides increasing the amount of the positive samples,  we  observe performance improvement by  increasing the  diversity  of relevant negative samples,  obtained  by hard mining . We mine  negative  samples,  by using  the pre -converged model on  a very large  public  videos  dataset  (that are not used in training,  development, or test sets). Then,  training is continued  using  the  mined  negative  samples  until convergence.  As shown in Fig. 2,  hard negative mining  yields decrease in FRR  for the test set .  3.4. !Noise robustness   Fig ure  3: FRR vs. FA per hour for the test set with  various SNR values.  For the test set with various SNR values, Fig. 3 shows the  FRR vs. FA per hour . For higher SNR, lower FRR is obtained,  and stable performance starts for a lower  FA rate. Note that the  SNR values  (in dB)  of the augmented training samples are  sampled from a distribution with a mean of 5 dB, and  deterioration in performance is observed beyond this value.  Performance for lower SNR values can be improved by  augmenting  with lower SNR, but this comes at the expense of  decreased performance for higher SNR,  which can be attributed  to the limited learning capacity of the model.  We observe the benefit of recurrent layers e specially for  lower SNR values.  The performance gap of  CRNN  architectures with CNN architectures (adapted from [ 7] as  explained in Section 3.1) reduces as the SNR increases.  We  hypothesize that the recurrent layers are better able to adapt to  the noise signature of individual samples, since each layer  process es information from the entire frame. CNNs, in contrast,  require wide filters and/or great depth for this level of  information propagation . 3.5. !Far -field robustness  Our dataset already consists  of  samples recorded at varying  distance values, which should be representative for most  applications such as smartphone KWS syst ems. Yet, s ome  applications, such as smart -home KWS systems, require high  performance at far -field conditions.    Figure  4: FRR at 1 FA/hour vs. additional distance for  far -field test sets with varying SNR values. Solid:  baseline performance, dashed: with  far -field  augmented training.  Fig. 4 shows  performance degradation with the additional  distance . Far-field test sets are constructed by augmenting the  original test set with impulse responses corresponding to  a variety of  configurations at the given distance (considering  different values for degrees of arrival etc.) . Significant  deterioration in performance is  observed  especially  in  conjunction with higher noise , as  also  explained in [ 19]. To  provide robustness  against  this deterioration , we consider  training with far -field -augmented training samples, using a  variety of impulse responses that are different than the ones in  the test set. This augmentation achieves  significantly less  degradation in performance for farther di stances. Yet, it yields  a worse performance for the original data set  due to the  training/testing mismatch .  4.!Conclusions  We studied CRNNs for small -footprint KWS systems. We  presente d the trade -off between model size and performance,  and demonstrated the optimal choice of parameters given the  tradeoff . The capaci ty limitation of the model  has various  implications. P erformance gain is limited by merely increasing  the number of positive  samples, yet  hard negative mining  improves the performance.  Training sets should be carefully  chosen to reflect the application environment, such as the nois e level  or far -field conditions.  Overall,  at 0.5 FA/hour (which is  an acceptable value from a user perspective),  our model  achieves  97.71% , 98.71% and 99.3%  accuracy for the test set  with 5 dB , 10 dB and 20 dB  SNR  values , respectively . Our  numerical performance results  may  seem better tha n other  KWS  models in the literature . However , a direct comparison is  not meaningful because of the difference in the data sets and the  actual keywords,  i.e.  the inference task . Given that human  performance is excellent in  the  KWS task, we still believe tha t there  is  further  room for improvement in terms of performance .   5.!Acknowledgements  Discussions with Andrew Ng, Sanjeev Satheesh, Jiaji Huang,  Jue Sun, and Bing Jiang  are gratefully acknowledged.  We thank  Hui Song for the impulse response measurements used for far -field augmentation.    6.!References  [1]  J.R. Rohlicek, W. Ru ssell, S. Roukos, and H. Gish, Ò Continuous  hidden Markov modeling for speaker -independent wordspotting,Ó in  IEEE  Proceedings of the  International Conference on Acoustics,  Speech and Signal Processing , 1990, pp. 627 Ð630. [2]  G. Chen, C. Parada, a nd G. Heigold, ÒSmall -footprint keyword  spotting using deep neural n etworks,Ó in  Proceedings  International  Conference on Acoustics, Speech, an d Signal Processing , 2014 , pp.  4087-4091. [3]  G. Tucker, M. Wu, M. Sun, S.  Panchapagesan,  G. Fu, and S.  Vitaladevuni, ÒModel compression applied to small -footprint keyword  spotting,Ó  in Proceedings  of Interspeech , 2016, pp. 1393 -1397 [4]  Vikas Sindhwani , Tara N. Sainath, and Sanjiv Kumar, ÒStructured  transforms for small -foo tprint deep learning,Ó in  Neural Information  Processing Systems , 2015 , pp. 3088 -3096. [5]  R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. N.  Sainath , ÒAutomatic gain control and multi -style training for robust  small -footprint keyword spotting with deep neural networks,Ó in  IEEE  Proceedings of the International Conference on Acoustics, Speech and  Signal Processing , pp. 4704 Ð4708. [6]  S. Panchapagesan,  M. Sun, A. Khare, S. Matsoukas, A. Mandal, B.  Hoffmeister, and S. Vitaladevuni, ÒMulti -task learning and weighted  cross -entropy for dnn -based keyword spotting,Ó in  Proceedings of  Interspeech , 2016, pp. 760 -764. [7] T. N.  Sainath  and  C. Parada, Ò Convolution al neural networks for  small -footprint keyword spotting,Ó in  Proceedings of Interspeech , 2015, pp. 1478 -1482 [8] Y. Wang, P. Getreuer , T. Hughes , R. F. Lyon , and R. A. Saurous,  ÒTrainable frontend for robust and  far -field keyword s potting,Ó  arXiv  preprint,  arXiv:1607.05666 , 2016. [9]  K. Hwang, M. Lee, and W. Sung, ÒOnline keyword spotting with a  character -level recurrent neural network,Ó arXiv preprint  arXiv:1512.08903, 2015.  [10]  S. Fernandez, A. Graves, and J. S chmidhuber, ÒAn application of  recurrent neural networks to discriminative keyword spotting, Ó in  Artificial Neural Networks . Springer, 2007, pp. 220 Ð229. [11] C.  Lengerich,  and A. Hannun, ÒAn end -to-end a rchitecture  for  keyword spotting and voice activity detection,Ó  arXiv preprint  arX iv:1611.09405 , 2016 . [12]  L. Deng, and J. C. Platt, Ò Ensemble deep learning for speech  recognition,Ó  in  Proceedings of Interspeech , 2014 . [13] T. N. Sainath, O.  Vinyals , A.  Senior A,  and H. Sak,  ÒConvolutional, long short -term memory, fully co nnected deep  neural  networks,Ó in  IEEE Proceedings of the International Conference on  Acoustics, Speech and Signal Processing , 2015, pp. 4580 -4584. [14] D.  Amode i et al. Ò Deep Speech 2: End -to-end speech recognition  in English and Mandarin. ,Ó arXiv preprint arXiv:1512. 02595, 2015 . [15]  K. Cho, B. van Merrienboer,  D. Bahdanau, and Y. Bengio, ÒOn  the  properties of neural machine  translation: Encoder -decoder  approaches ,Ó arXiv preprint arXiv:1409.1259 , 2014.  [16]  S. Hochreiter and J . Schmidhuber, ÒLong Short -Term Memory,Ó  Neural Computation , vol. 9, no. 8, pp.  1735-1780, 1997.  [17] D.  Kingma, an d J. Ba, ÒAdam: A method for stochastic  optimization,Ó  arXiv preprint  arXiv:1412.6980 , 2014 . [18] J. Shelton , and  G. P. Kumar GP,  ÒComparison between auditory  and visual simple react ion times,Ó  Neuroscience and medicine , vol. 1  no. 1, pp. 30 -32, 2010.  [19] K. Kumatani et al. Ò Microphone array  processing for distant  speech recognition:  Towards real -world deployment,Ó in  IEEE Asia -Pacific Signal & Information Processing Association Annual Summit  and Conference , 2012.      
SimpleStrategiesforRecoveringInnerProductsfrom CoarselyQuantizedRandomProjections PingLi BaiduResearch,and RutgersUniversity pingli98@gmail.com MartinSlawski DepartmentofStatistics GeorgeMasonUniversity mslawsk3@gmu.edu Abstract Randomprojectionshavebeenincreasinglyadoptedforadiversesetoftasksin machinelearninginvolvingdimensionalityreduction.Onelineofresearch onthistopichasinvestigatedtheuseofquantizationsubsequenttoprojection withtheaimofadditionaldatacompression.Motivatedbyapplicationsinnearest neighborsearchandlinearlearning,werevisittheproblemofrecoveringinner products(respectivelycosinesimilarities)insuchsetting.Weshowthatevenunder coarsescalarquantizationwith3to5bitsperprojection,thelossinaccuracytends torangefromﬁnegligibleﬂtoﬁmoderateﬂ.Oneimplicationisthatinmostscenarios ofpracticalinterest,thereisnoneedforasophisticatedrecoveryapproachlike maximumlikelihoodestimationasconsideredinpreviousworkonthesubject. Whatweproposehereinalsoyieldsconsiderableimprovementsintermsofaccuracy overtheHammingdistance-basedapproachinLietal.(ICML2014)whichis comparableintermsofsimplicity. 1Introduction Themethodofrandomprojections(RPs)forlineardimensionalityreductionhasbecomemore andmorepopularovertheyearsafterthebasictheoreticalfoundation,thecelebratedJohnson- Lindenstrauss(JL)Lemma[ 12 , 20 , 33 ],hadbeenlaidout.Inanutshell,itstatesthatitispossible toconsiderablylowerthedimensionofasetofdatapointsbymeansofalinearmapinsuchaway thatsquaredEuclideandistancesandinnerproductsareroughlypreservedinthelow-dimensional representation.Conveniently,alinearmapofthissortcanberealizedbyavarietyofrandom matrices[ 1 , 2 , 18 ].ThescopeofapplicationsofRPshasexpandeddramaticallyinthecourseof time,andincludesdimensionreductioninlinearandregression[ 14 , 30 ],similarity search[ 5 , 17 ],compressedsensing[ 8 ],clustering[ 7 , 11 ],randomizednumericallinearalgebraand matrixsketching[29],anddifferentialprivacy[21],amongothers. Theideaofachievingfurtherdatacompressionbymeansofsubsequentscalarquantizationofthe projecteddatahasbeenconsideredforawhile.Suchsettingcanbemotivatedfromconstraints concerningdatastorageandcommunication,locality-sensitivehashing[ 13 , 27 ],ortheenhancement ofprivacy[ 31 ].Theextremecaseofone-bitquantizationcanbeassociatedwithtwoseminalworks incomputerscience,theSDPrelaxationofthe MAXCUT problem[ 16 ]andthesimhash[ 10 ].One-bit compressedsensingisintroducedin[ 6 ],andalongwithitsnumerousextensions,hasmeanwhile developedintoawithinthecompressedsensingliterature.Aseriesofrecentpapersdiscuss quantizedRPswithafocusonsimilarityestimationandsearch.Thepapers[ 25 , 32 ]discussquantized RPswithafocusonimageretrievalbasedonnearestneighborsearch.Independentofthe application,[ 25 , 32 ]provideJL-typestatementsforquantizedRPs,andconsiderthetrade-offbetween thenumberofprojectionsandthenumberofbitsperprojectionunderagivenbudgetofbitsasitalso appearsinthecompressedsensingliterature[ 24 ].Thepaper[ 19 ]studiesapproximateJL-typeresults forquantizedRPsindetail.TheapproachtoquantizedRPstakeninthepresentpaperfollows[ 27 , 28 ] 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. inwhichtheproblemofrecoveringdistancesandinnerproductsisrecastwithintheframeworkof classicalstatisticalpointestimationtheory.Thepaper[ 28 ]discussesmaximumlikelihoodestimation inthiscontext,withanemphasisontheaforementionedtrade-offbetweenthenumberofRPsandthe bitdepthperprojection.Inthepresentpaperwefocusonthemuchsimplerandcomputationallymuch moreconvenientapproachinwhichthepresenceofthequantizerisignored,i.e.,quantizeddataare treatedinthesamewayasfull-precisiondata.Wehereinquantifythelossofaccuracyofthisapproach relativetothefull-precisioncase,whichturnsouttobeinmanyscenariosofpractical interestevenundercoarsequantizationwith3to5bitsperprojection.Moreover,weshowthat theapproachcomparesfavorablytotheHammingdistance-based(orequivalentlycollision-based) schemein[ 27 ]whichisofsimilarsimplicity.Wearguethatbothapproacheshavetheirmerits:the collision-basedschemeperformsbetterinpreservinglocalgeometry(thedistancesofnearbypoints), whereastheonestudiedinmoredetailhereinyieldsbetterpreservationglobally. Notation. Forapositiveinteger m ,welet [ m ]= f 1 ;:::;m g .For l 2 [ m ] , v ( l ) denotesthe l -th componentofavector v 2 R m ;ifthereisnodangerofconfusionwithanotherindex,thebracketsin thesubscriptareomitted. I ( P ) denotestheindicatorfunctionofexpression P . Supplement :Proofsandadditionalexperimentalresultscanbefoundinthesupplement. Basicsetup. Let X = f x 1 ;:::;x n gˆ R d beasetofinputdatawithsquaredEuclideannorms  2 i : = k x i k 2 2 , i 2 [ n ] .Wethinkof d beinglarge.RPsreducethedimensionalityoftheinputdata bymeansofalinearmap A : R d ! R k , k ˝ d .Weassumethroughoutthepaperthatthemap A isrealizedbyarandommatrixwithi.i.d.entriesfromthestandardGaussiandistribution,i.e., A lj ˘ N (0 ; 1) , l 2 [ k ] , j 2 [ d ] .OnestandardgoalofRPsistoapproximatelypreservedistancesin X whileloweringthedimension,i.e., k Ax i  Ax j k 2 2 =k ˇk x i  x j k 2 2 forall ( i;j ) .Thisisimplied byapproximateinnerproductpreservation h x i ;x j iˇh Ax i ;Ax j i =k forall ( i;j ) . Forthetimebeing,weassumethatitispossibletocomputeandstorethesquarednorms f  2 i g n i =1 , andtorescaletheinputdatatounitnorm,i.e.,oneforms e x i   x i  i , i 2 [ n ] ,beforeapplying A .Inthiscase,itsuftorecoverthe(cosine)similarities ˆ ij : = h x i ;x j i  i  j = h e x i ; e x j i , i;j 2 [ n ] ,of theinputdata X fromtheircompressedrepresentation Z = f z 1 ;:::;z n g ;z i : = A e x i ;i 2 [ n ] . 2Estimationofcosinesimilaritybasedonfull-precisionRPs Aspreparationforlatersections,westartbyprovidingbackgroundconcerningtheusualsetting withoutquantization.Let ( Z;Z 0 ) r berandomvariableshavingabivariateGaussiandistributionwith zeromean,unitvariance,andcorrelation r 2 (  1 ; 1) : ( Z;Z 0 ) r ˘ N 2  0 0  ;  1 r r 1  : (1) Letfurther x;x 0 beagenericpairofpointsfrom X ,andlet z : = A e x , z 0 : = A e x 0 bethecounterpartin Z .Thenthecomponents f ( z ( l ) ;z 0 ( l ) ) g k l =1 of ( z;z 0 ) aredistributedi.i.d.asin (1) with r = ˆ = : h e x; e x 0 i . Hencetheproblemofrecoveringthecosinesimilarityof x and x 0 canbere-castasestimatingthe correlationfromani.i.d.sampleof k bivariateGaussianrandomvariables.Tosimplifyourexposition, wehenceforthassumethat 0  ˆ< 1 asthiscaneasilybeachievedbythesignofoneof x or x 0 .Thestandardestimatorof ˆ iswhatiscalledtheﬁlinearestimatorﬂherein: b ˆ lin = 1 k h z;z 0 i = 1 k k X l =1 z ( l ) z 0 ( l ) : (2) Aspointedoutin[ 26 ]thisestimatorcanbeconsiderablyimproveduponbythemaximumlikelihood estimator(MLE)given(1): b ˆ MLE =argmax r ˆ  1 2 log(1  r 2 )  1 2 1 1  r 2  1 k k z k 2 2 + 1 k k z 0 k 2 2  1 k h z;z 0 i 2 r  : (3) Theestimator b ˆ MLE isnotavailableinclosedform,whichispotentiallyaseriousconcernsinceit needstobeevaluatedfornumerousdifferentpairsofdatapoints.However,thiscanbeaddressed 2 bytabulationofthetwostatistics  k z k 2 2 + k z 0 k 2 2  =k; h z;z 0 i =k o andthecorrespondingsolutions b ˆ MLE overasufgrid.Atprocessingtime,computationof b ˆ MLE canthenbereducedtoa look-upinapre-computedtable. Oneobviousissueof b ˆ lin isthatitdoesnotrespecttherangeoftheunderlyingparameter.Anatural istheuseoftheﬁnormalizedlinearestimatorﬂ b ˆ norm = h z;z 0 i = ( k z k 2 k z 0 k 2 ) : (4) Whencomparingdifferentestimatorsof ˆ intermsofstatisticalaccuracy,weevaluatethemean squarederror(MSE),possiblyasymptoticallyasthenumberofRPs k !1 .,weconsider MSE ˆ ( b ˆ )= E ˆ [( ˆ  b ˆ ) 2 ]=Bias 2 ˆ ( b ˆ )+Var ˆ ( b ˆ ) ; Bias ˆ ( b ˆ ) : = E ˆ [ b ˆ ]  ˆ; (5) where b ˆ issomeestimator,andthesubscript ˆ indicatesthatexpectationsaretakenwithrespecttoa sample ( z;z 0 ) followingthebivariatenormaldistributionin(1)with r = ˆ . Itturnsoutthat b ˆ norm and b ˆ MLE canhavedramaticallylower(asymptotic)MSEsthan b ˆ lin forlarge valuesof ˆ ,i.e.,forpointsofhighcosinesimilarity.Itcanbeshownthat(cf.[4],p.132,and[26]) Bias ˆ ( b ˆ lin )=0 ; Var ˆ ( b ˆ lin )=(1+ ˆ 2 ) =k; (6) Bias 2 ˆ ( b ˆ norm )= O (1 =k 2 ) ; Var ˆ ( b ˆ norm )=(1  ˆ 2 ) 2 =k + O (1 =k 2 ) ; (7) Bias 2 ˆ ( b ˆ MLE )= O (1 =k 2 ) ; Var ˆ ( b ˆ MLE )= (1  ˆ 2 ) 2 1+ ˆ 2 =k + O (1 =k 2 ) : (8) Whilefor ˆ =0 ,the(asymptotic)MSEsarethesame,wenotethattheleadingtermsoftheMSEs of b ˆ norm and b ˆ MLE decayatrate   ˆ ) 2 ) as ˆ ! 1 ,whereastheMSEof b ˆ lin growswith ˆ .The followingtableprovidestheasymptoticMSEratiosof b ˆ lin and b ˆ norm forselectedvaluesof ˆ . ˆ 0.5 0.6 0.7 0.8 0.9 0.95 0.99 MSE ˆ ( b ˆ lin ) MSE ˆ ( b ˆ norm ) 2 : 2 3 : 3 5 : 7 12 : 6 50 200 5000 Inconclusion,ifitispossibletopre-computeandstorethenormsofthedatapriortodimensionality reduction,asimpleformofnormalizationcanyieldimportantwithregardtotherecoveryof innerproductsanddistancesforpairsofpointshavinghighcosinesimilarity.TheMLEcanprovide afurtherbuttheimprovementover b ˆ norm canbeatmostbyafactorof 2 . 3EstimationofcosinesimilaritybasedonquantizedRPs Thefollowingsectioncontainsourmainresults.Afterintroducingpreliminariesregardingquantiza- tion,wereviewpreviousapproachestotheproblem,beforeanalyzingestimatorsfollowingadifferent paradigm.Weconcludewithacomparisonandsomerecommendationsaboutwhattouseinpractice. Quantization. Afterobtainingtheprojecteddata Z ,thenextstepisscalarquantization.Let t =( t 1 ;:::;t K  1 ) with 0= t 0 <t 1 <:::<t K  1 <t K =+ 1 beasetof thresholds inducingapartitioningofthepositivereallineinto K intervals f [ t s  1 ;t s ) ;s 2 [ K ] g ,andlet M = f  1 ;:::; K g beasetof codes with  s representinginterval [ t s  1 ;t s ) , s 2 [ K ] .Given t and M ,thescalarquantizer(orquantizationmap)isby Q : R !M  : = [M ;z 7! Q ( z )=sign( z ) P K s =1  s I ( j z j2 [ t s  1 ;t s )) : (9) Theprojectedandquantizeddataresultas Q = f q i g n i =1 ˆ ( M  ) k , q i =  Q ( z i ( l ) )  k l =1 ,where z i ( l ) denotesthe l -thcomponentof z i 2Z , l 2 [ k ] , i 2 [ n ] .Thebitdepth b ofthequantizerisgivenby b : =1+log 2 ( K ) .Forsimplicity,weonlyconsiderthecasewhere b isaninteger.Thecase b =1 is well-studied[10,27]andishencedisregardedinouranalysistokeepourexpositioncompact. Bin-basedvs.code-basedapproaches. Let q = Q ( z ) and q 0 = Q ( z 0 ) bethepointsresultingfrom quantizationofthegenericpair z;z 0 intheprevioussection.Inthispaper,wedistinguishbetween twobasicparadigmsforestimatingthecosinesimilarityoftheunderlyingpair x;x 0 from q;q 0 .The paradigm,whichwerefertoas bin-based estimation,doesnotmakeuseofthevaluesof 3 thecodes M  ,butonlyoftheintervals(ﬁbinsﬂ)associatedwitheachcode.Thisisoppositetothe secondparadigm,referredtoas code-based estimationwhichonlymakesuseofthevaluesofthe codes.Asweelaboratebelow,anadvantageofthebin-basedapproachisthatworkingwithintervals theprocessofquantizationmorefaithfullyandhencecanbestatisticallymoreaccurate;onthe otherhand,acode-basedapproachtendstobemoreconvenientfromthepointofviewcomputation. Inthispaper,wemakeacaseforthecode-basedapproachbyshowingthatthelossinstatistical accuracycanbefairlyminorinseveralregimesofpracticalinterest. Lloyd-Max(LM)quantizer. With b respectively K beinged,oneneedstochoosethethresholds t andthecodes M ofthequantizer(thesecondiscrucialonlyforacode-basedapproach).Inour setting,with z i ( l ) ˘ N (0 ; 1) , i 2 [ n ] , l 2 [ k ] ,whichisinheritedfromthedistributionoftheentries of A ,astandardchoiceisLMquantization[15]whichminimizesthesquareddistortionerror: ( t ? ;  ? )=argmin t ;  E g ˘ N (0 ; 1) [ f g  Q ( g ; t ;  ) g 2 ] : (10) Problem (10) canbesolvedbyaniterativeschemethatalternatesbetweenoptimizationof t fored  andviceversa.Thatschemecanbeshowntodelivertheglobaloptimum[ 22 ].Intheabsenceof anypriorinformationaboutthecosinesimilaritiesthatwewouldliketorecover, (10) appearsasa reasonabledefaultwhoseuseforbin-basedestimationhasbeenin[ 28 ].Inthelimitofcosine similarity ˆ ! 1 ,itmayseemmoreplausibletouse (10) with g replacedbyitssquare,andtakingthe rootoftheresultingoptimalthresholdsandcodes.However,itturnsoutthatempiricallythisyields reducedperformancemoreoftenthanimprovements,hencewestickto(10)inthesequel. 3.1Bin-basedapproaches MLE. Givenapair q =( q ( l ) ) k l =1 and q 0 =( q 0 ( l ) ) k l =1 ofprojectedandquantizedpoints,max- imumlikelihoodestimationoftheunderlyingcosinesimilarity ˆ isstudiedindepthin[ 28 ]. Theassociatedlikelihoodfunction L ( r ) isbasedonbivariatenormalprobabilitiesoftheform P r ( Z 2 [ t s  1 ;t s ) ;Z 0 2 [ t u  1 ;t u )) ; P  r ( Z 2 [ t s  1 ;t s ) ;Z 0 2 [ t u  1 ;t u )) with ( Z;Z 0 ) r asin (1) . Itisshownin[ 28 ]thattheMLEwith b  2 canbemoreefcientatthebitlevelthancommon single-bitquantization[ 10 , 16 ];theoptimalchoiceof b increaseswith ˆ .Whilestatisticallyoptimalin thegivensetting,theMLEremainscomputationallycumbersomeevenwhenusingtheapproximation in[ 28 ]becauseitrequirescross-tabulationoftheempiricalfrequenciescorrespondingtothebivariate normalprobabilitiesabove.ThismakestheuseoftheMLEunattractiveparticularlyinsituationsin whichitisnotfeasibletomaterializeall O ( n 2 ) pairwisesimilaritiesestimablefrom ( q i ;q j ) i<j so thattheywouldneedtobere-evaluatedfrequently. Collision-basedestimator. Thecollision-basedestimatorproposedin[ 27 ]isabin-basedapproach astheMLE.Thesimilarity ˆ isestimatedas b ˆ col =   1  P k l =1 I ( q ( l ) = q 0 ( l ) ) =k  ,wherethemap  :[0 ; 1] ! [0 ; 1] isby r 7!  ( r )= P r ( Q ( Z )= Q ( Z 0 )) ,showntobemonotonically increasingin[ 27 ].ComparedtotheMLE, b ˆ col useslessinformationŒitonlycountsﬁcollisionsﬂ, i.e.,events f q ( l ) = q 0 ( l ) g .Thelossinstatisticalefyismoderatefor b =2 ,inparticularfor ˆ closeto 1 .However,as b increasesthatlossbecomesmoreandmoresubstantial;cf.Figure1.On thepositiveside, b ˆ col isconvenienttocomputegiventhattheevaluationofthefunction   1 canbe approximatedbyemployingalook-uptableaftertabulating  onagrid. Figure1:(L):AsymptoticMSEs[ 27 ]of b ˆ col (tobedividedby k )for 2  b  4 .(M,R):Asymptotic relativeefMSE ˆ ( b ˆ col ) = MSE ˆ ( b ˆ MLE ) for b 2f 2 ; 4 g ,where b ˆ MLE istheMLEin[28]. 4 b boundonbias 2 2 1 : 2  10  1 3 7 : 2  10  3 4 4 : 5  10  4 5 2 : 8  10  5 6 1 : 8  10  6 Figure2:(L): Bias 2 ˆ ( b ˆ lin ) andtheboundofTheorem1.(M):uniformupperboundson Bias 2 ˆ ( b ˆ lin ) obtainedfromTheorem1bysetting ˆ =1 .(R): Var ˆ ( b ˆ lin ) (tobedividedby k ). 3.2Code-basedapproaches Inthecode-basedapproach,wesimplyignorethefactthatthequantizeddataactuallyrepresent intervalsandtreatthempreciselyinthesamewayasfull-precisiondata.Recoveryofcosinesimilarity isperformedbymeansoftheestimatorin x 2with z;z 0 replacedby q;q 0 .Perhapssurprisingly,it turnsoutthatdependingon ˆ thelossofinformationincurredbythisrathercrudeapproachcanbe smallalreadyforbitdepthsbetween b =3 and b =5 .Thatlossincreaseswith ˆ ,withafundamental gapcomparedtobin-basedapproachesandtothefullprecisioncaseinthelimit ˆ ! 1 . Linearestimator. Weconsider b ˆ lin = h q;q 0 i =k .Wenotethat b ˆ lin = b ˆ lin ;b dependson b ; b = 1 correspondstotheestimator b ˆ lin = b ˆ lin ; 1 in x 2denotedbythesamesymbol.Acrucialdifference betweenthecode-basedandthebin-basedapproachesdiscussedaboveisthatthelatterhavevanishing asymptoticsquaredbiasoftheorder O ( k  2 ) forany b [ 27 , 28 ].Thisisnotthecaseforcode-based approacheswhosebiasneedstobeanalyzedcarefully.Theexactbiasof b ˆ lin independenceof ˆ and b canbeevaluatedexactlynumerically.Numericalevaluationsofbiasandvarianceofestimators discussedinthepresentsectiononlyrelyonthecomputationofcoef   by   : = E ˆ [ Q ( Z )  Q ( Z 0 )  ]= X ˙;˙ 0  1 ; 1 g K X s;u =1 ˙  ( ˙ 0 )    s   u P ˆ  Z 2 ˙ ( t s  1 ;t s ) ;Z 0 2 ˙ 0 ( t u  1 ;t u )  ; (11) where ; arenon-negativeintegersand ( Z;Z 0 ) arebivariatenormal (1) with r = ˆ ., wehave E ˆ [ b ˆ lin ]=  1 ; 1 , Var ˆ ( b ˆ lin )=(  2 ; 2   2 1 ; 1 ) =k .Inadditiontoexactnumericalevaluation,we provideaboundonthebiasof b ˆ lin whichexplicitlytherateofdecayindependence b . Theorem1. Wehave Bias 2 ˆ ( b ˆ lin )  4 ˆ 2 D 2 b ,where D b = 3 3 = 2 2 ˇ 12 2  2 b ˇ 2 : 72  2  2 b . AsshowninFigure2(L),theboundonthesquaredbiasinTheorem1constitutesareasonable proxyoftheexactsquaredbias.Therateofdecayis O (2  4 b ) .Moreover,itcanbev numericallythatthevarianceinthefullprecisioncaseupperboundsthevariancefor b ,i.e., Var ˆ ( b ˆ lin ;b )  Var ˆ ( b ˆ lin ; 1 ) , ˆ 2 [0 ; 1) .Combiningbiasandvariance,wemayconcludethat dependingon k ,theMSEof b ˆ lin basedoncoarselyquantizeddatadoesnottendtobefarfromwhat isachievedwithfullprecisiondata.Thefollowingtwoexamplesillustratethispoint. (i)Suppose k =100 and b =3 .Withfullprecision,wehave MSE ˆ ( b ˆ lin ; 1 )=(1+ ˆ 2 ) =k 2 [ : 01 ;: 02] . FromFigure2(M)andtheobservationthat Var ˆ ( b ˆ lin ; 3 )  Var ˆ ( b ˆ lin ; 1 ) ,wethattheMSEcan goupbyatmost 7 : 2  10  3 ,i.e.,itcanatmostdoublerelativetothefullprecisioncase. (ii)Suppose k =1000 and b =4 .Withthesamereasoningasin(i),theMSEunderquantizationcan increaseatmostbyafactorof 1 : 45 ascomparedtofullprecisiondata. Figure3showsthatthesenumbersstilltendtobeconservative.Ingeneral,thedifferenceofthe MSEsfor b = 1 ontheonehandand b 2f 3 ; 4 ; 5 g ontheotherhandgetsmorepronouncedforlarge valuesofthesimilarity ˆ andlargevaluesof k .Thisisattributedtothe(squared)biasof b ˆ lin .In particular,itdoesnotpayofftochoose k largerthantheorderofthesquaredbias. 5 Figure3:MSEsof b ˆ lin forvarious k and b 2f 3 ; 4 ; 5 g (dotted).Thesolid(red)linesindicatethe correspondingMSEsfor b ˆ lin inthefull-precisioncase( b = 1 ). Normalizedestimator. Inthefullprecisioncasewehaveseenthatsimplenormalizationofthe form b ˆ norm = h z;z 0 i = ( k z k 2 k z 0 k 2 ) canyieldsubstantialInterestingly,itturnsoutthat thecounterpart b ˆ norm = h q;q 0 i = ( k q k 2 k q 0 k 2 ) forquantizeddataisevenmorevaluableasithelps reducingthebiasof b ˆ lin = h q;q 0 i =k .Thiseffectcanbeseeneasilyinthelimit ˆ ! 1 inwhichcase Bias ˆ ( b ˆ norm ) ! 0 byconstruction.Ingeneral,biasandvariancecanbeevaluatedasfollows. Proposition1. Intermsofthecoef   in (11) ,as k !1 ,wehave j Bias ˆ [ b ˆ norm ] j =    1 ; 1  2 ; 0  ˆ   + O ( k  1 ) Var( b ˆ norm )= 1 k   2 ; 2  2 2 ; 0  2  1 ; 1  3 ; 1  3 2 ; 0 +  2 1 ; 1 (  4 ; 0 +  2 ; 2 ) 2  4 2 ; 0  + O ( k  2 ) : Figure4(L,M)graphstheabovetwoexpressions.Inparticular,theplotshighlightthereduction inbiascomparedto b ˆ lin andthefactthatthevarianceisdecreasingin ˆ asfor b = 1 .While Proposition1isasymptotic,weverifyatightagreementinsimulationsforreasonablysmall k (cf.supplement). Figure4:(L):Asymptotic Bias 2 ˆ ( b ˆ norm ) relativeto Bias 2 ˆ ( b ˆ lin ) .(M): Var ˆ ( b ˆ norm ) (asymptotic,tobe dividedby k ).(R):MSEsof b ˆ lin ; 4 vs.theMSEsof b ˆ coll ; 2 usingtwicethenumberofRPs(comparison atthebitlevel).Thestarsindicatethevaluesof ˆ atwhichtheMSEsofthetwoestimatorsareequal. 3.3Coding-basedestimationvs.Collision-basedestimation Bothschemesarecomparableintermsofsimplicity,butatthelevelofstatisticalperformancenone ofthetwodominatestheother.Thecollision-basedapproachbehavesfavorablyinahighsimilarity regimeasshowsacomparisonof MSE ˆ ( b ˆ col ) ( b =2 )and MSE ˆ ( b ˆ norm ) ( b =4 )atthebitlevel (Figure4(R)):since b ˆ col usesonlytwobitsforeachofthe k RPs,while b ˆ norm usestwiceasmany bits,wehavedoubledthenumberofRPsfor b ˆ col .Thevaluesof ˆ forwhichthecurvesofthetwo approaches(fored k )intersectareindicatedbystars.As k decreasesfrom 10 4 to 10 2 ,thesevalues increasefromabout ˆ =0 : 55 to ˆ =0 : 95 .Inconclusion, b ˆ col ispreferableinapplicationsinwhich highsimilaritiesprevail,e.g.,induplicatedetection.Ontheotherhand,forgenerichigh-dimensional data,onewouldrathernotexpect ˆ totakehighvaluesgiventhattwopointsdrawnuniformlyat randomfromthesphereareclosetoorthogonalwithhighprobability. Figure1(L)showsthatas b israised, b ˆ col requires ˆ tobeincreasinglyclosertoonetoachievelower MSE.Bycontrast,increasing b forthecoding-basedschemesyieldsimprovementsessentiallyforthe 6 wholerangeof ˆ .Aninterestingphenomenonoccursinthelimit ˆ ! 1 .Itturnsoutthattherateof decayof Var ˆ ( b ˆ norm ) isconsiderablyslowerthantherateofdecayof Var ˆ ( b ˆ col ) . Theorem2. Forany b ,wehave Var ˆ ( b ˆ norm )=  ˆ ) 1 = 2 ) ; Var ˆ ( b ˆ col )=  ˆ ) 3 = 2 ) as ˆ ! 1 : Therate   ˆ ) 3 = 2 ) isthesameastheMLE[ 28 ]whichisslowerthantherate   ˆ ) 2 ) in thefullprecisioncase(cf.§2).Weconjecturethattherate   ˆ ) 1 = 2 ) isintrinsictocode-based estimationasthisrateisalsoobtainedwhencomputingthefullprecisionMLE (3) withquantized data(i.e., z;z 0 getsreplacedby q;q 0 ). 3.4Quantizationofnorms Letusrecallthataccordingtoourbasicsetupin x 1,wehaveassumedsofarthatitispossibleto computethenorms  i = k x i k 2 2 , i 2 [ n ] ,oftheoriginaldatapriortoprojectionandquantization,and storetheminfullprecisiontoapproximatelyrecoverinnerproductsandsquareddistancesvia h x i ;x j iˇ  i  j b ˆ ij ; k x i  x j k 2 2 ˇ  2 i +  2 j  2  i  j b ˆ ij ; where b ˆ ij isanestimateofthecosinesimilarityof x i and x j .Dependingonthesetting,itmaybe requiredtoquantizethe f  i g n i =1 aswell.ItturnsoutthattheMSEforestimatingdistancescanbe tightlyboundedintermsoftheMSEforestimatingcosinesimilaritiesand max 1  i  n j b  i   i j ,where f b  i g n i =1 denotethequantizedversionsof f  i g n i =1 ;thepreciseboundisstatedinthesupplement. 4Empiricalresults:linearusingquantizedRPs OnetraditionalapplicationofRPsisdimensionreductioninlinearregressionorwith high-dimensionalpredictors[ 14 , 30 ].Theresultsof x 3.2suggestthataslongasthenumberofRPs k arenomorethanafewthousand,subsequentscalarquantizationtofourbitsisnotexpectedto havemuchofanegativeeffectrelativetousingfullprecisiondata.Inthissection,weverifythis hypothesisforfourhigh-dimensionaldatasetsfromtheUCIrepository: arcene ( d =10 4 ), Dexter ( d =2  10 4 ), farm ( d =5 : 5  10 4 )and PEMS ( d =1 : 4  10 5 ). Setup. AlldatapointsarescaledtounitEuclideannormbeforedimensionreductionandscalar quantizationbasedontheLloyd-Maxquantizer (10) .ThenumberofRPs k isvariedaccordingto f 2 6 ; 2 7 ;:::; 2 12 g .Foreachofthesevaluesfor k ,weconsider 20 independentrealizationsofthe randomprojectionmatrix A .Givenprojectedandquantizeddata f q 1 ;:::;q n g ,weestimatethe underlyingcosinesimilarities ˆ ij as b ˆ ij = b ˆ ( q i ;q j ) , i;j 2 [ n ] ,where b ˆ ( q i ;q j ) isaplaceholder foreitherthecollision-basedestimator b ˆ coll basedon b =2 bitsorthenormalizedestimator b ˆ norm for b 2f 1 ; 2 ; 4 ; 1g usingdata f q i ( l ) ;q j ( l ) g k l =1 ;one-bitquantization( b =1 )ishereincludedasa reference.The f b ˆ ij g 1  i;j  n arethenusedasakernelmatrixfedinto LIBSVM [ 9 ]totrainabinary .Predictionontestsetsisperformedaccordingly. LIBSVM isrunwith30differentvaluesof itstuningparameter C rangingfrom 10  3 to 10 4 . Results. AsubsetoftheresultsisdepictedinFigure5whichiscomposedofthreecolumns(onefor eachtypeofplot)andfourrows(oneforeachdataset).Allresultsareaveragesover 20 independent setsofrandomprojections.Theplotsintheleftcolumnshowtheminimumtesterrorsoverall30 choicesofthetuningparameter C underconsiderationindependencyofthenumberofRPs k .The plotsinthemiddlecolumnshowthetesterrorsindependencyof C foraselectedvalueof k (thefull setofplotscanbefoundinthesupplement).Theplotsintherightcolumnprovideacomparisonof theminimum(w.r.t. C )testerrorsof b ˆ coll ; 2 and b ˆ norm ; 4 atthebitlevel,i.e.,with k doubledfor b ˆ coll ; 2 . Inallplots,performanceimprovesas b increases.Whatismorenotablethoughisthat thegapbetween b =4 and b = 1 isindeedminorasanticipated.Regardingtheperformanceof b ˆ coll ; 2 and b ˆ norm ; 4 ,thelatterconsistentlyachievesbetterperformance. 5Conclusion Inthispaper,wehavepresentedtheoreticalandempiricalevidencethatitispossibletoachieve additionaldatacompressionintheuseofrandomprojectionsbymeansofcoarsescalarquantization. 7 Figure5:Resultsoftheexperiments.Eachrowcorrespondstoonedataset.(L): Accuracyonthetestset(optimizedover C )independenceofthenumberofRPs k ( log 2 scale).(M): Accuracyonthetestsetforaselectedvalueof k independenceof log 10 ( C ) .(R):Comparisonofthe testaccuracieswhenusingtheestimators b ˆ norm ; 4 respectively b ˆ coll ; 2 withtwicethenumberofRPs. Thelossofinformationincurredatthissteptendstobemildevenwiththenaiveapproachinwhich quantizeddataaretreatedinthesamewayastheirfullprecisioncounterparts.Anexceptiononly arisesforcosinesimilaritiescloseto1(Theorem2).Wehavealsoshownthatthesimpleformof normalizationemployedintheconstructionoftheestimator b ˆ norm canbeextremelyeven moresoforcoarselyquantizeddatabecauseofacrucialbiasreduction. Regardingfuturework,itisworthwhiletoconsidertheextensiontothecaseinwhichtherandom projectionsarenotGaussianbutarisefromoneofthevariousstructuredJohnson-Lindenstrauss transforms,e.g.,thosein[ 2 , 3 , 23 ].Aseconddirectionofinterestistoanalyzetheoptimaltrade-off betweenthenumberofRPs k andthebitdepth b independenceofthesimilarity ˆ ;inthepresent work,thechoiceof b hasbeendrivenwiththegoalofroughlymatchingthefullprecisioncase. 8 Acknowledgments TheworkwaspartiallysupportedbyNSF-Bigdata-1419210,NSF-III-1360971.PingLialsothanks MichaelMitzenmacherforhelpfuldiscussions. References [1] D.Achlioptas.Database-friendlyrandomprojections:Johnson-Lindenstrausswithbinarycoins. Journal ofComputerandSystemSciences ,66:671Œ687,2003. [2] N.AilonandB.Chazelle.ApproximatenearestneighborsandthefastJohnson-Lindenstrausstransform. In ProceedingsoftheSymposiumonTheoryofComputing(STOC) ,pages557Œ563,2006. [3] N.AilonandE.Liberty.AlmostoptimalunrestrictedfastJohnsonŒLindenstrausstransform. ACM TransactionsonAlgorithms ,9:21,2013. [4] T.Anderson. AnIntroductiontoMultivariateStatisticalAnalysis .Wiley,2003. [5] E.BinghamandH.Mannila.Randomprojectionindimensionalityreduction:applicationstoimageand textdata.In ConferenceonKnowledgeDiscoveryandDataMining(KDD) ,pages245Œ250,2001. [6] P.BoufounosandR.Baraniuk.1-bitcompressivesensing.In InformationScienceandSystems ,2008. [7] C.Boutsidis,A.Zouzias,andP.Drineas.RandomProjectionsfork-meansClustering.In Advancesin NeuralInformationProcessingSystems(NIPS) ,pages298Œ306.2010. [8] E.CandesandT.Tao.Near-optimalsignalrecoveryfromrandomprojections:Universalencoding strategies? IEEETransactionsonInformationTheory ,52:5406Œ5425,2006. [9] C-C.ChangandC-J.Lin.LIBSVM:Alibraryforsupportvectormachines. ACMTransactionsonIntelligent SystemsandTechnology ,2:27:1Œ27:27,2011. http://www.csie.ntu.edu.tw/~cjlin/libsvm . [10] M.Charikar.Similarityestimationtechniquesfromroundingalgorithms.In ProceedingsoftheSymposium onTheoryofComputing(STOC) ,pages380Œ388,2002. [11] S.Dasgupta.LearningmixturesofGaussians.In SymposiumonFoundationsofComputerScience(FOCS) , pages634Œ644,1999. [12] S.Dasgupta.AnelementaryproofofatheoremofJohnsonandLindenstrauss. RandomStructuresand Algorithms ,22:60Œ65,2003. [13] M.Datar,N.Immorlica,P.Indyk,andV.Mirrokni.Locality-SensitiveHashingSchemeBasedonp-Stable Distributions.In SymposiumonComputationalGeometry(SCG) ,pages253Œ262,2004. [14] D.FradkinandD.Madigan.Experimentswithrandomprojectionsformachinelearning.In Conferenceon KnowledgeDiscoveryandDataMining(KDD) ,pages517Œ522,2003. [15] A.GershoandR.Gray. VectorQuantizationandSignalCompression .Springer,1991. [16] M.GoemansandD.Williamson.ImprovedApproximationAlgorithmsforMaximumCutand ProblemsUsingProgramming. JournaloftheACM ,42:1115Œ1145,1995. [17] P.IndykandR.Motwani.Approximatenearestneighbors:towardsremovingthecurseofdimensionality. In ProceedingsoftheSymposiumonTheoryofComputing(STOC) ,pages604Œ613,1998. [18] J.Matousek.OnvariantsoftheJohnson-Lindenstrausslemma. RandomStructuresandAlgorithms , 33:142Œ156,2008. [19] L.Jacques.AQuantizedJohnson-LindenstraussLemma:TheFindingofBuffon'sneedle. IEEETransac- tionsonInformationTheory ,61:5012Œ5027,2015. [20] W.JohnsonandJ.Lindenstrauss.ExtensionsofLipschitzmappingsintoaHilbertspace. Contemporary Mathematics ,pages189Œ206,1984. [21] K.Kenthapadi,A.Korolova,I.Mironov,andN.Mishra.PrivacyviatheJohnson-LindenstraussTransform. JournalofPrivacyand ,5,2013. [22] J.Kieffer.Uniquenessoflocallyoptimalquantizerforlog-concavedensityandconvexerrorweighting function. IEEETransactionsonInformationTheory ,29:42Œ47,1983. 9 [23] F.KrahmerandR.Ward.NewandimprovedJohnson-LindenstraussembeddingsviatheRestricted IsometryProperty. SIAMJournalonMathematicalAnalysis ,43:1269Œ1281,2011. [24] J.LaskaandR.Baraniuk.Regimechange:Bit-depthversusmeasurement-rateincompressivesensing. IEEETransactionsonSignalProcessing ,60:3496Œ3505,2012. [25] M.Li,S.Rane,andP.Boufounos.Quantizedembeddingsofscale-invariantimagefeaturesformobile augmentedreality.In InternationalWorkshoponMultimediaSignalProcessing(MMSP) ,pages1Œ6,2012. [26] P.Li,T.Hastie,andK.Church.ImprovingRandomProjectionsUsingMarginalInformation.In Annual ConferenceonLearningTheory(COLT) ,pages635Œ649,2006. [27] P.Li,M.Mitzenmacher,andA.Shrivastava.CodingforRandomProjections.In Proceedingsofthe InternationalConferenceonMachineLearning(ICML) ,pages676Œ678,2014. [28] P.Li,M.Mitzenmacher,andM.Slawski.QuantizedRandomProjectionsandNon-LinearEstimationof CosineSimilarity.In AdvancesinNeuralInformationProcessingSystems(NIPS) ,pages2756Œ2764.2016. [29] M.Mahoney.RandomizedAlgorithmsforMatricesandData. FoundationsandTrendsinMachine Learning ,3:123Œ224,2011. [30] O.MaillardandR.Munos.Compressedleast-squaresregression.In AdvancesinNeuralInformation ProcessingSystems(NIPS) ,pages1213Œ1221.2009. [31] S.RaneandP.Boufounos.Privacy-preservingnearestneighbormethods:Comparingsignalswihtout revealingthem. IEEESignalProcessingMagazine ,30:18Œ28,2013. [32] S.Rane,P.Boufounos,andA.Vetro.Quantizedembeddings:Anefcientanduniversalnearestneighbor methodforcloud-basedimageretrieval.In SPIEOpticalEngineeringandApplications ,pages885609Œ 885609.InternationalSocietyforOpticsandPhotonics,2013. [33] S.Vempala. TheRandomProjectionMethod .AmericanMathematicalSociety,2005. 10  
R2SDH:RobustRotatedSupervisedDiscreteHashing JieGui RutgersUniversity Piscataway,NJ,08854,USA guijiejie@gmail.com PingLi BaiduResearch Bellevue,WA,98004,USA pingli98@gmail.com ABSTRACT Learning-basedhashinghasrecentlyreceivedconsiderableatten- tionsduetoitscapabilityofsupportinge˝cientstorageandre- trievalofhigh-dimensionaldatasuchasimages,videos,anddoc-  uments.Inthispaper,weproposealearning-basedhashingalgo- rithmcalledobustRotatedSupervisedDiscreteHashing(R 2SDH), byextendingthepreviousworkonupervisedDiscreteHash- ing(SDH).InR 2SDH, correntropy isadoptedtoreplacetheleast squareregression(LSR)modelinSDHforachievingbetterrobust-  ness.Furthermore,consideringthecommonlyuseddistancemet-  ricssuchascosineandEuclideandistanceareinvarianttorota- tionaltransformation, rotation isintegratedintotheoriginalzero- onelabelmatrixusedinSDH,asadditionalfreedomtopromote ˚exibilitywithoutsacri˙cingaccuracy.Therotationmatrixislearned throughanoptimizationprocedure.Experimentalresultsonthree imagedatasets(MNIST,CIFAR-10,andNUS-WIDE)con˙rmthat R2SDHgenerallyoutperformsSDH. KEYWORDS superviseddiscretehashing,robustM-estimator,rotation ACMReferenceFormat: JieGuiandPingLi.2018.R 2SDH:RobustRotatedSupervisedDiscreteHash- ing.In KDD'18:The24thACMSIGKDDInternationalConferenceonKnowl- edgeDiscovery&DataMining,August1923,2018,London,UnitedKingdom. ACM,NewYork,NY,USA,9pages.https://doi.org/10.1145/3219819.3219955 1INTRODUCTION  Inlarge-scaleapplicationsinsearch,datamining,andmachine learning,itishighlydesirablethatthedatashouldbeorganized andindexede˝cientlyandaccurately.Asawell-establishedand  powerfullarge-scaletechnique, hashing hasshownpromisingper- formanceandhasreceivedgreatattentionsfromresearchersin  datamining,machinelearning,computervision,informationre-  trieval,andrelatedareasduetoitspracticalutility.Hashinggen- erallyinvolvesgeneratingarangeofhashfunctionstomapeach instancesuchasanimage,avideo,adocumentorothertypesof  dataintoavectorofbinarycode.Theproducedhashcodesshould preservetheoriginalinstancestructure(e.g.,similaritiesbetween theoriginalinstances).Ingeneral,distancecomputationscanbe Permissiontomakedigitalorhardcopiesofallorpartofthis workforpersonalor classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed forpro˙torcommercialadvantageandthatcopiesbearthisnoticeandthefullcita- tiononthe˙rstpage.Copyrightsforcomponentsofthisworkownedbyothersthan theauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyother- wise,orrepublish,topostonserversortoredistributetolists,requirespriorspeci˙c permissionand/orafee.Requestpermissionsfrompermissions@acm.org. KDD'18,August1923,2018,London,UnitedKingdom ©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtotheAs- sociationforComputingMachinery. ACMISBN978-1-4503-5552-0/18/08...$15.00 https://doi.org/10.1145/3219819.3219955 conductedverye˝cientlyintheHammingspace.Thus,afterthe dataarehashed,subsequentalgorithmsbasedonpairwisecompar- isonscanbecalculatedmoree˝cientlyinlarge-scaledatasets.Due tothe˚exibilityofbinaryrepresentations,hashingmethodscan beappliedinmanysituations,forinstance,searchinge˝ciently byinvestigatingonlyinstancesfallingintobucketsclosetothe  querybasedontheHammingdistanceorusingthebinarycodes forothertaskssuchasfacerecognition,imageclassi˙cation,digit recognition,andfaceindexing. Generallyspeaking,hashingalgorithmscanbeclassi˙edinto twomaincategories:(A)learning-freehashingmethods;and(B)  learning-based(alsoknownasdata-dependent)hashingmethods. 1.1Learning-FreeHashingMethods Learning-freehashingmethodsdonotneedtrainingdataandran- domlyestablishaseriesofhashfunctionswithoutanylearning. Therearenumerouslearning-freemethods.Ifwerestrictthedis- cussiontomethodswhichproducebinary(orinteger)outputs,then popularlearning-freealgorithmsincludeatleastthefollowing: Sign(Gaussian)randomprojections :thecollisionprobability (orempiricallythehammingdistance)ofthehashcodeis proportionaltothecosinesimilarityofthedata[6,8,20]. SignCauchyrandomprojections :thecollisionprobabilityis proportionaltothe ˜2similarityofthedata[22]. (b-bit)minwisehashing (forbinarydata):thecollisionprob- abilityisproportionaltotheresemblance[24,19,21]. Consistentweightedsampling(CWS) :thecollisionprobabil- ityisproportionaltothemin-maxsimilarity[13,17,18,31]. Apparently,theadvantageoflearning-freemethodsisthatthey donotrequireanytraining.However,onewillthenhavetochoose appropriatehashingmethodsbasedondata,otherwisetheperfor- mancemightbedisappointing.Forexample,whilerandomprojec- tionmethods(andvariantssuchasrandomfourierfeatures)are verypopular,theyareoftennotasaccurate[18](i.e.,requiring longhashingcode).Minwisehashing(andvariants)andconsistent weightedsamplinggenerateintegerhashcodewhichareequiva- lenttohigh-dimensionalsparsebinarydataandtypicallyoutper- formrandomprojection-basedmethods(intermsofaccuracyver- susnumberofhashes)inrealdata. 1.2Learning-BasedHashingMethods Recently,data-dependent(learning-based)hashingalgorithmshave becomepopular,since(potentially)learnedcompacthashcodes wouldbeabletoe˝cientlyande˛ectivelyorganizeandindexlarge- scaledata.Unlikelearning-freemethodswhichrandomlyconstruct  hashfunctions,learning-basedhashingalgorithmsaimtogenerate shorthashcodesbyutilizingtrainingdata.Anenormousvolume ofliteraturehasbeendevotedtodevelopawidevarietyofhas h- ingalgorithmswhichcanbefurtherdividedinintothefollowing sub-categories. Unsupervisedhashing :Thetraininginstancelabelsarenot requiredinlearning.Forexample,Gongetal.[10]presentedanit- erativequantization(ITQ)methodthatminimizedthebinarization lossbetweentheoriginalinstancesandhashcodes.Weissetal.[44] proposedaspectralhashing(SH)methodwhoseobjectivefunc- tionwassimilartothatofLaplacianeigenmaps[1].Otherunsu-  pervisedlearning-basedhashingalgorithmssuchasanchorgraph hashing(AGH)[28],inductivemanifoldhashing(IMH)[38]with t-distributedstochasticneighborembedding(t-SNE)[30],andscal-  ablegraphhashingwithfeaturetransformation(SGH)[14]have alsobeenproposed.Becauseunsupervisedhashingdoesnottake intoconsiderationthetrainingexamplelabels,usefulinformation keytoclassi˙cationmaybelost.Therefore,manysemi-supervised hashingandsupervisedhashingmethodshavebeenproposed. Semi-supervisedlearning-basedhashing :Hashfunctionis learnedfromlabeledexamplesandconsiderablymoreunlabeled  examples.Forexample,KulisandDarrell[15]presentedabinary reconstructiveembedding(BRE)algorithmthatminimizedthere-  constructionerrorbetweentheoriginalEuclideandistanceand thelearnedHammingdistance.Wangetal.[43]presentedasemi- supervisedhashing(SSH)methodthatsimultaneouslymaximized  thevarianceofall(unlabeledandlabeled)traininginstancesand minimizedtheempiricallossforpairwiselabeledtraininginstances. Supervisedlearning-basedhashing :Hashfunctionsarelearned usingtraininginstancelabels.Forexample,Rastegarietal.[35] proposedpredictabledual-viewhashingwhichaddedtheideaof supportvectormachines(SVMs)tohashlearning.Liuetal.[27] presentedakernel-basedsupervisedhashing(KSH)algorithmthat  neededalimitedamountoflabelinformation,i.e.,dissimilarand similarinstancepairs.Othersuperviseddata-dependenthashing methodsincludelineardiscriminantanalysisbasedhashing(LDA-  Hash)[41]andfastsupervisedhashingusinggraphcutsandde- cisiontrees(FastHash)[23,24].Inouropinion,ranking-basedal- gorithms[33]inwhichthesupervisedinformationarecomposed ofrankinglabelssuchastripletsarealsopartofthesupervised hashingalgorithms. Multimodalhashing :Itincludescross-modalhashingand multi-sourcehashing.Incross-modalhashing[16],thequeryde- notesonemodalitywhiletheoutputdenotesanothermodality.For instance,givenatextquery,imagesarereturnedwhichrepresent  thetext.Inmulti-sourcehashing[39,45],itisassumedthatall viewsaregivenforaqueryandthegoalistolearnbetterhash codesthanunimodalhashingbyfullyutilizingalltheseviews.Thus,  bothcross-modalhashingandmulti-sourcehashingutilizemulti- modalinformation[5].However,theyareutilizedindi˛erentsitu- ationsandcross-modalhashingmayhavewiderapplicationsthan multi-sourcehashinginrealapplications. Deeplearning-basedhashing :Manyalgorithmshavebeen proposedoverthepastfewyears,someofwhichhavebeensuc- cessfullyappliedtomanyrealapplicationssuchasimageretrieval, actionrecognition,andimageclassi˙cation.Asfarasweknow, semantichashing[36]isthe˙rsttousedeeplearningforhashing. Thisseminalworkusedthestacked-restrictedBoltzmannmachine (RBM)tolearncompactbinarycodesforvisualsearch.However, themodelwascomplicatedandneededpre-training,whichisin- e˝cientinrealapplications.Someotherrelatedalgorithmscanbe foundin[25,40]. Ingeneral,hashcodesconsistof-1and1or0and1.Thediscrete constraintsimposedonthebinarycodesleadtocomplexbinary optimizationproblems,whicharegenerallyNP-hard.Tofacilitate theoptimizationinhashcodelearning,mosthashingalgorithms ˙rstdonottakeintoconsiderationthediscreteconstraints,then solvearelaxedandcontinuousproblem,and˙nallyturnrealval-  uesintotheapproximatebinarycodesbythresholding(orquanti- zation).Thisrelaxationskillgreatlysimpli˙estheoriginalcompli- catedmixedintegeroptimizationproblem.However,theapproxi-  matesolutionisapparentlysuboptimal,oftenoflowquality,and reducesthee˛ectivenessofthe˙nalbinarycode.Itispossiblydue totheaccumulatedquantizationerror,especiallywhenlearning longbinarycodes.Mostexistinghashingmethodsdonottakeinto considerationthesigni˙canceofdiscreteoptimization.Shenetal. [37]proposedanovelsuperviseddiscretehashing(SDH)method thatdirectlylearnedthebinarycodeswithoutrelaxation.Tofully utilizelabelinformation,SDHwasformulatedasaleastsquaresre- gressionthatregressedthecorrespondinglabeloneachhashcode.  Luoetal.[29]proposedrobustdiscretecodemodelingforSDH. Guietal.[11]proposedSDHwithrelaxation(SDHR). 1.3SummaryofContribution Leastsquaresregression(LSR)isawidely-utilizedstatisticalanaly- sismethod,whichhasbeensuccessfullyusedinmanydatamining andmachinelearningtasks,suchasclassi˙cation,clustering,di-  mensionalityreduction,andmulti-viewlearning.However,theor-  dinaryLSRissensitivetooutliers[32],whichmotivatesustocon- siderstrategiesforenhancingtherobustnessinSDH.Tothisend, weproposearobustSDHmodelwhichusesrobustM-estimator[26] andcanbede˙nedsuchasWelschM-estimatorandCauchyM-  estimator.Furthermore,theoriginalLSRutilizethelabelmatrix astheregressiontarget,whichis˙xed.Toincreasethe˚exibility, weaddarotationmatrixtothelabelmatrix.Therotationmatrix issolvedthroughoptimizationwhichismore˚exibleandmaybe  moreaccurate.WenameouralgorithmobustRotatedSuper- visedDiscreteHashing(R 2SDH). Therestofthepaperisorganizedasfollows:Section2brie˚yin- troducessuperviseddiscretehashing.Section3presentstheR 2SDH. Section4reportsonourexperimentalresults.Finally,Section5  concludesthepaper. 2REVIEWOFSUPERVISEDDISCRETE HASHING(SDH) Webrie˚yreviewtherelatedworkSDH[37]inthissection.Given n(SeeTable1forthenotationsusedinthispaper)examples X=fxigni=1,ouraimistolearnthecorrespondinghashcodes B=fbigni=12f 1;1gnltoretaintheirsimilaritiesintheoriginal space,wherethe i-throwvector biisthe l-bitshashcodesfor xi.Thelabelmatrixisdenotedas Y=fyigni=12Rnc,where cde- notesthenumberofclasses.Theterm yik isthe kthelementof yiand yik =1if xiisfromclass kand0otherwise. Table1:Notation Symbol Description X2Rndthedatamatrix xi2R1dthe i-thexample B2Rnlthehashcodes bi2R1lthehashcodefor xi(the i-throwvectorof B)Y2Rnctheclasslabelsofalltrainingexamples yi2R1cthe i-throwvectorofthelabelmatrix YP2Rmltheprojectionmatrixforthenonlinearembedding W2Rlctheprojectionmatrixforthehashcode R2Rcctherotationmatrix nthetrainingexamplesize:thenumberofalltraininginstanc es lthehashcodelength cthenumberofclasses F¹º thenonlinearembeddingtoapproximatethehashcode mthenumberofanchorpoints fajgmi=1randomlyselected manchorpointsfromthetraininginstances ˚¹º the m-dimensionalvectorobtainedbythekernelfunction thekernelparameteroflasttermofSDH'sobjectivefunction g¹º therobustM-estimator ˙2thekernelwidthoftherobustM-estimator rtheHammingradius SDH'sobjectivefunctionisde˙nedas: min B;F;WnÕi=1kyibiWk22+kWk2F+vnÕi=1kbiF¹xiºk 2 2(1) s:t:8ib i2f 1;1gl;whichcanbewritten,equivalently,as min B;F;WkYBW k2F+kWk2F+vkBF¹Xºk 2F(2) s:t:B2f 1;1gnl;where kk FisthematrixFrobeniusnormand Wistheprojection matrix.The˙rsttermof(2)istheleastsquaresregression,which isusedtoregressthecorrespondingclasslabeloneachhashcode. Thesecondtermof(2)isusedforregularization. F¹º inthelast termof(2)isaneasyyetusefulnonlinearembeddingtoapproxi- matethehashcode F¹xº=˚¹xºP;(3) where ˚¹xºisan m-dimensionalvectoracquiredbytheRBFkernel: ˚¹xº=»exp kxa1k2; ;exp kxamk2¼:(4) Theterms fajgmj=1arerandomlyselected manchorpointsfrom thetraininginstances,and istheGaussiankernelwidthparame- ter.Thematrix P2Rmlprojects ˚¹xºontothelow-dimensional space.Similarformulationsasequation(3)arewidelyadoptedin suchasbinaryreconstructiveembedding(BRE)[15]andkernel- basedsupervisedhashing(KSH)[27]. ThereasonwhyaGaussiankernelfunctionisusedisshown asfollows:whentheunderlyingfeatureembeddingforthekernel isunknown,existingmethodssuchasLSHdonotapplyforker- nelizeddatainhigh-dimensionalspace.Furthermore,theusage  ofkernelgeneralizessuchmethodsasLSHtoadjusttoarbitrary kernels,makingitpossibletokeepthealgorithm'ssublineartime similaritysearchguaranteesforalargenumberofusefulsimilarity  functions. SDH'soptimizationhasthreesteps:theF-steptosolve P,the W-steptosolve W,andtheB-steptosolve B:F-step: If Bis˙xed,itiseasytosolvetheprojectionmatrix P:P=˚¹XºT˚¹Xº1˚¹XºTB:(5) W-step: Wiseasilycomputedandhasaclosed-formsolution by˙xing B:W=BTB+I 1BTY:(6) B-step: By˙xingallothervariables,[37]proposedsolving Bit- erativelythroughadiscretecycliccoordinatedescentprocedure. 3R 2SDH:ROBUSTROTATEDSUPERVISED DISCRETEHASHING Inthissection,weintroduceourproposedmethodindetail.  3.1Correntropy Asusingtheordinaryleastsquareregressionmightbesensitiveto outliers,inthisstudy,weadopttheideaofcorrentropy[26]in ordertoincreaserobustnessofthehashingprocedure.Speci˙cally, wemodifytheoriginalSDHformulationasfollows: min B;F;WÕni=1g¹YBW ºi+kWk2F+vkBF¹Xºk 2F(7) s:t:B2f 1;1gnlwherewerecall ¹YBW ºiisthe i-throwof ¹YBW º.Theterm g¹º in(7)istherobustM-estimatorandcanbede˙nedsuchas WelschM-estimatorandCauchyM-estimator.Inthisstudy,wefo- cusontheWelschM-estimator: g¹xº=1exp  kxk22˙2!;(8) where ˙2isthekernelwidth.Wewilldescribeaprocedurefor determining ˙2.3.2R 2SDH:RobustRotatedSupervisedDiscrete Hashing Ifwerotatethelabelmatrix,thedistancebetweenthecodewords ofdi˛erentclassesremainsthesame.Forexample,forbinaryclas- si˙cation,thecodewordsforthe˙rstclassandsecondclassare y1=»10 ¼(Fig.1)and y2=»01 ¼,respectively.Ifwerandomly rotate y1and y2clockwiseorcounterclockwise,thedistancebe- tween y1and y2remainsthesame.Therefore,weaddrotationto (7)toobtainmore˚exibilities.R 2SDH'sobjectivefunctionis min B;F;W;RÕni=1g¹YR BW ºi+kWk2F+vkBF¹Xºk 2F(9) s:t:B2f 1;1gnl;RR T=I;where Risarotationmatrix.  Figure1:Randomrotationof y1and y2.3.3Optimization Theproblemformulatedin(9)isamixedbinaryintegerprogram with˙veunknownvariables.Alternatingoptimizationisadopted tosolvetheproblemiteratively. D-step: Basedonhalf-quadraticoptimization[12],theproblem (9)canbesolvedinanalternateminimizationwayasfollows: Di=exp ¹YR BW ºi 22.˙2;(10) ˙2=¹YR BW ºi¹YR BW ºiT˚c(11) where isatuningparameterandwerecall cisthenumberof classesand ¹YR BW ºiisthe i-throwof ¹YR BW º.Inthenextstep,weneedtosolvethefollowingprogram: min B;F;W;Rtr ¹YR BW ºTD¹YR BW º+kWk2F+vkBF¹Xºk 2Fs:t:B2f 1;1gnl;RR T=I;(12) where tr ¹º isthetracenormand Disadiagonalmatrixandthe i-thdiagonalelementis Di.Theproblem(12)canbesolvedwith regardto B,F,W,and R,respectively,by˙xingtheothervariables. W-step: For˙xed B,F,and R,Wcanbesolvedbysettingthe derivativeof(12)tozero.Therefore, Whasaclosed-formsolution asfollowing: W=BTDB +I 1BTDYR :(13) R-step: For˙xed B,F,and W,werewrite(12)as min R D1š2¹YR BW º  2Fs:t:RR T=I:(14) Theorem1. Supposethesingularvaluedecompositionof YTDBW is UVT,theclosedformsolutionof Ris R=UV T.Proof: TheLagrangefunctionis L¹R;Aº=D1š2¹YR BW º2F+tr ARR TI ;(15) whichequals L¹R;Aº=tr  RTYTWTBTD¹YR BW º+tr ARR TI =tr YTDYRR T2tr WTBTDYR +tr RTAR +const ;(16) where A2RccistheLagrangemultiplierandisasymmetric matrix.Since RR T=RTR=I,(16)isequalto L¹R;Aº=2tr WTBTDYR +tr RTAR +const :(17) Bysettingthederivativeof L¹R;Aºwithrespectto Rtozero,we have @L¹R;AºR=YTDBW +AR =0:(18) Therefore,weget R=A1YTDBW :(19) Since RR T=I,weobtain A1YTDBWW TBTDYA 1=I:(20) Thus,wehave A=YTDBW YTDBW T1š2:(21) Bysubstituting(21)in(19),weobtain R=YTDBW YTDBW T1š2YTDBW (22) =UVTVUT1š2UVT=UV T:Thiscompletestheproof. B-step: Inordertosolve B,(12)canberewrittenas: min Btr ¹YR BW ºTD¹YR BW º+vkBF¹Xºk 2F(23) s:t:B2f 1;1gnlwhichcanbeequivalentlywrittenas min Btr WTBTDBW 2tr ¹BTMº(24) s:t:B2f 1;1gnl;where M=DYRW T+vF ¹Xº.LikeinSDH[37],wealsousethedis- cretecycliccoordinatedescentmethodistosolve B.Duetothe Dmatrixintheformulation,wecannotsimplyusetheresultin[37] viaastraightforwardmatrixtransformation. Let bTbethe mthcolumnof B,m=1; ;land B0thematrix of Bexcluding bT,wbethe mthrowof Wand W0thematrixof Wexcluding w.Similarly,let qthe mcolumnof Mand M0thematrix Mexcluding q.Thenwehave tr WTBTDBW =tr  ¹W0ºT¹B0ºT+wTbDB0W0+bTw =const +tr wTbDb Tw+2tr bDB 0W0wT:(25) Since bDb T=ÍiDib2i=ÍiDiwhere biisthe i-thelementof b,bDb Tisaconstant. Similarly,since BTM=¹B0ºTM0¹B0ºTqbM 0bq ,wehave tr ¹BTMº=const +tr ¹bq º:(26) Therefore,(24)isequalto min tr ¹bDB 0W0wTqºs:t:b2f 1;1g1l:(27) Thesolutionto(27)is b=sign qDB 0W0wTT:(28) F-step: ThisstepisthesameasthatofSDH.Thepseudocode ofR 2SDHisshowninAlgorithm1. Algorithm1 RobustRotatedSupervisedDiscreteHashing (R 2SDH) Inputs: trainingdatamatrix X=fxigni=1;labelmatrix Y=fyigni=1;hashcodelength l;parameter ;maximumiteration number t.Output: hashcodes fbigni=12f 1;1gnlRandomlyselect minstances fajgmi=1fromthetrainingdataand obtainthe ˚¹xºthroughtheGaussiankernelfunction; Initialize Basa f 1;1gnlmatrixrandomly; Initialize Dand Rasidentitymatrixes; Use(13)toinitialize W;Use(5)toinitialize P;repeat D-step Use(10)tosolve D;B-step Usediscretecycliccoordinatedescenttosolve Bbased on(28); W-step Use(13)tosolve W;F-step Use(5)tosolve P;R-step Use(22)tosolve R;until convergence 4EXPERIMENTS Inthissection,weinvestigatetheperformanceofourproposed R2SDHalgorithm.Allofourexperimentshavebeenconductedon aserverwithanIntelXeonprocessor(2.80GHz),128GBRAM, andcon˙guredwithMicrosoftWindowsServer2008andMAT-  LAB2014b. Weconductedexperimentsusingthreelarge-scaleimagedatasets, MNIST 1,CIFAR-10 2,andNUS-WIDE 3.TheproposedalgorithmR 2SDH iscomparedwithrepresentativehashingalgorithmssuchasBRE [15],SDHR[11],KSH[27],SSH[43],FastHash[23,24],AGH[28], andIMH[38]witht-SNE[30].Foriterativequantization(ITQ)  [9,10],weutilizebothitsunsupervisedversionPCA-ITQandsu- pervisedversionCCA-ITQ.Canonicalcorrelationanalysis(CCA) 1http://yann.lecun.com/exdb/mnist/ 2https://www.cs.toronto.edu/~kriz/cifar.html 3http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm isutilizedasthepreprocessingforCCA-ITQ.Weutilizethepub-  licMATLABcodesandtheparameterssuggestedbytheauthors. Morespeci˙cally,forSDHandR 2SDH, and vareempiricallyset to1and1e-5,respectively;themaximumiterationnumber tisset to5.Theparameter inR 2SDHisestimatedby10-foldcrossval- idation.ForAGH,IMH,SDH,andR 2SDH,weuse1,000randomly sampledanchorpoints. TheexperimentalresultsarereportedbasedonHammingrank- ing(themeanofaverageprecision,MAP),accuracy,trainingtime, andhashlookupincludingprecision,recall,F-measureofHam- mingradiustwo(@ r=2).WesettheHammingradius rtobe2asin [7,37].TheF-measure'sde˙nitionis2 precision recall/(precision +recall).Thefollowingevaluationmetricisalsousedtomeasure theperformanceofthevariousmethods:precisionat Nsamples (precision@sample= N),whichisde˙nedasthepercentageoftrue neighborsamongthetop Nretrievedsamples.Weset Nto500as in[7].Notethataqueryisconsideredtobeafalseexampleifnoin- stanceisreturnedwhencalculatingprecisions.Groundtruthsare theexamplelabels. 4.1ResultsontheMNISTDataset MNISThas70,000784-dimensionalhandwrittendigitimagesfrom `0'to`9'.Everyimageiscroppedandnormalizedto28 28pixels. Thisdatasetissplitintoatestsetwith1,000instancesandatrain- ingsetwithallremaininginstances.Theexperimentalresultson MNISTarelistedinTable2.R 2SDHoutperformstheothermethods intermsofprecision@ r=2,recall@ r=2,F-measure@ r=2,MAP,and accuracy,whilePCA-ITQperformsbestintermsoftrainingtime.  However,R 2SDH'soverallperformanceismuchbetterthanthat ofPCA-ITQ.Forexample,R 2SDH'sF-measure@ r=2andMAPare morethanfourtimesandtwotimesbetterthanthatofPCA-ITQ onthissituation.Moreover,theprecision@sample=500,precision ofHammingradiustwo,recallofHammingradiustwo,F-measure ofHammingradiustwo,MAP,andaccuracycurvesareplottedin Figs.2-7,respectively.Onlysomemethodsarepresenteddueto  spacelimitations.R 2SDHperformsbetterthantheothermethods inmostcases.Moreover,R 2SDHparticularlyperformsbetterthan theothermethodsintermsofprecision@sample=500.  Figure2:Precision@sample=500ontheMNISTdatasetwith di˛erentnumberofhashingbits. Table2:ExperimentalresultsontheMNISTdatasetwhenthenu mberofhashingbitsis16. Method precision@ r=2recall@ r=2 F-measure@ r=2MAP accuracy trainingtime(sec) R2SDH 0.9373 0.8801 0.9078 0.9364 0.967 52.3 SDH 0.8908 0.8569 0.8735 0.9276 0.925 39.1 SDHR 0.9301 0.8693 0.8987 0.9206 0.955 78.4 BRE 0.5741 0.1263 0.2071 0.3646 0.701 2884.7 KSH 0.8869 0.6287 0.7358 0.8670 0.906 318.8 SSH 0.3070 0.4235 0.3559 0.3569 0.525 68.6 CCA-ITQ 0.8136 0.4875 0.6097 0.7515 0.863 6.6 FastHash 0.5560 0.2310 0.3264 0.5253 0.588 1017.4 PCA-ITQ 0.6965 0.1107 0.1910 0.4260 0.732 4.5 AGH 0.6132 0.4712 0.5329 0.5995 0.785 6.8 IMH 0.6694 0.4238 0.5190 0.5996 0.813 32.1   Figure3:PrecisionofHammingradiustwoontheMNIST datasetwithdi˛erentnumberofhashingbits.  Figure4:RecallofHammingradiustwoontheMNISTdata setwithdi˛erentnumberofhashingbits. 4.2ResultsontheCIFAR-10Dataset Asasubsetofthecelebrated80Mtinyimagecollection[42],there are60,000imagesonCIFAR-10from10classeswith6,000instances   Figure5:F-measureofHammingradiustwoontheMNIST datasetwithdi˛erentnumberofhashingbits.  Figure6:MAPontheMNISTdatasetwithdi˛erentnumber ofhashingbits. foreveryclass.Everyimageinthisdatasetisrepresentedasa512- dimensionalGISTfeaturevector[34].Thedatasetissplitintoa trainingsetwith59,000instancesandatestsetwithallremain- inginstances.TheexperimentalresultsonCIFAR-10arelistedin  Figure7:AccuracyontheMNISTdatasetwithdi˛erent numberofhashingbits. Table3whenthenumberofhashingbitsis32.Precision@ r=2, recall@ r=2,F-measure@ r=2,MAP,accuracy,andtrainingtime,are reported.ForSSH,5,000labeledexamplesareusedforsimilarity  matrixconstruction.R 2SDHperformsbetterthanSDHinterms ofprecision@ r=2,recall@ r=2,F-measure@ r=2,MAP,andaccu- racy.Forinstance,therecall@ r=2ofR 2SDHis1.02-timeshigher thanthatofSDH.Furthermore,fromthelastcolumninTable3,it  canbeseenthatthetrainingtimeofbothR 2SDHandSDHare aboutoneminute,whichisquitee˝cient.Incontrast,ittakes  FastHashandKSHabout20minutesandmorethan40minutesto  train,respectively.Speci˙cally,thetrainingofR 2SDHisabout16- timesand36-timesmoree˝cientthanFastHashandKSH,respec- tively,onthissituation.SDHR,SSH,CCA-ITQ,PCA-ITQ,AGH, andIMHarealsoverye˝cient;however,R 2SDHgenerallyout- performsthem.Theprecisionat500instances(precision@sample  =500),precision@ r=2,andaccuracyversusthenumberofhashing bitsareplottedinFigs.8-10,respectively.Duetospacelimitations, onlysomealgorithmsareillustratedinthecorresponding˙gure.In regardtoprecision@ r=2,R 2SDHperformsbetterthantheotheral- gorithmswhenthenumberofhashingbitsislargerthan32,and KSHoutperformstheothermethodswhenthenumberofhashing bitsis16.R 2SDHperformsthebestaccordingtoprecisionat500 instances(precision@sample=500)andaccuracy,demonstrating  thee˛ectivenessofR 2SDH. 4.3ResultsontheNUS-WIDEDataset TheNUS-WIDEdatasethasabout270,000imagescollectedfrom Flickr.NUS-WIDEcontains81groundtruthconceptlabels,with everyimagehavingmultiplelabels.Thetrueneighborsofaquery arede˙nedastheimagessharingatleastonelabelswiththequery  image.Weutilizetheprovided500-dimensionalbag-of-wordsfea- tures.Asin[28],the21mostfrequentlabelareusedfortesting. Foreachclass,100imagesareuniformlysampledforthequeryset andtheremainingimagesareusedfortraining.Forthislargedata- base,allthetrainingexamplesareutilizedforthee˝cientR 2SDH, SDH,andCCA-ITQ.ForBRE,MLHandKSH,5,000imagesaresam- pledfortraining.ExperimentalresultsaregiveninTable4.Since SDHRcannotsolvemulti-labellearningandcanonlybeusedfor   Figure8:Precision@sample=500ontheCIFAR-10dataset withthenumberofhashingbitsfrom16to128.    Figure9:PrecisionofHammingradiustwoontheCIFAR-10 datasetwiththenumberofhashingbitsfrom16to128.     Figure10:AccuracyontheCIFAR-10datasetwiththenum- berofhashingbitsfrom16to128. Table3:ExperimentalresultsontheCIFAR-10datasetwhenth enumberofhashingbitsis32. Method precision@ r=2recall@ r=2 F-measure@ r=2MAP accuracy trainingtime(sec) R2SDH 0.5134 0.1622 0.2465 0.4417 0.643 72.3 SDH 0.5121 0.1586 0.2422 0.4396 0.641 47.0 SDHR 0.5169 0.1445 0.2259 0.4408 0.627 61.6 BRE 0.2423 0.0122 0.0232 0.1505 0.353 232.4 KSH 0.3244 0.0312 0.0569 0.4421 0.567 2639.3 SSH 0.2229 0.1097 0.1470 0.1894 0.386 32.9 CCA-ITQ 0.4299 0.0426 0.0775 0.3330 0.553 7.1 FastHash 0.4358 0.0899 0.1490 0.6009 0.657 1200 PCA-ITQ 0.1801 4.8e-4 9.5e-4 0.1714 0.433 4.8 AGH 0.2775 0.0058 0.0113 0.1547 0.357 7.6 IMH 0.2300 0.0244 0.0441 0.1724 0.329 45.6 Table4:ExperimentalresultsontheNUS-WIDEdatabasewhent henumberofhashingbitsis64. Method precision@ r=2recall@ r=2 F-measure@ r=2MAP trainingtime(sec) R2SDH 0.3605 0.9950 0.5293 0.4918 890.7 SDH 0.4957 0.3025 0.3757 0.5609 593.8 BRE 0.0129 6.8375e-7 1.3674e-6 0.5022 41660.2 KSH 0.2528 0.0329 0.0582 0.7334 4142.7 SSH 0.4405 0.3713 0.4030 0.5786 114.4 CCA-ITQ 0.2839 8.0079e-4 0.0016 0.6127 46.1 FastHash 0.2043 0.0032 0.0062 0.5253 12564.7 PCA-ITQ 0.0944 0.0026 0.0051 0.5556 31.6 AGH 0.4500 0.0040 0.0080 0.5256 21.6 IMH 0.4434 0.0117 0.0228 0.5478 61.1 single-labellearning,thereisnoresultforSDHRinTable4. R2SDH performsthebestintwoevaluationmetrics,demonstratingtheef- fectivenessofourmethodontheretrievaltaskofmulti-labeldata. 5CONCLUSION Inthispaper,weproposeadata-dependent(learning-based)hash- ingalgorithmnamedobustrotatedsuperviseddiscretehashing" (R 2SDH)byextendinguperviseddiscretehashing"(SDH).R 2SDH usedcorrentropyratherthanleastsquaresregressionasinSDH. Thus,R 2SDHismorerobustthanSDH.Furthermore,R 2SDHused rotationforthelabelmatrix,whicho˛eradditional˚exibility.As expected,theR 2SDH'sperformanceisbetterthanthatofSDHin mostcases.Real-worldimageclassi˙cationanddigitrecognition experimentsdemonstratethee˛ectivenessande˝ciencyofthe  proposedmethod. Hashingalgorithmscanreducecomputationalcosts,butthecosts arestillhighforlarge-scalehigh-dimensionaldata.Howtospeed  hashingalgorithmssuchasR 2SDHisanissueworthstudying.There aresomepossibleways,forinstance,selectingrepresentativein- stancestorepresenteachclassratherthanusingallinstances. ACKNOWLEDGEMENT JieGuiwassupportedbyNSFC-61572463,NSF-Bigdata-1419210 andNSF-III-1360971. REFERENCES [1]MikhailBelkinandParthaNiyogi.2003.Laplacianeigenmapsfordimensionality reductionanddatarepresentation. Neuralcomputation 15,6(2003),1396. [2]AndreiZ.Broder.1997.OntheResemblanceandContainmentofDocuments. In theCompressionandComplexityofSequences .Positano,Italy,29. [3]AndreiZ.Broder,MosesCharikar,AlanM.Frieze,andMichaelMitzenmacher. 1998.Min-WiseIndependentPermutations.In STOC .Dallas,TX,336. [4]AndreiZ.Broder,StevenC.Glassman,MarkS.Manasse,andGeo˛reyZweig. 1997.SyntacticclusteringoftheWeb.In WWW .SantaClara,CA,11571166. [5]GuoqingChaoandShiliangSun.2016.AlternativeMultiviewMaximumEntropy Discrimination. IEEETransactionsonNeuralNetworksandLearningSystems 27,7(2016),1456. [6]MosesS.Charikar.2002.Similarityestimationtechniquesfromroundingalgo- rithms.In STOC .Montreal,Canada,388. [7]VeniceErinLiong,JiwenLu,GangWang,PierreMoulin,andJieZhou.2015. Deephashingforcompactbinarycodeslearning.In ConferenceonComputer VisionandPatternRecognition .2483. [8]MichelX.GoemansandDavidP.Williamson.1995.Improvedapproximation algorithmsformaximumcutandsatis˙abilityproblemsusingsemide˙nitePro- gramming. JournalofACM 42,6(1995),1145. [9]YunchaoGongandSvetlanaLazebnik.2011.Iterativequantization:Apro- crusteanapproachtolearningbinarycodes.In ConferenceonComputerVision andPatternRecognition .824. [10]YunchaoGong,SvetlanaLazebnik,AlbertGordo,andFlorentPerronnin.2013.It- erativequantization:Aprocrusteanapproachtolearningbinarycodesforlarge- scaleimageretrieval. IEEETransactionsonPatternAnalysisandMachineIntelli- gence 35,12(2013),2929. [11]JieGui,TongliangLiu,ZhenanSun,DachengTao,andTieniuTan.2018.Super- visedDiscreteHashingwithRelaxation. IEEETransactionsonNeuralNetworks andLearningSystems 29,3(2018),617. [12]RanHe,TieniuTan,LiangWang,andWei-ShiZheng.2012. l2;1Regularized correntropyforrobustfeatureselection.In ConferenceonComputerVisionand PatternRecognition .2511. [13]SergeyIo˛e.2010.ImprovedConsistentSampling,WeightedMinhashandL1 Sketching.In ICDM .Sydney,AU,255. [14]Qing-YuanJiangandWu-JunLi.2015.Scalablegraphhash ingwithfeaturetrans- formation.In InternationalJointConferenceonArti˙cialIntelligence .2254. [15]BrianKulisandTrevorDarrell.2009.Learningtohashwithbinaryreconstruc- tiveembeddings.In NeuralInformationProcessingSystems .1050. [16]ShaishavKumarandRaghavendraUdupa.2011.Learninghashfunctionsfor cross-viewsimilaritysearch.In InternationalJointConferenceonArti˙cialIntel- ligence .1365. [17]PingLi.2015.0-BitConsistentWeightedSampling.In KDD .Sydney,Australia, 674. [18]PingLi.2017.LinearizedGMMKernelsandNormalizedRandomFourierFea- tures.In KDD .324. [19]PingLiandArndChristianKönig.2010.b-BitMinwiseHashing.In WWW .Raleigh,NC,680. [20]PingLi,MichaelMitzenmacher,andAnshumaliShrivastava.2014.Codingfor RandomProjections.In ICML .[21]PingLi,ArtBOwen,andCun-HuiZhang.2012.OnePermutationHashing.In NIPS .LakeTahoe,NV. [22]PingLi,GennadySamorodnitsky,andJohnHopcroft.2013.SignCauchyProjec- tionsandChi-SquareKernel.In NIPS .LakeTahoe,NV. [23]GuoshengLin,ChunhuaShen,QinfengShi,AntonvandenHengel,andDavid Suter.2014.Fastsupervisedhashingwithdecisiontreesforhigh-dimensional data.In ConferenceonComputerVisionandPatternRecognition .1970. [24]GuoshengLin,ChunhuaShen,andAntonvandenHengel.2015.Supervised hashingusinggraphcutsandboosteddecisiontrees. IEEETransactionsonPat- ternAnalysisandMachineIntelligence 37,11(2015),2331. [25]HaomiaoLiu,RuipingWang,ShiguangShan,andXilinChen.2016.DeepSuper- visedHashingforFastImageRetrieval.In ConferenceonComputerVisionand PatternRecognition .2072. [26]WeifengLiu,PuskalPPokharel,andJoséCPríncipe.2007.Correntropy:prop- ertiesandapplicationsinnon-Gaussiansignalprocessing. IEEETransactionson SignalProcessing 55,11(2007),5298. [27]WeiLiu,JunWang,RongrongJi,Yu-GangJiang,andShih-FuChang.2012.Su- pervisedhashingwithkernels.In ConferenceonComputerVisionandPattern Recognition .2081. [28]WeiLiu,JunWang,SanjivKumar,andShih-FuChang.2011.Hashingwith graphs.In InternationalConferenceonMachineLearning .8. [29]YadanLuo,YangYang,FuminShen,ZiHuang,PanZhou,andHengTaoShen. 2018.Robustdiscretecodemodelingforsupervisedhashing. PatternRecognition 75(2018),135. [30]LaurensvanderMaatenandGeo˛reyHinton.2008.Visualizingdatausingt- SNE. JournalofMachineLearningResearch 9,Nov(2008),2605. [31]MarkManasse,FrankMcSherry,andKunalTalwar.2010. ConsistentWeighted Sampling .TechnicalReportMSR-TR-2010-73.MicrosoftResearch. [32]FeipingNie,HengHuang,XiaoCai,andChrisDing.2010.E˝cientandrobust featureselectionviajoint l2;1-normsminimization.In NeuralInformationPro- cessingSystems .1821. [33]MohammadNorouzi,DavidJFleet,andRuslanRSalakhutdinov.2012.Hamming distancemetriclearning.In NeuralInformationProcessingSystems .1069. [34]AudeOlivaandAntonioTorralba.2001.Modelingtheshapeofthescene:A holisticrepresentationofthespatialenvelope. InternationalJournalofComputer Vision 42,3(2001),175. [35]MohammadRastegari,JonghyunChoi,ShobeirFakhraei,HalDauméIII,and LarrySDavis.2013.Predictabledual-Viewhashing.In InternationalConference onMachineLearning .1336. [36]RuslanSalakhutdinovandGeo˛reyHinton.2009.Semantichashing. Interna- tionalJournalofApproximateReasoning 50,7(2009),978. [37]FuminShen,ChunhuaShen,WeiLiu,andHengTaoShen.2015.Supervised discretehashing.In ConferenceonComputerVisionandPatternRecognition . 45.[38]FuminShen,ChunhuaShen,QinfengShi,AntonVanDenHengel,andZhenmin Tang.2013.Inductivehashingonmanifolds.In ConferenceonComputerVision andPatternRecognition .1569. [39]JingkuanSong,YiYang,ZiHuang,HengTaoShen,andRichangHong.2011. Multiplefeaturehashingforreal-timelargescalenear-duplicatevideoretrieval.  In ACMInternationalConferenceonMultimedia .432. [40]RyanSpringandAnshumaliShrivastava.2017.Scalableandsustainabledeep learningviarandomizedhashing.In ACMSIGKDDInternationalConferenceon KnowledgeDiscoveryandDataMining .454. [41]ChristophStrecha,AlexBronstein,MichaelBronstein,andPascalFua.2012. LDAHash:Improvedmatchingwithsmallerdescriptors. IEEETransactionson PatternAnalysisandMachineIntelligence 34,1(2012),78. [42]AntonioTorralba,RobFergus,andWilliamTFreeman.2008.80milliontiny images:Alargedatasetfornonparametricobjectandscenerecognition. IEEE TransactionsonPatternAnalysisandMachineIntelligence 30,11(2008), 1970.[43]JunWang,SanjivKumar,andShih-FuChang.2012.Semi-supervisedhashingfor large-scalesearch. IEEETransactionsonPatternAnalysisandMachineIntelligence 34,12(2012),2406. [44]YairWeiss,AntonioTorralba,andRobFergus.2009.Spectralhashing.In Neural InformationProcessingSystems .1760. [45]DanZhang,FeiWang,andLuoSi.2011.Compositehashingwithmultipleinfor- mationsources.In InternationalACMSIGIRConferenceonResearchandDevelop- mentinInformationRetrieval .234.  
DeepVoice2:Multi-SpeakerNeuralText-to-Speech SercanÖ.  sercanarik@baidu.com GregoryDiamos  gregdiamos@baidu.com AndrewGibiansky  gibianskyandrew@baidu.com JohnMiller  millerjohn@baidu.com KainanPeng  pengkainan@baidu.com WeiPing  pingwei01@baidu.com JonathanRaiman  jonathanraiman@baidu.com YanqiZhou  zhouyanqi@baidu.com BaiduSiliconValleyIntelligenceLab 1195BordeauxDr.Sunnyvale,CA94089 Abstract Weintroduceatechniqueforaugmentingneuraltext-to-speech(TTS)withlow- dimensionaltrainablespeakerembeddingstogeneratedifferentvoicesfroma singlemodel.Asastartingpoint,weshowimprovementsoverthetwostate-of- the-artapproachesforsingle-speakerneuralTTS:DeepVoice1andTacotron. WeintroduceDeepVoice2,whichisbasedonasimilarpipelinewithDeep Voice1,butconstructedwithhigherperformancebuildingblocksanddemonstrates aaudioqualityimprovementoverDeepVoice1.WeimproveTacotron byintroducingapost-processingneuralvocoder,anddemonstratea audioqualityimprovement.Wethendemonstrateourtechniqueformulti-speaker speechsynthesisforbothDeepVoice2andTacotronontwomulti-speakerTTS datasets.WeshowthatasingleneuralTTSsystemcanlearnhundredsofunique voicesfromlessthanhalfanhourofdataperspeaker,whileachievinghighaudio qualitysynthesisandpreservingthespeakeridentitiesalmostperfectly. 1Introduction speechsynthesis,commonlyknownastext-to-speech(TTS),hasavarietyofapplicationsin technologyinterfaces,accessibility,media,andentertainment.MostTTSsystemsarebuiltwitha singlespeakervoice,andmultiplespeakervoicesareprovidedbyhavingdistinctspeechdatabasesor modelparameters.Asaresult,developingaTTSsystemwithsupportformultiplevoicesrequires muchmoredataanddevelopmenteffortthanasystemwhichonlysupportsasinglevoice. Inthiswork,wedemonstratethatwecanbuildall-neuralmulti-speakerTTSsystemswhichsharethe vastmajorityofparametersbetweendifferentspeakers.Weshowthatnotonlycanasinglemodel generatespeechfrommultipledifferentvoices,butalsothatlessdataisrequiredper speakerthanwhentrainingsingle-speakersystems. Concretely,wemakethefollowingcontributions: 1. WepresentDeepVoice2,animprovedarchitecturebasedonDeepVoice1( Ariketal. , 2017 ). 2. WeintroduceaWaveNet-based( Oordetal. , 2016 )spectrogram-to-audioneuralvocoder,and useitwithTacotron( Wangetal. , 2017 )asareplacementforGrifaudiogeneration.  Listedalphabetically. 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. arXiv:1705.08947v2  [cs.CL]  20 Sep 20173. Usingthesetwosingle-speakermodelsasabaseline,wedemonstratemulti-speakerneural speechsynthesisbyintroducingtrainablespeakerembeddingsintoDeepVoice2andTacotron. Weorganizetherestofthispaperasfollows.Section 2 discussesrelatedworkandwhatmakesthe contributionsofthispaperdistinctfrompriorwork.Section 3 presentsDeepVoice2andhighlights thedifferencesfromDeepVoice1.Section 4 explainsourspeakerembeddingtechniqueforneural TTSmodelsandshowsmulti-speakervariantsoftheDeepVoice2andTacotronarchitectures. Section 5.1 theimprovementforsinglespeakerTTSthroughameanopinionscore(MOS) evaluationandSection 5.2 presentsthesynthesizedaudioqualityofmulti-speakerDeepVoice2and TacotronviabothMOSevaluationandamulti-speakerdiscriminatoraccuracymetric.Section 6 concludeswithadiscussionoftheresultsandpotentialfuturework. 2RelatedWork WediscusstherelatedworkrelevanttoeachofourclaimsinSection 1 inorder,startingfrom single-speakerneuralspeechsynthesisandmovingontomulti-speakerspeechsynthesisandmetrics forgenerativemodelquality. Withregardstosingle-speakerspeechsynthesis,deeplearninghasbeenusedforavarietyofsubcom- ponents,includingdurationprediction( Zenetal. , 2016 ),fundamentalfrequencyprediction( Ronanki etal. , 2016 ),acousticmodeling( ZenandSak , 2015 ),andmorerecentlyautoregressivesample-by- sampleaudiowaveformgeneration(e.g., Oordetal. , 2016 ; Mehrietal. , 2016 ).Ourcontributions builduponrecentworkinentirelyneuralTTSsystems,includingDeepVoice1( Ariketal. , 2017 ), Tacotron( Wangetal. , 2017 ),andChar2Wav( Soteloetal. , 2017 ).Whiletheseworksfocuson buildingsingle-speakerTTSsystems,ourpaperfocusesonextendingneuralTTSsystemstohandle multiplespeakerswithlessdataperspeaker. Ourworkisnotthetoattemptamulti-speakerTTSsystem.Forinstance,intraditionalHMM- basedTTSsynthesis(e.g., Yamagishietal. , 2009 ),anaveragevoicemodelistrainedusingmultiple speakers'data,whichisthenadaptedtodifferentspeakers.DNN-basedsystems(e.g., Yangetal. , 2016 )havealsobeenusedtobuildaveragevoicemodels,withi-vectorsrepresentingspeakersas additionalinputsandseparateoutputlayersforeachtargetspeaker.Similarly, Fanetal. ( 2015 ) usesasharedhiddenrepresentationamongdifferentspeakerswithspeaker-dependentoutputlayers predictingvocoderparameters(e.g.,linespectralpairs,aperiodicityparametersetc.).Forfurther context, Wuetal. ( 2015 )empiricallystudiesDNN-basedmulti-speakermodeling.Morerecently, speakeradaptationhasbeentackledwithgenerativeadversarialnetworks(GANs)( Hsuetal. , 2017 ). Weinsteadusetrainablespeakerembeddingsformulti-speakerTTS.Theapproachwasinvestigated inspeechrecognition( Abdel-HamidandJiang , 2013 ),butisanoveltechniqueinspeechsynthesis. Unlikepriorworkwhichdependsonedembeddings(e.g.i-vectors),thespeakerembeddingsused inthisworkaretrainedjointlywiththerestofthemodelfromscratch,andthuscandirectlylearn thefeaturesrelevanttothespeechsynthesistask.Inaddition,thisworkdoesnotrelyonper-speaker outputlayersoraveragevoicemodeling,whichleadstohigher-qualitysynthesizedsamplesandlower datarequirements(astherearefeweruniqueparametersperspeakertolearn). Inordertoevaluatethedistinctivenessofthegeneratedvoicesinanautomatedway,weproposeusing theaccuracyofaspeakerdiscriminator.SimilarmetricssuchasanﬁInceptionscoreﬂ havebeenusedforquantitativequalityevaluationsofGANsforimagesynthesis(e.g., Salimans etal. , 2016 ).SpeakerhasbeenstudiedwithbothtraditionalGMM-basedmethods(e.g., Reynoldsetal. , 2000 )andmorerecentlywithdeeplearningapproaches(e.g., Lietal. , 2017 ). 3Single-SpeakerDeepVoice2 Inthissection,wepresentDeepVoice2,aneuralTTSsystembasedonDeepVoice1( Ariketal. , 2017 ).WekeepthegeneralstructureoftheDeepVoice1( Ariketal. , 2017 ),asdepictedinFig. 1 (the correspondingtrainingpipelineisdepictedinAppendix A ).Ourprimarymotivationforpresenting animprovedsingle-speakermodelistouseitasthestartingpointforahigh-qualitymulti-speaker model. OnemajordifferencebetweenDeepVoice2andDeepVoice1istheseparationofthephoneme durationandfrequencymodels.DeepVoice1hasasinglemodeltojointlypredictphonemeduration 2 Figure1:Inferencesystemdiagram:text-phonemesdictionaryconversion,secondpredict phonemedurations,thirdupsampleandgenerate F 0 ,feed F 0 andphonemestovocalmodel. andfrequency(voicednessandtime-dependentfundamentalfrequency, F 0 ).InDeepVoice2, thephonemedurationsarepredictedandthenareusedasinputstothefrequencymodel. Inthesubsequentsubsections,wepresentthemodelsusedinDeepVoice2.Allmodelsaretrained separatelyusingthehyperparametersinAppendix B .Wewillprovideaquantitative comparisonofDeepVoice1andDeepVoice2inSection 5.1 . 3.1Segmentationmodel EstimationofphonemelocationsistreatedasanunsupervisedlearningprobleminDeepVoice 2,similartoDeepVoice1.Thesegmentationmodelisconvolutional-recurrentarchitecturewith connectionisttemporal(CTC)loss( Gravesetal. , 2006 )appliedtoclassifyphoneme pairs,whicharethenusedtoextracttheboundariesbetweenthem.Themajorarchitecturechangesin DeepVoice2aretheadditionofbatchnormalizationandresidualconnectionsintheconvolutional layers.,DeepVoice1'ssegmentationmodelcomputestheoutputofeachlayeras h ( l ) =relu  W ( l )  h ( l  1) + b ( l )  ; (1) where h ( l ) istheoutputofthe l -thlayer, W ( l ) istheconvolution b ( l ) isthebiasvector,and  istheconvolutionoperator.Incontrast,DeepVoice2'ssegmentationmodellayersinsteadcompute h ( l ) =relu  h ( l  1) +BN  W ( l )  h ( l  1)  ; (2) where BN isbatchnormalization( IoffeandSzegedy , 2015 ).Inaddition,wethatthesegmentation modeloftenmakesmistakesforboundariesbetweensilencephonemesandotherphonemes,whichcan reducesegmentationaccuracyonsomedatasets.Weintroduceasmallpost-processing steptocorrectthesemistakes:wheneverthesegmentationmodeldecodesasilenceboundary,we adjustthelocationoftheboundarywithasilencedetectionheuristic. 2 3.2DurationModel InDeepVoice2,insteadofpredictingacontinuous-valuedduration,weformulatedurationprediction asasequencelabelingproblem.Wediscretizethephonemedurationintolog-scaledbuckets,and assigneachinputphonemetothebucketlabelcorrespondingtoitsduration.Wemodelthesequence byaconditionalrandom(CRF)withpairwisepotentialsatoutputlayer( Lampleetal. , 2016 ). Duringinference,wedecodediscretizeddurationsfromtheCRFusingtheViterbiforward-backward algorithm.Wethatquantizingthedurationpredictionandintroducingthepairwisedependence impliedbytheCRFimprovessynthesisquality. 3.3FrequencyModel Afterdecodingfromthedurationmodel,thepredictedphonemedurationsareupsampledfroma per-phonemeinputfeaturestoaper-frameinputforthefrequencymodel. 3 DeepVoice2frequency 2 Wecomputethesmoothednormalizedaudiopoweras p [ n ]=( x [ n ] 2 =x max 2 )  g [ n ] ,where x [ n ] isthe audiosignal, g [ n ] istheimpulseresponseofaGaussian, x max isthemaximumvalueof x [ n ] and  is one-dimensionalconvolutionoperation.Weassignthesilencephonemeboundarieswhen p [ n ] exceedsaed threshold.TheoptimalparametervaluesfortheGaussianandthethresholddependonthedatasetand audiosamplingrate. 3 Eachframeisensuredtobe10milliseconds.Forexample,ifaphonemelasts20milliseconds,theinput featurescorrespondingtothatphonemewillberepeatedin2frames.Ifitlastslessthan10milliseconds,itis extendtoasingleframe. 3 modelconsistsofmultiplelayers:,bidirectionalgatedrecurrentunit(GRU)layers( Choetal. , 2014 )generatehiddenstatesfromtheinputfeatures.Fromthesehiddenstates,anafprojection followedbyasigmoidnonlinearityproducestheprobabilitythateachframeisvoiced.Hiddenstates arealsousedtomaketwoseparatenormalized F 0 predictions.Theprediction, f GRU ,ismade withasingle-layerbidirectionalGRUfollowedbyanafprojection.Thesecondprediction, f conv , ismadebyaddingupthecontributionsofmultipleconvolutionswithvaryingconvolutionwidths andasingleoutputchannel.Finally,thehiddenstateisusedwithanafprojectionandasigmoid nonlinearitytopredictamixtureratio ! ,whichisusedtoweighthetwonormalizedfrequency predictionsandcombinetheminto f = !  f GRU +(1  ! )  f conv : (3) Thenormalizedprediction f isthenconvertedtothetruefrequency F 0 predictionvia F 0 =  F 0 + ˙ F 0  f; (4) where  F 0 and ˙ F 0 are,respectively,themeanandstandarddeviationof F 0 forthespeakerthe modelistrainedon.Wethatpredicting F 0 withamixtureofconvolutionsandarecurrentlayer performsbetterthanpredictingwitheitheroneindividually.Weattributethistothehypothesisthat includingthewideconvolutionsreducestheburdenfortherecurrentlayerstomaintainstateovera largenumberofinputframes,whileprocessingtheentirecontextinformationef. 3.4VocalModel TheDeepVoice2vocalmodelisbasedonaWaveNetarchitecture( Oordetal. , 2016 )withatwo-layer bidirectionalQRNN( Bradburyetal. , 2017 )conditioningnetwork,similartoDeepVoice1.However, weremovethe 1  1 convolutionbetweenthegatedtanhnonlinearityandtheresidualconnection.In addition,weusethesameconditionerbiasforeverylayeroftheWaveNet,insteadofgeneratinga separatebiasforeverylayeraswasdoneinDeepVoice1. 4 4Multi-SpeakerModelswithTrainableSpeakerEmbeddings Inordertosynthesizespeechfrommultiplespeakers,weaugmenteachofourmodelswithasingle low-dimensionalspeakerembeddingvectorperspeaker.Unlikepreviouswork,ourapproachdoes notrelyonper-speakerweight matrices or layers .Speaker-dependentparametersarestoredinavery low-dimensionalvectorandthusthereisnear-completeweightsharingbetweenspeakers.Weuse speakerembeddingstoproducerecurrentneuralnetwork(RNN)initialstates,nonlinearitybiases, andmultiplicativegatingfactors,usedthroughoutthenetworks.Speakerembeddingsareinitialized randomlywithauniformdistributionover [  0 : 1 ; 0 : 1] andtrainedjointlyviabackpropagation;each modelhasitsownsetofspeakerembeddings. Toencourageeachspeaker'suniquevoicesignaturetothemodel,weincorporatethespeaker embeddingsintomultipleportionsofthemodel.Empirically,wethatsimplyprovidingthe speakerembeddingstotheinputlayersdoesnotworkaswellforanyofthepresentedmodelsbesides thevocalmodel,possiblyduetothehighdegreeofresidualconnectionspresentintheWaveNetand duetothedifcultyoflearninghigh-qualityspeakerembeddings.Weobservedthatseveralpatterns tendtoyieldhighperformance:  SpeakerEmbeddings: Foreveryusesiteinthemodelarchitecture,transformthe sharedspeakerembeddingtotheappropriatedimensionandformthroughanafprojection andanonlinearity.  RecurrentInitialization: Initializerecurrentlayerhiddenstateswithspeaker embeddings.  InputAugmentation: Concatenateaspeakerembeddingtotheinputatevery timestepofarecurrentlayer.  FeatureGating: Multiplylayeractivationselementwisewithasite-specispeakerembedding torenderadaptableinformationw. 5 4 Wethatthesechangesreducemodelsizebyafactorof ˘ 7andspeedupinferenceby ˘ 25%,while yieldingnoperceptualchangeinquality.However,wedonotfocusondemonstratingtheseclaimsinthispaper. 5 Wehypothesizethatfeaturegatingletsthemodellearntheunionofallnecessaryfeatureswhileallowing speakerembeddingstodeterminewhatfeaturesareusedforeachspeakerandhowmuchtheywill haveontheactivations. 4 (a) (b) (c) Figure2:Architectureforthemulti-speaker(a)segmentation,(b)duration,and(c)frequencymodel. Next,wedescribehowspeakerembeddingsareusedineacharchitecture. 4.1Multi-SpeakerDeepVoice2 TheDeepVoice2modelshaveseparatespeakerembeddingsforeachmodel.Yet,theycanbeviewed aschunksofalargerspeakerembedding,whicharetrainedindependently. 4.1.1SegmentationModel Inmulti-speakersegmentationmodel,weusefeaturegatingintheresidualconnectionsofthe convolutionlayers.InsteadofEq. ( 2 ) ,wemultiplythebatch-normalizedactivationsbya speakerembedding: h ( l ) =relu  h ( l  1) +BN  W  h ( l  1)   g s  ; (5) where g s isaspeakerembedding.Thesameembeddingissharedforall theconvolutionallayers.Inaddition,weinitializeeachoftherecurrentlayerswithasecondsite embedding.Similarly,eachlayersharesthesameembedding,ratherthanhaving aseparateembeddingperlayer. 4.1.2DurationModel Themulti-speakerdurationmodelusesspeaker-dependentrecurrentinitializationandinputaugmen- tation.AembeddingisusedtoinitializeRNNhiddenstates,andanothersi embeddingisprovidedasinputtotheRNNlayerbyconcatenatingittothefeaturevectors. 4.1.3FrequencyModel Themulti-speakerfrequencymodelusesrecurrentinitialization,whichinitializestherecurrent layers(exceptfortherecurrentoutputlayer)withasinglespeaker-embedding.As describedinSection 3.3 ,therecurrentandconvolutionaloutputlayersinthesingle-speakerfrequency modelpredicta normalized frequency,whichisthenconvertedintothetrue F 0 byaedlinear transformation.Thelineartransformationdependsonthemeanandstandarddeviationofobserved F 0 forthespeaker.Thesevaluesvarygreatlybetweenspeakers:malespeakers,forinstance,tendto haveamuchlowermean F 0 .Tobetteradapttothesevariations,wemakethemeanandstandard deviationtrainablemodelparametersandmultiplythembyscalingtermswhichdependonthespeaker embeddings.,insteadofEq.( 4 ),wecomputethe F 0 predictionas F 0 =  F 0   1+softsign  V  T g f  + ˙ F 0   1+softsign  V ˙ T g f   f; (6) where g f isaspeakerembedding,  F 0 and ˙ F 0 aretrainablescalarparametersinitialized tothe F 0 meanandstandarddeviationonthedataset,and V  and V ˙ aretrainableparametervectors. 5 Figure3:TacotronwithspeakerconditioningintheEncoderCBHGmoduleanddecoderwithtwo waystoconvertspectrogramtoaudio:Griforourspeaker-conditionedVocalmodel. 4.1.4VocalModel Themulti-speakervocalmodelusesonlyinputaugmentation,withthespeakerembedding concatenatedontoeachinputframeoftheconditioner.Thisdiffersfromtheglobalconditioning suggestedin Oordetal. ( 2016 )andallowsthespeakerembeddingtothelocalconditioning networkaswell. Withoutspeakerembeddings,thevocalmodelisstillabletogeneratesomewhatdistinct-sounding voicesbecauseofthedisctinctivefeaturesprovidedbythefrequencyanddurationmodels.Yet, havingspeakerembeddingsinthevocalmodelincreasestheaudioquality.Weindeedobservethat theembeddingsconvergetoameaningfullatentspace. 4.2Multi-SpeakerTacotron InadditiontoextendingDeepVoice2withspeakerembeddings,wealsoextendTacotron( Wang etal. , 2017 ),asequence-to-sequencecharacter-to-waveformmodel.Whentrainingmulti-speaker Tacotronvariants,wethatmodelperformanceishighlydependentonmodelhyperparameters, andthatsomemodelsoftenfailtolearnattentionmechanismsforasmallsubsetofspeakers.Wealso thatifthespeechineachaudioclipdoesnotstartatthesametimestep,themodelsaremuchless likelytoconvergetoameaningfulattentioncurveandrecognizablespeech;thus,wetrimallinitial andsilenceineachaudioclip.Duetothesensitivityofthemodeltohyperparametersanddata preprocessing,webelievethatadditionaltuningmaybenecessarytoobtainmaximalquality.Thus, ourworkfocusesondemonstratingthatTacotron,likeDeepVoice2,iscapableofhandlingmultiple speakersthroughspeakerembeddings,ratherthancomparingthequalityofthetwoarchitectures. 4.2.1Character-to-SpectrogramModel TheTacotroncharacter-to-spectrogramarchitectureconsistsofaconvolution-bank-highway-GRU (CBHG)encoder,anattentionaldecoder,andaCBHGpost-processingnetwork.Duetothecomplexity ofthearchitecture,weleaveoutacompletedescriptionandinsteadfocusonour WethatincorporatingspeakerembeddingsintotheCBHGpost-processingnetworkdegrades outputquality,whereasincorporatingspeakerembeddingsintothecharacterencoderisnecessary. Withoutaspeaker-dependentCBHGencoder,themodelisincapableoflearningitsattentionmech- anismandcannotgeneratemeaningfuloutput(seeAppendix D.2 forspeaker-dependentattention visualizations).Inordertoconditiontheencoderonthespeaker,weuseoneembedding asanextrainputtoeachhighwaylayerateachtimestepandinitializetheCBHGRNNstatewitha secondembedding. Wealsothataugmentingthedecoderwithspeakerembeddingsishelpful.Weuseone embeddingasanextrainputtothedecoderpre-net,oneextraembeddingastheinitial attentioncontextvectorfortheattentionalRNN,oneembeddingastheinitialdecoder GRUhiddenstate,andoneembeddingasabiastothetanhinthecontent-basedattention mechanism. 6 Model Samp.Freq. MOS DeepVoice1 16KHz 2 : 05  0 : 24 DeepVoice2 16KHz 2 : 96  0 : 38 Tacotron(Grif 24KHz 2 : 57  0 : 28 Tacotron(WaveNet) 24KHz 4 : 17  0 : 18 Table1:MeanOpinionScore(MOS)evaluationswith95%intervalsofDeepVoice1, DeepVoice2,andTacotron.UsingthecrowdMOStoolkit,batchesofsamplesfromthesemodels werepresentedtoratersonMechanicalTurk.Sincebatchescontainedsamplesfromallmodels,the experimentnaturallyinducesacomparisonbetweenthemodels. 4.2.2Spectrogram-to-WaveformModel TheoriginalTacotronimplementationin( Wangetal. , 2017 )usestheGrifn-Limalgorithmtoconvert spectrogramstotime-domainaudiowaveformsbyiterativelyestimatingtheunknownphases. 6 We observethatminornoiseintheinputspectrogramcausesnoticeableestimationerrorsintheGrif Limalgorithmandthegeneratedaudioqualityisdegraded.Toproducehigherqualityaudiousing Tacotron,insteadofusingGrifwetrainaWaveNet-basedneuralvocodertoconvertfrom linearspectrogramstoaudiowaveforms.ThemodelusedisequivalenttotheDeepVoice2vocal model,buttakeslinear-scaledlog-magnitudespectrogramsinsteadofphonemeidentityand F 0 as input.ThecombinedTacotron-WaveNetmodelisshowninFig. 3 .AswewillshowinSection 5.1 , WaveNet-basedneuralvocoderindeedimprovessingle-speakerTacotronaswell. 5Results Inthissection,wewillpresenttheresultsonbothsingle-speakerandmulti-speakerspeechsynthesis usingthedescribedarchitectures.AllmodelhyperparametersarepresentedinAppendix B . 5.1Single-SpeakerSpeechSynthesis WetrainDeepVoice1,DeepVoice2,andTacotrononaninternalEnglishspeechdatabasecontaining approximately20hoursofsingle-speakerdata.TheintermediateevaluationsofmodelsinDeepVoice 1andDeepVoice2canbefoundinTable 3 withinAppendix A .WerunanMOSevaluationusingthe crowdMOSframework( Ribeiroetal. , 2011 )tocomparethequalityofsamples(Table 1 ).Theresults showconclusivelythatthearchitectureimprovementsinDeepVoice2yieldgainsin qualityoverDeepVoice1.TheyalsodemonstratethatconvertingTacotron-generatedspectrograms toaudiousingWaveNetispreferabletousingtheiterativeGrifalgorithm. 5.2Multi-SpeakerSpeechSynthesis WetrainalltheaforementionedmodelsontheVCTKdatasetwith44hoursofspeech,whichcontains 108speakerswithapproximately400utteranceseach.Wealsotrainallmodelsonaninternaldataset ofaudiobooks,whichcontains477speakerswith30minutesofaudioeach(foratotalof ˘ 238 hours).Theconsistentsamplequalityobservedfromourmodelsindicatesthatourarchitecturescan easilylearnhundredsofdistinctvoiceswithavarietyofdifferentaccentsandcadences.Wealso observethatthelearnedembeddingslieinameaningfullatentspace(seeFig. 4 asanexampleand Appendix D formoredetails). Inordertoevaluatethequalityofthesynthesizedaudio,werunMOSevaluationsusingthecrowdMOS framework,andpresenttheresultsinTable 2 .Wepurposefullyincludegroundtruthsamplesinthe setbeingevaluated,becausetheaccentsindatasetsarelikelytobeunfamiliartoourNorthAmerican crowdsourcedratersandwillthusberatedpoorlyduetotheaccentratherthanduetothemodel quality.Byincludinggroundtruthsamples,weareabletocomparetheMOSofthemodelswith thegroundtruthMOSandthusevaluatethemodelqualityratherthanthedataquality;however,the resultingMOSmaybelower,duetotheimplicitcomparisonwiththegroundtruthsamples.Overall, weobservethattheDeepVoice2modelcanapproachanMOSvaluethatisclosetothegroundtruth, whenlowsamplingrateandcompanding/expandingtakenintoaccount. 6 Estimationoftheunknownphasesisdonebyrepeatedlyconvertingbetweenfrequencyandtimedomain representationsofthesignalusingtheshort-timeFouriertransformanditsinverse,substitutingthemagnitudeof eachfrequencycomponenttothepredictedmagnitudeateachstep. 7 Dataset Multi-SpeakerModel Samp.Freq. MOS Acc. VCTK DeepVoice2(20-layerWaveNet) 16KHz 2.87  0.13 99.9% VCTK DeepVoice2(40-layerWaveNet) 16KHz 3.21  0.13 100% VCTK DeepVoice2(60-layerWaveNet) 16KHz 3.42  0.12 99.7% VCTK DeepVoice2(80-layerWaveNet) 16KHz 3.53  0.12 99.9% VCTK Tacotron(Grif 24KHz 1.68  0.12 99.4% VCTK Tacotron(20-layerWaveNet) 24KHz 2.51  0.13 60.9% VCTK GroundTruthData 48KHz 4.65  0.06 99.7% Audiobooks DeepVoice2(80-layerWaveNet) 16KHz 2.97  0.17 97.4% Audiobooks Tacotron(Grif 24KHz 1.73  0.22 93.9% Audiobooks Tacotron(20-layerWaveNet) 24KHz 2.11  0.20 66.5% Audiobooks GroundTruthData 44.1KHz 4.63  0.04 98.8% Table2:MOSandaccuracyforallmulti-speakermodels.ToobtainMOS,weuse crowdMOStoolkitasdetailedinTable 1 .Wealsopresentaccuraciesofthespeaker discriminativemodels(seeAppendix E fordetails)onthesamples,showingthatthesynthesized voicesareasdistinguishableasgroundtruthaudio. (a) (b) Figure4:Principalcomponentsofthelearnedspeakerembeddingsforthe(a)80-layervocalmodel and(b)character-to-spectrogrammodelforVCTKdataset.SeeAppendix D.3 fordetails. Amulti-speakerTTSsystemwithhighsamplequalitybutindistinguishablevoiceswouldresultin highMOS,butfailtomeetthedesiredobjectiveofreproducingtheinputvoicesaccurately.Toshow thatourmodelsnotonlygeneratehighqualitysamples,butalsogenerate distinguishable voices,we alsomeasuretheaccuracyofaspeakerdiscriminativemodelonourgeneratedsamples. Thespeakerdiscriminativeisaconvolutionalnetworktrainedtoclassifyutterancestotheirspeakers, trainedonthesamedatasetastheTTSsystemsthemselves.Ifthevoiceswereindistinguishable (ortheaudioqualitywaslow),theaccuracywouldbemuchlowerforsynthesized samplesthanitisforthegroundtruthsamples.AswedemonstrateinTable 2 ,accuracy demonstratesthatsamplesgeneratedfromourmodelsareasdistinguishableasthegroundtruth samples(seeAppendix E formoredetails).Theaccuracyisonlylowerfor TacotronwithWaveNet,andwesuspectthatgenerationerrorsinthespectrogramareexacerbatedby theWaveNet,asitistrainedwithgroundtruthspectrograms. 6Conclusion Inthiswork,weexplorehowentirely-neuralspeechsynthesispipelinesmaybeextendedtomulti- speakertext-to-speechvialow-dimensionaltrainablespeakerembeddings.Westartbypresenting DeepVoice2,animprovedsingle-speakermodel.Next,wedemonstratetheapplicabilityofour techniquebytrainingbothmulti-speakerDeepVoice2andmulti-speakerTacotronmodels,and evaluatetheirqualitythroughMOS.Inconclusion,weuseourspeakerembeddingtechniquetocreate highqualitytext-to-speechsystemsandconclusivelyshowthatneuralspeechsynthesismodelscan learneffectivelyfromsmallamountsofdataspreadamonghundredsofdifferentspeakers. Theresultspresentedinthisworksuggestmanydirectionsforfutureresearch.Futureworkmaytest thelimitsofthistechniqueandexplorehowmanyspeakersthesemodelscangeneralizeto,howlittle dataistrulyrequiredperspeakerforhighqualitysynthesis,whethernewspeakerscanbeaddedtoa systembymodelparametersandsolelytrainingnewspeakerembeddings,andwhetherthe speakerembeddingscanbeusedasameaningfulvectorspace,asispossiblewithwordembeddings. 8 References O.Abdel-HamidandH.Jiang.FastspeakeradaptationofhybridNN/HMMmodelforspeechrecognitionbased ondiscriminativelearningofspeakercode.In ICASSP ,2013. S.O.Arik,M.Chrzanowski,A.Coates,G.Diamos,A.Gibiansky,Y.Kang,X.Li,J.Miller,J.Raiman, S.Sengupta,andM.Shoeybi.Deepvoice:Real-timeneuraltext-to-speech.In ICML ,2017. J.Bradbury,S.Merity,C.Xiong,andR.Socher.Quasi-recurrentneuralnetworks.In ICLR ,2017. K.Cho,B.VanMerriënboer,C.Gulcehre,D.Bahdanau,F.Bougares,H.Schwenk,andY.Bengio.Learning phraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation. arXiv:1406.1078 ,2014. Y.Fan,Y.Qian,F.K.Soong,andL.He.Multi-speakermodelingandspeakeradaptationforDNN-basedTTS synthesis.In IEEEICASSP ,2015. A.Graves,S.Fernández,F.Gomez,andJ.Schmidhuber.Connectionisttemporallabelling unsegmentedsequencedatawithrecurrentneuralnetworks.In ICML ,2006. C.-C.Hsu,H.-T.Hwang,Y.-C.Wu,Y.Tsao,andH.-M.Wang.Voiceconversionfromunalignedcorporausing variationalautoencodingwassersteingenerativeadversarialnetworks. arXiv:1704.00849 ,2017. S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariate shift. arXivpreprintarXiv:1502.03167 ,2015. D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization. arXiv:1412.6980 ,2014. G.Lample,M.Ballesteros,K.Kawakami,S.Subramanian,andC.Dyer.Neuralarchitecturesfornamedentity recognition.In Proc.NAACL-HLT ,2016. C.Li,X.Ma,B.Jiang,X.Li,X.Zhang,X.Liu,Y.Cao,A.Kannan,andZ.Zhu.Deepspeaker:anend-to-end neuralspeakerembeddingsystem. arXivpreprintarXiv:1705.02304 ,2017. S.Mehri,K.Kumar,I.Gulrajani,R.Kumar,S.Jain,J.Sotelo,A.Courville,andY.Bengio.SampleRNN:An unconditionalend-to-endneuralaudiogenerationmodel. arXiv:1612.07837 ,2016. A.v.d.Oord,S.Dieleman,H.Zen,K.Simonyan,O.Vinyals,A.Graves,N.Kalchbrenner,A.Senior,and K.Kavukcuoglu.Wavenet:Agenerativemodelforrawaudio. arXiv:1609.03499 ,2016. D.A.Reynolds,T.F.Quatieri,andR.B.Dunn.Speakervusingadaptedgaussianmixturemodels. Digitalsignalprocessing ,10(1-3):19Œ41,2000. F.Ribeiro,D.Florêncio,C.Zhang,andM.Seltzer.Crowdmos:Anapproachforcrowdsourcingmeanopinion scorestudies.In IEEEICASSP ,2011. S.Ronanki,O.Watts,S.King,andG.E.Henter.Median-basedgenerationofsyntheticspeechdurationsusinga non-parametricapproach. arXiv:1608.06134 ,2016. T.Salimans,I.Goodfellow,W.Zaremba,V.Cheung,A.Radford,andX.Chen.Improvedtechniquesfortraining gans.In NIPS ,2016. J.Sotelo,S.Mehri,K.Kumar,J.F.Santos,K.Kastner,A.Courville,andY.Bengio.Char2wav:End-to-end speechsynthesis.In ICLR2017workshopsubmission ,2017. Y.Wang,R.Skerry-Ryan,D.Stanton,Y.Wu,R.J.Weiss,N.Jaitly,Z.Yang,Y.Xiao,Z.Chen,S.Bengio,etal. Tacotron:Towardsend-to-endspeechsynthesis.In Interspeech ,2017. Z.Wu,P.Swietojanski,C.Veaux,S.Renals,andS.King.AstudyofspeakeradaptationforDNN-basedspeech synthesis.In Interspeech ,2015. J.Yamagishi,T.Nose,H.Zen,Z.-H.Ling,T.Toda,K.Tokuda,S.King,andS.Renals.Robustspeaker-adaptive hmm-basedtext-to-speechsynthesis. IEEETransactionsonAudio,Speech,andLanguageProcessing ,2009. S.Yang,Z.Wu,andL.Xie.OnthetrainingofDNN-basedaveragevoicemodelforspeechsynthesis.In Signal andInformationProcessingAssociationAnnualSummitandConference(APSIPA),Asia-P ,2016. H.ZenandH.Sak.Unidirectionallongshort-termmemoryrecurrentneuralnetworkwithrecurrentoutputlayer forlow-latencyspeechsynthesis.In IEEEICASSP ,2015. H.Zen,Y.Agiomyrgiannakis,N.Egberts,F.Henderson,andP.Szczepaniak.Fast,compact,andhighquality LSTM-RNNbasedstatisticalparametricspeechsynthesizersformobiledevices. arXiv:1606.06061 ,2016. 9 Appendices ATrainingDeepVoice2 Figure5:SystemdiagramfortrainingprocedureforDeepVoice2. Forconvenience,wedepictthetrainingprocedureforDeepVoice2inFig. 5 ,similartothein Ariketal. ( 2017 ).Thepronunciationdictionarycanbesubstitutedbyagrapheme-to-phonememodel asin Ariketal. ( 2017 ),toadaptforunseenwordsinthepronunciationdictionary. Forfrequencyextractionduringtraining,similarto Ariketal. ( 2017 ),weusethePraatsoftware, whichmaybesubstitutedbyanotherfundamentalfrequencyandvoicednessestimationalgorithm. TheintermediateevaluationmetricsforDeepVoice2modelsarecomparedtotheirDeepVoice1 counterpartsinTable 3 .Weobtainimprovementsintheseevaluationmetricsforallthe models,butwealsonotethattheoverallsynthesisqualitycannotbefullybythesemetrics. Model Evaluationmetric DeepVoice1 DeepVoice2 Segmentation Phonemepairerrorrate 7% 0.1% Duration Meanabsoluteerror 38.0ms 17.5ms Frequency Meanabsoluteerror 29.4Hz 24.5Hz Table3:Comparisonofsegmentation,duration,frequencymodelsforDeepVoice1andDeepVoice 2,bothtrainedwiththesamesingle-speakerdataandevaluatedasdescribedin Ariketal. ( 2017 ). BModelHyperparameters AllhyperparametersofthemodelsusedinthispaperareprovidedinTable 4 . Tospeedupthetrainingofcharacter-to-spectrogrammodelinTacotroninourexperiments,weadded apenaltytermintheformCTCloss(obtainedfromtheattentionhiddenstates)totheoverallloss function.Wedonothaveaclearconclusiveevidencethatitimprovestheoverallaudioqualitybutwe observedfasterconvergenceinsomecases. Learningrateispresentedasatriple ` Œ r Œ s ,whichmeansthattheinitiallearningrateof ` was decayedbyafactorof r every s iterations.AllmodelsusetheAdamoptimizationtechnique( Kingma andBa , 2014 )with  1 =0 : 9 ,  2 =0 : 99 ,and " =10  8 . Convolutionallayersarepresentedas l  ;o;h  w whichmeansthatthereare l convolutional layers,and o (outputchannels)ineachlayer.Thesizeis h  w ,whereheight h isin frequencybinsandwidth w isintimeframes. AnydetailsorhyperparametersforDeepVoice2areidenticaltothosefromthebest modelsusedintheoriginalimplementationofDeepVoice1( Ariketal. , 2017 ).Similarly,any detailsorhyperparametersforourTacotronimplementationareidenticaltothosefrom thebestmodelsusedintheoriginalimplementationofTacotron( Wangetal. , 2017 ). 10 Single-Speaker VCTK Audiobooks Segmentation NumberofMFCCs 40 40 40 Convolutionallayers 4  ,128 9  5  4  ,64 9  5  5  ,128 9  5  Recurrentlayers(Bi-GRU) 4  ,512-wide 4  ,1024-wide 4  ,1024-wide Dropoutkeepprobability 0.95 0.85 0.85 Learningrate 10  3 Œ0.95Œ400 2  10  4 Œ0.95Œ1000 2  10  4 Œ0.95Œ2000 Silencethreshold 0.1 0.1 0.1 Gaussianwidth 0.25ms 0.25ms 0.25ms Batchsize 8 8 8 Speakerembeddingsize N/A 16 32 Duration Fully-connected 2  ,256units 4  ,256units 4  ,256units Recurrentlayers(Bi-GRU) 4  ,256-wide 4  ,512-wide 4  ,512-wide Dropoutkeepprobability 0.8 0.85 0.85 Outputbuckets 100 250 300 Learningrate 3  10  4 Œ 0 : 9 Œ300 6  10  4 Œ 0 : 9 Œ400 3  10  4 Œ 0 : 9 Œ800 Batchsize 128 32 32 Speakerembeddingsize N/A 16 32 Frequency Hiddenlayers(Bi-GRU) 3  ,256-wide 3  ,512-wide 3  ,512-wide Outputdimension 32 32 64 Convolutionwidths 5,10,20 3,6,15,30 691835 Learningrate 10  3 Œ 0 : 9 Œ300 4  10  4 Œ 0 : 9 Œ300 4  10  4 Œ 0 : 9 Œ300 Batchsize 32 32 32 Speakerembeddingsize N/A 16 16 Vocal Layers 60 20/40/60/80 80 Learningrate 10  3 Œ 0 : 9886 Œ1000 10  3 Œ 0 : 9886 Œ1000 10  3 Œ 0 : 9886 Œ1000 Batchsize 8 8 8 Speakerembeddingsize N/A 16 16 Character-to-spectrogram Enc.-CBHGbanksize 16 16 16 Enc.-CBHGchannels 128 128 128 Enc.-CBHGrecurrentsize 128 128 128 Enc.-CBHGhighwaylayers 4 4 4 Enc.-CBHGmaxpoolwidth 2 2 2 Enc.-CBHGproj.sizes 128,128 128,128 128,128 Enc.-CBHGproj.width 3 3 3 Decoderlayers 3 3 3 Dropoutkeepprobability 0.5 0.8 0.8 Attentionsize 128 256 512 Attentionstatesize 256 256 256 Decoderprenetsizes 256,128 256,128 256,128,64 Post-CBHGbanksize 8 8 8 Post-CBHGchannels 128 512 512 Post-CBHGconv.widths 3 3 3 Post-CBHGrecurrentsize 128 256 256 Post-CBHGhighwaylayers 4 4 4 Post-CBHGmaxpoolwidth 2 2 2 Reductionfactor 4 4 4 CTClosscoef 0.01 0.01 0.01 Learningrate 10  3 Œ1ŒN/A 10  3 Œ 0 : 95 Œ3000 10  3 Œ 0 : 95 Œ3000 Batchsize 16 16 16 Speakerembeddingsize N/A 32 32 Table4:Modelhyperparametersforallmodelspresentedinthispaper. 11 CTrainingTimeforMulti-speakerModels Wepresentthedetailsoftrainingtimeformulti-speakermodelsinTable 5 .Weusethesameoptimized WaveNetinferencekernelsdescribedinDeepVoice1.Fordetailedanalysisandcorresponding techniquestooptimizetheinferencetime,wereferthereadersto( Ariketal. , 2017 ). Model Hardware Timeperiteration Numberofiterations Totaltime Segmentation 1TitanXGPU 1200ms 90k 30hours Duration 1TeslaK40GPU 320ms 60k 4.5hours Frequency 1TeslaK40GPU 1100ms 25k 7.5hours Vocal(20layer) 8TitanXGPUs 280ms 350k 27hours Vocal(40layer) 8TitanXGPUs 450ms 350k 44hours Vocal(60layer) 8TitanXGPUs 600ms 500k 83hours Vocal(80layer) 8TitanXGPUs 780ms 500k 108hours Character-to-spectrogram 1TitanXGPU 450ms 200k 25hours Table5:Trainingtimedetailsofmulti-speakerDeepVoice2andTacotronmodelsonVCTKdataset. DInterpretationofLearnedEmbeddings Inthissection,weexploretheconsequencesofspeaker-dependentmodelsonintermediatemodel outputs,modelactivations,andthedistributionsofthelearnedembeddings. D.1Speaker-DependentFundamentalFrequencyPr Figure6:Time-dependentfundamentalfrequency(collectivelyincludingthe F 0 andvoiced- ness)generatedbythemodelwhentheinputsandembeddingscorrespondtoSpeaker S 6 or S 24 . Speaker S 6 isa23-year-oldfemalewithaSouthernEnglandaccentandSpeaker S 24 isa24-year-old malewithanIndianaccent.Thepronouncedsentenceis"Sixspoonsoffreshsnowpeasvethick slabsofbluecheeseandmaybeasnackforherbrotherBob".Spikesinthegroundtrutharecausedby theestimationerrorsusingthePraatsoftware( Ariketal. , 2017 ),andweobservethatourfrequency modelcancompensatetheminsomecases,learningfromtheothercorrectgroundtruthsamples. 12 Todemonstratetheofthespeakerembeddings,weruninferenceforthefrequencymodel withthespeakerembeddingvectorscorrespondingtotheactualspeakerandadifferentspeaker.As showninFig. 6 ,whiletheinputphonemefeaturesaredominantindeterminingtheoverallshapeof thefundamentalfrequencytheactualvaluesarehighlyspeakerdependent.Forexample, whenthespeakerembeddingvectorofamaleissubstitutedwiththespeakerembeddingvectorofa female,theoutputfrequencylewouldcausegenerationofafemalevoicedespitealltheother inputfeaturescorrespondtothecorrectmalespeaker. D.2Speaker-DependentAttentionPlots Figure7:Attentionplotsforthreespeakersspeakingatdifferentspeeds.Speaker S 25 isa22year-old malewithaScottishaccent,Speaker S 4 isa23-year-oldfemalewithaSouthernEnglandaccent,and Speaker S 16 isa22-year-oldmalewithaLondonaccent.Thepronouncedsentenceis"Sixspoonsof freshsnowpeasvethickslabsofbluecheeseandmaybeasnackforherbrotherBob". Fig. 7 showsthelearnedattentionplotsforthreedifferentspeakerswhotalkatdifferentspeeds.It demonstratesthattheintheTacotronencoderarchitecturearehighlyeffectivemaking theattentionmodelspeakerdependentsuchthatdifferentportionsoftheinputtextcanbefocused dependingonthespeechfeaturesofthespeaker. D.3PrincipalComponentsoftheEmbeddings Figure8:Principalcomponentsofthelearnedembeddingsofthe80-layervocalmodel,shownwith thegendersandregionsofthespeakers. Weexplorethelatentspaceofthelearnedspeakerembeddingsbyvisualizingtheminalowerdimen- sionalspace.Fig. 8 andFig. 9 showthetwoprincipalcomponentsofthelearnedembeddings ofthevocalmodelandcharacter-to-spectrogrammodelrespectively.Althoughtheyareinitialized randomlyandcompletelytrainedbasedonalossfunctionrelatedtothegenerativequality,wecan observediscriminativepatternsinthelearnedembeddings.Genderofthespeakeristhemostapparent discriminativepatternintheselearnedembeddingsthatevenalinearontheshowntwo- dimensionalspacecanclassifythegenderwithaveryhighaccuracy.Besides,weobserveapparent discriminativepatternsfortheregionofthespeaker. 7 Inthetwo-dimensionalspace,especiallyGreat BritainandNorthAmericaregionsseemhighlyseparable. 7 Theregionsaredeterminedaccordingto https://en.wikipedia.org/wiki/Regional_accents_of_ English 13 Figure9:Principalcomponentsofthelearnedembeddingsofthecharacter-to-spectrogrammodel, shownwiththegendersandregionsofthespeakers. Figure10:Architectureforspeakerdiscriminator. ESpeakerDiscriminativeModel Tocomputemulti-speakeraccuracy,weuseaspeakerdiscriminativemodeltrainedon thegroundtruthdatasetofmultiplespeakers.Althoughusinganotherdiscriminatormodelsuchas DeepSpeaker( Lietal. , 2017 )orothermethodswouldalsosufwechoosetocreateourowndeep learningbaseddiscriminativemodel.Wenotethatouraccuracyresultsonthetestsetareonparwith thestate-of-the-artspeakermethodsintheliterature.OurarchitectureisdepictedinFig. 10 .Weusemel-frequencycepstralcoefcients(MFCCs)computedafterresamplingtheinputtoa constantsamplingfrequency.Then,weemploytwo-dimensionalconvolutionallayersconvolving overbothtimeandcepstralfrequencybands,witharelunonlinearityclippedtoamaximumofsix aftereachconvolutionallayer.Thelastconvolutionallayerisfollowedbymax-poolinglayer.We thenmean-poolovertimeforallutterancetimestepsandapplyafullyconnectedlayerwitharelu nonlinearityfollowedbyafullyconnectedoutputlayerwithasoftmaxnonlinearityandcross-entropy loss.Inordertoavoidovtothedataset,weapplydropoutaftereveryrelu nonlinearity. Inordertodemonstratethattheresultsarenotsensitivetothechoiceofthehyperpa- rametersofthediscriminativemodel,wedemonstratetheaccuracyforotherchoices inthissection.HyperparametersforallthediscriminatormodelsareavailableinTable 7 .Onlythe resultsforthemodels, D 3 and D 8 ,arepresentedinTable 2 ,astheyyieldedthehighestvalidationset accuracy. 14 Param. D1 D2 D3 (inTable 2 ) D4 Audioresamplingfreq. 16KHz 16KHz 16KHz 16KHz NumberofMFCCs 20 20 80 80 Hoplength 160 160 160 160 Convolutionlayers 5  ,32 2  10  5  ,32 9  5 5  ,32 2  20 5  ,32 9  5 Maxpoolwidth&stride 2 2 2 2 Fullyconnectedsize 16 16 32 32 Dropoutprobability 0.75 0.75 0.75 0.75 Learningrate 10  3 Œ0.95Œ1000 10  3 Œ0.95Œ1000 10  3 Œ0.95Œ1000 10  3 Œ0.95Œ1000 Table6:HyperparametersforspeakerdiscriminativemodelsforVCTKdataset. Param. D5 D6 D7 D8 (inTable 2 ) Audioresamplingfreq. 16KHz 16KHz 16KHz 16KHz NumberofMFCCs 20 20 80 80 Hoplength 160 160 160 160 Convolutionlayers 3  ,32 9  5  5  32 2  10 7  ,32 9  5 5  ,32 2  10 Maxpoolwidth&stride 2 2 2 2 Fullyconnectedsize 32 32 32 32 Dropoutprobability 0.75 0.75 0.75 0.75 Learningrate 10  3 Œ0.95Œ1000 10  3 Œ0.99Œ2000 10  3 Œ0.95Œ1000 10  3 Œ0.99Œ2000 Table7:HyperparametersforspeakerdiscriminativemodelsforAudiobookdataset. Dataset Multi-SpeakerModel D1 D2 D3 (inTable 2 ) D4 VCTK DeepVoice2(20-layerWaveNet) 97.87% 97.60% 99.92% 99.84% VCTK DeepVoice2(40-layerWaveNet) 98.56% 98.68% 100.00% 100.00% VCTK DeepVoice2(60-layerWaveNet) 98.56% 98.44% 99.68% 99.80% VCTK DeepVoice2(80-layerWaveNet) 99.06% 99.21% 99.96% 99.96% VCTK Tacotron(Grif 95.89% 96.24% 99.37% 99.60% VCTK Tacotron(20-layerWaveNet) 87.93% 85.19% 60.87% 65.43% VCTK GroundTruthData 98.07% 98.00% 99.74% 99.66% Table8:accuracyusingspeakerdiscriminativemodelsD1,D2,D3andD4.The correspondingmodelhyperparametersaregiveninTable 6 . Dataset Multi-SpeakerModel D5 D6 D7 D8 (inTable 2 ) Audiobooks DeepVoice2(80-layerWaveNet) 98.24% 95.66% 96.80% 97.42% Audiobooks Tacotron(Grif 96.93% 94.89% 92.24% 93.87% Audiobooks Tacotron(20-layerWaveNet) 83.36% 80.81% 60.00% 66.53% Audiobooks GroundTruthData 96.30% 97.49% 98.39% 98.80% Table9:accuracyusingspeakerdiscriminativemodelsD5,D6,D7andD8.The correspondingmodelhyperparametersaregiveninTable 7 . 15  
Proceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics ,pages800Œ810, Berlin,Germany,August7-12,2016. c  2016AssociationforComputationalLinguistics 800 801 802 803 804 805 806 807 808 809 810  
VideoParagraphCaptioningUsingHierarchicalRecurrentNeuralNetworks HaonanYu 1  JiangWang 3 ZhihengHuang 2  YiYang 3 WeiXu 3 1 PurdueUniversity 2 Facebook haonanu@gmail.comzhiheng@fb.com 3 BaiduResearch-InstituteofDeepLearning f wangjiang03,yangyi05,wei.xug @baidu.comAbstractWepresentanapproachthatexploitshierarchicalRecur- rentNeuralNetworks(RNNs)totacklethevideocaptioning  problem,i.e.,generatingoneormultiplesentencestode-  scribearealisticvideo.Ourhierarchicalframeworkcon-  tainsasentencegeneratorandaparagraphgenerator.The  sentencegeneratorproducesonesimpleshortsentencethat  describesashortvideointerval.Itexploitsboth  temporal-andspatial-attentionmechanismstoselectively  focusonvisualelementsduringgeneration.Theparagraph  generatorcapturestheinter-sentencedependencybytaking  asinputthesententialembeddingproducedbythesentence  generator,combiningitwiththeparagraphhistory,and  outputtingthenewinitialstateforthesentencegenerator.  Weevaluateourapproachontwolarge-scalebenchmark  datasets:YouTubeClipsandTACoS-MultiLevel.Theexper-  imentsdemonstratethatourapproachoutper-  formsthecurrentstate-of-the-artmethodswithBLEU@4  scores0.499and0.305respectively.  1.Introduction Inthispaper,weconsidertheproblemofvideocaption- ing,i.e.generatingoneormultiplesentencestodescribe thecontentofavideo.Thegivenvideocouldbeasgen-  eralasthoseuploadedtoYouTube,oritcouldbeas  ascookingvideoswithactivities.Thisability  togeneratelinguisticdescriptionsforunconstrainedvideo  isimportantbecausenotonlyitisacriticalsteptowards  machineintelligence,butalsoithasmanyapplicationsin  dailyscenariossuchasvideoretrieval,automaticvideosub-  titling,blindnavigation, etc.Figure 1showssomeexample sentencesgeneratedbyourapproach. Thevideocaptioningproblemhasbeenstudiedforover onedecadeeversincetherule-basedsystemondescrib-  inghumanactivitieswithnaturallanguage[ 23].Inavery limitedsetting,Kojima etal. designedsomesimpleheuris-  ThisworkwasdonewhiletheauthorswereatBaidu. ticsforidentifyingvideoobjectsandasetofrulesforpro-  ducingverbsandprepositions.Asentenceisthengenerated  bytemplateswiththerecognizedparts  ofspeech.Followingtheirwork,severalsucceedingap-  proaches[ 26,20,21,15,3]appliedsimilarrule-basedsys- temstodatasetswithlargernumbersofobjectsandevents,  indifferenttasksandscenarios.With adhoc rules,they manuallyestablishthecorrespondencebetweenlinguistic  termsandvisualelements,andanalyzetherelationsamong  thevisualelementstogeneratesentences.Amongthem,the  mostcomplexrule-basedsystem[ 3]supportsavocabulary of118lexicalentries(including 48verbsand 24nouns).Toeliminatethetediouseffortofruleengineeringwhen theproblemscales,somerecentmethodstrainstatistical  modelsforlexicalentries,eitherinafully[ 10,14,24,42]or weakly[ 37,36,57,55]supervisedfashion.Thestatistical modelsofdifferentpartsofspeechusuallyhavedifferent  mathematicalrepresentationsandtrainingstrategies( e.g .,[14,24]).Withmostofthemanualeffortgone,thetrain- ingprocessexposesthesemethodstoevenlargerdatasets  (e.g .,YouTubeClips[ 6]andTACoS-MultiLevel[ 36])which containthousandsoflexicalentriesanddozensofhoursof  videos.Asaresult,thevideocaptioningtaskbecomesmuch  morechallenging,andthegenerationperformanceofthese  methodsisusuallylowontheselarge-scaledatasets. Sincethen,inspiringresultshavebeenachievedbyare- centlineofwork[ 11,48,47,32,54,56]which fromtherapiddevelopmentofdeepneuralnetworks,es-  peciallyRecurrentNeuralNetwork(RNN).ApplyingRNN  totranslatingvisualsequencetonaturallanguageislargely  inspiredbytherecentadvancesinNeuralMachineTransla-  tion(NMT)[ 1,43]inthenaturallanguageprocessingcom- munity.Theideaistotreattheimagesequenceofavideoas  theﬁsourcetextﬂandthecorrespondingcaptionasthetarget  text.Givenasequenceofdeepconvolutionalfeatures( e.g .,VggNet[ 40]andC3D[ 45])extractedfromvideoframes,a compactrepresentationofthevideoisobtainedby:average  pooling[ 48,32],weightedaveragepoolingwithanatten- tionmodel[ 56],ortakingthelastoutputfromanRNNen- 4584 Amanispouringoilintoapot. Adogisplayinginabowl. Thepersonopenedthedrawer.  Thepersontookoutapot.  Thepersonwenttothesink.  Thepersonwashedthepot.  Thepersonturnedonthestove. Thepersonpeeledthefruit.  Thepersonputthefruitinthebowl.  Thepersonslicedtheorange.  Thepersonputthepiecesintheplate.  Thepersonrinsedtheplateinthesink. Figure1.Someexamplesentencesgeneratedbyourapproach.TherowshowsexamplestrainedonYouTubeClips,whereonlyone  sentenceisgeneratedforeachvideo.ThesecondrowshowsexamplestrainedonTACoS-MultiLevel,whereparagraphsaregenerated.  coderwhichsummarizesthefeaturesequence[ 11,47,54].ThenanRNNdecoderacceptsthiscompactrepresentation  andoutputsasentenceofavariablelength. WhilepromisingresultswereachievedbytheseRNN methods,theyonlyfocusongeneratingasinglesentence  forashortvideoclip.Sofartheproblemofgeneratingmul-  tiplesentencesoraparagraphforalongvideohasnotbeen  attemptedbydeeplearningapproaches.Somegraphical-  modelmethods,suchasRohrbach etal. [36],areableto generatemultiplesentences,buttheirresultsarestillfar  fromperfect.Themotivationofgeneratingaparagraphis  thatmostvideosdepictfarmorethanjustoneevent.Us-  ingonlyoneshortsentencetodescribeasemanticallyrich  videousuallyyieldsuninformativeandevenboringresults.  Forexample,insteadofsaying thepersonslicedthepota- toes,cuttheonionsintopieces,andputtheonionsandpota-  toesintothepot ,amethodthatisonlyabletoproduceone shortsentencewouldprobablysay thepersoniscooking .Inspiredbytherecentprogressofdocumentmodel- ing[ 27,28]innaturallanguageprocessing,weproposea hierarchical-RNNframeworkfordescribingalongvideo  withaparagraphconsistingofmultiplesentences.Theidea  behindourhierarchicalframeworkisthatwewanttoex-  ploitthetemporaldependencyamongsentencesinapara-  graph,sothatwhenproducingtheparagraph,thesentences  arenotgeneratedindependently.Instead,thegenerationof  onesentencemightbeaffectedbythesemanticcontextpro-  videdbytheprevioussentences.Forexample,inavideo  ofcookingdishes,asentence thepersonpeeledthepota- toesismorelikelytooccur,thanthesentence theperson turnedonthestove ,afterthesentence thepersontookout somepotatoesfromthefridge .Towardsthisend,ourhierar- chicalframeworkconsistsoftwogenerators, i.e.asentence generatorandaparagraphgenerator,bothofwhichusere-  currentlayersforlanguagemodeling.Atthelowlevel,the  sentencegeneratorproducessingleshortsentencesthatde- scribetimeintervalsandvideoregions.Weexploit  bothtemporal-andspatial-attentionmechanismstoselec-  tivelyfocusonvisualelementswhengeneratingasentence.  Theembeddingofthegeneratedsentenceisencodedbythe  outputoftherecurrentlayer.Atthehighlevel,thepara-  graphgeneratortakesthesententialembeddingasinput,and  usesanotherrecurrentlayertooutputtheparagraphstate,  whichisthenusedasthenewinitialstateofthesentence  generator(seeSection 3).Figure 2illustratesourover- allframework.Weevaluateourapproachontwopublic  datasets:YouTubeClips[ 6]andTACoS-MultiLevel[ 36].Weshowthatourapproachoutperformsother  state-of-the-artmethods.Toourknowledge,thisisthe  applicationofhierarchicalRNNtovideocaptioningtask.  2.RelatedWork  NeuralMachineTranslation. ThemethodsforNMT[ 18,9,1,43,27,28]incomputationallinguisticsgenerallyfol- lowtheencoder-decoderparadigm.Anencodermapsthe  sourcesentencetoaed-lengthfeaturevectorintheem-  beddingspace.Adecoderthenconditionsonthisvectorto  generateatranslatedsentenceinthetargetlanguage.On  topofthisparadigm,severalimprovementswereproposed.  Bahdanauetal. [1]proposedasoftattentionmodeltodo alignmentduringtranslation,sothattheirapproachisable  tofocusondifferentpartsofthesourcesentencewhengen-  eratingdifferenttranslatedwords.Li etal. [27]andLin etal. [28]employedhierarchicalRNNtomodelthehier- archyofadocument.Ourapproachismuchsimilartoa  neuralmachinetranslatorwithaattentionmodel  andahierarchicalarchitecture.  ImagecaptioningwithRNNs. Theattemptofvisual- to-texttranslationusingRNNswasseenintheworkofim-  agecaptioning[ 29,22,19,50,8],whichcanbetreatedas aspecialcaseofvideocaptioningwheneachvideohasa  singleframeandnotemporalstructure.Asaresult,image 4585 captioningonlyrequirescomputingobjectappearancefea-  tures,butnotaction/motionfeatures.Theamountofdata  handledbyanimagecaptioningmethodismuch(dozensof  times)lessthanthathandledbyavideocaptioningmethod.  Theoverallstructureofanimagecaptioner(instance-to-  sequence)isalsousuallysimplerthanthatofavideocap-  tioner(sequence-to-sequence).Someothermethods,such  asParkandKim[ 34],addressedtheproblemofretrieving sentencesfromtrainingdatabasetodescribeasequenceof  images.Theyproposedalocalcoherencemodelfor  sentencetransitions,whichservesasimilarpurposeofour  paragraphgenerator.  VideocaptioningwithRNNs. Theveryearlyvideocap- tioningmethod[ 48]basedonRNNsextendstheimage captioningmethodsbysimplyaveragepoolingthevideo  frames.Thentheproblembecomesexactlythesameasim-  agecaptioning.However,thisstrategyworksonlyforshort  videoclipswherethereisonlyonemajorevent,usuallyap-  pearinginonevideoshotfromthebeginningtotheend.To  avoidthisissue,moresophisticatedwaysofencodingvideo  featureswereproposedinlaterwork,usingeitherarecur-  rentencoder[ 11,47,54]oranattentionmodel[ 56].Our sentencegeneratoriscloselyrelatedtoYao etal. [56],in thatwealsouseattentionmechanismtoselectivelyfocuson  videofeatures.Onedifferencebetweenourframeworkand  theirsisthatweadditionallyexploitspatialattention.The  otherdifferenceisthatafterweighingvideofeatureswith  attentionweights,wedonotconditionthehiddenstateof  ourrecurrentlayerontheweightedfeatures(Section 3.2).3.HierarchicalRNNforVideoCaptioning Ourapproachstacksaparagraphgeneratorontopofa sentencegenerator.Thesentencegeneratorisbuiltupon  1)aRecurrentNeuralNetwork(RNN)forlanguagemodel-  ing,2)amultimodallayer[ 29]forintegratinginformation fromdifferentsources,and3)anattentionmodel[ 56,1]for selectivelyfocusingontheinputvideofeatures.Thepara-  graphgeneratorissimplyanotherRNNwhichmodelsthe  inter-sentencedependency.Itreceivesthecompactsenten-  tialrepresentationencodedbythesentencegenerator,com-  binesitwiththeparagraphhistory,andoutputsanewini-  tialstateforthesentencegenerator.TheRNNsexploited  bythetwogeneratorsincorporatethe GatedRecurrentUnit (GRU)[ 9]whichisaoftheLongShort-Term Memory(LSTM)architecture[ 16].Inthefollowing,we reviewtheRNNwiththeGRU(orthe gatedRNN),andthendescribeourframeworkindetails. 3.1.GatedRecurrentUnit AsimpleRNN[ 12]canbeconstructedbyaddingfeed- backconnectionstoafeedforwardnetworkthatconsistsof  threelayers:theinputlayer x ,thehiddenlayer h ,andthe outputlayer y .Thenetworkisupdatedbyboththeinput andthepreviousrecurrenthiddenstateasfollows: h t = ˚  W h x t + U h h t  1 + b h  (hiddenstate) y t = ˚ ( U y h t + b y ) (output)whereW ; U andb areweightmatricesandbiasestobe learned,and ˚ (  ) areelement-wiseactivationfunctions. WhilethesimpleRNNisabletomodeltemporalde- pendencyforasmalltimegap,itusuallyfailstocapture  long-termtemporalinformation.Toaddressthisissue,the  GRU[ 9]isdesignedtoadaptivelyrememberandforgetthe past.Insidetheunit,thehiddenstateismodulatedbynon-  lineargates.,let  denotetheelement-wise multiplicationoftwovectors,theGRUcomputesthehid-  denstate h as:r t = ˙ ( W r x t + U r h t  1 + b r ) (resetgate) z t = ˙ ( W z x t + U z h t  1 + b z ) (updategate) e h t = ˚  W h x t + U h ( r t  h t  1 )+ b h  h t = z t  h t  1 +(1  z t )  e h t (hiddenstate) where˙ (  ) areelement-wiseSigmoidfunctions.Thereset gate r determineswhetherthehiddenstatewantstodrop anyinformationthatwillbeirrelevantinthefuture.The  updategate z controlshowmuchinformationfromthepre- vioushiddenstatewillbepreservedforthecurrentstate.  DuringthetrainingofagatedRNN,theparameterscanbe  estimatedbyBackpropagationThroughTime(BPTT)[ 53]asintraditionalRNNarchitectures.  3.2.SentenceGenerator TheoverallstructureofourhierarchicalRNNisillus- tratedinFigure 2.Thesentencegeneratoroperatesatevery timestepwhenaone-hotinput(1-of- N encoding,where N isthevocabularysize)arrivesattheembeddinglayer.The  embeddinglayerconvertstheone-hotvectortoadenserep-  resentationinalowerdimensionalspacebymultiplyingit  withanembeddingtable( 512 N ),ofwhicheachrowisa wordembeddingtobelearned.Theresultingwordembed-  dingistheninputtoourRNN, i.e.,therecurrentlayer I.Thisgatedrecurrentlayerhas 512dimensionsandacts similarlytothosethatarecommonlyemployedbyavari-  etyofimage/videocaptioningmethods( e.g .,[ 47,29,56]),i.e.,modelingthesyntaxofalanguage.Itupdatesitshid- denstateeverytimeanewwordarrives,andencodesthe  sentencesemanticsinacompactformuptothewordsthat  havebeenfedin.Wesettheactivationfunction ˚ ofthis recurrentlayertobetheLinearUnit(ReLU)[ 31],sinceitperformsbetterthannon-linearactivationfunctions  suchasSigmoidaccordingtoourobservation. Asonebranch,theoutputoftherecurrentlayerIisdi- rectedtotheattentionlayerstocomputeattentionweights  forthefeaturesinthevideofeaturepool.Ourattention  modelisinspiredbytherecentsoft-alignmentmethodthat 4586 !"# !"# $%&'()*+,-.  /012--3%4  526',,2%()$  7'890+-:8  ;,2-36(2-)*+,-.  <3--2%  =+>0:?  7:?$@  A:.()$%.(:%62  !"# "B#C !"# /012--3%4) DE2,:42  526',,2%()$$  512 =2%(2%62)) /012--3%4)  512 ;:,:4,:&F)=(:(2  (a) Sentence Generator  (b) Paragraph Generator  G3-2+)H2:(',2);++8  DI2%9+%)$  *234F(2-)DE2,:42  )DI2%9+%)$$  =2J'2%9:8)=+>0:?  Figure2.OurhierarchicalRNNforvideocaptioning. Greendenotestheinputtotheframework, bluedenotestheoutput,and reddenotestherecurrentcomponents.The orangearrowrepresentsthereinitializationofthesentencegeneratorwiththecurrentparagraphstate.For simplicity,weonlydrawasinglevideofeaturepoolintheInfact,bothappearanceandactionfeaturesgothroughasimilarattention  processbeforetheyarefedintothemultimodallayer.  hasbeensuccessfullyappliedinthecontextofNeuralMa-  chineTranslation(NMT)[ 1],andwaslateradaptedtovideo captioningbyYao etal. [56].Thedifferencebetweenour modelandtheoneusedbyYao etal. isthattheirmodel onlyfocusesontemporalattention.Weadditionallyin-  cludespatialattentionbycomputingfeaturesformultiple  imagepatchesatdifferentlocationsonavideoframeand  poolthefeaturestogether.Thissimpleimprovementisim-  portantwhenobjectsaresmallanddiftobelocalized  onsomedatasets( e.g .,TACoS-MultiLevel[ 36]).Inthis case,whole-frame-basedvideofeatureswillfailtocapture  theobjectinformationandmultipleobjectproposalsare  neededforgoodperformance(seeSection 5fordetails).Let thefeaturesinthepoolbedenotedas fv 1 ; v 2 ;:::; v KM g,whereM isthevideolengthand K isthenumberofpatches oneachframe.Wewanttocomputeasetofweights  f t 1 ; t 2 ;:::; t KM gforthesefeaturesateachtimestep t suchthat P KM  m =1  t m =1 .Todoso,wecomputean attentionscore q t m foreachframe m,conditioningonthe previoushiddenstate h t  1 :q t m = w > ˚ ( W q v m + U q h t  1 + b q ) wherew ,W q ,U q ,and b q aretheparametersshared byallthefeaturesatallthetimesteps,and ˚ issetto theelement-wiseScaledHyperbolicTangent(stanh)func-  tion[ 25]:1: 7159 tanh( 2 x 3 ) .Theabovecomputationisper- formedbytheattentionlayersIandIIinFigure 2(a),where theattentionlayerIprojectsthefeature v andthehidden stateh intoalowerdimensionalspacewhosedimension canrangefrom 32to256.TheattentionlayerIIthenfur- thercompressestheactivationoftheprojectedvectorintoa  scalar,oneforeachfeature.Afterthis,wesetupasequen- tialsoftmaxlayertogettheattentionweights:  t m =exp  q t m  . KM X m 0 =1 exp  q t m 0  Finally,asinglefeaturevectorisobtainedbyweightedav-  eraging:u t = P KM  m =1  t m v m .Theaboveprocessisaso- phisticatedversionofthetemporalmeanpooling.Itallows  thesentencegeneratortoselectivelyfocusonasubsetofthe  featuresduringgeneration.Notethatwhileonlyonefeature  channelisshowninFigure 2(a),oursentencegeneratorin factpumpsfeaturesofseveralchannelsthroughthesame  attentionprocess.Eachfeaturechannelhasadifferentset  ofweightsandbiasestobelearned.Inourexperiments,we  employtwofeaturechannels,oneforobjectappearanceand  theotherforaction/motion.(Section 5).Aftertheattentionprocess,theweightedsumsofthe videofeaturesarefedintothemultimodallayerwhichhas  1024dimensions.Themultimodallayeralsoreceivesthe outputoftherecurrentlayerI,thusconnectingthevision  componentwiththelanguagemodel.Supposewehavetwo  videofeaturechannels,ofwhichtheweightedfeaturesout-  putbytheattentionmodelare u t  o andu t  a respectively.The multimodallayermapsthetwofeatures,togetherwiththe  hiddenstate h t oftherecurrentlayerI,intoa 1024dimen-sionalfeaturespaceandaddthemup: m t = ˚ ( W m;o u t  o + W m;a u t  a + U m h t + b m ) where˚ issettotheelement-wisestanhfunction.Toreduce ovweadddropout[ 41]withadroprateof0.5to thislayer. Themultimodallayerisfollowedbyahiddenlayerand asoftmaxlayer(seeFigure 2(a)),bothwiththeelement- wisestanhfunctionastheiractivationfunctions.Thehid-  denlayerhasexactlythesamedimension 512withthe 4587 wordembeddinglayer,andthesoftmaxlayerhasadimen-  sionthatisequaltothesizeofthevocabularywhichis  dataset-dependent.Inspiredbythetransposedweightshar-  ingschemerecentlyproposedbyMao etal. [30],weset theprojectionmatrixfromthehiddenlayertothesoftmax  layerasthetransposeofthewordembeddingtable.Ithas  beenshownthatthisstrategyallowstheuseofawordem-  beddinglayerwithamuchlargerdimensionduetothepa-  rametersharing,andhelpsregularizethewordembedding  tablebecauseofthematrixtranspose.Asthestepof  thesentencegenerator,themaxidlayerpickstheindexthat  pointstothemaximalvalueintheoutputofthesoftmax  layer.Theindexisthentreatedasthepredictedwordid.  Notethatduringtest,thepredictedwordwillbefedbackto  thesentencegeneratoragainasthenextinputword.While  inthetraining,thenextinputwordisalwaysprovidedby  theannotatedsentence.  3.3.ParagraphGenerator Thesentencegeneratoraboveonlyhandlesonesingle sentenceatatime.Forthesentenceintheparagraph,  theinitialstateoftherecurrentlayerIissettoallzeros,  i.e.,h 0 = 0.However,anysentenceafterthatwillhave itsinitialstateconditionedonthesemanticcontextofallits  precedingsentences.Thissemanticcontextisencodedby  ourparagraphgenerator. Duringthegenerationofasentence,anembeddingaver- agelayer(seeFigure 2(b))accumulatesallthewordembed- dingsofthesentenceandtakestheaveragetogetacompact  embeddingvector.Theaveragestrategyisinspiredbythe  QAembedding[ 5]inwhichquestionsandanswersareboth representedasacombinationoftheembeddingsoftheirin-  dividualwordsand/orsymbols.Wealsotakethelaststate  oftherecurrentlayerIasacompactrepresentationforthe  sentence,followingtheideabehindtheEncoder-Decoder  framework[ 9]inNMT.Afterthat,theaveragedembedding andthelastrecurrentstateareconcatenatedtogether,and  fullyconnectedtothesentenceembeddinglayer( 512di-mensions)withstanhastheactivationfunction.Wetreat  theoutputofthesentenceembeddinglayerasthesen-  tencerepresentation. Thesentenceembeddinglayerislinkedtooursecond gatedRNN(seeFigure 2(b)).TherecurrentlayerIIop- erateswheneverafullsentencegoesthroughthesentence  generatorandthesentenceembeddingisproducedbythe  sentenceembeddinglayer.Thusthetworecurrentlayers  areasynchronous:whiletherecurrentlayerIkeepsupdat-  ingitshiddenstateateverytimestep,therecurrentlayerII  onlyupdatesitshiddenstatewhenafullsentencehasbeen  processed.TherecurrentlayerIIencodestheparagraphse-  manticsinacompactformuptothesentencesthathave  beenfedin.Finally,wesetupaparagraphstatelayerto  combinethehiddenstateoftherecurrentlayerIIandthe sentenceembedding.Thisparagraphstateisusedasthe  initialhiddenstatewhentherecurrentlayerIisreinitialized  forthenextsentence.Itessentiallyprovidesthesentence  generatorwiththeparagraphhistorysothatthenextsen-  tenceisproducedinthecontext.  4.TrainingandGeneration Wetrainallthecomponentsinourhierarchicalframe- worktogetherfromscratchwithrandomlyinitializedpa-  rameters.Wetreattheactivationvalueindexedbyatraining  word w n t inthesoftmaxlayerofoursentencegeneratoras thelikelihoodofgeneratingthatword: P w n t j s 1: n  1 ;w n 1: t  1 ; V  given1)alltheprecedingsentences s 1: n  1 intheparagraph, 2)allthepreviouswords w n 1: t  1 inthesamesentence n ,and 3)thecorrespondingvideo V .Thecostofgeneratingthat trainingwordisthenasthenegativelogarithmof  thelikelihood.Wefurtherthecostofgeneratingthe  wholeparagraph s 1: N (N isthenumberofsentencesinthe paragraph)as: PPL ( s 1: N j V ) =  N X n =1 T n X t =1 log P w n t j s 1: n  1 ;w n 1: t  1 ; V  , N X n =1 T n whereT n isthenumberofwordsinthesentence n .The abovecostisinfactthe perplexity oftheparagraphgiven thevideo.Finally,thecostfunctionovertheentiretraining  setisas: PPL = Y X y =1 0 @PPL ( s y  1: N y j V y )  N y X n =1 T y n 1 A, Y X y =1 N y X n =1 T y n (1)whereY isthetotalnumberofparagraphsinthetraining set.ToreduceovL2andL1regularizationterms  areaddedtotheabovecostfunction.WeuseBackpropa-  gationThroughTime(BPTT)[ 53]tocomputethegradients oftheparametersandStochasticGradientDescent(SGD)  totheoptimum.Forbetterconvergence,wedividethe  gradientbyarunningaverageofitsrecentmagnitudeac-  cordingtotheRMSPROPalgorithm[ 44].Wesetasmall learningrate 10 4 toavoidthegradientexplosionproblem thatiscommoninthetrainingprocessofRNNs. Aftertheparametersarelearned,weperformthegen- erationwithBeamSearch.Supposethatweuseabeam  widthof L .ThebeamsearchprocessstartswiththeBOS (begin-of-sentence)symbol w BOS(i.e.,w 0 )whichistreated asa1-wordsequencewithzerocostat t =0 .Assume thatatanytimestep t ,thereareatmost Lt -wordsequences thatwerepreviouslyselectedwiththelowestsequencecosts 4588 (asequencecostisthesumofthewordcostsinthatse-  quence).Foreachofthe t -wordsequences,givenitslast wordasinput,thesentencegeneratorcalculatesthecostof  thenextword  log P( w t j w 1: t  1 ; V ) andthesequencecost ifthewordisappendedtothesequence.Thenfromallthe  t +1 -wordsequencesexpandedfromtheexisting t -wordse- quences,wepickthetop L withthelowestsequencecosts. Ofthenew t +1 -wordsequences,anyonethatisacom- pletesentence( i.e.,thelastword w t +1 istheEOS(end-of- sentence)symbol w EOS)willberemovedfromthesearch tree.Itwillbeputintooursentencepoolif1)thereareless  thanJ (J  L )sentencesinthepoolor,2)itssequence costislowerthanoneofthe J sentencesinthepool.Inthe secondcase,thesentencewiththehighestcostwillbere-  movedfromthepool,replacedbythenewaddedsentence.  Alsoofthenew t +1 -wordsequences,anyonethathasa highersequencecostthanallofthe J sentencesinthepool willberemovedfromthesearchtree.Thereasonisthatex-  pandingawordsequencemonotonicallyincreasesitscost.  Thebeamsearchprocessstopswhenthereisnowordse-  quencetobeexpandedinthenexttimestep.Intheend,  J candidatesentenceswillbegeneratedforpost-processing andevaluation. Afterthis,thegenerationprocessgoesonbypickingthe sentencewiththelowestcostfromthe J candidatesen- tences.Thissentenceisfedintoourparagraphgenerator  whichreinitializesthesentencegenerator.Thesentence  generatorthenacceptsanewBOSandagainproduces J candidatesentences.Thiswholeprocessstopswhenthe  sentencereceivedbytheparagraphgeneratoristheEOP  (end-of-paragraph)whichconsistsofonlytheBOSandthe  EOS.Finally,wewillhaveaparagraphthatisasequence  oflists,eachlistwith J sentences.Inourexperiments, weset L = J =5 .Excludingthecalculationofvisual features,theaveragecomputationaltimeforthesentence  generatortoproducetop5candidatesentenceswithabeam  widthof5is0.15seconds,onasinglethreadwithCPU  Intel(R)Core(TM)i7-5960X@3.00GHz.  5.Experiments Weevaluateourapproachontwobenchmarkdatasets: YouTubeClips[ 6]andTACoS-MultiLevel[ 36].YouTubeClips Thisdatasetconsistsof 1; 967shortvideo clips( 9secondsonaverage)downloadedfromYouTube. Thevideoclipsareopen-domain,containingdifferentpeo-  ple,animals,actions,scenarios,landscapes, etc.Eachvideo clipisannotatedwithmultipleparallelsentencesbydiffer-  entturkers.Thereare 80; 839sentencesintotal,withabout 41annotatedsentencesperclip.Eachsentenceonaver- agecontainsabout 8words.Thewordscontainedinall thesentencesconstituteavocabularyof 12; 766uniquelex- icalentries.Weadoptthetrainandtestsplitsprovidedby  Guadarramaetal. [14],where 1; 297and670videosare usedfortrainingandtestingrespectively.Itshouldbenoted  thatwhilemultiplesentencesareannotatedforeachvideo  clip,theyare parallel andindependentinthetemporalex- tent,i.e.,thesentencesdescribeexactlythesamevideoin- terval,fromthebeginningtotheendofthevideo.Asa  result,weusethisdatasetasaspecialtestcaseforourap-  proach,whentheparagraphlength N =1 .TACoS-MultiLevel Thisdatasetconsistsof 185longvideos(6minutesonaverage)inanindoorenviron-  ment.Thevideosareclosed-domain,containingdifferent  actors,activities,andsmallinteractingobjects  indailycookingscenarios.Eachvideoisannotatedbymul-  tipleturkers.Aturkerannotatesasequenceoftemporal  intervalsacrossthevideo,pairingeveryintervalwithasin-  gleshortsentence.Thereare 16; 145distinctintervalsand 52; 478sentencesintotal,withabout 87intervalsand 284sentencespervideo.Thesentenceswereoriginallyprepro-  cessedsothattheyallhavethepasttense,anddifferent  genderiweresubstitutedwithﬁ theper- sonﬂ.Eachsentenceonaveragecontainsabout 8words. Thewordscontainedinallthesentencesconstituteavocab-  ularyof 2; 864uniquelexicalentries.Weadoptthetrainand testsplitsusedbyRohrbach etal. [36],where 143and42videosareusedfortrainingandtestingrespectively.Note  thatthecookingactivitiesinthisdatasethavestrongtem-  poraldependencies.Suchdependencyinavideoisim-  pliedbythesequenceofintervalsannotatedbythesame  turkeronthatvideo.FollowingDonahue etal. [11]and Rohrbachetal. [36],weemploytheintervalinformation toalignoursentencesintheparagraphduringbothtraining  andgeneration.Thisdatasetisusedasageneraltestcase  forourapproach,whentheparagraphlength N> 1.Tomodelvideoobjectappearance,weusethepre- trainedVggNet[ 40](ontheImageNetdataset[ 38])forboth datasets.SincetheobjectsinYouTubeClipsareusually  prominent,weonlyextractoneVggNetfeatureforeach  entireframe.Thisresultsinonlytemporalattentioninour  sentencegenerator( i.e.,K =1 inSection 3.2).ForTACoS- MultiLevel,theinteractingobjectsareusuallyquitesmall  anddiftobelocalized.Tosolvethisproblem,both  Donahueetal. [11]andRohrbach etal. [36]designedaspe- cializedhanddetector.Oncethehandregionsaredetected,  theyextractfeaturesintheneighborhoodtorepresentthe  interactingobjects.Insteadoftryingtoaccuratelylocate  handswhichrequiresalotofengineeringeffortasintheir  case,werelyonasimpleroutinetoobtainmultipleobject  proposals.WeuseOpticalFlow[ 13]toroughlydetect aboundingboxfortheactorineachframe.Wethenextract  K imagepatchesofsize 220 220alongthelowerpartof theboxborder,whereeverytwoneighboringpatcheshave  anoverlapofhalftheirsize.Oursimpleobservationisthat  thesepatchestogetherhaveahighrecallofcontainingthe  interactingobjectswhiletheactoriscooking.Finally,we 4589 B@1B@2B@3B@4 MCLSTM-YT[ 48]---0.333 0.291-S2VT[ 47]---- 0.298-MM-VDN[ 54]---0.376 0.290-TA[ 56]0.8000.6470.5260.419 0.2960.517LSTM-E[ 32]0.7880.6600.5540.453 0.310-h-RNN-Vgg0.7730.6450.5460.443 0.3110.621h-RNN-C3D0.7970.6790.5790.474 0.3030.536h-RNN(Ours) 0.8150.7040.6040.499 0.3260.658Table1.ResultsonYouTubeClips,whereB,M,andCareshort  forBLEU,METEOR,andCIDErrespectively.  computetheVggNetfeatureforeachpatchandpoolallthe  patchfeatures.When K> 1,theaboveroutineleadsto bothtemporalandspatialattentioninoursentencegenera-  tor.Inpractice,wethatasmallvalueof K (e.g .,3˘ 5)isenoughtoyieldgoodperformance. Tomodelvideomotionandactivities,weusethepre- trainedC3D[ 45](ontheSports-1Mdataset[ 19])for YouTubeClips.TheC3Dnetreadsinavideoandoutputs  aed-lengthfeaturevectorevery 16frames.Thuswhen applyingtheattentionmodeltotheC3Dfeaturepool,we  setK =1 anddivide M by16(Section3.2).Forthe TACoS-MultiLeveldataset,sincethecookingactivitiesare  thesamemodeltrainedonsportsvideosdoes  notworkwell.AlternativelywecomputetheDenseTrajec-  tories[ 51]foreachvideointervalandencodethemwiththe Fishervector[ 17].Fortheattentionmodel,weset K =1 andM =1 .Weemploythreedifferentevaluationmetrics:BLEU [33],METEOR[ 2],andCIDEr[ 46].Becausethe YouTubeClipsdatasetwastestedonbymostexistingvideo-  captioningmethods,thepriorresultsofallthethreemet-  ricshavebeenreported.TheTACoS-MultiLeveldataset  isrelativelynewandonlytheBLEUscoreswerereported  inthepreviouswork.Wecomputetheothermetricsfor  thecomparisonmethodsbasedonthegeneratedsentences  thatcomewiththedataset.Generally,thehigherthemetric  scoresare,thebetterthegeneratedsentencecorrelateswith  humanjudgment.Weusetheevaluationscriptprovidedby  Chenetal. [7]tocomputescoresonbothdatasets. 5.1.Results Wecompareourapproach(h-RNN)onYouTube- Clipswithsixstate-of-the-artmethods:LSTM-YT[ 48],S2VT[ 47],MM-VDN[ 54],TA[ 56],andLSTM-E[ 32].Notethatinthisexperimentasinglesentenceisgener-  atedforeachvideo.Thusonlyoursentencegeneratoris  evaluatedincomparisontoothers.Toevaluatetheimpor-  tanceofourvideofeatures,wealsoreporttheresultsoftwo  baselinemethods:h-RNN-Vggandh-RNN-C3D.Thefor-  merusesonlytheobjectappearancefeatureandthelatter  usesonlythemotionfeature,withothercomponentsofour B@1B@2B@3B@4 MCCRF-T[ 37]0.5640.4470.3320.253 0.2601.248CRF-M[ 36]0.5840.4670.3520.273 0.2721.347LRCN[ 11]0.5930.4820.3700.292 0.2821.534h-RNN-Vgg0.5610.4450.3290.256 0.2601.267h-RNN-DT0.5570.4510.3460.274 0.2611.400RNN-sent0.5680.4690.3670.295 0.2781.580RNN-cat0.6050.4890.3760.297 0.2841.555h-RNN(Ours) 0.6080.4960.3850.305 0.2871.602Table2.ResultsonTACoS-MultiLevel,whereB,M,andCare  shortforBLEU,METEOR,andCIDErrespectively.  frameworkunchanged.Theevaluationresultsareshown  inTable 1.Wecanseethatourapproachperformsmuch betterthanthecomparisonmethods,inallthethreemet-  rics.Theimprovementsonthemostrecentstate-of-the-art  method( i.e.,LSTM-E[ 32])are 0 : 499  0 : 453 0 : 453 =10 : 15% intheBLEU@4score,and 0 : 326  0 : 310 0 : 310 =5 : 16% intheME- TEORscore.SinceLSTM-EalsoexploitsVggNetand  C3Dfeatures,thisdemonstratesthatoursentencegenerator  frameworkissuperiortotheirjointembeddingframework.  Moreover,althoughTA[ 56]alsoemploystemporalatten- tion,ourapproachproducesmuchbetterresultsduetothe  factthatthehiddenstateofourRNNisnotconditionedon  thevideofeatures.Instead,thevideofeaturesaredirectly  inputtoourmultimodallayer.Ourapproachalsooutper-  formsthetwobaselinemethodsbylargemargins,indicat-  ingthatbothvideofeaturesareindeedcrucialinthevideo  captioningtask. WecompareourapproachonTACoS-MultiLevelwith threestate-of-the-artmethods:CRF-T[ 37],CRF-M[ 36],andLRCN[ 11].Likeabove,wehavetwobaselinemethods h-RNN-Vggandh-RNN-DTwhichuseonlytheappearance  andmotionfeaturesrespectively.Wealsoaddanothertwo  baselinemethodsRNN-sentandRNN-catthathavenohi-  erarchy( i.e.,withonlythesentencegenerator,butnotthe paragraphgenerator).RNN-sentistrainedandtestedon  individualvideoclipsthataresegmentedfromtheoriginal  185longvideosaccordingtotheannotatedintervals.The initialstateofthesentencegeneratorissettozeroforeach  sentence.Asaresult,sentencesaretrainedandgenerated  independently.RNN-catinitializesthesentencegenerator  withzeroonlyforthesentenceinaparagraph.Then  thesentencegeneratormaintainsitsstateforthefollowing  sentencesuntiltheendoftheparagraph.Thisconcatenation  strategyfortrainingaparagraphhasbeenexploitedinare-  centneuralconversationalmodel[ 49].WeuseRNN-send andRNN-cattoevaluatetheimportanceofourhierarchical  structure.TheresultsonTACoS-MultiLevelareshowninTable 2.Ourapproachoutperformsthestate-of-the-artmethods,in-  cludingtheveryrecentlyproposedone( i.e.,LRCN)with animprovementof 0 : 305  0 : 292 0 : 292 =4 : 45% intheBLEU@4 4590 RNN-sent:Thepersonenteredthekitchen.  Thepersonwenttotherefrigerator. Thepersonplacedthecucumberonthecuttingboard.  Thepersonrinsedthecuttingboard. h-RNN:Thepersonwalkedintothekitchen.  Thepersonwenttotherefrigerator. Thepersonwalkedovertothesink.  Thepersonrinsedthecarrotinthesink. RNN-sent:Thepersontookoutacuttingboardfromthedrawer. Thepersongotaknifeandacuttingboardfromthedrawer. Thepersoncuttheendsoffthecuttingboard. h-RNN:Thepersontookoutacuttingboard. Thepersongotaknifefromthedrawer. Thepersoncutthecucumberonthecuttingboard. Figure3.Examplesofgeneratedparagraphs. Redindicatesincor- rectsentencesproducedbyRNN-sentand greenshowstheones generatedbyourh-RNNinthecorrespondingtimeintervals.In  theexample,ourhierarchicalmodelsuccessfullycapturesthe  highlikelihoodoftheevent walktothesink aftertheevent opentherefrigerator .Inthesecondexample,RNN-sentgeneratesthe event takethecuttingboard twiceduetothefactthatthesentences intheparagraphareproducedindependently.Incontrast,ourhi-  erarchicalmodelavoidsthismistake.  score.Giventhatourstrategyofextractingobjectregions  isrelativelysimplecomparedtothesophisticatedhandde-  tector[ 11,36],weexpecttohaveevenbetterperformance ifourobjectlocalizationisimproved.Ourmethodisalso  superiortoallthebaselinemethods.AlthoughRNN-cat  modelstemporaldependencyamongsentencesbysentence-  levelconcatenation,itperformsworsethanourhierarchical  architecture.Again,itshowsthatboththevideofeatures  andthehierarchicalstructurearecrucialinourtask.Fig-  ure3illustratessomeexampleparagraphsgeneratedbyour approachonTACoS-MultiLevel. Tofurtherdemonstratethatourmethodh-RNNgen- eratesbettersentencesthanRNN-catingeneral,weper-  formhumanevaluationtocomparethesetwomethodson  TACoS-MultiLevel.,wediscard 1; 166testvideointervals,eachofwhichhasexactlythesamesen-  tencegeneratedbyRNN-catandh-RNN.Thisresultsina  totalnumberof 4; 314 1; 166=3 ; 148videointervalsfor humanevaluation.Wethenputthevideointervalsandthe  generatedsentencesonAmazonMechanicalTurk(AMT).  Eachvideointervalispairedwithonesentencegenerated  byRNN-catandtheotherbyh-RNN,sidebyside.Foreach  videointerval,weaskoneturkertoselectthesentencethat  betterdescribesthevideocontent.Theturkeralsohasa thirdchoiceifhebelievesthatbothsentencesareequally  goodorbad.Intheend,weobtained 773selectionsforh- RNNand 472selectionsforRNN-cat,withagapof 301selections.Thush-RNNhasatleast 301 472+3069 =8 : 50% improvementoverRNN-cat. h-RNNRNN-catEquallygoodorbad Total 773472306943145.2.DiscussionsandLimitations Althoughourapproachisabletoproduceparagraphsfor videoandhasachievedencouragingresults,itissubjectto  severallimitations.First,ourobjectdetectionroutinehas  difhandlingverysmallobjects.Mostofourfail-  urecasesonTACoS-MultiLevelproduceincorrectobject  namesinthesentences, e.g .,confusingsmallobjectsthat havesimilarshapesorappearances( cucumbervs.carrot ,mangovs.orange ,kiwivs.avocado ,etc.).SeeFigure 1fora concreteexample: slicedtheorange shouldreallybe slicedthemango .Accuratelydetectingsmallobjects(sometimes withocclusion)incomplexvideoscenariosstillremainsan  openproblem.Second,thesententialinformationwsuni-  directionallythroughtheparagraphrecurrentlayer,from  thebeginningoftheparagraphtotheend,butnotalsoin  thereverseway.Misleadinginformationwillbepotentially  passeddownwhentheseveralsentencesinaparagraph  aregeneratedincorrectly.UsingbidirectionalRNN[ 39,52]forsentencegenerationisstillanopenproblem.Lastly,our  approachsuffersfromaknownproblemasinmostother  image/videocaptioningmethods,namely,thereisdiscrep-  ancybetweentheobjectivefunctionusedbytrainingandthe  oneusedbygeneration.Thetrainingprocesspredictsthe  nextwordgiventhepreviouswordsfromgroundtruth,while  thegenerationprocessconditionsthepredictionontheones  previouslygeneratedbyitself.Thisproblemis  inourhierarchicalframeworkwheretheparagraphgenera-  torconditionsongroundtruthsentencesduringtrainingbut  ongeneratedonesduringgeneration.Apotentialcurefor  thiswouldbeaddingScheduledSampling[ 4]tothetrain- ingprocess,whereonerandomlyselectsbetweenthetrue  previouswordsandthewordsgeneratedbythemodel.An-  othersolutionmightbetodirectlyoptimizethemetric( e.g .,BLEU)usedattesttime[ 35].6.Conclusion Wehaveproposedahierarchical-RNNframeworkfor videoparagraphcaptioning.Theframeworkmodelsinter-  sentencedependencytogenerateasequenceofsentences  givenvideodata.Theexperimentsshowthatourapproach  isabletogenerateaparagraphforalongvideoandachieves  thestate-of-the-artresultsontwolarge-scaledatasets.  Acknowledgments TheprimaryauthorwouldliketothankBaiduResearch forprovidingthesummerinternship. 4591 References [1]D.Bahdanau,K.Cho,andY.Bengio.Neuralmachinetrans- lationbyjointlylearningtoalignandtranslate.In Interna-tionalConferenceonLearningRepresentations ,2015. [2]S.BanerjeeandA.Lavie.METEOR:Anautomaticmet- ricforMTevaluationwithimprovedcorrelationwithhuman  judgments.In ProceedingsoftheACLWorkshoponIntrinsic andExtrinsicEvaluationMeasuresforMachineTranslation  and/orSummarization ,pages65Œ72,June2005. [3]A.Barbu,A.Bridge,Z.Burchill,D.Coroian,S.Dick- inson,S.Fidler,A.Michaux,S.Mussman,N.Siddharth,  D.Salvi,L.Schmidt,J.Shangguan,J.M.Siskind,J.Wag-  goner,S.Wang,J.Wei,Y.Yin,andZ.Zhang.Videoinsen-  tencesout.In ProceedingsoftheConferenceonUncertainty inIntelligence ,pages102Œ112,2012. [4]S.Bengio,O.Vinyals,N.Jaitly,andN.Shazeer.Scheduled samplingforsequencepredictionwithrecurrentneuralnet-  works.In AdvancesinNeuralInformationProcessingSys- tems,pages1171Œ1179,2015. [5]A.Bordes,S.Chopra,andJ.Weston.Questionanswering withsubgraphembeddings.In ConferenceonEmpirical MethodsinNaturalLanguageProcessing ,pages615Œ620, 2014.[6]D.L.ChenandW.B.Dolan.Collectinghighlyparalleldata forparaphraseevaluation.In Proceedingsofthe49thAn- nualMeetingoftheAssociationforComputationalLinguis-  tics(ACL-2011) ,Portland,OR,June2011. [7]X.Chen,H.Fang,T.Lin,R.Vedantam,S.Gupta,P.Doll ´ar, andC.L.Zitnick.MicrosoftCOCOcaptions:Datacollec-  tionandevaluationserver. CoRR,abs/1504.00325,2015. [8]X.ChenandC.L.Zitnick.Learningarecurrentvisualrepre- sentationforimagecaptiongeneration.In Proceedingsofthe IEEEConferenceonComputerVisionandPatternRecogni-  tion,2015. [9]K.Cho,B.vanMerrienboer,C¸.G ¨ulc¸ehre,F.Bougares, H.Schwenk,andY.Bengio.Learningphraserepresentations  usingRNNencoder-decoderforstatisticalmachinetransla-  tion.In ConferenceonEmpiricalMethodsinNaturalLan- guageProcessing ,2014. [10]P.Das,C.Xu,R.F.Doell,andJ.J.Corso.Athousandframes injustafewwords:Lingualdescriptionofvideosthroughla-  tenttopicsandsparseobjectstitching.In Proceedingsofthe IEEEConferenceonComputerVisionandPatternRecogni-  tion,pages2634Œ2641,2013. [11]J.Donahue,L.A.Hendricks,S.Guadarrama,M.Rohrbach, S.Venugopalan,K.Saenko,andT.Darrell.Long-termrecur-  rentconvolutionalnetworksforvisualrecognitionandde-  scription.In ProceedingsoftheIEEEConferenceonCom- puterVisionandPatternRecognition ,2015. [12]J.L.Elman.Findingstructureintime. COGNITIVESCI- ENCE,14(2):179Œ211,1990. [13]G.Farneb ¨ack.Two-framemotionestimationbasedonpoly- nomialexpansion.In Proceedingsofthe13thScandinavian ConferenceonImageAnalysis ,pages363Œ370,2003. [14]S.Guadarrama,N.Krishnamoorthy,G.Malkarnenkar, S.Venugopalan,T.D.R.Mooney,andK.Saenko. Youtube2text:Recognizinganddescribingarbitraryactiv-  itiesusingsemantichierarchiesandzero-shotrecognition.  InICCV'13Int.Conf.onComputerVision2013 ,December 2013.[15]P.Hanckmann,K.Schutte,andG.J.Burghouts.Automated textualdescriptionsforawiderangeofvideoeventswith48  humanactions.In ProceedingsoftheEuropeanConference onComputerVisionWorkshopsandDemonstrations ,pages 372Œ380,2012. [16]S.HochreiterandJ.Schmidhuber.Longshort-termmemory. NeuralComput. ,9(8):1735Œ1780,Nov.1997. [17]H.Jegou,F.Perronnin,M.Douze,J.S&#x00E1;nchez, P.Perez,andC.Schmid.Aggregatinglocalimagedescrip-  torsintocompactcodes. IEEETrans.PatternAnal.Mach. Intell.,34(9):1704Œ1716,Sept.2012. [18]N.KalchbrennerandP.Blunsom.Recurrentcontinuous translationmodels.In ConferenceonEmpiricalMethodsin NaturalLanguageProcessing ,pages1700Œ1709,2013. [19]A.Karpathy,G.Toderici,S.Shetty,T.Leung,R.Sukthankar, andL.Fei-Fei.Large-scalevideowithconvo-  lutionalneuralnetworks.In CVPR,2014. [20]M.U.G.Khan,L.Zhang,andY.Gotoh.Humanfocused videodescription.In ProceedingsoftheIEEEInternational ConferenceonComputerVisionWorkshops ,pages1480Œ 1487,2011. [21]M.U.G.Khan,L.Zhang,andY.Gotoh.Towardscoher- entnaturallanguagedescriptionofvideostreams.In Pro- ceedingsoftheIEEEInternationalConferenceonComputer  VisionWorkshops ,pages664Œ671,2011. [22]R.Kiros,R.Salakhutdinov,andR.S.Zemel.Unifying visual-semanticembeddingswithmultimodalneurallan-  guagemodels.In NIPSDeepLearningWorkshop ,2014. [23]A.Kojima,T.Tamura,andK.Fukunaga.Naturallanguage descriptionofhumanactivitiesfromvideoimagesbasedon  concepthierarchyofactions. InternationalJournalofCom- puterVision ,50(2):171Œ184,2002. [24]N.Krishnamoorthy,G.Malkarnenkar,R.J.Mooney, K.Saenko,andS.Guadarrama.Generatingnatural-language  videodescriptionsusingtext-minedknowledge.In AAAIConferenceonIntelligence ,pages541Œ547,2013. [25]Y.LeCun,L.Bottou,G.Orr,andK.M ¨uller.Efback- prop.In NeuralNetworks:TricksoftheTrade ,page546. 1998.[26]M.W.Lee,A.Hakeem,N.Haering,andS.-C.Zhu.SAVE:A frameworkforsemanticannotationofvisualevents.In Pro- ceedingsoftheIEEEConferenceonComputerVisionand  PatternRecognitionWorkshops ,pages1Œ8,2008. [27]J.Li,M.Luong,andD.Jurafsky.Ahierarchicalneuralau- toencoderforparagraphsanddocuments.In Proceedings oftheAnnualMeetingoftheAssociationforComputational  Linguistics,pages1106Œ1115,2015. [28]R.Lin,S.Liu,M.Yang,M.Li,M.Zhou,andS.Li.Hi- erarchicalrecurrentneuralnetworkfordocumentmodeling.  pages899Œ907.ConferenceonEmpiricalMethodsinNatu-  ralLanguageProcessing,Sept.2015. [29]J.Mao,W.Xu,Y.Yang,J.Wang,Z.Huang,andA.Yuille. Deepcaptioningwithmultimodalrecurrentneuralnetworks  (m-rnn).ICLR,2015. 4592 [30]J.Mao,W.Xu,Y.Yang,J.Wang,Z.Huang,andA.L.Yuille. Learninglikeachild:Fastnovelvisualconceptlearning  fromsentencedescriptionsofimages.2015. [31]V.NairandG.E.Hinton.linearunitsimprove restrictedboltzmannmachines.In ICML,pages807Œ814, 2010.[32]Y.Pan,T.Mei,T.Yao,H.Li,andY.Rui.Jointlymodel- ingembeddingandtranslationtobridgevideoandlanguage.  CoRR,abs/1505.01861,2015. [33]K.Papineni,S.Roukos,T.Ward,andW.jingZhu.Bleu:a methodforautomaticevaluationofmachinetranslation.In  ACL ,pages311Œ318,2002. [34]C.C.ParkandG.Kim.Expressinganimagestreamwitha sequenceofnaturalsentences.In AdvancesinNeuralInfor- mationProcessingSystems ,pages73Œ81,2015. [35]M.Ranzato,S.Chopra,M.Auli,andW.Zaremba.Se- quenceleveltrainingwithrecurrentneuralnetworks. CoRR,abs/1511.06732,2015. [36]A.Rohrbach,M.Rohrbach,W.Qiu,A.Friedrich,M.Pinkal, andB.Schiele.Coherentmulti-sentencevideodescription  withvariablelevelofdetail.In GermanConferenceonPat- ternRecognition(GCPR) ,September2014. [37]M.Rohrbach,W.Qiu,I.Titov,S.Thater,M.Pinkal,and B.Schiele.Translatingvideocontenttonaturallanguage  descriptions.In ProceedingsoftheIEEEInternationalCon- ferenceonComputerVision ,pages433Œ440,2013. [38]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh, S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,  A.C.Berg,andL.Fei-Fei.Imagenetlargescalevisualrecog-  nitionchallenge. InternationalJournalofComputerVision (IJCV),pages1Œ42,Apr.2015. [39]M.SchusterandK.Paliwal.Bidirectionalrecurrentneu- ralnetworks. IEEETransactionsonSignalProcessing ,45(11):2673Œ2681,Nov.1997. [40]K.SimonyanandA.Zisserman.Verydeepconvolutional networksforlarge-scaleimagerecognition.In InternationalConferenceonLearningRepresentations ,2014. [41]N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,and R.Salakhutdinov.Dropout:Asimplewaytopreventneu-  ralnetworksfromov JournalofMachineLearning Research ,15:1929Œ1958,2014. [42]C.SunandR.Nevatia.Semanticawarevideotranscription usingrandomforestIn ProceedingsoftheEuro- peanConferenceonComputerVision ,pages772Œ786,2014. [43]I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequence learningwithneuralnetworks.In AdvancesinNeuralInfor- mationProcessingSystems ,2014. [44]T.TielemanandG.Hinton.Lecture6.5-rmsprop:Divide thegradientbyarunningaverageofitsrecentmagnitude.  COURSERA:NeuralNetworksforMachineLearning ,4, 2012.[45]D.Tran,L.D.Bourdev,R.Fergus,L.Torresani,and M.Paluri.C3D:genericfeaturesforvideoanalysis.In Pro- ceedingsoftheIEEEInternationalConferenceonComputer  Vision ,2015. [46]R.Vedantam,C.L.Zitnick,andD.Parikh.Cider: Consensus-basedimagedescriptionevaluation.In Proceed- ingsoftheIEEEConferenceonComputerVisionandPattern  Recognition ,pages4566Œ4575,2015. [47]S.Venugopalan,M.Rohrbach,J.Donahue,R.J.Mooney, T.Darrell,andK.Saenko.Sequencetosequence-videoto  text.In ProceedingsoftheIEEEInternationalConference onComputerVision ,pages4534Œ4542,2015. [48]S.Venugopalan,H.Xu,J.Donahue,M.Rohrbach,R.J. Mooney,andK.Saenko.Translatingvideostonaturallan-  guageusingdeeprecurrentneuralnetworks.In Proceedings oftheNorthAmericanChapteroftheAssociationforCom-  putationalLinguistics ,pages1494Œ1504,2015. [49]O.VinyalsandQ.V.Le.Aneuralconversationalmodel.In ICMLDeepLearningWorkshop ,2015. [50]O.Vinyals,A.Toshev,S.Bengio,andD.Erhan.Showand tell:Aneuralimagecaptiongenerator.In Proceedingsofthe IEEEConferenceonComputerVisionandPatternRecogni-  tion,pages3156Œ3164,2015. [51]H.Wang,A.Kl ¨aser,C.Schmid,andC.-L.Liu.Action RecognitionbyDenseTrajectories.In IEEEConferenceon ComputerVision&PatternRecognition ,pages3169Œ3176, June2011. [52]T.Wen,M.Gasic,N.Mrksic,P.Su,D.Vandyke,andS.J. Young.Semanticallyconditionedlstm-basednaturallan-  guagegenerationforspokendialoguesystems.In Confer- enceonEmpiricalMethodsinNaturalLanguageProcessing ,2015.[53]P.Werbos.Backpropagationthroughtime:whatdoesitdo andhowtodoit.In ProceedingsofIEEE ,volume78,pages 1550Œ1560,1990. [54]H.Xu,S.Venugopalan,V.Ramanishka,M.Rohrbach,and K.Saenko.Amulti-scalemultipleinstancevideodescription  network. CoRR,abs/1505.05914,2015. [55]R.Xu,C.Xiong,W.Chen,andJ.J.Corso.Jointlymodel- ingdeepvideoandcompositionaltexttobridgevisionand  languageinaframework.In ProceedingsofAAAI ConferenceonIntelligence ,2015. [56]L.Yao,A.Torabi,K.Cho,N.Ballas,C.Pal,H.Larochelle, andA.Courville.Describingvideosbyexploitingtemporal  structure.In ProceedingsoftheIEEEInternationalConfer- enceonComputerVision ,pages4507Œ4515,2015. [57]H.YuandJ.M.Siskind.Learningtodescribevideowith weaksupervisionbyexploitingnegativesententialinforma-  tion.In AAAIConferenceonalIntelligence ,pages 3855Œ3863,Jan.2015. 4593  
Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) ,pages147Œ152, October25-29,2014,Doha,Qatar. c  2014AssociationforComputationalLinguistics 147 148 149 150 151 152  
TopicCompositionalNeuralLanguageModel WenlinWang 1 ZheGan 1 WenqiWang 3 DinghanShen 1 JiajiHuang 2 WeiPing 2 SanjeevSatheesh 2 LawrenceCarin 1 1 DukeUniversity 2 BaiduSiliconValleyAILab 3 PurdueUniversity Abstract WeproposeaTopicCompositionalNeural LanguageModel(TCNLM),anovelmethod designedtosimultaneouslycaptureboththe global semanticmeaningandthe local word- orderingstructureinadocument.TheTC- NLMlearnstheglobalsemanticcoherence ofadocumentviaaneuraltopicmodel, andtheprobabilityofeachlearnedlatent topicisfurtherusedtobuildaMixture-of- Experts(MoE)languagemodel,whereeach expert(correspondingtoonetopic)isare- currentneuralnetwork(RNN)thataccounts forlearningthelocalstructureofawordse- quence.InordertotraintheMoEmodel ciently,amatrixfactorizationmethodisap- plied,byextendingeachweightmatrixofthe RNNtobeanensembleoftopic-dependent weightmatrices.Thedegreetowhicheach memberoftheensembleisusedistiedtothe document-dependentprobabilityofthecor- respondingtopics.Experimentalresultson severalcorporashowthattheproposedap- proachoutperformsbothapureRNN-based modelandothertopic-guidedlanguagemod- els.Further,ourmodelyieldssensibletopics, andalsohasthecapacitytogeneratemean- ingfulsentencesconditionedongiventopics. 1Introduction Alanguagemodelisafundamentalcomponenttonat- urallanguageprocessing(NLP).Itplaysakeyrole inmanytraditionalNLPtasks,rangingfromspeech recognition(Mikolovetal.,2010;Arisoyetal.,2012; Srirametal.,2017),machinetranslation(Schwenk Proceedingsofthe21 st InternationalConferenceonAr- IntelligenceandStatistics(AISTATS)2018,Lan- zarote,Spain.PMLR:Volume84.Copyright2018bythe author(s). etal.,2012;Vaswanietal.,2013)toimagecaption- ing(Maoetal.,2014;Devlinetal.,2015).Training agoodlanguagemodeloftenimprovestheunderlying metricsoftheseapplications, e.g., worderrorratesfor speechrecognitionandBLEUscores(Papinenietal., 2002)formachinetranslation.Hence,learningapow- erfullanguagemodelhasbecomeacentraltaskin NLP.Typically,theprimarygoalofalanguagemodel istopredictdistributionsoverwords,whichhasto encodeboththesemanticknowledgeandgrammat- icalstructureinthedocuments.RNN-basedneural languagemodelshaveyieldedstate-of-the-artperfor- mance(Jozefowiczetal.,2016;Shazeeretal.,2017). However,theyaretypicallyappliedonlyatthesen- tencelevel,withoutaccesstothebroaddocumentcon- text.Suchmodelsmayconsequentlyfailtocapture long-termdependenciesofadocument(Diengetal., 2016). Fortunately,suchbroadercontextinformationisof asemanticnature,andcanbecapturedbyatopic model.Topicmodelshavebeenstudiedfordecades andhavebecomeapowerfultoolforextractinghigh- levelsemanticstructureofdocumentcollections,by inferringlatenttopics.TheclassicalLatentDirichlet Allocation(LDA)method(Bleietal.,2003)andits variants,includingrecentworkonneuraltopicmod- els(Wanetal.,2012;Caoetal.,2015;Miaoetal., 2017),havebeenusefulforaplethoraofapplications inNLP. Althoughlanguagemodelsthatleveragetopicshave shownpromise,theyalsohaveseverallimitations.For example,someoftheexistingmethodsuseonlypre- trainedtopicmodels(MikolovandZweig,2012),with- outconsideringtheword-sequencepredictiontaskof interest.Anotherkeylimitationoftheexistingmeth- odsliesintheintegrationofthelearnedtopicsintothe languagemodel; e.g. ,eitherthroughconcatenatingthe topicvectorasanadditionalfeatureofRNNs(Mikolov andZweig,2012;Lauetal.,2017),orre-scoringthe predicteddistributionoverwordsusingthetopicvec- tor(Diengetal.,2016).Theformerrequiresabal- ancebetweenthenumberofRNNhiddenunitsand arXiv:1712.09783v3  [cs.LG]  26 Feb 2018TopicCompositionalNeuralLanguageModel Figure1:Theoverallarchitectureoftheproposedmodel. thenumberoftopics,whilethelatterhastocarefully designthevocabularyofthetopicmodel. Motivatedbytheaforementionedgoalsandlimitations ofexistingapproaches,weproposetheTopicCompo- sitionalNeuralLanguageModel(TCNLM),anewap- proachtosimultaneouslylearnaneuraltopicmodel andaneurallanguagemodel.AsdepictedinFigure1, TCNLMlearnsthelatenttopicswithinavariational autoencoder(KingmaandWelling,2013)framework, andthedesignedlatentcode t quantheproba- bilityoftopicusagewithinadocument.Latentcode t isfurtherusedinaMixture-of-Expertsmodel(Hu etal.,1997),whereeachlatenttopichasacorrespond- inglanguagemodel(expert).Acombinationofthese \experts,"weightedbythetopic-usageprobabilities, resultsinourpredictionforthesentences.Ama- trixfactorizationapproachisfurtherutilizedtoreduce computationalcostaswellaspreventovThe entiremodelistrainedend-to-endbymaximizingthe variationallowerbound.Throughacomprehensive setofexperiments,wedemonstratethattheproposed modelisabletotlyreducetheperplexityof alanguagemodelandelyassemblethemean- ingoftopicstogeneratemeaningfulsentences.Both quantitativeandqualitativecomparisonsareprovided toverifythesuperiorityofourmodel. 2Preliminaries WereviewRNN-basedlanguagemodelsand traditionalprobabilistictopicmodels. LanguageModel Alanguagemodelaimstolearn aprobabilitydistributionoverasequenceofwordsin apre-denedvocabulary.Wedenote V asthevocab- ularysetand f y 1 ;:::;y M g tobeasequenceofwords, witheach y m 2V .Alanguagemodeldenesthelike- lihoodofthesequencethroughajointprobabilitydis- tribution p ( y 1 ;:::;y M )= p ( y 1 ) M Y m =2 p ( y m j y 1: m  1 ) : (1) RNN-basedlanguagemodelstheconditional probabiltiyofeachword y m givenalltheprevious words y 1: m  1 throughthehiddenstate h m : p ( y m j y 1: m  1 )= p ( y m j h m )(2) h m = f ( h m  1 ;x m ) : (3) Thefunction f (  )istypicallyimplementedasaba- sicRNNcell,aLongShort-TermMemory(LSTM) cell(HochreiterandSchmidhuber,1997),oraGated RecurrentUnit(GRU)cell(Choetal.,2014).The inputandoutputwordsarerelatedviatherelation x m = y m  1 . TopicModel Atopicmodelisaprobabilisticgraph- icalrepresentationforuncoveringtheunderlyingse- manticstructureofadocumentcollection.Latent DirichletAllocation(LDA)(Bleietal.,2003),forex- ample,providesarobustandscalableapproachfor documentmodeling,byintroducinglatentvariables foreachtoken,indicatingitstopicassignment.Specif- ically,let t denotethetopicproportionfordocument d ,and z n representthetopicassignmentforword w n . TheDirichletdistributionisemployedasthepriorof t .ThegenerativeprocessofLDAmaybesummarized as: t ˘ Dir(  0 ) ;z n ˘ Discrete( t ) ;w n ˘ Discrete(  z n ) ; where  z n representsthedistributionoverwordsfor topic z n ,  0 isthehyper-parameteroftheDirichlet prior, n 2 [1 ;N d ],and N d isthenumberofwordsin document d .Themarginallikelihoodfordocument d canbeexpressedas p ( d j  0 ;  )= Z t p ( t j  0 ) Y n X z n p ( w n j  z n ) p ( z n j t ) d t : 3TopicCompositionalNeural LanguageModel WedescribetheproposedTCNLM,asillustratedin Figure1.Ourmodelconsistsoftwokeycomponents: W.Wang,Z.Gan,W.Wang,D.Shen,J.Huang,W.Ping,S.Satheesh,L.Carin ( i )aneuraltopicmodel(NTM),and( ii )aneural languagemodel(NLM).TheNTMaimstocapturethe long-rangesemanticmeaningsacrossthedocument, whiletheNLMisdesignedtolearnthelocalsemantic andsyntacticrelationshipsbetweenwords. 3.1NeuralTopicModel Let d 2 Z D + denotethebag-of-wordsrepresentation ofadocument,with Z + denotingnonnegativeinte- gers. D isthevocabularysize,andeachelementof d acountofthenumberoftimesthecorre- spondingwordoccursinthedocument.Distinctfrom LDA(Bleietal.,2003),wepassa Gaussian random vectorthroughasoftmaxfunctiontoparameterizethe multinomialdocumenttopicdistributions(Miaoetal., 2017).Sp,thegenerativeprocessoftheNTM is  ˘N (  0 ;˙ 2 0 ) t = g (  ) z n ˘ Discrete( t ) w n ˘ Discrete(  z n ) ; (4) where N (  0 ;˙ 2 0 )isanisotropicGaussiandistribution, withmean  0 andvariance ˙ 2 0 ineachdimension; g (  )isatransformationfunctionthatmapssample  tothetopicembedding t ,hereas g (  )= softmax( ^ W  + ^ b ),where ^ W and ^ b aretrainablepa- rameters. Themarginallikelihoodfordocument d is: p ( d j  0 ;˙ 0 ;  )= Z t p ( t j  0 ;˙ 2 0 ) Y n X z n p ( w n j  z n ) p ( z n j t ) d t = Z t p ( t j  0 ;˙ 2 0 ) Y n p ( w n j  ; t ) d t = Z t p ( t j  0 ;˙ 2 0 ) p ( d j  ; t ) d t : (5) Thesecondequationin(5)holdsbecausewecanread- ilymarginalizedoutthesampledtopicwords z n by p ( w n j  ; t )= X z n p ( w n j  z n ) p ( z n j t )= t : (6)  = f  1 ;  2 ;:::;  T g isthetransitionmatrixfromthe topicdistributiontotheworddistribution,whichare trainableparametersofthedecoder; T isthenumber oftopicsand  i 2 R D isthetopicdistributionover words(allelementsof  i arenonnegative,andthey sumtoone). There-parameterizationtrick(KingmaandWelling, 2013)canbeappliedtobuildanunbiasedandlow- variancegradientestimatorforthevariationaldistri- bution.Theparameterupdatescanstillbederived directlyfromthevariationallowerbound,asdiscussed inSection3.3. DiversityRegularizer Redundanceininferred topicsisacommonissueexisitingingeneraltopic models.Inordertoaddressthisissue,itisstraightfor- wardtoregularizetherow-wisedistancebetweeneach pairedtopicstodiversifythetopics.FollowingXie etal.(2015);Miaoetal.(2017),weapplyatopicdi- versityregularizationwhilecarryingouttheinference. Sp,thedistancebetweenapairoftopics aremeasuredbytheircosinedistance a (  i ;  j )= arccos  j  i   j j jj  i jj 2 jj  j jj 2  .Themeanangleofallpairsof T topicsis ˚ = 1 T 2 P i P j a (  i ;  j ),andthevariance is  = 1 T 2 P i P j ( a (  i ;  j )  ˚ ) 2 .Finally,thetopic diversityregularizationisas R = ˚   . 3.2NeuralLanguageModel WeproposeaMixture-of-Experts(MoE)language model,whichconsistsasetof\expertnetworks", i.e. , E 1 ;E 2 ;:::;E T .EachexpertisitselfanRNNwithits ownparameterscorrespondingtoalatenttopic. Withoutlossofgenerality,webeginbydiscussingan RNNwithasimpletransitionfunction,whichisthen generalizedtotheLSTM.Sp,wetwo weighttensors W2 R n h  n x  T and U2 R n h  n h  T , where n h isthenumberofhiddenunitsand n x isthe dimensionofwordembedding.Eachexpert E k corre- spondstoasetofparameters W [ k ]and U [ k ],which denotesthe k -th2D\slice"of W and U ,respectively. All T expertsworkcooperativelytogenerateanout- put y m .Sep, p ( y m )= T X k =1 t k  softmax( V h ( k ) m )(7) h ( k ) m = ˙ ( W [ k ] x m + U [ k ] h m  1 ) ; (8) where t k istheusageoftopic k (component k of t ), and ˙ (  )isasigmoidfunction; V istheweightmatrix connectingtheRNN'shiddenstate,usedforcomput- ingadistributionoverwords.Biastermsareomitted forsimplicity. However,suchanMoEmoduleiscomputationallypro- hibitiveandstorageexcessive.Thetrainingprocessis tandeveninfeasibleinpractice.Toremedy this,insteadofensemblingtheoutputofthe T experts asin(7),weextendtheweightmatrixoftheRNNto beanensembleoftopic-dependentweightmatrices. Sp,the T expertsworktogetherasfollows: p ( y m )=softmax( V h m )(9) h m = ˙ ( W ( t ) x m + U ( t ) h m  1 ) ; (10) TopicCompositionalNeuralLanguageModel and W ( t )= T X k =1 t k W [ k ] ; U ( t )= T X k =1 t k U [ k ] : (11) Inordertoreducethenumberofmodelparameters, motivatedbyGanetal.(2016);Songetal.(2016), insteadofimplementingatensorasin(11),wede- compose W ( t )intoamultiplicationofthreeterms W a 2 R n h  n f , W b 2 R n f  T and W c 2 R n f  n x , where n f isthenumberoffactors.Sp, W ( t )= W a  diag( W b t )  W c = W a  ( W b t  W c ) ; (12) where  representstheHadamardoperator. W a and W c aresharedparametersacrossalltopics,tocapture thecommonlinguisticpatterns. W b arethefactors whichareweightedbythelearnedtopicembedding t .Thesamefactorizationisalsoappliedfor U ( t ). Thetopicdistribution t RNNparametersas- sociatedwiththedocumentwhenpredictingthesuc- ceedingwords,whichimplicitlyesanensemble of T languagemodels.Inthisfactorizedmodel,the RNNweightmatricesthatcorrespondtoeachtopic share\structure". NowwegeneralizetheaboveanalysisbyusingLSTM units.Sp,wesummarizethenewtopiccom- positionalLSTMcellas: i m = ˙ ( W ia ~ x i;m  1 + U ia ~ h i;m  1 ) f m = ˙ ( W fa ~ x f;m  1 + U fa ~ h f;m  1 ) o m = ˙ ( W oa ~ x o;m  1 + U oa ~ h o;m  1 ) ~ c m = ˙ ( W ca ~ x c;m  1 + U ca ~ h c;m  1 ) c m = i m  ~ c m + f m  c m  1 h m = o m  tanh( c m ) : (13) For  = i;f;o;c ,we ~ x  ;m  1 = W  b t  W  c x m  1 (14) ~ h  ;m  1 = U  b t  U  c h m  1 : (15) ComparedwithastandardLSTMcell,ourLSTMunit hasatotalnumberofparametersinsizeof4 n f  ( n x + 2 T +3 n h )andtheadditionalcomputationalcostcomes from(14)and(15).Further,empiricalcomparison hasbeenconductedinSection5.6toverifythatour proposedmodelissuperiorthanusingthenaiveMoE implementationasin(7). 3.3ModelInference Theproposedmodel(seeFigure1)followsthevaria- tionalautoencoder(KingmaandWelling,2013)frame- work,whichtakesthebag-of-wordsasinputandem- bedsadocumentintothetopicvector.Thisvectoris thenusedtoreconstructthebag-of-wordsinput,and alsotolearnanensembleofRNNsforpredictinga sequenceofwordsinthedocument. Thejointmarginallikelihoodcanbewrittenas: p ( y 1: M ; d j  0 ;˙ 2 0 ;  )= Z t p ( t j  0 ;˙ 2 0 ) p ( d j  ; t ) M Y m =1 p ( y m j y 1: m  1 ; t ) d t : (16) Sincethedirectoptimizationof(16)isintractable,we employvariationalinference(Jordanetal.,1999).We denote q ( t j d )tobethevariationaldistributionfor t . Hence,weconstructthevariationalobjectivefunction, alsocalledtheevidencelowerbound(ELBO),as L = E q ( t j d ) (log p ( d j t ))  KL  q ( t j d ) jj p ( t j  0 ;˙ 2 0 )  | {z } neuraltopicmodel + E q ( t j d )   M X m =1 log p ( y m j y 1: m  1 ; t ) ! | {z } neurallanguagemodel (17)  log p ( y 1: M ; d j  0 ;˙ 2 0 ;  ) : MoredetailscanbefoundintheSupplementaryMate- rial.Inexperiments,weoptimizetheELBOtogether withthediversityregularisation: J = L +   R: (18) 4RelatedWork TopicModel Topicmodelshavebeenstudiedfor avarietyofapplicationsindocumentmodeling.Be- yondLDA(Bleietal.,2003),textensions havebeenproposed,includingcapturingtopiccor- relations(Bleiandy,2007),modelingtempo- raldependencies(Bleiandy,2006),discover- inganunboundednumberoftopics(Tehetal.,2005), learningdeeparchitectures(Henaoetal.,2015;Zhou etal.,2015),amongmanyothers.Recently,neural topicmodelshaveattractedmuchattention,build- inguponthesuccessfulusageofrestrictedBoltzmann machines(HintonandSalakhutdinov,2009),auto- regressivemodels(LarochelleandLauly,2012),sig- moidbeliefnetworks(Ganetal.,2015),andvariational autoencoders(Miaoetal.,2016). Variationalinferencehasbeensuccessfullyappliedin avarietyofapplications(Puetal.,2016;Wangetal., 2017;Chenetal.,2017).TherecentworkofMiao etal.(2017)employsvariationalinferencetotrain topicmodels,andiscloselyrelatedtoourwork.Their modelfollowstheoriginalLDAformulationandex- tendsitbyparameterizingthemultinomialdistribu- tionwithneuralnetworks.Incontrast,ourmodel W.Wang,Z.Gan,W.Wang,D.Shen,J.Huang,W.Ping,S.Satheesh,L.Carin Dataset VocabularyTrainingDevelopmentTesting LMTM#Docs#Sents#Tokens#Docs#Sents#Tokens#Docs#Sents#Tokens APNEWS 32 ; 4007 ; 79050 K 0 : 7 M 15 M 2 K 27 : 4 K 0 : 6 M 2 K 26 : 3 K 0 : 6 M IMDB 34 ; 2568 ; 71375 K 0 : 9 M 20 M 12 : 5 K 0 : 2 M 0 : 3 M 12 : 5 K 0 : 2 M 0 : 3 M BNC 41 ; 3709 ; 74115 K 0 : 8 M 18 M 1 K 44 K 1 M 1 K 52 K 1 M Table1:Summarystatisticsforthedatasetsusedintheexperiments. enforcestheneuralnetworknotonlymodelingdoc- umentsasbag-of-words,butalsotransferingthein- ferredtopicknowledgetoalanguagemodelforword- sequencegeneration. LanguageModel Neurallanguagemodelshavere- centlyachievedremarkableadvances(Mikolovetal., 2010).TheRNN-basedlanguagemodel(RNNLM) issuperiorforitsabilitytomodellonger-termtem- poraldependencieswithoutimposingastrongcondi- tionalindependenceassumption;ithasrecentlybeen showntooutperformcarefully-tunedtraditionaln- gram-basedlanguagemodels(Jozefowiczetal.,2016). AnRNNLMcanbefurtherimprovedbyutilizingthe broaddocumentcontext(MikolovandZweig,2012). Suchmodelstypicallyextractlatenttopicsviaatopic model,andthensendthetopicvectortoalanguage modelforsentencegeneration.Importantworkin thisdirectionincludeMikolovandZweig(2012);Dieng etal.(2016);Lauetal.(2017);Ahnetal.(2016).The keyofthesemethodsisineitherthetopic modelitselforthemethodofintegratingthetopicvec- torintothelanguagemodel.Intermsofthetopic model,MikolovandZweig(2012)usesapre-trained LDAmodel;Diengetal.(2016)usesavariationalau- toencoder;Lauetal.(2017)introducesanattention- basedconvolutionalneuralnetworktoextractseman- tictopics;andAhnetal.(2016)utilizesthetopicas- sociatedtothefactpairsderivedfromaknowledge graph(VinyalsandLe,2015). Concerningthemethodofincorporatingthetopicvec- torintothelanguagemodel,MikolovandZweig(2012) andLauetal.(2017)extendtheRNNcellwithaddi- tionaltopicfeatures.Diengetal.(2016)andAhnetal. (2016)useahybridmodelcombiningthepredicted worddistributiongivenbybothatopicmodeland astandardRNNLM.Distinctfromtheseapproaches, ourmodellearnsthetopicmodelandthelanguage modeljointlyundertheVAEframework,allowingan tend-to-endtrainingprocess.Further,the topicinformationisusedasguidanceforaMixture- of-Experts(MoE)modeldesign.Underourfactoriza- tionmethod,themodelcanyieldboostedperformance tly(ascorroboratedintheexperiments). Recently,Shazeeretal.(2017)proposesaMoEmodel forlarge-scalelanguagemodeling.tfromours, theyintroduceaMoElayer,inwhicheachexpert standsforasmallfeed-forwardneuralnetworkonthe previousoutputoftheLSTMlayer.Therefore,it yieldsatquantityofadditionalparameters andcomputationalcost,whichisinfeasibletotrain onasingleGPUmachine.Moreover,theyprovide nosemanticmeaningsforeachexpert,andallexperts aretreatedequally;theproposedmodelcangenerate meaningfulsentencesconditionedongiventopics. OurTCNLMissimilartoGanetal.(2016).However, Ganetal.(2016)usesatwo-steppipline,learning amulti-labelonagroupofimage tags,andthengeneratingimagecaptionsconditioned onthem.Incomparison,ourmodeljointlylearnsa topicmodelandalanguagemodel,andfocusesonthe languagemodelingtask. 5Experiments Datasets Wepresentexperimentalresultsonthree publiclyavailablecorpora: APNEWS , IMDB and BNC . APNEWS 1 isacollectionofAssociatedPress newsarticlesfrom2009to2016. IMDB isasetof moviereviewscollectedbyMaasetal.(2011),and BNC (BNCConsortium,2007)isthewrittenpor- tionoftheBritishNationalCorpus,whichcontains excerptsfromjournals,books,letters,essays,mem- oranda,newsandothertypesoftext.Thesethree datasetscanbedownloadedfromGitHub 2 . WefollowthepreprocessingstepsinLauetal.(2017). Sp,wordsandsentencesaretokenizedusing StanfordCoreNLP(Manningetal.,2014).Welower- caseallwordtokens,andoutwordtokensthat occurlessthan10times.Fortopicmodeling,wead- ditionallyremovestopwords 3 inthedocumentsand excludethetop0 : 1%mostfrequentwordsandalso wordsthatappearinlessthan100documents.All thesedatasetsaredividedintotraining,development andtestingsets.Asummarystatisticofthesedatasets isprovidedinTable1. 1 https://www.ap.org/en-gb/ 2 https://github.com/jhlau/topically-driven-language- model 3 Weusethefollowingstopwordslist:https://github. com/mimno/Mallet/blob/master/stoplists/en.txt TopicCompositionalNeuralLanguageModel Dataset LSTM basic-LSTM  LDA+LSTM  LCLM  Topic-RNNTDLM  TCNLM type 50100150501001505010015050100150 APNEWS small64 : 1357 : 0555 : 5254 : 8354 : 1856 : 7754 : 5454 : 1253 : 0052 : 7552 : 6552 : 7552 : 63 52.59 large58 : 8952 : 7250 : 7550 : 1750 : 6353 : 1950 : 2450 : 0148 : 9648 : 9748 : 2148 : 0747 : 81 47.74 IMDB small72 : 1469 : 5869 : 6469 : 6267 : 7868 : 7467 : 8366 : 4563 : 6763 : 4563 : 8263 : 9862 : 64 62.59 large66 : 4763 : 4863 : 0462 : 7867 : 8663 : 0261 : 5960 : 1458 : 9959 : 0458 : 5957 : 0656 : 38 56.12 BNC small102 : 8996 : 4296 : 5096 : 3887 : 4794 : 6693 : 5793 : 5587 : 4285 : 9986 : 4387 : 9886 : 44 86.21 large94 : 2388 : 4287 : 7787 : 2880 : 6885 : 9084 : 6284 : 1282 : 6281 : 8380 : 5880 : 2980 : 14 80.12 Table2:Testperplexitiesoftmodelson APNEWS , IMDB and BNC .(  )takenfromLauetal.(2017). Setup FortheNTMpart,weconsidera2-layer feed-forwardneuralnetworktomodel q ( t j d ),with256 hiddenunitsineachlayer;ReLU(NairandHinton, 2010)isusedastheactivationfunction.Thehyper- parameter  forthediversityregularizerisedto0.1 acrossalltheexperiments.Allthesentencesinapara- graph,excludingtheonebeingpredicted,areusedto obtainthebag-of-wordsdocumentrepresentation d . Themaximumnumberofwordsinaparagraphisset to300. IntermsoftheNLMpart,weconsider2settings:( i ) asmall1-layerLSTMmodelwith600hiddenunits, and( ii )alarge2-layerLSTMmodelwith900hidden unitsineachlayer.Thesequencelengthisto30. Inordertoalleviateovdropoutwitharateof 0 : 4isusedineachLSTMlayer.Inaddition,adaptive softmax(Graveetal.,2016)isusedtospeedupthe trainingprocess. Duringtraining,theNTMandNLMparametersare jointlylearnedusingAdam(KingmaandBa,2014). Allthehyper-parametersaretunedbasedontheper- formanceonthedevelopmentset.Weempirically thattheoptimalsettingsarefairlyrobustacrossthe 3datasets.Alltheexperimentswereconductedusing TwandtrainedonNVIDIAGTXTITANX with3072coresand12GBglobalmemory. 5.1LanguageModelEvaluation Perplexityisusedasthemetrictoevaluatetheperfor- manceofthelanguagemodel.Inordertodemonstrate theadvantageoftheproposedmodel,wecompareTC- NLMwiththefollowingbaselines:  basic-LSTM :AbaselineLSTM-basedlanguage model,usingthesamearchitectureandhyper- parametersasTCNLMwhereverapplicable.  LDA+LSTM :Atopic-enrolledLSTM-based languagemodel.WepretrainanLDA model(Bleietal.,2003)tolearn50/100/150top- icsfor APNEWS , IMDB and BNC .Givenadoc- ument,theLDAtopicdistributionisincorporated byconcatenatingwiththeoutputofthehidden statestopredictthenextword.  LCLM (WangandCho,2016):Acontext-based languagemodel,whichincorporatescontextinfor- mationfromprecedingsentences.Thepreceding sentencesaretreatedasbag-of-words,andanat- tentionmechanismisusedwhenpredictingthe nextword.  TDLM (Lauetal.,2017):Aconvolutionaltopic modelenrolledlangugemodel.Itstopicknowl- edgeisutilizedbyconcatenatingtoadenselayer ofarecurrentlanguagemodel.  Topic-RNN (Diengetal.,2016):Ajointlearn- ingframeworkthatlearnsatopicmodelanda languagemodelsimutaneously.Thetopicinfor- mationisincorporatedthroughalineartransfor- mationtorescorethepredictionofthenextword. Topic-RNN(Diengetal.,2016)isimplementedbyour- selvesandothercomparisonsarecopiedfrom(Lau etal.,2017).ResultsarepresentedinTable2.We highlightsomeobservations.( i )Allthetopic-enrolled methodsoutperformthebasic-LSTMmodel,indicat- ingtheenessofincorporatingglobalseman- tictopicinformation.( ii )OurTCNLMperformsthe bestacrossalldatasets,andthetrendkeepsimprov- ingwiththeincreaseoftopicnumbers.( iii )Theim- provedperformanceofTCNLMoverLCLMimplies thatencodingthedocumentcontextintomeaningful topicsprovidesabetterwaytoimprovethelanguage modelcomparedwithusingtheextracontextwordsdi- rectly.( iv )ThemarginbetweenLDA+LSTM/Topic- RNNandourTCNLMindicatesthatourmodelsup- pliesamoretwaytoutilizethetopicinforma- tionthroughthejointvariationallearningframework toimplicitlytrainanensemblemodel. 5.2TopicModelEvaluation Weevaluatethetopicmodelbyinspectingtheco- herenceofinferredtopics(Changetal.,2009;New- manetal.,2010;Mimnoetal.,2011).FollowingLau etal.(2014),wecomputetopiccoherenceusingnor- malizedPMI(NPMI).Giventhetop n wordsofa topic,thecoherenceiscalculatedbasedonthesum ofpairwiseNPMIscoresbetweentopicwords,where thewordprobabilitiesusedintheNPMIcalculation arebasedonco-occurrencestatisticsminedfromEn- glishWikipediawithaslidingwindow.Inpractice, W.Wang,Z.Gan,W.Wang,D.Shen,J.Huang,W.Ping,S.Satheesh,L.Carin Datasetarmyanimalmedicalmarketlottoryterrorismlawarttransportationeducation APNEWS afghanistananimalspatientszackscasinosyrialawsuitalbumairlinesstudents veteransdogsdrugcentsmegairandamagesmusicfraudmath soldierszoofdaearningslotterymilitantsplaintisschemeschools brigadebeardiseasekeywordsgamblingal-qaidasongsconspiracyeducation infantrywildlifevirussharejackpotkoreasuitcomedyightsteachers IMDB horroractionfamilychildrenwardetectivenegativeethicepsiode zombiemartialramplingkidswareyrealienawfulgayseason slasherkungrelationshipsnoopycherochestergodzillaunfunnyschoolepisodes massacrelibinochesantadocumentarybooktarzansexgirlsseries chainsawchanmariecartoonmuslimsaustenplanetpoorwomencolumbo gorefumotherparentsjewsholmesaliensworstsexbatman BNC environmenteducationpoliticsbusinessfacilitiessportsartawardexpressioncrime pollutioncourseselectionscorpbedroomsgoalalbumjohneyepolice emissionstrainingeconomicturnoverhotelscorebandawardlookedmurder nuclearstudentsministerunixgardencupguitarresearchhairkilled wastemedaupoliticalnetsituatedballmusicdarlingtonlipsjury environmentaleducationdemocraticroomsseasonspeakerstaredtrail Table3:10topicslearnedfromourTCNLMon APNEWS , IMDB and BNC . #TopicModel Coherence APNEWSIMDBBNC 50 LDA  0.1250.0840.106 NTM  0.0750.0640.081 TDLM(s)  0.1490.1040.102 TDLM(l)  0.1300.0880.095 Topic-RNN(s)0.1340.1030.102 Topic-RNN(l)0.1270.0960.100 TCNLM(s) 0.1590.1060.114 TCNLM(l)0 : 1520 : 1000 : 101 100 LDA  0.1360.092 0.119 NTM  0.0850.0710.070 TDLM(s)  0.1520.0870.106 TDLM(l)  0.1420.0970.101 Topic-RNN(s)0.1580.0960.108 Topic-RNN(l)0.1430.0930.105 TCNLM(s) 0.1600.101 0 : 111 TCNLM(l)0 : 1520 : 0980 : 104 150 LDA  0.1340.094 0.119 NTM  0.0780.0750.072 TDLM(s)  0.1470.0850.100 TDLM(l)  0.1450.0910.104 Topic-RNN(s)0.1460.0890.102 Topic-RNN(l)0.1370.0920.097 TCNLM(s)0 : 153 0.096 0 : 107 TCNLM(l) 0.155 0 : 0930 : 102 Table4:Topiccoherencescoresofderentmodels on APNEWS , IMDB and BNC .(s)and(l)indicate smallandlargemodel,respectively.(  )takenfromLau etal.(2017). weaveragetopiccoherenceoverthetop5 = 10 = 15 = 20 topicwords.Toaggregatetopiccoherencescorefor atrainedmodel,wethenfurtheraveragethecoher- encescoresovertopics.Forcomparison,weusethe followingbaselinetopicmodels:  LDA :LDA(Bleietal.,2003)isusedasabase- linetopicmodel.WeuseLDAtolearnthetopic distributionsforLDA+LSTM.  NTM :Weevaluatetheneuraltopicmodelpro- posedinCaoetal.(2015).Thedocument-topic andtopic-wordsmultinomialsareexpressedusing neuralnetworks.N-gramsembeddingsareincor- poratedasinputsofthemodel.  TDLM (Lauetal.,2017):Thesamemodelas usedinthelanguagemodelevaluation.  Topic-RNN (Diengetal.,2016):Thesame modelasusedinthelanguagemodelevaluation. ResultsaresummarizedinTable4.OurTCNLM achievespromisingresults.Sp,( i )weachieve thebestcoherenceperformanceover APNEWS and IMDB ,andarerelativelycompetitivewithLDAon BNC .( ii )Wealsoobservethatalargermodelmay resultinaslightlyworsecoherenceperformance.One possibleexplanationisthatalargerlanguagemodel mayhavemoreimpactonthetopicmodel,andthein- heritedstrongersequentialinformationmaybeharm- fultothecoherencemeasurement.( iii )Additionally, theadvantageofourTCNLMoverTopic-RNNindi- catesthatourTCNLMsuppliesamorepowerfultopic guidance. Inordertobetterunderstandthetopicmodel,wepro- videthetop5wordsfor10randomlychosentopics oneachdataset(theboldfacewordisthetopicname summarizedbyus),asshowninTable3.Theseresults correspondtothesmallnetworkwith100neurons.We alsopresentsomeinferredtopicdistributionsforsev- eraldocumentsfromourTCNLMinFigure2.The topicusageforaspdocumentissparse,demon- stratingtheenessofourNTM.Moreinferred topicdistributionexamplesareprovidedintheSup- plementaryMaterial. 5.3SentenceGeneration AnotheradvantageofourTCNLMisitscapacityto generatemeaningfulsentencesconditionedongiven topics.Giventopic i ,weconstructanLSTMgenera- torbyusingonlythe i -thfactorof W b and U b .Then westartfromazerohiddenstate,andgreedilysample wordsuntilanendtokenoccurs.Table5showsthe generatedsentencesfromourTCNLMlearnedwith 50topicsusingthesmallnetwork.Mostofthesen- tencesarestronglycorrelatedwiththegiventopics. Moreinterestingly,wecanalsogeneratereasonable sentencesconditionedonamixedcombinationoftop- ics,evenifthetopicpairsaredivergent, e.g. ,\an- imal"and\lottory"for APNEWS .Moreexamples areprovidedintheSupplementaryMaterial.Itshows TopicCompositionalNeuralLanguageModel DataTopicGeneratedSentences APNEWS army  afemalesergeant,servinginthefortworth,hasservedassheservedinthemilitaryiniraq. animal  mostofthebearwillhavestumbledtothelake. medical  physiciansseekinghelpinutahandthenihhashadanysolutionstousingthepolicyandusestobewithatestingorbody. market  thecompanysaiditexpectsrevenueof$ < unk > millionto$ < unk > millioninthethirdquarter. lottory  wherethewinningnumbersdrawnupforamegaballwassold. army+terrorism  thetaliban'spresencehasearnedadegreefromthe1950-53koreanwarinpakistan'shistoriclifesince1964,withtwoexampleof < unk > soldiersfromwoundediraqiarmyshootingsandbahrainintheeasternarmy. animal+lottory  shetoldthenewspaperthatshewasconcernedthatthebuyerwasinaneighborhoodlastyearandhadagraywolf. IMDB horror  thekillerisaguywhoisn'tevenazombie. action  theactionisabittoomuch,buttheactionisn'tverygood. family  theisalsothestoryofayoungwomanwhose < unk > and < unk > andveryyetultimatelysympathetic, < unk > relationship, < unk > , andpalestinebeingequal,andtheoldman,a < unk > . children  iconsiderthismovietobeachildren'sforkids. war  thedocumentaryisadocumentaryaboutthewarandthe < unk > ofthewar. horror+negative  ifthismoviewasindeedahorriblemovieithinkiwillbebetterthe. hildren  paulthinkshimhastomakeupwhenthe < unk > eugenediscoversdefeatinordertotaketoomuchtimewithoutresortingtomortalbugs, andthenndshiswifeandboys. BNC environment  environmentalistsimmediatebasecallstodefendtheworld. education  theschoolhasrecentlybeenfoundedbya < unk > ofthenextgenerationfortwoyears. politics  aneweconomyinwhichprivatizationwasannouncedonjuly4. business  netearningspersharerose < unk > %to$ < unk > inthequarter,and$ < unk > m,onturnoverthatrose < unk > %to$ < unk > m. facilities  allroomshaveexcellentamenities. environment+politics  thecommission'sreportonoct.2,1990,onjan.7deniedthegovernment'sgrantto"thenationallevelofwater". art+crime  aswellas36,heisreturningfreelanceintotheredarmyofdramawherehehasbeenstruckfortheirpremiere. Table5:Generatedsentencesfromgiventopics.MoreexamplesareprovidedintheSupplementaryMaterial. Figure2:Inferredtopicdistributionsononesampledocumentineachdataset.Contentofthethreedocuments isprovidedintheSupplementaryMateiral. thatourTCNLMisabletogeneratetopic-relatedsen- tences,providinganinterpretablewaytounderstand thetopicmodelandthelanguagemodelsimulaneously. Thesequalitativeanalysisfurtherdemonstratethat ourmodelectivelyassemblesthemeaningoftopics togeneratesentences. 5.4EmpiricalComparisonwithNaiveMoE WeexploretheusageofanaiveMoElanguagemodel asin(7).InordertothemodelonasingleGPUma- chine,wetrainaNTMwith30topicsandeachNLMof theMoEisa1-layerLSTMwith100hiddenunits.Re- sultsaresummarizedinTable6.BoththenaiveMoE andourTCNLMprovidebetterperformancethanthe basicLSTM.Interestingly,thoughrequiringlesscom- putationalcostandstorageusage,ourTCNLMout- performsthenaiveMoEbyanon-trivialmargin.We attributethisboostedperformancetothe\structure" designofourmatrixfactorizationmethod.Theinher- enttopic-guidedfactorcontroltlyprevents ovandyieldsttraining,demonstrating theadvantageofourmodelfortransferringsemantic knowledgelearnedfromthetopicmodeltothelan- guagemodel. Datasetbasic-LSTMnaiveMoETCNLM APNEWS 101.6285.87 82.67 IMDB 105.2996.16 94.64 BNC 146.50130.01 125.09 Table6:Testperplexitycomparisonbetweenthenaive MoEimplementationandourTCNLMon APNEWS , IMDB and BNC . 6Conclusion WehavepresentedTopicCompositionalNeuralLan- guageModel(TCNLM),anewmethodtolearnatopic modelandalanguagemodelsimultaneously.The topicmodelpartcapturestheglobalsemanticmeaning inadocument,whilethelanguagemodelpartlearns thelocalsemanticandsyntacticrelationshipsbetween words.Theinferredtopicinformationisincorporated intothelanguagemodelthroughaMixture-of-Experts modeldesign.Experimentsconductedonthreecor- poravalidatethesuperiorityoftheproposedapproach. Further,ourmodelinferssensibletopics,andhasthe capacitytogeneratemeaningfulsentencesconditioned ongiventopics.Onepossiblefuturedirectionistoex- tendtheTCNLMtoaconditionalmodelandapplyit forthemachinetranslationtask. W.Wang,Z.Gan,W.Wang,D.Shen,J.Huang,W.Ping,S.Satheesh,L.Carin References S.Ahn,H.Choi,T.arnamaa,andY.Bengio.A neuralknowledgelanguagemodel. arXivpreprint arXiv:1608.00318 ,2016. E.Arisoy,T.N.Sainath,B.Kingsbury,andB.Ram- abhadran.Deepneuralnetworklanguagemodels. In NAACL-HLTWorkshop ,2012. D.M.BleiandJ.D.y.Dynamictopicmodels. In ICML ,2006. D.M.BleiandJ.D.y.Acorrelatedtopicmodel ofscience. TheAnnalsofAppliedStatistics ,2007. D.M.Blei,A.Y.Ng,andM.I.Jordan.Latentdirich- letallocation. JMLR ,2003. B.BNCConsortium.Thebritishnationalcor- pus,version3(bncxmledition). Dis- tributedbyBodleianLibraries,Universityof Oxford,onbehalfoftheBNCConsortium. URL:http://www.natcorp.ox.ac.uk/ ,2007. Z.Cao,S.Li,Y.Liu,W.Li,andH.Ji.Anovelneural topicmodelanditssupervisedextension.In AAAI , 2015. J.Chang,S.Gerrish,C.Wang,J.L.Boyd-Graber, andD.M.Blei.Readingtealeaves:Howhumans interprettopicmodels.In NIPS ,2009. C.Chen,C.Li,L.Chen,W.Wang,Y.Pu,and L.Carin.Continuous-timewsfordeepgenerative models. arXivpreprintarXiv:1709.01179 ,2017. K.Cho,B.VanMerrienboer,C.Gulcehre,D.Bah- danau,F.Bougares,H.Schwenk,andY.Ben- gio.LearningphraserepresentationsusingRNN encoder-decoderforstatisticalmachinetranslation. In EMNLP ,2014. J.Devlin,H.Cheng,H.Fang,S.Gupta,L.Deng, X.He,G.Zweig,andM.Mitchell.Languagemodels forimagecaptioning:Thequirksandwhatworks. arXivpreprintarXiv:1505.01809 ,2015. A.B.Dieng,C.Wang,J.Gao,andJ.Pais- ley.Topicrnn:Arecurrentneuralnetworkwith long-rangesemanticdependency. arXivpreprint arXiv:1611.01702 ,2016. Z.Gan,C.Chen,R.Henao,D.Carlson,andL.Carin. Scalabledeeppoissonfactoranalysisfortopicmod- eling.In ICML ,2015. Z.Gan,C.Gan,X.He,Y.Pu,K.Tran,J.Gao, L.Carin,andL.Deng.Semanticcompositional networksforvisualcaptioning. arXivpreprint arXiv:1611.08002 ,2016.  E.Grave,A.Joulin,M.Cisse,D.Grangier,and H.Jegou.tsoftmaxapproximationforgpus. arXivpreprintarXiv:1609.04309 ,2016. R.Henao,Z.Gan,J.Lu,andL.Carin.Deeppoisson factormodeling.In NIPS ,2015. G.E.HintonandR.R.Salakhutdinov.Replicated softmax:anundirectedtopicmodel.In NIPS ,2009. S.HochreiterandJ.Schmidhuber.Longshort-term memory.In Neuralcomputation ,1997. Y.H.Hu,S.Palreddy,andW.J.Tompkins.Apatient- adaptableecgbeatusingamixtureofex- pertsapproach. IEEEtransactionsonbiomedical engineering ,1997. M.I.Jordan,Z.Ghahramani,T.S.Jaakkola,and L.K.Saul.Anintroductiontovariationalmethods forgraphicalmodels. Machinelearning ,1999. R.Jozefowicz,O.Vinyals,M.Schuster,N.Shazeer, andY.Wu.Exploringthelimitsoflanguagemod- eling. arXivpreprintarXiv:1602.02410 ,2016. D.KingmaandJ.Ba.Adam:Amethodforstochastic optimization. arXivpreprintarXiv:1412.6980 ,2014. D.P.KingmaandM.Welling.Auto-encodingvaria- tionalbayes. arXivpreprintarXiv:1312.6114 ,2013. H.LarochelleandS.Lauly.Aneuralautoregressive topicmodel.In NIPS ,2012. J.H.Lau,D.Newman,andT.Baldwin.Machine readingtealeaves:Automaticallyevaluatingtopic coherenceandtopicmodelquality.In EACL ,2014. J.H.Lau,T.Baldwin,andT.Cohn.Topically drivenneurallanguagemodel. arXivpreprint arXiv:1704.08012 ,2017. A.L.Maas,R.E.Daly,P.T.Pham,D.Huang,A.Y. Ng,andC.Potts.Learningwordvectorsforsenti- mentanalysis.In ACL ,2011. C.D.Manning,M.Surdeanu,J.Bauer,J.R.Finkel, S.Bethard,andD.McClosky.Thestanfordcorenlp naturallanguageprocessingtoolkit.In ACL ,2014. J.Mao,W.Xu,Y.Yang,J.Wang,Z.Huang,and A.Yuille.Deepcaptioningwithmultimodalre- currentneuralnetworks(m-rnn). arXivpreprint arXiv:1412.6632 ,2014. Y.Miao,L.Yu,andP.Blunsom.Neuralvariational inferencefortextprocessing.In ICML ,2016. Y.Miao,E.Grefenstette,andP.Blunsom.Discov- eringdiscretelatenttopicswithneuralvariational inference. arXivpreprintarXiv:1706.00359 ,2017. T.MikolovandG.Zweig.Contextdependentrecur- rentneuralnetworklanguagemodel. SLT ,2012. T.Mikolov,M.at,L.Burget,J.Cernocky,and S.Khudanpur.Recurrentneuralnetworkbasedlan- guagemodel.In Interspeech ,2010. D.Mimno,H.M.Wallach,E.Talley,M.Leenders, andA.McCallum.Optimizingsemanticcoherence intopicmodels.In EMNLP ,2011. TopicCompositionalNeuralLanguageModel V.NairandG.E.Hinton.linearunitsim- proverestrictedboltzmannmachines.In ICML , 2010. D.Newman,J.H.Lau,K.Grieser,andT.Bald- win.Automaticevaluationoftopiccoherence.In NAACL ,2010. K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu. Bleu:amethodforautomaticevaluationofmachine translation.In ACL ,2002. Y.Pu,Z.Gan,R.Henao,X.Yuan,C.Li,A.Stevens, andL.Carin.Variationalautoencoderfordeep learningofimages,labelsandcaptions.In NIPS , 2016. H.Schwenk,A.Rousseau,andM.Attik.Large,pruned orcontinuousspacelanguagemodelsonagpufor statisticalmachinetranslation.In NAACL-HLT Workshop ,2012. N.Shazeer,A.Mirhoseini,K.Maziarz,A.Davis, Q.Le,G.Hinton,andJ.Dean.Outrageously largeneuralnetworks:Thesparsely-gatedmixture- of-expertslayer. arXivpreprintarXiv:1701.06538 , 2017. J.Song,Z.Gan,andL.Carin.Factoredtemporal sigmoidbeliefnetworksforsequencelearning.In ICML ,2016. A.Sriram,H.Jun,S.Satheesh,andA.Coates.Cold fusion:Trainingseq2seqmodelstogetherwithlan- guagemodels. arXivpreprintarXiv:1708.06426 , 2017. Y.W.Teh,M.I.Jordan,M.J.Beal,andD.M.Blei. Sharingclustersamongrelatedgroups:Hierarchical dirichletprocesses.In NIPS ,2005. A.Vaswani,Y.Zhao,V.Fossum,andD.Chiang.De- codingwithlarge-scaleneurallanguagemodelsim- provestranslation.In EMNLP ,2013. O.VinyalsandQ.Le.Aneuralconversationalmodel. arXivpreprintarXiv:1506.05869 ,2015. L.Wan,L.Zhu,andR.Fergus.Ahybridneural network-latenttopicmodel.In AISTAT ,2012. T.WangandH.Cho.Larger-contextlanguagemod- ellingwithrecurrentneuralnetwork. ACL ,2016. W.Wang,Y.Pu,V.K.Verma,K.Fan,Y.Zhang, C.Chen,P.Rai,andL.Carin.Zero-shotlearning viaclass-conditioneddeepgenerativemodels. arXiv preprintarXiv:1711.05820 ,2017. P.Xie,Y.Deng,andE.Xing.Diversifyingrestricted boltzmannmachinefordocumentmodeling.In KDD ,2015. M.Zhou,Y.Cong,andB.Chen.Thepoissongamma beliefnetwork.In NIPS ,2015. W.Wang,Z.Gan,W.Wang,D.Shen,J.Huang,W.Ping,S.Satheesh,L.Carin SupplementaryMaterialfor: TopicCompositionalNeuralLanguageModel ADetailedmodelinference Weprovidethedetailedderivationforthemodelin- ference.Startfrom(16),wehave log p ( y 1: M ; d j  0 ;˙ 2 0 ;  ) =log Z t p ( t ) q ( t j d ) q ( t j d ) p ( d j t ) M Y m =1 p ( y m j y 1: m  1 ; t ) d t =log E q ( t j d )   p ( t ) q ( t j d ) p ( d j t ) M Y m =1 p ( y m j y 1: m  1 ; t ) !  E q ( t j d )   log p ( d j t )  log q ( t j d ) p ( t ) + M X m =1 log p ( y m j y 1: m  1 ; t ) ! = E q ( t j d ) (log p ( d j t ))  KL  q ( t j d ) jj p ( t j  0 ;˙ 2 0 )  | {z } neuraltopicmodel + E q ( t j d )   M X n =1 log p ( y m j y 1: m  1 ; t ) ! | {z } neurallanguagemodel : BDocumentsusedtoinfertopic distributions Thedocumentsusedtoinferthetopicdistributions plotedinFigure2areprovidedbelow. Apnews : colombia'spolicedirectorsayssixpolice ershavebeenkilledandaseventhwoundedinam- bushinaruralsouthwesternareawhereleftistrebels operate.gen.joserobertoleontellstheassociated pressthattheerswereridingonfourmotorcycles whentheywereattackedwithemondayafter- noononaruralstretchofhighwayinthecaucastate townofpadilla.hesaidafrontoftherevolutionary armedforcesofcolombia,orfarc,operatesinthearea .ifthefarcisresponsible,thedeathswouldbringto 15thenumberofsecurityforcememberskilledsince thegovernmentandrebelsformallyopenedpeacetalks innorwayonoct.18.thetalkstoendanearly decade-oldcaresettobegininearnestincuba onnov.15. IMDB : havingjustwatchedthismovieforasecond time,someyearsaftermyinitialviewing,myfeel- ingsremainunchanged.thisisasoliddrama thatienjoyverymuch.whatelementsthereare ,areprimarilyofaddedinterestratherthanthemain substanceofthe.whatthismovieisreallyabout iswartimecict,butinasetting.ithasa solidcast,fromtheeverreliabledavidwarnertothe upandcomingfreddieprinzejr,alsoincludingmany britishtvregulars(thatobviouslyaddatouchofclass :),notforgettingthesuperbtchekykaryo.ifeelthis ismoreofanensemblepiecethanastarringvehicle .reminiscentofwwiibasedaroundsubmarine combatandair-combat(theseemlikeadapta- tionsofwwiicorsairsintheirdesign,evokingaretro feel)thisisoneoffewamericanthatifeltwas notoverwhelmedbysentimentorsaccharine.thesets andspecialctsareallwelldone,neverdetracting formthebelievabilityofthestory,althoughthekil- rathithemselvesareratherunderdevelopedandone dimensional.thisisamoreabouthumanityin cratherthanaaboutexploringanewand originalalienraceorhigh-browconcepts.forget thatit's,justwatchandenjoy. BNC : anarmyandcivilianexercisewentaheadin secretyesterdayacasualtyofthegeneralelection. thesimulateddisasterinexercisegryphon'sliftwasa midaircollisionbetweenmilitaryandcivilianplanes overcatterickgarrison.hamishlumsden,themin- istryofdefence'schiefpresserwhoarrivedfrom london,said:'there'sanabsolutebanonproactive prduringanelection.'journalistswhoreportedto gazabarracksat7.15amasinstructedweretoldthey wouldnotbeallowedtowitnesstheexercise,which involved24airmobilebrigade,northyorkshirepolice ,eandambulanceservices,thecountyemergency planningdepartmentand'casualties'fromlonglands college,middlesbrough.theaimofthegryphonlift wastotestarmysupportforcivilemergencies.brief detailssuppliedtothepressoutlinedthedisaster.a fullyloadedcivilianplanecrashesinmid-airwithan armedmilitaryplaneovercatterickgarrison.the1st battalionthegreenhowardsandabombdisposalsquad cordonandclearanareascatteredwitharmaments. 24airmobileambulance,whichservedinthegulf war,tendsaburning,packedyouthhostelhitby piecesofaircraft.38engineerregimentfromclaro barracks,ripon,searchalakewherealightaircraft crashedwhenhitbywreckage.civilianemer- gencyservices,includingthepoliceunderwaterteam ,wereduetoworkalongsidemilitarypersonnelunder theoverallco-ordinationofthepolice.mrlumsden said:'thereisaveryverystrictrulethatduringa generalelectionnothingthatthegovernmentdoescan intrudeontheelectionprocess.' TopicCompositionalNeuralLanguageModel Figure3:Inferredtopicdistributionsforthe5documentsinthedevelopmentsetovereachdataset. CMoreinferredtopicdistribution examples Wepresenttheinferredtopicdistributionsforthe 5documentsinthedevelopmentsetovereachdataset inFigure3. DMoregeneratedsentences Wepresentgeneratedsentencesusingthetopicslisted inTable3foreachdataset.Thegeneratedsentences forasingletopicareprovidedinTable7,8,9;the generatedsentencesforamixedcombinationoftopics areprovidedinTable10. W.Wang,Z.Gan,W.Wang,D.Shen,J.Huang,W.Ping,S.Satheesh,L.Carin TopicGeneratedSentences army  afemalesergeant,servinginthefortworth,hasservedassheservedinthemilitaryiniraq.  obamasaidtheobamaadministrationisseekingthestate'sexpectedendorsementofafamilybyafghansoldiersatthemilitary inworldwarii,whoselivesatthebaseofkandahar.  thevfwannouncedresultsontheu.s.andatotalof$5milliononthe,buthe'sstillrunningforthedemocraticnomination forthesenate. animal  mostofthebearwillhavestumbledtothelake.  feralhorsestakesauniquemixtoforagefortheirpetsandisbirthtohumans.  thezoohasestimatedsuchalossofhalfincaptivity,whichissoakedinayear. medical  physiciansseekinghelpinutahandthenihhashadanysolutionstousingthepolicyandusestobewithatestingorbody.  thattriggersmondayforthestudywerefoundbehindalistofbreastcancertreatmentuntilthestudy,asdoesinnationwide,has60days tosleepthere.  rightstronger,includingthevirusisinonetypeofdrugnowcalledintheclinicalformofradiation. market  thecompanysaiditexpectsrevenueof$ < unk > millionto$ < unk > millioninthethirdquarter.  biglarisaidoutsideparliamentdistrictofjanuary,up$4.30to100centspershare,thelastsummerofitsyearto$2.  fouranalystssurveyedbyzacksexpected$ < unk > billion. lottory  thenumbersdrawnfridaynightwere < unk > .  wherethewinningnumbersdrawnupforamegaballwassold.  thejackpotisexpectedtobeinjuly. terrorism  therussianhavepreviouslysaidtheseniorpresidentmadenothreats.  obamabeganhaltingcontrolofthetalksfridayandlastyearinanotherroundofthepeacetalksafterthenorth'sartilleryattackthere wednesdayhave < unk > theirhighestdebateoverhisculturalweaponssoon.  theturkishregimeisusingmilitantsintomercenariesabroadtotakeongateswasbythewestandeastjerusaleminrecentyears law  theeeoclawsuitsaysit'sentitledtoworktimeforrepeatedreportingproblemsthatwouldkickanodoncheapsteelfromtheowner  thestateallowedmarathontoemployment,andthencaahasabrokenrecordofsaleandfor$ < unk > foracheck  thetaxpayersinthelawsuitwerelegallyaliveandmarch < unk > ,orpastatimpropertimesoflosalamos art  quentintarantino'sannouncementsthatreceivedthemovie < unk > spanish  cathyjohnson,jane'sshort-livedsingerstevedeanand"thebroadwaymusicmusicalshow,"theearlyshow,"addsclassics, < unk > or5,500,whilerestaurantshavepickedother < unk > next  katiesaid,he'snevercreatedthedramaseries:themoviescoulddropsintohismusicloungeandknifeawayinalongforgottengown transportation  youngandbernardwouldpaymorethan$11millioninbanks,theairlinesaidannouncedbythe < unk >  thefraudcaseincludedadelta'sformerbusinesstravelbusinesswhosefakecards"ledtothescheme,"andtohavebeenmore than$10,000.  aformeru.s.attorney'scitedinafraudschemeinvolvingtwoengines,includingminingcompaniesledtothegovernmentfrom thegovernment. education  thestate's < unk > schoolboardofeducationisnota < unk > .  assemblymember < unk > ,charterschoolschairmanwhowereborninnewyorkwhomarrieddistrictsmakingmorethanlifelongeducation playtheissue,tellsthesamestorythatthey'llbeabletosupportthelegislation.  thestate'sleadingschoolofgranthasaddedthecurrentschoolsto < unk > studentsina < unk > classandripleyaimstoserve child < unk > andsocialsciencesareasintoinmayandthelatestresources Table7:Moregeneratedsentencesusingtopicslearnedfrom APNEWS . TopicGeneratedSentences horror  thekillerisaguywhoisn'tevenazombie.  thecannibalswanttoonescene,thegirlhassomethingoutoftheheadandchoppingaconcertbyasharkinthehead,andheheadsinthe showerwhilesomebodyissentto < unk > .  abunchofteenageslashertypesaretreatedtoagirlwhogetsmurderedbyagroupoffriends. action  theactionisabittoomuch,buttheactionisn'tverygood.  somereallymaybethatthewarscenewasatrademark < unk > whentingsequenceswereusedbymodernkungfu'srubbish.  actionpackedneatly,afairamountofgunplayandscienceactsasalegacytoacrossbetween80sand,greatgunplayandscenery. family  theisalsothestoryofayoungwomanwhose < unk > and < unk > andveryyetultimatelysympathetic, < unk > relationship, < unk > . ,andpalestinebeingequal,andtheoldman,a < unk > .  catherineseeksworkandhershareofeachother,a < unk > desire,andsubmittoher,buthedoesnotreallywanttorethinkherissues, orwhereheabortedhismother'squestto < unk > .  theni'maboutthelove,butafterafamilymeeting,herfriendaditya(tatum < unk > )marriesa16yearoldgirl,willbeableto understandtheamountofherboyfriendanytime children  snoopystartstolearnwhatwedobuti'mstillbringingupinthere.  iconsiderthismovietobeachildren'sforkids.  myfavoritechildhoodisatouchthatdepictsherhowthemotherwaswhattheyapparentlywould'vebroughtittotherightplacetofox. war  thedocumentaryisadocumentaryaboutthewarandthe < unk > ofthewar.  oneofthemajorfailingsofthewaristhatthegermansalsostruggletooverthrowthedeathofthemuslimsandthenaziregime,and < unk > .  thegoes,farastothepolitical,butthenewsthatwillbe < unk > athowthesepeoplecanbereducedtoarescue. detective  hopefullythat'sstarting < unk > ashalfofrochestertakesthecharacterinjane'sway,thoughholmesmanagedtomaketyronepower perfectedalotofmagicalstu,playingtheonewithhamlettoday.  whilethewasbasedonthestageadaptation,iknowshelookeduptosuspectfromtheentireproduction.  therewasnopreviousversioninmybookisaw,onlytothosethatreadthenovel,andithoughtthatnopartthathewastowhyarefar moreprofessional.   themonsterismuchbetterthanthealieninwhichthe < unk > wasrequiredfornearlyeverymomentofthe.  weretheastronautsfeellikeenoughtochallengethespacegodzilla,whereitprevails  buttheadventurethatwillarisefromtheearthishavingamonsterthatcanmakeitclearthatthealiensarenotwearingthe < unk > allthat will < unk > thelaser. negative  themoviereinforcesmytokenbadratings-it'stheworstmovieihaveeverseen.  itwasprettybad,butasidefromashowtothe2idiotsintheircastmembers,i'mpsychotic.  wehadthegarbageusingpeckinpah'smovieswithsomany < unk > ,icannotrecommendthistoanyoneelse. ethic  englundearlierinasupportingrole,aclosetedgaygalreporterwhoapparentlyhopestodisgracethegirlsinbeingsexual.  thisisjustplainstupidandinsane,andalittlebitofcheesy.  theiswellmadeduringitsturbulent,exquisite,warmandsinisterjoys,whileaglimpseofteenrelationships. episode  3episodesasa < unk > won3emmyseries!  irememberthesopranosontv,early80's,(andinmyopinionitwasanabcshowmadetoaminimum)..  theshowisnotable(moreofcourse,notwiththeaudience),andmostoftheactorsareexcellentandtheoveralldialogueisniceto watch(theshowmayhavebeenagreatepisode) Table8:Moregeneratedsentencesusingtopicslearnedfrom IMDB . TopicCompositionalNeuralLanguageModel TopicGeneratedSentences environment  environmentalistsimmediatebasecallstodefendtheworld.  ontheformationofthepoliticalandthefederalspaceagency,theec'slong-terminterestswerescheduledtoassessglobalwarming andaimedatdevelopingprogrammesbasedontheamericanindustrialplants.  companiescanreconstituteoruselargesyntheticoilorgas. education  theschoolhasrecentlybeenfoundedbya < unk > ofthenextgenerationfortwoyears.  theinstituteforstudenteducationcommitteesdependsontheattentionofthereceivedfromthetopoflasmo'sroundof theyear.  66yearslatertheteamjoinedblackpool,with < unk > livesfromtheverygoodandbeatenforallsupportcrew{andanew < unk > studentcallingunderulsternewsmayprovidean < unk > modernenterprise. politics  therestorationofthegovernmentannouncedonnov.4thattherepublic'sindependencetodirectcuba(threeyearsofelectoral < unk > wouldbealsoheldintheunitedkingdom( < unk > ).  aneweconomyinwhichprivatizationwasannouncedonjuly4.  agreementswerehithertoacceptedfromtermsthefollowingaugust1969intheaprilelections[seepp. < unk > .] business  netearningspersharerose < unk > %to$ < unk > inthequarter,and$ < unk > m,onturnoverthatrose < unk > %to$ < unk > m.  theinsurancemanagementhasissuedthequarternetup < unk > -$ < unk > m,duringturnoverup < unk > %at$ < unk > m;netforthesixmonthshasreportedthecumulativelossesof$  thequarterwouldalsohaveasmalllossofein,duefollowingof$7.3. facilities  thehotelissituatedinthe < unk > andthe < unk > .  allroomshaveexcellentamenities.  therestaurantisinasmallgarden,withitsownviewsand < unk > . sports  the < unk > ,whohada < unk > winoverthe < unk > ,wasa < unk > goal.  harveyhasbeenanthrashinggoalforboth < unk > andtheinstitutionalstrikerfellgentlyontheplatipand,throughinregular < unk > tointheseason.  botham'steamtookthefourthchanceoveraseasonwithoutplaying. art  radiocodesaidtheband'salbum'newest'armycluband < unk > album.  theyhavea < unk > ofthealbum,'run'fora' < unk > ',theorchestra,whichincludesthe < unk > ,whichisandtheband'slife.  nearlyallthisyear'salbum'sasell-outtour! award  superfrenchforthemtomeetwinners:thelabel < unk > withjust#50,000shouldbepaidforaverylargesize.  aspokeswomanfor < unk > said:thismayhavebeenamatterofpractice.  femalespeakerthey'llstartdiscussingmusicattheirhomeandwhydecisionsarecelebratingchildrenagain,buttheirpopularityhas arisenforenvironmentalresearch. expression  roirbakstaredathim,andthesmilehovering.  buttheymusthaveseenitinthegreatnight,aunt,soyoumadetheblushpoorthatfool.  makinghercry,itwasfair. crime  theprosecutionsaythecaseisnotthesame.  thechiefinspectorsuptmichael < unk > ,acrossbristolroad,anddeliveredonthesitethatthepolicehadbeenaccusedtotakehim jobbecauseitisnotabovethelegalservices.  shewasnearthesametimeasoneoftheeightmenwhodiedtakenprisonerandsubsequentlystabbed,whereshewashitaway. Table9:Moregeneratedsentencesusingtopicslearnedfrom BNC . DataTopicGeneratedSentences APNEWS army+terrorism  thetaliban'spresencehasearnedadegreefromthe1950-53koreanwarinpakistan'shistoriclifesince1964,withtwo exampleof < unk > soldiersfromwoundediraqiarmyshootingsandbahrainintheeasternarmy.  atthesamelevel,itwouldeventuallybegiventhegovernmentadministration'senhancedmilitaryforcesincethewar.  the < unk > previouslyblamedfortheattacksinafghanistan,whichnowcoverstheafghanarmy,andtheunitednationswillbe agreatopportunitytopracticeit. animal+lottory  whenthenumbersbegan,theu.s.andwildlifeserviceunveiledagamblingmeasurebyagreeingtoacquireapermitbyanimal protectionafterthepreviouspermitsbecameselectedfromthegovernor's.  shetoldthenewspaperthatshewasconcernedthatthebuyerwasinaneighborhoodlastyearandhadagraywolf.  thetippecanoecountyhistoricalsocietysaysitwasn'tsellinganywolfhunts. IMDB horror+negative  ifthismoviewasindeedahorriblemovieithinkiwillbebetterthe.  hestartstalkingtothewomanwhenthepersongetsthetown,shesuddenlyloseschildrenforbloodandit'sannoyingtodeath eventhoughitisuptoherfansandbaby.  what'sreallyscaryaboutthismovieisit'snotthatbad. hildren  mysteryincisalotofsmoke,whenatrivial,whinygirl < unk > troy < unk > andawomangetsattackedbythe < unk > captain( playedbyhurley).  paulthinkshimhastomakeupwhenthe < unk > eugenediscoversdefeatinordertotaketoomuchtimewithoutresortingto mortalbugs,andthenndshiswifeandboys.  theturtlesaregrownuptobilly(ashetakestherestofthe)andthescepterisafamilyandisdying. BNC environment+politics  < unk > shallowwaterareacomplexinadditionaninternationalactivityhadbeenpurchasedtohit < unk > tonnesofnuclearpower attheunplantin < unk > ,whichhadbegunstrikeactiontothepeopleofsoutherncountries.  thenationalenergyminister,michael < unk > of < unk > ,hasgivena"right"routetotheunitedkingdom'seuropeanparliament ,buttobepassedby < unk > ,theandfourthstates.  thecommission'sreportonoct.2,1990,onjan.7deniedthegovernment'sgrantto"thenationallevelofwater". art+crime  aswellas36,heisreturningfreelanceintotheredarmyofdramawherehehaslybeenstruckfortheirpremiere.  byalandonovan,twoarrestedbyoneguestofastarissupportedbysometeenagewomenforthequeen.  afterthetalks,therecordisalreadyfeaturingshaun < unk > 'splay' < unk > 'inthequartetoftheirabomb. Table10:Moregeneratedsentencesusingamixedcombinationoftopics.  
ProceedingsofNAACL-HLT2013 ,pages563Œ568, Atlanta,Georgia,9Œ14June2013. c  2013AssociationforComputationalLinguistics 563 564 565 566 567 568  
LearninglikeaChild: FastNovelVisualConceptLearningfromSentenceDescriptionsofImages JunhuaMao 1 WeiXu 2 YiYang 2 JiangWang 2 ZhihengHuang 2 AlanL.Yuille 1 1 UniversityofCalifornia,LosAngeles 2 BaiduResearch mjhustc@ucla.edu,f wei.xu,yangyi05,wangjiang03,huangzhihengg @baidu.com,yuille@stat.ucla.edu AbstractInthispaper,weaddressthetaskoflearningnovelvi- sualconcepts,andtheirinteractionswithotherconcepts,  fromafewimageswithsentencedescriptions.Usinglin-  guisticcontextandvisualfeatures,ourmethodisableto  efhypothesizethesemanticmeaningofnewwords  andaddthemtoitsworddictionarysothattheycanbe  usedtodescribeimageswhichcontainthesenovelcon-  cepts.Ourmethodhasanimagecaptioningmodulebased  on[ 38]withseveralimprovements.Inparticular,wepro- poseatransposedweightsharingscheme,whichnotonly  improvesperformanceonimagecaptioning,butalsomakes  themodelmoresuitableforthenovelconceptlearningtask.  Weproposemethodstopreventothenewcon-  cepts.Inaddition,threenovelconceptdatasetsarecon-  structedforthisnewtask,andarepubliclyavailableonthe  projectpage.Intheexperiments,weshowthatourmethod  effectivelylearnsnovelvisualconceptsfromafewexam-  pleswithoutdisturbingthepreviouslylearnedconcepts.  Theprojectpageis: www.stat.ucla.edu/Ÿjunhua.mao/projects/child_learning.html .1.Introduction Recognizing,learningandusingnovelconceptsisone ofthemostimportantcognitivefunctionsofhumans.When  wewereveryyoung,welearnednewconceptsbyobserv-  ingthevisualworldandlisteningtothesentencedescrip-  tionsofourparents.Theprocesswasslowatthebeginning,  butgotmuchfasterafterweaccumulatedenoughlearned  concepts[ 4].Inparticular,itisknownthatchildrencan formquickandroughhypothesesaboutthemeaningofnew  wordsinasentencebasedontheirknowledgeofprevious  learnedwords[ 5,21],associatethesewordstotheobjects ortheirproperties,anddescribenovelconceptsusingsen-  tenceswiththenewwords[ 4].Thisphenomenonhasbeen researchedforover30yearsbythepsychologistsandlin-  guistswhostudytheprocessofwordlearning[ 52].Forthecomputervisionseveralmethodsarepro- posed[ 16,46,53,31]tohandletheproblemoflearningnew   Figure1:AnillustrationoftheNovelVisualConceptlearn-  ingfromSentences(NVCS)task.Westartwithamodel  (i.e.model-base)trainedwithimagesthatdonotcontain  theconceptofﬁquidditchﬂ 1.Usingafewﬁquidditchﬂim- ageswithsentencedescriptions,ourmethodisabletolearn  thatﬁquidditchﬂisplayedbypeoplewithaball.  categoriesofobjectsfromahandfulofexamples.Thistask  isimportantinpracticebecausewesometimesdonothave  enoughdatafornovelconceptsandhenceneedtotransfer  knowledgefrompreviouslylearnedcategories.Moreover,  wedonotwanttoretrainthewholemodeleverytimewe  addafewimageswithnovelconcepts,especiallywhenthe  amountofdataormodelparametersisverybig. However,thesepreviousmethodsconcentrateonlearn- ingormappings,betweensinglewords(e.g.a  novelobjectcategory)andimages.Weareunawareofany  computervisionstudiesintothetaskoflearningnovelvi-  sualconceptsfromafewsentencesandthenusingthese  conceptstodescribenewimagesŒataskthatchildrenseem  todoeffortlessly.Wecallthisthe NovelVisualConcept learningfromSentences(NVCS) task(seeFigure 1).Inthispaper,wepresentanovelframeworktoaddress theNVCStask.Westartwithamodelthathasalready  beentrainedwithalargeamountofvisualconcepts.We  proposeamethodthatallowsthemodeltoenlargeitsword  dictionarytodescribethenovelconceptsusingafewex-  amplesandwithoutextensiveretraining.Inparticular,we  donotneedtoretrainmodelsfromscratchonallofthedata  (allthepreviouslylearnedconceptsandthenovelconcepts).  WeproposethreedatasetsfortheNVCStasktovalidateour 1ﬁquidditchﬂisasportcreatedinﬁHarryPotterﬂ.Itisplayedbyteams ofpeopleholdingbroomswithaball(seeFigure 1).12533 model,whichareavailableontheprojectpage. Ourmethodrequiresa basemodel forimagecaption- ingwhichwillbeadaptedtoperformtheNVCStask.We  choosethem-RNNmodel[ 38],whichperformsatthestate oftheart,asourbasemodel.Notethatwecouldusemost  ofthecurrentimagecaptioningmodelsasthebasemodel  inourmethod.Butwemakeseveralchangestothemodel  structureofm-RNNpartlymotivatedbythedesiretoavoid  ovwhichisaparticulardangerforNVCSbecause  wewanttolearnfromafewnewimages.Wenotethatthese  changesalsoimproveperformanceontheoriginalimage  captioningtask,althoughthisimprovementisnotthemain  focusofthispaper.Inparticular,weintroduceatransposed  weightsharing(TWS)strategy(motivatedbyauto-encoders  [3])whichreduces,byafactorofonehalf,thenumberof modelparametersthatneedtobelearned.Thisallowsusto  increasethedimensionoftheword-embeddingandmulti-  modallayers,withoutovthedata,yieldingaricher  wordandmultimodaldenserepresentation.Wetrainthis  imagecaptioningmodelonalargeimagedatasetwithsen-  tencedescriptions.Thisisthebasemodelwhichweadapt  fortheNVCStask. Nowweaddressthetaskoflearningthenewconcepts fromasmallnewsetofdatathatcontainstheseconcepts.  TherearetwomaindifFirstly,theweightsfor  thepreviouslylearnedconceptsmaybedisturbedbythe  newconcepts.Althoughthiscanbesolvedbythese  weights.Secondly,learningthenewconceptsfrompos-  itiveexamplescanintroducebias.Intuitively,themodel  willassignabaselineprobabilityforeachword,whichis  roughlyproportionaltothefrequencyofthewordsinthe  sentences.Whenwetrainthemodelonnewdata,thebase-  lineprobabilitiesofthenewwordswillbeunreliablyhigh.  Weproposeastrategythataddressesthisproblemby  thebaselineprobabilityofthenewwords. Weconstructthreedatasetstovalidateourmethod, whichinvolvesnewconceptsofman-madeobjects,ani-  mals,andactivities.Thetwodatasetsarederivedfrom  theMS-COCOdataset[ 34].Thethirdnewdatasetiscon- structedbyaddingthreeuncommonconceptswhichdonot  occurinMS-COCOorotherstandarddatasets.Thesecon-  ceptsare:quidditch,t-rexandsamisen(seesection 5)2.The experimentsshowthattrainingourmethodononlyafew  examplesofthenewconceptsgivesusasgoodperformance  asretrainingtheentiremodelonalltheexamples.  2.RelatedWork  Deepneuralnetwork Recentlytherehavebeendramatic progressindeepneuralnetworksfornaturallanguageand 2Thedatasetisavailableat www.stat.ucla.edu/Ÿjunhua.mao/projects/child_learning.html.Weareaddingmore novelconceptsinthisdataset.Thelatestversionofthedatasetcontains  8additionalnovelconcepts:tai-ji,huangmeiopera,kiss,rocketgun,tem-  pura,waterfall,weddingdress,andwindmill. computervision.Fornaturallanguage,RecurrentNeural  Networks(RNNs[ 14,40])andLong-ShortTermMemories (LSTMs[ 22])achievethestate-of-the-artperformancefor manyNLPtaskssuchasmachinetranslation[ 23,8,51 ]and speechrecognition[ 40].Forcomputervision,deepConvo- lutionalNeuralNetworks(CNN[ 33])outperformprevious methodsbyalargemarginforthetasksofobject  cation[ 27,48]anddetection[ 19,43,59].Thesuccessof thesemethodsforlanguageandvisionmotivatetheiruse  formultimodallearningtasks(e.g.imagecaptioningand  sentence-imageretrieval).  Multimodallearningoflanguageandvision Themeth- odsofimage-sentenceretrieval[ 17,50],imagedescrip- tiongeneration[ 28,42,20]andvisualquestion-answering [18,36,1]havedevelopedveryfastinrecentyears.Veryre- centworksofimagecaptioningincludes[ 39,25,24,55,10,15,7,32,37,26,57,35].Manyofthem(e.g.[ 38,55])adopt anRNN-CNNframeworkthatoptimizesthelog-likelihood  ofthecaptiongiventheimage,andtrainthenetworksinan  end-to-endway.Anexceptionis[ 15],whichincorporates visualdetectors,languagemodels,andmultimodalsimi-  laritymodelsinahigh-performingpipeline.Theevalua-  tionmetricsoftheimagecaptioningtaskisalsodiscussed  [13,54].Alloftheseimagecaptioningmethodsuseapre- andedworddictionary,andtraintheirmodel  onalargedataset.Ourmethodcanbedirectlyappliedto  anycaptioningmodelsthatadoptanRNN-CNNframework,  andourstrategytoavoidovisusefulformostofthe  modelsinthenovelvisualconceptlearningtask.  Zero-shotandone-shotlearning Forzero-shotlearning, thetaskistoassociatedensewordvectorsorattributeswith  imagefeatures[ 49,17,12,2,31].Thedensewordvec- torsinthesepapersarepre-trainedfromalargeamountof  textcorpusandthewordsemanticrepresentationiscaptured  fromco-occurrencewithotherwords[ 41].[ 31]developed thisideabyonlyshowingthenovelwordsafewtimes.In  addition,[ 47]adoptedauto-encoderswithattributerepre- sentationstolearnnewclasslabelsand[ 56 ]proposeda methodthatscalestolargedatasetsusinglabelembeddings. Anotherrelatedtaskisone-shotlearningtaskofnewcat- egories[ 16,29,53].Theylearnnewobjectsfromonlyafew examples.However,theseworkonlyconsiderwordsorat-  tributesinsteadofsentencesandsotheirlearningtargetis  differentfromthatofthetaskinthispaper.  3.TheImageCaptioningModel Weneedanimagecaptioningasthebasemodelwhich willbeadaptedintheNVCStask.Thebasemodelisbased  onthem-RNNmodel[ 38].ItsarchitectureisshowninFig- ure2(a).Wemaketwomainofthearchi- tecturetomakeitmoresuitablefortheNVCStaskwhich,  asasideeffect,alsoimprovesperformanceontheorigi-  nalimagecaptioningtask.Firstlyandmostimportantly,  weproposeatransposedweightsharingstrategywhichsig- 2534        Figure2:(a).Ourimagecaptioningmodel.Foreachwordinasentence,themodeltakesthecurrentwordindexandthe  imageasinputs,andoutputsthenextwordindex.Theweightsaresharedacrossthesub-modelsforthewordsinasentence.  Thenumberonthetoprightofeachlayerdenotesitsdimension.Asinthem-RNNmodel[ 38],weaddastartsign w start andanendsign w end toeachtrainingsentence.(b).Thetransposedweightsharingof U D andU M .(Bestviewedincolor) reducesthenumberofparametersinthemodel  (seesection 3.2).Secondly,wereplacetherecurrentlayer in[ 38]byaLong-ShortTermMemory(LSTM)layer[ 22].LSTMisarecurrentneuralnetworkwhichisdesignedto  solvethegradientexplosionandvanishingproblems.We  introducetheframeworkofthemodelinsection 3.1anddescribethedetailsofthetransposedweightsharing  strategyinsection 3.2.3.1.TheModelArchitecture AsshowninFigure 2(a),theinputofourmodelforeach wordinasentenceistheindexofthecurrentwordinthe  worddictionaryaswellastheimage.Werepresentthisin-  dexasaone-hotvector(abinaryvectorwithonlyonenon-  zeroelementindicatingtheindex).Theoutputistheindex  ofthenextword.Themodelhasthreecomponents:the  languagecomponent,thevisioncomponentandthemulti-  modalcomponent.Thelanguagecomponentcontainstwo  wordembeddinglayersandaLSTMlayer.Itmapsthe  indexofthewordinthedictionaryintoasemanticdense  wordembeddingspaceandstoresthewordcontextinfor-  mationintheLSTMlayer.Thevisioncomponentcontains  a16-layerdeepconvolutionalneuralnetwork(CNN[ 48])pre-trainedontheImageNettask[ 45].Were- movetheSoftMaxlayerofthedeepCNNandconnect  thetopfullyconnectedlayer(a4096dimensionallayer)to  ourmodel.Theactivationofthis4096dimensionallayer  canbetreatedasimagefeaturesthatcontainrichvisualat-  tributesforobjectsandscenes.Themultimodalcomponent  containsaone-layerrepresentationwheretheinformation  fromthelanguagepartandthevisionpartmergetogether.  WebuildaSoftMaxlayerafterthemultimodallayertopre-  dicttheindexofthenextword.Theweightsareshared  acrossthesub-modelsofthewordsinasentence.Asin  them-RNNmodel[ 38],weaddastartsign w start andan endsign w end toeachtrainingsentence.Inthetestingstage forimagecaptioning,weinputthestartsign w start intothe modelandpickthe K bestwordswithmaximumprobabil- itiesaccordingtotheSoftMaxlayer.Werepeattheprocess  untilthemodelgeneratestheendsign w end .3.2.TheTransposedWeightSharing(TWS) Fortheoriginalm-RNNmodel[ 38],mostoftheweights (i.e.98.49%)arecontainedinthefollowingtwoweight  matrices:U D 2 R512  N andU M 2 RN  1024 whereN representsthesizeoftheworddictionary. Theweightmatrix U D betweentheone-hotlayerand wordembeddinglayerisusedtocomputetheinputof  thewordembeddinglayer w ( t ) :w ( t )= f ( U D h ( t )) (1)wheref ( : ) isanelement-wisenon-linearfunction, h ( t ) 2 RN  1 istheone-hotvectorofthecurrentword.Notethat itisfasttocalculateEquation 1becausethereisonlyone non-zeroelementin h ( t ) .Inpractice,wedonotneedto calculatethefullmatrixmultiplicationoperationsinceonly  onecolumnof U D isusedforeachwordintheforwardand backwardpropagation. Theweightmatrix U M betweenthemultimodallayer andtheSoftMaxlayerisusedtocomputetheactivationof  theSoftMaxlayer y ( t ) :y ( t )= g ( U M m ( t )+ b ) (2)wherem ( t ) istheactivationofthemultimodallayerand g ( : ) istheSoftMaxnon-linearfunction. Intuitively,theroleoftheweightmatrix U D inEquation 1istoencodetheone-hotvector h ( t ) intoadensesemantic vector w ( t ) .Theroleoftheweightmatrix U M inEqua- tion2istodecodethedensesemanticvector m ( t ) backtoapseudoone-hotvector y ( t ) withthehelpoftheSoft- Maxfunction,whichisverysimilartotheinverseoperation  ofEquation 1.Thedifferenceisthat m ( t ) isinthedense multimodalsemanticspacewhile w ( t ) isinthedenseword semanticspace. 2535    Figure3:Illustrationoftrainingnovelconcepts.Weonly  updatethesub-matrix U D n(thegreenpartsinthe inU D thatisconnectedtothenodeofnewwordsinthe One-HotlayerandtheSoftMaxlayerduringthetraining  fornovelconcepts.(Bestviewedincolor) Toreducethenumberoftheparameters,wedecompose U M intotwoparts.Thepartmapsthemultimodallayer activationvectortoanintermediatevectorinthewordse-  manticspace.Thesecondpartmapstheintermediatevector  tothepseudoone-hotwordvector,whichistheinverseop-  erationofEquation 1.Thesub-matrixofthesecondpart isabletoshareparameterswith U D inatransposedman- ner,whichismotivatedbythetiedweightsstrategyinauto-  encodersforunsupervisedlearningtasks[ 3].Hereisan exampleoflineardecomposition: U M = U T  D U I ,where U I 2 R512  1024 .Equation 2isaccordinglychangedto: y ( t )= g [ U T  D f ( U I m ( t ))+ b ] (3)wheref ( : ) isaelement-wisefunction.If f ( : ) isanidentity mappingfunction,itisequivalenttolinearlydecomposing  U M intoU T  D andU I .Inourexperiments,wethatset- tingf ( : ) asthescaledhyperbolictangentfunctionleadstoa slightlybetterperformancethanlineardecomposition.This  strategycanbeviewedasaddinganintermediatelayerwith  dimension512betweenthemultimodalandSoftMaxlay-  ersasshowninFigure 2(b).Theweightmatrixbetween theintermediateandtheSoftMaxlayerissharedwith U D inatransposedmanner.ThisTransposedWeightSharing  (TWS)strategyenablesustouseamuchlargerdimensional  word-embeddinglayerthanthem-RNNmodel[ 38]with- outincreasingthenumberofparameters.Wealso  fromthisstrategywhenaddressingthenovelconceptlearn-  ingtask.  4.TheNovelConceptLearning(NVCS)Task Supposewehavetrainedamodelbasedonalarge amountofimagesandsentences.Thenwemeetwithim-  agesofnovelconceptswhosesentenceannotationscon-  tainwordsnotinourdictionary,whatshouldwedo?It  istime-consumingandunnecessarytore-trainthewhole  modelfromscratchusingallthedata.Inmanycases,we  cannotevenaccesstheoriginaltrainingdataofthemodel.  Butthewholemodelusingonlythenewdata  causessevereovonthenewconceptsanddecrease  theperformanceofthemodelfortheoriginallytrainedones. Tosolvetheseproblems,weproposethefollowing strategiesthatlearnthenewconceptswithafewimages  withoutlosingtheaccuracyontheoriginalconcepts.  4.1.Fixingtheoriginallylearnedweights Undertheassumptionthatwehavelearnedtheweights oftheoriginalwordsfromalargeamountofdataandthat  theamountofthedatafornewconceptsisrelativelysmall,  itisstraightforwardtotheoriginallylearnedweightsof  themodelduringtheincrementaltraining.More  cally,theweightmatrix U D canbeseparatedintotwoparts: U D =[ U D o ; U D n] ,where U D o andU D nassociatewith theoriginalwordsandthenewwordsrespectively.E.g.,as  showninFigure 3,forthenovelvisualconceptﬁcatﬂ, U D nisassociatedwith29newwords,suchascat,kittenand  pawing.Wethesub-matrix U D o andupdatethesub- matrixU D nasillustratedinFigure 3.4.2.Fixingthebaselineprobability InEquation 3,thereisabiasterm b .Intuitively,each elementin b representsthetendencyofthemodeltooutput thecorrespondingword.Wecanthinkofthistermasthe  baselineprobabilityofeachword.Similarto U D ,b canbeseparatedintotwoparts: b =[ b o ; b n ] ,where b o andb n associatewiththeoriginalwordsandthenewwordsre- spectively.Ifweonlypresentthenewdatatothenetwork,  theestimationof b n isunreliable.Thenetworkwilltendto increasethevalueof b n whichcausesovtothenew data.Theeasiestwaytosolvethisproblemisto b n dur- ingthetrainingfornovelconcepts.Butthisisnotenough.  Becausetheaverageactivation x oftheintermediatelayer acrossallthetrainingsamplesisnot 0,theweightmatrix U D playsasimilarroleto b inchangingthebaselineprob- ability.Toavoidthisproblem,wecentralizetheactivation  oftheintermediatelayer x andturntheoriginalbiasterm b intob 0 asfollows: y ( t )= g [ U T  D ( x  x )+ b 0 ]; b 0 o = b o + U T  D o x (4)Afterthat,weseteveryelementin b 0  n tobetheaverage valueoftheelementsin b 0  o and b 0  n whenwetrainon thenewimages.WecallthisstrategyBaselineProbability  Fixation(BPF). Intheexperiments,weadoptastochasticgradientde- scentalgorithmwithaninitiallearningrateof0.01anduse  AdaDelta[ 58]astheadaptivelearningratealgorithmfor boththebasemodelandthenovelconceptmodel.  4.3.TheRoleofLanguageandVision Inthenovelconceptlearning(NVCS)task,thesentences serveasaweaklabelingoftheimage.Thelanguagepartof  themodel(thewordembeddinglayersandtheLSTMlayer)  hypothesizesthebasicproperties(e.g.thepartsofspeech)  ofthenewwordsandwhetherthenewwordsareclosely  relatedtothecontentoftheimage.Italsohypothesizes 2536            Figure4:Organizationofthenovelconceptdatasets whichwordsintheoriginaldictionaryaresemanticallyand  syntacticallyclosetothenewwords.Forexample,suppose  themodelmeetsanewimagewiththesentencedescription  ﬁAwomanisplayingwithacatﬂ.Alsosupposethereare  imagesintheoriginaldatacontainingsentencedescription  suchasﬁAmanisplayingwithadogﬂ.Thenalthoughthe  modelhasnotseenthewordﬁcatﬂbefore,itwillhypothe-  sizethatthewordﬁcatﬂandﬁdogﬂareclosetoeachother. Thevisionpartispre-trainedontheImageNet tiontask[ 45]with1.2millionimagesand1,000categories. Itprovidesrichvisualattributesoftheobjectsandscenes  thatareusefulnotonlyforthe1,000taskit-  self,butalsoforothervisiontasks[ 11].Combiningcuesfrombothlanguageandvision,our modelcaneffectivelylearnthenewconceptsusingonlya  fewexamplesasdemonstratedintheexperiments.  5.Datasets  5.1.StrategiestoConstructDatasets WeusetheannotationsandimagesfromtheMSCOCO [34]toconstructourNovelConcept(NC)learningdatasets. ThecurrentreleaseofCOCOcontains82,783trainingim-  agesand40,504validationimages,withobjectinstancean-  notationsand5sentencedescriptionsforeachimage.To  constructtheNCdatasetwithanewconcept(e.g.  ﬁcatﬂ),weremoveallimagescontainingtheobjectﬁcatﬂ  accordingtotheobjectannotations.Wealsocheckwhether  therearesomeimagesleftwithsentencesdescriptionscon-  tainingcatrelatedwords.Theremainingimagesaretreated  asthe BaseSet wherewewilltrain,validateandtestour basemodel.Theremovedimagesareusedtoconstructthe  NovelConceptset(NCset) ,whichisusedtotrain,validate andtestourmodelforthetaskofnovelconceptlearning.  5.2.TheNovelVisualConceptsDatasets Weconstructthreedatasetsinvolvingvedifferentnovel visualconcepts: NewObj-CatandNewObj-MotorThecorresponding newconceptsofthesetwodatasetsare ﬁcatﬂandﬁmotor- cycleﬂrespectively.Themodelneedtolearnalltherelated TrainNCTestValidation NewObj-Cat28401000490  NewObj-Motor1854600349  NC-3150( 50  3 )120( 40  3 )30( 10  3 )Table1:Thenumberofimagesforthethreedatasets.                   Figure5:SampleimagesandannotationsfromNovel  Concept-3(NC-3)dataset(seemoreontheprojectpage).  wordsthatdescribetheseconceptsandtheiractivities. NC-3dataset 3Thetwodatasetsmentionedaboveare allderivedfromtheMSCOCOdataset.Tofurtherver-  ifytheeffectivenessofourmethod,weconstructanew  datasetcontainsthreenovelconcepts: ﬁquidditchﬂ (are- centlycreatedsportderivedfromﬁHarryPotterﬂ), ﬁt-rexﬂ (adinosaur),and ﬁsamisenﬂ(aninstrument).Itcontains notonlyobjectconcepts(e.g.t-rexandsamisen),butalso  activityconcepts(e.g.quidditch).Welabeled100images  foreachconceptwith5sentenceannotationsforeachim-  age.Todiversifythelabeledsentencesfordifferentimages  inthesamecategory,theannotatorsareinstructedtolabel  theimageswithdifferentsentencesbydescribingthedetails  ineachimage.Itleadstoadifferentstyleofannotation  fromthatoftheMSCOCOdataset.Theaveragelength  ofthesentencesisalso26%longerthanthatoftheMS  COCO(13.5v.s.10.7).Weconstructthisdatasetfortwo  reasons.Firstly,thethreeconceptsarenotincludedinthe  1,000categoriesoftheImageNettask[ 45]wherewepre-trainedthevisioncomponentofourmodel.  Secondly,thisdatasethasricherandmoredivsen-  tencedescriptionscomparedtoNewObj-CatandNewObj-  Motor.WedenotethisdatasetasNovelConcept-3dataset  (NC-3).Somesamplesimagesandannotationsareshown  inFigure 5.Werandomlyseparatetheabovethreedatasetsintotrain- ing,testingandvalidationsets.Thenumberofimagesfor  thethreedatasetsareshowninTable 1.Toinvestigatethe possibleovissuesonthesedatasets,inthetesting  stage,werandomlypickedimagesfromthetestingsetof  theBaseSetandtreatedthemasaseparatesetoftestingim-  ages.Thenumberofaddedimagesisequaltothesizeofthe  originaltestset(e.g.1000imagesarepickedforNewObj-  Cattestingset).Wedenotetheoriginalnewconcepttesting  imagesas NovelConcept(NC)test setandtheaddedbase testingimagesas Basetest set.Agoodnovelvisualcon- ceptlearningmethodshouldperformbetterthanthebase  modelonNCtestsetandcomparableonBasetestset.The  organizationofNCdatasetsisillustratedinFigure 4.3Thedatasetispubliclyavailableat www.stat.ucla.edu/Ÿjunhua.mao/projects/child_learning.html.Weareac- tivelyexpandingthedataset.Thelatestversioncontains11novelconcepts. 2537 B-1B-2B-3B-4METEORCIDErROUGE Lm-RNN[ 38]0.6800.5060.3690.2720.2250.7910.499 ours-TWS0.6850.5120.3760.2790.2290.8190.504 Table2:Theperformancecomparisonsofourmodeland  m-RNN[ 38]forthestandardimagecaptioningtask. 6.Experiments  6.1.EvaluationMetrics Toevaluatetheoutputsentencedescriptionsfornovel visualconcepts,weadopttwoevaluationmetricsthatare  widelyusedinrecentimagecaptioningwork:BLEUscores  [44](BLEUscoreforn-gramisdenotedasasB-ninthe paper)andMETEOR[ 30].BothBLEUscoresandMETEORtargetonevaluating theoverallqualityofthegeneratedsentences.IntheNVCS  task,however,wefocusmoreontheaccuracyforthenew  wordsthanthepreviouslylearnedwordsinthesentences.  Therefore,toconductacomprehensiveevaluation,wealso  calculatethe f scoreforthewordsthatdescribethenew concepts.E.g.forthecatdataset,thereare29newwords  suchascat,cats,kitten,andpawing.Theprecision p andrecallr foreachnewwordinthedictionary( w d n )arecalcu- latedasfollows: p =N( w d n 2 S gen ^ w d n 2 S ref ) N( w d n 2 S gen ) ; r =N( w d n 2 S gen ^ w d n 2 S ref ) N( w d n 2 S ref ) whereS gen denotesgeneratedsentence, S ref denotesrefer- encesentences, N( condition ) representsnumberoftesting imagesthatconformtothecondition.Notethat p andr arecalculatedonthecombinedtestingsetoftheNCtestsetand  thebasetestset(i.e.Alltest). Ahigh r withalow p indicatesthatthemodelov thenewdata(Wecanalwaysget r =1 ifweoutputthe newwordeverytime)whileahigh p withalow r indicatesWeusethe f = 2 p  1 +r  1 asabalancedmea- surementbetween p andr .Best f scoreis1.Notethat f =0 ifeither p =0 orr =0 .ComparedtoMETEORand BLEU,the f scoreshowtheeffectivenessofthemodelto learnnewconceptsmoreexplicitly.  6.2.EffectivenessofTWSandBPF WetestourbasemodelwiththeTransposedWeight Sharing(TWS)strategyintheoriginalimagecaptaining  taskontheMSCOCO[ 6]andcomparetom-RNN[ 38],whichdoesnotuseTWS.Ourmodelperformsbetterthan  m-RNNinthistaskasshowninTable 2.Wechoosethe layerdimensionsofourmodelsothatthenumberofpa-  rametersmatchesthatof[ 38].Modelswithdifferenthyper- parameters,featuresorpipelinesmightleadtobetterper-  formance,whichisbeyondthescopeofthispaper.E.g.  [38,55,15]furtherimprovetheirresultsafterthesubmis- sionofthisdraftandachieveaB-4scoreof0.302,0.309and  0.308respectivelyusing,e.g.,imagefeatureson  COCOorconsensusreranking[ 9,38],whicharecomple- mentarywithTWS. BiasFixCentralizeTWS f edBias  p 0.851Deep-NVCS-FixedBias p  p 0.860Deep-NVCS-NoBPF-NoTWS 0.839Deep-NVCS-BPF-NoTWSpp  0.850Deep-NVCS-BPF-TWSppp 0.875Table3:PerformanceofDeep-NVCSmodelswithdifferent  novelconceptlearningstrategiesonNewObj-Cat.TWSand  BPFimprovetheperformance. WealsovalidatetheeffectivenessofourTransposed WeightSharing(TWS)andBaselineProbabilityFixation  (BPF)strategiesforthenovelconceptlearningtaskonthe  NewObj-Catdataset.Wecomparetheperformanceofve  Deep-NVCSmodels.Theirpropertiesandperformancein  termsof f scoreforthewordﬁcatﬂaresummarizedinTable 3.ﬁBiasFixﬂmeansthatwethebiasterm b n inEqua- tion3.ﬁCentralizeﬂmeansthatwecentralizetheintermedi- atelayeractivation x (seeEquation 4)sothat U D willnot affectthebaselineprobability. Weachieve2.5%increaseofperformanceintermsof f usingTWS(Deep-NVCS-BPF-TWSv.s.Deep-NVCS- BPF-noTWS4),andachieves2.4%increaseusingBPF (Deep-NVCS-BPF-TWSv.s.edBias).  WeuseDeep-NVCStorepresentDeep-NVCS-BPF-TWS  inshortfortherestofthepaper.  6.3.Resultsonand  6.3.1Usingalltrainingsamples  WeshowtheperformanceofourDeep-NVCSmodelscom-  paredtostrongbaselinesontheNewObj-CatandNewObj-  MotordatasetsinTable 4.ForDeep-NVCS,weonlyusethe trainingdatafromthenovelconceptset.ForDeep-NVCS-  Inc1:1,weaddtrainingdatarandomlysampledfromthe  trainingsetofthebaseset.Thenumberofaddedtraining  imagesisthesameasthatofthetrainingimagesfornovel  concepts.Model-basestandsforthemodeltrainedonlyon thebaseset(nonovelconceptimages).Weimplementa  baselinemodel, Model-word2vec ,wheretheweightsofnew words( U D n)arecalculatedusingaweightedsumofthe weightsof10similarconceptsmeasuredbytheunsuper-  visedlearnedword-embeddingsfromword2vec[ 41].We alsoimplementastrongbaseline, Model-retrain ,byretrain- ingthewholemodelfromscratchonthecombinedtraining  set(trainingimagesfromboththebasesetandtheNCset). TheresultsshowthatcomparedtotheModel-basewhich isonlytrainedonbaseset,theDeep-NVCSmodelsperform  muchbetteronthenovelconcepttestsetwhilereaching  comparableperformanceonthebasetestset.Deep-NVCS  alsoperformsbetterthantheModel-word2vecmodel.The 4WetriedtwoversionsofthemodelwithoutTWS:(I).themodelwith multimodallayerdirectlyconnectedtosoftmaxlayerlike[ 38],(II).the modelwithanadditionalintermediatelayerlikeTWSbutdoesnotshare  theweights.Inourexperiments,(I)performsslightlybetterthan(II)sowe  reporttheperformanceof(I)here. 2538 AlltestNCtestBasetest EvaluationMetrics fB-1B-2B-3B-4 METEORB-1B-2B-3B-4 METEORB-1B-2B-3B-4 METEORNewObj-Cat Model-retrain0.8660.6890.5310.3910.2910.2280.6960.5490.4030.3050.2270.6830.5130.3790.2770.229  Model-base0.0000.6450.4740.3390.2470.2010.6070.4360.3030.2170.1750.6830.5110.3750.2770.227  Model-word2vec0.1830.6420.4710.3410.2450.2000.6100.4320.3070.2170.1760.6740.5100.3750.2730.224  Deep-NVCS0.8750.6820.5210.3820.2860.2240.6840.5340.3920.2990.2240.6800.5080.3720.2740.225  Deep-NVCS-1:1Inc0.8810.6830.5230.3850.2880.2260.6860.5380.3980.3030.2260.6790.5070.3710.2730.225 NewObj-Motor Model-retrain0.7970.6970.5260.3860.2840.2400.6870.5120.3680.2630.2440.7070.5390.4040.3050.236  Model-base0.0000.6460.4600.3270.2350.2180.5860.3800.2450.1600.2030.7050.5360.4010.3010.235  Model-word2vec0.2790.6630.4760.3380.2430.2260.6240.4230.2790.1830.2230.7010.5300.3970.3030.229  Deep-NVCS0.7840.6880.5120.3730.2760.2360.6770.4940.3490.2520.2410.6980.5300.3980.2990.231  Deep-NVCS-1:1Inc0.7900.6870.5120.3740.2800.2350.6720.4920.3470.2560.2370.7020.5320.4010.3030.234 Table4:ResultsontheNewObj-CatandNewObj-Motordatasetusingallthetrainingsamples.TheDeepNVCSmodels  outperformthesimplebaselines.Theyachievecomparableperformancewiththestrongbaseline(i.e.Model-retrain)but  onlyneed  2%ofthetime.Model-baseandModel-retrainstandforthemodeltrainedonbaseset(nonovelconcepts)and themodelretrainedonthecombineddata(alltheimagesofbasesetandnovelconceptset)respectively.Model-word2vec  isabaselinemodelbasedonword2vec[ 41].Deep-NVCSstandsforthemodeltrainedonlywiththenewconceptdata. Deep-NVCS-1:1IncstandsfortheDeep-NVCSmodeltrainedbyaddingequalnumberoftrainingimagesfromthebaseset. Figure6:PerformancecomparisonofourmodelwithdifferentnumberoftrainingimagesonNewObj-catandNewObj-Motor  datasets.Thered,blue,andmagentadashedlineindicatestheperformanceofDeep-NVCSusingallthetrainingimageson  thebasetestset,theNCtestsetandthealltestsetrespectively.ThegreendashedlineindicatestheperformanceofModel-  base.TheblackdotsstandfortheperformanceofModel-retrainforNCtest.Weshowthatourmodeltrainedwith10to50  imagesachievescomparableperformancewiththemodeltrainedonthefulltrainingset.(Bestviewedincolor)  performanceofourDeep-NVCSmodelsisveryclosetothat  ofthestrongbaselineModel-retrainbutneeds onlylessthan 2%ofthetime .Thisdemonstratestheeffectivenessofour novelconceptlearningstrategies.Themodellearnsthenew  wordsforthenovelconceptswithoutdisturbingtheprevi-  ouslearnedwords. TheperformanceofDeep-NVCSisalsocomparable with,thoughslightlylowerthanDeep-NVCS-1:1Inc.In-  tuitively,iftheimagefeaturescansuccessfullycapturethe  differencebetweenthenewconceptsandtheexistingones,  itissuftolearnthenewconceptonlyfromthenew  data.However,ifthenewconceptsareverysimilartosome  previouslylearnedconcepts,suchascatanddog,itishelp-  fultopresentthedataofbothnovelandexistingconcepts  tomakeiteasierforthemodeltothedifference.  6.3.2Usingafewtrainingsamples  Wealsotestourmodelundertheoneorfew-shotscenarios.  ,werandomlysampled k imagesfromthetrain- ingsetofNewObj-CatandNewObj-Motor,andtrainedour  Deep-NVCSmodelonlyontheseimages( k rangesfrom1 to1000).Weconducttheexperiments10timesandaverage  theresultstoavoidtherandomnessofthesampling. Weshowtheperformanceofourmodelwithdifferent numberoftrainingimagesinFigure 6.Weonlyshowthe resultsintermsof f score,METEOR,B-3andB-4because ofspacelimitation.TheresultsofB-1andB-2andconsis-  tentwiththeshownmetrics.Theperformanceofthemodel  trainedwiththefullNCtrainingsetinthelastsectionis  indicatedbytheblue(Basetest),red(NCtest)ormagenta  (Alltest)dashedlinesinFigure 6.Theselinesrepresentthe experimentalupperboundsofourmodelundertheoneor  few-shotscenario.TheperformanceoftheModel-baseis  shownbyagreendashedline.Itservesasanexperimental  lowerbound.WealsoshowtheresultsofModel-retrainfor  NCtestwithblackdotsinFigure 6trainedwith10and500 novelconceptsimages. Theresultsshowthatusingabout10to50trainingim- 2539                               Figure7:Thegeneratedsentencesforthetestimagesfromnovelconceptdatasets.Intheseexamples,cat,motorcycle,  quidditch,t-rexandsamisenarethenovelconceptsrespectively.Weshowmoreinthesupplementarymaterial.  ages,themodelachievescomparableperformancewiththe  Deep-NVCSmodeltrainedonthefullnovelconcepttrain-  ingset.Inaddition,usingabout5trainingimages,weob-  serveanontrivialincreaseofperformancecomparedtothe  basemodel.Ourdeep-NVCSalsobetterhandlesthecase  forafewimagesandrunsmuchfasterthanModel-retrain.  6.4.Resultson TheNC-3datasethasthreemaindifFirstly,the conceptshaveverysimilarcounterpartsintheoriginalim-  ageset,suchasSamisenv.s.Guitar,Quidditchv.s.foot-  ball.Secondly,thethreeconceptsrarelyappearindaily  life.TheyarenotincludedintheImageNet1,000cate-  gorieswherewepre-trainedourvisiondeepCNN.Thirdly,  thewaywedescribethethreenovelconceptsissomewhat  differentfromthatofthecommonobjectsincludedinthe  baseset.Therequirementtodiversifytheannotatedsen-  tencesmakesthedifferenceofthestylefortheannotated  sentencesbetweenNC-3andMSCOCOevenlarger.The  effectofthedifferenceinsentencestyleleadstodecreased  performanceofthebasemodelcomparedtothatonthe  NewObj-CatandNewObj-Motordataset(seeModel-base  inTable 5comparedtothatinTable 4onNCtest).Further- more,itmakesitharderforthemodeltohypothesizethe  meaningsofnewwordsfromafewsentences. Facedwiththesedifourmodelstilllearnsthe semanticmeaningofthenewconceptsquitewell.The f scoresofthemodelshowninTable 5indicatethatthemodel successfullylearnsthenewconceptswithahighaccuracy  fromonly50examples. ItisinterestingthatModel-retrainperformsverybadly EvaluationMetrics fB-3B-4MET. fB-3B-4MET. quidditcht-rex Model-retrain0.0000.1960.1380.1200.2130.2240.1410.105  Model-base0.0000.1930.1390.1220.0000.1660.1020.088  Deep-NVCC0.8540.2370.1670.1680.8610.2470.1440.187  Deep-NVCC-1:1Inc0.8630.2440.1700.1700.8560.2420.1320.186 samisenBaseTest Model-retrain0.0000.2090.1330.122-0.4120.3280.234  Model-base0.0000.1770.1050.122-0.4140.3250.240  Deep-NVCC0.6300.2290.1400.161-0.4140.3260.239  Deep-NVCC-1:1Inc0.6420.2330.1440.164-0.4140.3270.239 Table5:ResultsofourmodelontheNC-3Datasets. NewWordFivenearestneighbours catkitten;tabby;puppy;calico;doll;  motorcyclemotorbike;moped;vehicle;motor;motorbikes;  quidditchsoccer;football;softball;basketball;frisbees;  t-rexgiraffe's;bull;pony;goat;burger;  samisenguitar;wii;toothbrushes;purse;contents; Table6:Thevenearestneighborsofthenewwordsas  measuredbytheactivationoftheword-embeddinglayer.  onthisdataset.Itdoesnotoutputthewordﬁquidditchﬂand  ﬁsamisenﬂinthegeneratedsentences.TheBLEUscores  andMETEORarealsoverylow.Thisisnotsurprisingsince  thereareonlyafewtrainingexamples(i.e.50)forthese  threenovelconceptsandsoitiseasytobeoverwhelmedby  otherconceptsfromtheoriginalMSCOCOdataset.  6.5.QualitativeResults InTable 6,weshowthevenearestneighborsofthenew conceptsusingtheactivationoftheword-embeddinglayer  learnedbyourDeep-NVCSmodel.Itshowsthatthelearned  novelwordembeddingvectorscapturesthesemanticinfor-  mationfrombothlanguageandvision.Wealsoshowsome  samplegeneratedsentencedescriptionsofthebasemodel  andourDeep-NVCSmodelinFigure 7.7.Conclusion Inthispaper,weproposetheNovelVisualConcept learningfromSentences(NVCS)task.Inthistask,methods  needtolearnnovelconceptsfromsentencedescriptionsof  afewimages.Wedescribeamethodthatallowsustotrain  ourmodelonasmallnumberofimagescontainingnovel  concepts.Thisperformscomparablywiththemodelre-  trainedfromscratchonallofthedataifthenumberofnovel  conceptimagesislarge,andperformsbetterwhenthereare  onlyafewtrainingimagesofnovelconceptsavailable.We  constructthreenovelconceptdatasetswherewevalidatethe  effectivenessofourmethod.Thesedatasetshavebeenre-  leasedtoencouragefutureresearchinthisarea.  Acknowledgement Wethankthecommentsandsuggestionsoftheanonymousre- viewers,andhelpfromXiaochenLianinthedatasetcollection  process.WeacknowledgesupportfromNSFSTCawardCCF-  1231216andARO62250-CS. 2540 References [1]S.Antol,A.Agrawal,J.Lu,M.Mitchell,D.Batra,C.L.Zitnick,andD.Parikh. Vqa:Visualquestionanswering. arXiv,2015. 2[2]S.Antol,C.L.Zitnick,andD.Parikh.Zero-shotlearningviavisualabstraction. InECCV,pages401Œ416.2014. 2[3]Y.Bengio.Learningdeeparchitecturesforai. Foundationsandtrends R inMachineLearning ,2(1):1Œ127,2009. 2,4[4]P.Bloom. Howchildrenlearnthemeaningsofwords .MITpress,2002. 1[5]S.CareyandE.Bartlett.Acquiringasinglenewword.1978. 1[6]X.Chen,H.Fang,T.Lin,R.Vedantam,S.Gupta,P.Dollr,andC.L.Zitnick. Microsoftcococaptions:Datacollectionandevaluationserver. arXivpreprint arXiv:1504.00325,2015. 6[7]X.ChenandC.L.Zitnick.Learningarecurrentvisualrepresentationforimage captiongeneration. arXivpreprintarXiv:1411.5654 ,2014. 2[8]K.Cho,B.vanMerrienboer,C.Gulcehre,F.Bougares,H.Schwenk,and Y.Bengio.Learningphraserepresentationsusingrnnencoder-decoderforsta-  tisticalmachinetranslation. arXivpreprintarXiv:1406.1078 ,2014. 2[9]J.Devlin,S.Gupta,R.Girshick,M.Mitchell,andC.L.Zitnick.Ex- ploringnearestneighborapproachesforimagecaptioning. arXivpreprint arXiv:1505.04467,2015. 6[10]J.Donahue,L.A.Hendricks,S.Guadarrama,M.Rohrbach,S.Venugopalan, K.Saenko,andT.Darrell.Long-termrecurrentconvolutionalnetworksfor  visualrecognitionanddescription. arXivpreprintarXiv:1411.4389 ,2014. 2[11]J.Donahue,Y.Jia,O.Vinyals,J.Hoffman,N.Zhang,E.Tzeng,andT.Darrell. Decaf:Adeepconvolutionalactivationfeatureforgenericvisualrecognition.  arXivpreprintarXiv:1310.1531 ,2013. 5[12]M.Elhoseiny,B.Saleh,andA.Elgammal.WriteaZero-shotlearning usingpurelytextualdescriptions.In ICCV,pages2584Œ2591,2013. 2[13]D.ElliottandF.Keller.Comparingautomaticevaluationmeasuresforimage description.In ACL ,volume2,pages452Œ457,2014. 2[14]J.L.Elman.Findingstructureintime. Cognitivescience ,14(2):179Œ211,1990. 2[15]H.Fang,S.Gupta,F.Iandola,R.Srivastava,L.Deng,P.Doll ´ar,J.Gao,X.He, M.Mitchell,J.Platt,etal.Fromcaptionstovisualconceptsandback. arXivpreprintarXiv:1411.4952 ,2014. 2,6[16]L.Fei-Fei,R.Fergus,andP.Perona.One-shotlearningofobjectcategories. TPAMI ,28(4):594Œ611,2006. 1,2[17]A.Frome,G.S.Corrado,J.Shlens,S.Bengio,J.Dean,T.Mikolov,etal. Devise:Adeepvisual-semanticembeddingmodel.In NIPS,pages2121Œ2129, 2013.2[18]H.Gao,J.Mao,J.Zhou,Z.Huang,L.Wang,andW.Xu.Areyoutalkingtoa machine?datasetandmethodsformultilingualimagequestionanswering.In  NIPS,2015. 2[19]R.Girshick,J.Donahue,T.Darrell,andJ.Malik.Richfeaturehierarchiesfor accurateobjectdetectionandsemanticsegmentation.In CVPR,2014. 2[20]A.GuptaandP.Mannem.Fromimageannotationtoimagedescription.In ICONIP,2012. 2[21]T.H.HeibeckandE.M.Markman.Wordlearninginchildren:Anexamination offastmapping. Childdevelopment ,pages1021Œ1034,1987. 1[22]S.HochreiterandJ.Schmidhuber.Longshort-termmemory. Neuralcomputa- tion,9(8):1735Œ1780,1997. 2,3[23]N.KalchbrennerandP.Blunsom.Recurrentcontinuoustranslationmodels.In EMNLP,pages1700Œ1709,2013. 2[24]A.KarpathyandL.Fei-Fei.Deepvisual-semanticalignmentsforgenerating imagedescriptions. arXivpreprintarXiv:1412.2306 ,2014. 2[25]R.Kiros,R.Salakhutdinov,andR.S.Zemel.Unifyingvisual-semantic embeddingswithmultimodalneurallanguagemodels. arXivpreprint arXiv:1411.2539,2014. 2[26]B.Klein,G.Lev,G.Sadeh,andL.Wolf.Fishervectorsderivedfromhy- bridgaussian-laplacianmixturemodelsforimageannotation. arXivpreprint arXiv:1411.7399,2014. 2[27]A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenetwith deepconvolutionalneuralnetworks.In NIPS,pages1097Œ1105,2012. 2[28]G.Kulkarni,V.Premraj,S.Dhar,S.Li,Y.Choi,A.C.Berg,andT.L.Berg. Babytalk:Understandingandgeneratingimagedescriptions.In CVPR,2011. 2[29]B.M.Lake,R.Salakhutdinov,J.Gross,andJ.B.Tenenbaum.Oneshotlearning ofsimplevisualconcepts.In CogSci ,volume172,2011. 2[30]A.LavieandA.Agarwal.Meteor:Anautomaticmetricformtevaluationwith highlevelsofcorrelationwithhumanjudgements.In WorkshoponStatistical MachineTranslation ,pages228Œ231,2007. 6[31]A.Lazaridou,E.Bruni,andM.Baroni.Isthisawampimuk?cross-modal mappingbetweendistributionalsemanticsandthevisualworld.In ACL ,pages 1403Œ1414,2014. 1,2[32]R.Lebret,P.O.Pinheiro,andR.Collobert.Simpleimagedescriptiongenerator viaalinearphrase-basedapproach. arXivpreprintarXiv:1412.8419 ,2014. 2[33]Y.A.LeCun,L.Bottou,G.B.Orr,andK.-R.M ¨uller.Efbackprop.In Neuralnetworks:Tricksofthetrade ,pages9Œ48.Springer,2012. 2[34]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Doll ´ar, andC.L.Zitnick.Microsoftcoco:Commonobjectsincontext. arXivpreprint arXiv:1405.0312,2014. 2,5[35]L.Ma,Z.Lu,L.Shang,andH.Li.Multimodalconvolutionalneuralnetworks formatchingimageandsentence. arXivpreprintarXiv:1504.06063 ,2015. 2[36]M.MalinowskiandM.Fritz.Amulti-worldapproachtoquestionanswering aboutreal-worldscenesbasedonuncertaininput.In NIPS,pages1682Œ1690, 2014.2[37]M.MalinowskiandM.Fritz.Apoolingapproachtomodellingspatialrelations forimageretrievalandannotation. arXivpreprintarXiv:1411.5190 ,2014. 2[38]J.Mao,W.Xu,Y.Yang,J.Wang,Z.Huang,andA.Yuille.Deepcaptioning withmultimodalrecurrentneuralnetworks(m-rnn).In ICLR,2015. 1,2,3,4,6[39]J.Mao,W.Xu,Y.Yang,J.Wang,andA.L.Yuille.Explainimageswithmul- timodalrecurrentneuralnetworks.In NIPSDeepLearningWorkshop ,2014. 2[40]T.Mikolov,M. ´at,L.Burget,J.Cernock ˚y,andS.Khudanpur.Recurrent neuralnetworkbasedlanguagemodel.In INTERSPEECH,pages1045Œ1048, 2010.2[41]T.Mikolov,I.Sutskever,K.Chen,G.S.Corrado,andJ.Dean.Distributed representationsofwordsandphrasesandtheircompositionality.In NIPS,pages 3111Œ3119,2013. 2,6,7[42]M.Mitchell,X.Han,J.Dodge,A.Mensch,A.Goyal,A.Berg,K.Yamaguchi, T.Berg,K.Stratos,andH.Daum ´eIII.Midge:Generatingimagedescriptions fromcomputervisiondetections.In EACL ,2012. 2[43]W.Ouyang,P.Luo,X.Zeng,S.Qiu,Y.Tian,H.Li,S.Yang,Z.Wang,Y.Xiong, C.Qian,etal.Deepid-net:multi-stageanddeformabledeepconvolutional  neuralnetworksforobjectdetection. arXivpreprintarXiv:1409.3505 ,2014. 2[44]K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu.Bleu:amethodforautomatic evaluationofmachinetranslation.In ACL ,pages311Œ318,2002. 6[45]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,S.Ma,Z.Huang, A.Karpathy,A.Khosla,M.Bernstein,A.C.Berg,andL.Fei-Fei.ImageNet  LargeScaleVisualRecognitionChallenge,2014. 3,5[46]R.Salakhutdinov,J.Tenenbaum,andA.Torralba.One-shotlearningwitha hierarchicalnonparametricbayesianmodel.2010. 1[47]V.Sharmanska,N.Quadrianto,andC.H.Lampert.Augmentedattributerepre- sentations.In ECCV,pages242Œ255.2012. 2[48]K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge- scaleimagerecognition. arXivpreprintarXiv:1409.1556 ,2014. 2,3[49]R.Socher,M.Ganjoo,C.D.Manning,andA.Ng.Zero-shotlearningthrough cross-modaltransfer.In NIPS,pages935Œ943,2013. 2[50]R.Socher,Q.Le,C.Manning,andA.Ng.Groundedcompositionalsemantics foranddescribingimageswithsentences.In TACL ,2014. 2[51]I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequencelearningwith neuralnetworks.In NIPS,pages3104Œ3112,2014. 2[52]D.Swingley.Fastmappingandslowmappinginchildren'swordlearning. LanguagelearningandDevelopment ,6(3):179Œ183,2010. 1[53]T.Tommasi,F.Orabona,andB.Caputo.Learningcategoriesfromfewexam- pleswithmultimodelknowledgetransfer. TPAMI ,36(5):928Œ941,2014. 1,2[54]R.Vedantam,C.L.Zitnick,andD.Parikh.Cider:Consensus-basedimage descriptionevaluation. arXivpreprintarXiv:1411.5726 ,2014. 2[55]O.Vinyals,A.Toshev,S.Bengio,andD.Erhan.Showandtell:Aneuralimage captiongenerator. arXivpreprintarXiv:1411.4555 ,2014. 2,6[56]J.Weston,S.Bengio,andN.Usunier.Largescaleimageannotation:learning torankwithjointword-imageembeddings. Machinelearning ,81(1):21Œ35, 2010.2[57]K.Xu,J.Ba,R.Kiros,C.A.Cho,Kyunghyun,R.Salakhutdinov,R.Zemel, andY.Bengio.Show,attendandtell:Neuralimagecaptiongenerationwith  visualattention. arXivpreprintarXiv:1502.03044 ,2015. 2[58]M.D.Zeiler.Adadelta:anadaptivelearningratemethod. arXivpreprint arXiv:1212.5701,2012. 4[59]J.Zhu,J.Mao,andA.L.Yuille.Learningfromweaklysuperviseddatabythe expectationlosssvm(e-svm)algorithm.In NIPS,pages1125Œ1133,2014. 22541  
Linear-TimeConstituencyParsingwithRNNs andDynamicProgramming JunekiHong 1 1 SchoolofEECS OregonStateUniversity,Corvallis,OR f juneki.hong,liang.huang.sh g @gmail.com LiangHuang 1 ; 2 2 SiliconValleyAILab BaiduResearch,Sunnyvale,CA Abstract Recently,span-basedconstituencyparsing hasachievedcompetitiveaccuracieswith extremelysimplemodelsbyusingbidirec- tionalRNNstomodelﬁspansﬂ.However, theminimalspanparserof Sternetal. ( 2017a )whichholdsthecurrentstateof theartaccuracyisachartparserrunningin cubictime, O ( n 3 ) ,whichistooslowfor longersentencesandforapplicationsbe- yondsentenceboundariessuchasend-to- enddiscourseparsingandjointsentence boundarydetectionandparsing.Wepro- posealinear-timeconstituencyparserwith RNNsanddynamicprogrammingusing graph-structuredstackandbeamsearch, whichrunsintime O ( nb 2 ) where b is thebeamsize.Wefurtherspeedthisup to O ( nb log b ) byintegratingcubeprun- ing.Comparedwithchartparsingbase- lines,thislinear-timeparserissubstan- tiallyfasterforlongsentencesonthePenn Treebankandordersofmagnitudefaster fordiscourseparsing,andachievesthe highestF1accuracyonthePennTreebank amongsinglemodelend-to-endsystems. 1Introduction Span-basedneuralconstituencyparsing( Cross andHuang , 2016 ; Sternetal. , 2017a )hasattracted attentionduetoitshighaccuracyandextreme simplicity.Comparedwithotherrecentneural constituencyparsers( Dyeretal. , 2016 ; Liuand Zhang , 2016 ; DurrettandKlein , 2015 )whichuse neuralnetworkstomodeltreestructures,thespan- basedframeworkisconsiderablysimpler,onlyus- ingbidirectionalRNNstomodelthe inputse- quence andnotthe outputtree .Becauseofthis factorization,theoutputspaceisdecomposable whichenablesefdynamicprogrammingal- gorithmsuchasCKY.Butexistingspan-based parserssufferfromacruciallimitationinterms ofsearch:ontheonehand,agreedyspanparser ( CrossandHuang , 2016 )isfast(linear-time)but onlyexploresonesinglepathintheexponentially largesearchspace,andontheotherhand,achart- basedspanparser( Sternetal. , 2017a )performs exactsearchandachievesstate-of-the-artaccu- racy,butincubictime,whichistooslowfor longersentencesandforapplicationsthatgobe- yondsentenceboundariessuchasend-to-enddis- courseparsing( Hernaultetal. , 2010 ; Zhaoand Huang , 2017 )andintegratedsentenceboundary detectionandparsing( Bj ¨ orkelundetal. , 2016 ). Weproposetocombinethemeritsofboth greedyandchart-basedapproachesanddesigna linear-timespan-basedneuralparserthatsearches overexponentiallylargespace.Following Huang andSagae ( 2010 ),weperformleft-to-rightdy- namicprogramminginanaction-synchronous style,with (2 n  1) actions(i.e.,steps)forasen- tenceof n words.Whilepreviousnon-neuralwork inthisarearequiressophisticatedfeatures( Huang andSagae , 2010 ; MiandHuang , 2015 )andthus hightimecomplexitysuchas O ( n 11 ) ,ourstates areassimpleas ` :( i;j ) where ` isthestepin- dexand ( i;j ) isthespan,modeledusingbidirec- tionalRNNswithoutanysyntacticfeatures.This givesarunningtimeof O ( n 4 ) ,withtheextra O ( n ) forstepindex.Wefurtheremploybeamsearch tohaveapracticalruntimeof O ( nb 2 ) atthecost ofexactsearchwhere b isthebeamsize.How- ever,onthePennTreebank,mostsentencesare lessthan40words( n< 40 ),andevenwithasmall beamsizeof b =10 ,the observed complexityof an O ( nb 2 ) parseris not exactlylinearin n (see Experiments).Tosolvethisproblem,weapply cubepruning( Chiang , 2007 ; HuangandChiang , 2007 )toimprovetheruntimeto O ( nb log b ) which rendersanobservedcomplexitythatislinearin n (withminorextrainexactness). Wemakethefollowingcontributions:  Wedesigntheneuralparserthatisboth lineartimeandcapableofsearchingoverex- ponentiallylargespace. 1  Wearethetoapplycubepruningtoin- crementalparsing,andachieves,forthe time,thecomplexityof O ( nb log b ) ,i.e.,lin- earinsentencelengthand(almost)linearin beamsize.Thisleadstoanobservedcom- plexitystrictlylinearinsentencelength n .  Wedeviseanovellossfunctionwhichpenal- izeswrongspansthatcrossgold-treespans, andemploymax-violationupdate( Huang etal. , 2012 )totrainthisparserwithstruc- turedSVMandbeamsearch.  Comparedwithchartparsingbaselines,our parserissubstantiallyfasterforlongsen- tencesonthePennTreebank,andorders ofmagnitudefasterforend-to-enddiscourse parsing.ItalsoachievesthehighestF1score onthePennTreebankamongsinglemodel end-to-endsystems.  Wedeviseanewformulationofgraph- structuredstack( Tomita , 1991 )whichre- quiresnoextrabookkeeping,provinganew theoremthatgivesdeepinsightintoGSS. 2Preliminaries 2.1Span-BasedShift-ReduceParsing Aspan-basedshift-reduceconstituencyparser ( CrossandHuang , 2016 )maintainsastackof spans ( i;j ) ,andprogressivelyaddsanewspan eachtimeittakesashiftorreduceaction.With ( i;j ) ontopofthestack,theparsercaneither shift topushthenextsingletonspan ( j;j +1) onthe stack,oritcan reduce tocombinethetoptwo spans, ( k;i ) and ( i;j ) ,formingthelargerspan ( k;j ) .Aftereachshift/reduceaction,thetop-most spanislabeledaseitheraconstituentorwitha null label ? ,whichmeansthatthesubsequenceisnota subtreeinthedecodedparse.Parsinginitial- izeswithanemptystackandcontinuesuntil (0 ;n ) isformed,representingtheentiresentence. 1 https://github.com/junekihong/beam-span-parser input w 0 :::w n  1 state ` : h i;j i : ( c;v ) init 0 : h 0 ; 0 i :(0,0) goal 2 n  1 : h 0 ;n i :( c , c ) shift ` : h ;j i :( c; ) ` +1: h j;j +1 i :( c + ˘;˘ ) j<n reduce ` 0 : h k;i i :( c 0 ;v 0 ) ` : h i;j i :( ;v ) ` +1: h k;j i :( c 0 + v + ˙;v 0 + v + ˙ ) Figure1:Ourshift-reducedeductivesystem.Here ` isthestepindex, c and v areandinside scores.Unlike HuangandSagae ( 2010 )and Cross andHuang ( 2016 ), ˘ and ˙ arenotshift/reduce scores;instead,theyarethe(best)labelscoresof theresultingspan: ˘ =max X s ( j;j +1 ;X ) and ˙ =max X s ( k;j;X ) where X isanonterminal symbol(couldbe ? ).Here ` 0 = `  2( j  i )+1 . 2.2Bi-LSTMfeatures Togetthefeaturerepresentationofaspan ( i;j ) , weusetheoutputsequenceofabi-directional LSTM( CrossandHuang , 2016 ; Sternetal. , 2017a ).TheLSTMproduces f 0 ;:::; f n forwards and b n ;:::; b 0 backwardsoutputs,whichwecon- catenatethedifferencesof ( f j  f i ) and ( b i  b j ) as therepresentationforspan ( i;j ) .Thiseliminates theneedforcomplexfeatureengineering,andcan bestoredforefqueryingduringdecoding. 3DynamicProgramming 3.1ScoreDecomposition Like Sternetal. ( 2017a ),wealsodecomposethe scoreofatree t tobethesumofthespanscores: s ( t )= X ( i;j;X ) 2 t s ( i;j;X ) (1) = X ( i;j ) 2 t max X s (( f j  f i ; b i  b j ) ;X ) (2) Notethat X isanonterminallabel,aunarychain (e.g.,S-VP),ornulllabel ? . 2 Inashift-reduce setting,thereare 2 n  1 steps( n shiftsand n  1 reduces)andaftereachstepwetakethebestlabel fortheresultingspan;thereforethereareexactly 2 Theactualcodebaseof Sternetal. ( 2017b )forces s ( i;j; ? ) tobe0,whichtheirCKYparserand slightlyimprovestheirparsingaccuracy.However,inour incrementalparser,thischangefavorsshiftoverreduceand degradesaccuracy,soourparserkeepsalearnedscorefor ? . 2 n  1 such(labeled)spans ( i;j;X ) intree t .Also notethatthechoiceofthelabelforanyspan ( i;j ) isonlydependenton ( i;j ) itself(andnotdepend- ingonanysubtreeinformation),thusthemaxover label X isindependentofotherspans,whichis anicepropertyofspan-basedparsing( Crossand Huang , 2016 ; Sternetal. , 2017a ). 3.2Graph-Struct.Stackw/oBookkeeping WenowreformulatethisDPparserintheabove sectionasashift-reduceparser.Wemaintainastep index ` inordertoperformaction-synchronous beamsearch(seebelow).Figure 1 showshow torepresentaparsingstackusingonlythetop span ( i;j ) .Ifthetopspan ( i;j ) shifts,itpro- duces ( j;j +1) ,butifitreduces,itneedstoknow thesecondlastspanonthestack, ( k;i ) ,whichis not representedinthecurrentstate.Thisproblem canbesolvedbygraph-structurestack( Tomita , 1991 ; HuangandSagae , 2010 ),whichmaintains, foreachstate p ,asetofpredecessorstates ˇ ( p ) that p cancombinewithontheleft. Thisisthewayouractualcodeworks( ˇ ( p ) is implementedasalistofpointers,orﬁleftpoint- ersﬂ),buthereforsimplicityofpresentationwe deviseanovelbuteasier-to-understandformula- tioninFig. 1 ,whereweexplicitlyrepresenttheset ofpredecessorstatesthatstate ` :( i;j ) cancom- binewithas ` 0 :( k;i ) where ` 0 = `  2( j  i )+1 , i.e., ( i;j ) atstep ` cancombinewithany ( k;i ) for any k atstep ` 0 .Therationalebehindthisnewfor- mulationisthefollowingtheorem: Theorem1 Thepredecessorstates ˇ ( ` :( i;j )) areallinthesamestep ` 0 = `  2( j  i )+1 . Proof. Byinduction. ThisTheorembringnewanddeepinsightsand suggestsanalternativeimplementationthatdoes notrequireanyextrabookkeeping.Thetimecom- plexityofthisalgorithmis O ( n 4 ) withtheextra O ( n ) duetostepindex. 3 3.3Action-SynchronousBeamSearch Theincrementalnatureofourparserallowsusto furtherlowertheruntimecomplexityatthecostof inexactsearch.Ateachtimestep,wemaintainthe top b parsingstates,pruningofftherest.Thus,a candidateparsethatmadeittotheendofdecod- inghadtosurvivewithinthetop b ateverystep. 3 Theword-synchronousalternativedoesnotneedthestep index ` andenjoysacubictimecomplexity,beingalmost identicaltoCKY.However,beamsearchbecomesverytricky. With O ( n ) parsingactionsourtimecomplexity becomeslinearinthelengthofthesentence. 3.4CubePruning However,Theorem1suggeststhataparsingstate p canhaveupto b predecessorstates(ﬁleftpoint- ersﬂ),i.e., j ˇ ( p ) j b because ˇ ( p ) areallinthe samestep,areduceactioncanproduceupto b subsequentnewreducedstates.With b itemson abeamand O ( n ) actionstotake,thisgivesusan overallcomplexityof O ( nb 2 ) .Eventhough b 2 is aconstant,evenmodestvaluesof b canmake b 2 dominatethelengthofthesentence. 4 Toimprovethisatthecostofadditionalinex- actness,weintroducecubepruningtoourbeam search,whereweputcandidateactionsintoaheap andretrievethetop b statestobeconsideredin thenexttime-step.Weheapifythetop b shift- mergedstatesandthetop b reducedstates.To avoidinsertingall b 2 reducedstatesfromthepre- viousbeam,weonlyconsidereachstate'shigh- estscoringleftpointer, 5 andwheneverwepopa reducedstatefromtheheap,weiteratedownits leftpointerstoinsertthenextnon-duplicatere- ducedstatebackintotheheap.Thisprocess isheswhenwepop b itemsfromtheheap.The initializationoftheheaptakes O ( b ) andpopping b itemstakes O ( b log b ) ,givingusanoverallim- provedruntimeof O ( nb log b ) . 4Training WeuseaStructuredSVMapproachfortraining ( Sternetal. , 2017a ; Shietal. , 2017 ).Wewant themodeltoscorethegoldtree t  higherthanany othertree t byatleastamargin  t;t  ) : 8 t;s ( t  )  s ( t )   t;t  ) : Notethat  t;t )=0 forany t and  t;t  ) > 0 forany t 6 = t  .Attrainingtimeweperformloss- augmenteddecoding: ^ t =argmax t s  ( t )=argmax t s ( t )+ t;t  ) : 4 TheaveragelengthofasentenceinthePennTreebank trainingsetisabout24.Evenwithabeamsizeof10,weal- readyhave b 2 =100 ,whichwouldbeafactorin ourruntime.Inpractice,eachparsingstatewillrarelyhave themaximum b leftpointerssothisendsupbeingaloose upper-bound.Nevertheless,thebeamsearchshouldbeper- formedwiththeinputlengthinmind,orelseas b increases werisklosingalinearruntime. 5 Ifeachpreviousbeamissorted,andifthebeamsearch isconductedbygoingtop-to-bottom,theneachstate'sleft pointerswillimplicitlybekeptinsortedorder. Figure2:Runtimeplotsofdecodingonthetrain- ingsetofthePennTreebank.Thedifferencesbe- tweenthedifferentalgorithmsbecomeevidentaf- tersentencesoflength40.Theregressioncurves havebeenempirically where s  (  ) istheloss-augmentedscore.If ^ t = t  ,thenallconstraintsared(whichimplies argmax t s ( t )= t  ),otherwiseweperformanup- datebybackpropagatingfrom s  ( ^ t )  s ( t  ) . 4.1Cross-SpanLoss Thebaselinelossfunctionfrom Sternetal. ( 2017a )countstheincorrectlabels ( i;j;X ) inthe predictedtree:  base ( t;t  )= X ( i;j;X ) 2 t 1  X 6 = t  ( i;j )  : Notethat X canbenull ? ,and t  ( i;j ) denotes thegoldlabelforspan ( i;j ) ,whichcouldalso be ? . 6 However,therearetwocaseswhere t  ( i;j ) = ? :asubspan ( i;j ) duetobinarization (e.g.,aspancombiningthetwosubtreesin aternarybranchingnode),oraninvalidspanin t thatcrossesagoldspanin t  .Inthebaseline functionabove,thesetwocasesaretreatedequiv- alently;forexample,aspan (3 ; 5 ; ? ) 2 t isnotpe- nalizedevenifthereisagoldspan (4 ; 6 ; VP) 2 t  . Sowereviseourlossfunctionas:  new ( t;t  )= X ( i;j;X ) 2 t 1  X 6 = t  ( i;j ) _ cross( i;j;t  )  6 Notethatthepredictedtree t hasexactly 2 n  1 spans but t  hasmuchfewerspans(onlylabeledspanswithout ? ). Figure3:Runtimeplotofdecodingthediscourse treebanktrainingset.Thelog-logplotontheright showsthecubiccomplexityofbaselinechartpars- ing.Whereasbeamsearchdecodingmaintainslin- eartimeevenforsequencesofthousandsofwords. where cross( i;j;t  )= 9 ( k;l ) 2 t  ,and i<k< j<l or k<i<l<j . 4.2MaxViolationUpdates Giventhatwemaintainloss-augmentedscores evenforpartialtrees,wecanperformatraining updateonagivenexamplesentencebychoos- ingtotakethelosswhereitisthegreatestalong theparsetrajectory.Ateachparsingtime-step ` , the violation isthedifferencebetweenthehigh- estaugmented-scoringparsetrajectoryuptothat pointandthegoldtrajectory( Huangetal. , 2012 ; Yuetal. , 2013 ).Notethatcomputingtheviola- tiongivesusthemax-marginlossdescribedabove. Takingthelargestviolationfromalltime-steps givesusthemax-violationloss. 5Experiments WepresentexperimentsonthePennTreebank ( Marcusetal. , 1993 )andthePTB-RSTdiscourse treebank( ZhaoandHuang , 2017 ).Inbothcases, thetrainingsetisshufbeforeeachepoch,and dropout( Hintonetal. , 2012 )isemployedwith probability0.4totherecurrentoutputsforregu- larization.Updateswithminibatchesofsize10 and1areusedforPTBandthePTB-RSTrespec- tively.WeuseAdam( KingmaandBa , 2014 )with defaultsettingstoschedulelearningratesforall theweights.Toaddressunknownwordsduring training,weadoptthestrategydescribedbyKiper- wasserandGoldberg( KiperwasserandGoldberg , 2016 );wordsinthetrainingsetarereplacedwith theunknownwordsymbol UNK withprobability p unk = 1 1+ f ( w ) ,with f ( w ) beingthenumberof DevelopmentSet22(F1) Baseline+cross-span Time ThisWorkBeam1 89.4189.93 0.042 ThisWorkBeam5 91.2791.91 0.050 ThisWorkBeam10 91.5692.09 0.058 ThisWorkBeam15 91.7492.16 0.062 ThisWorkBeam20 91.6592.20 0.066 Chart 92.0292.21 0.076 Table1:ComparisonofPTBdevelopmentset results,withthetimemeasuredinseconds-per- sentence.Thebaselinechartparserisfrom Stern etal. ( 2017b ),withnull-labelscoresunconstrained tobenonzero,replicatingtheirpaper. TestSet23 End-to-End&SingleModel LRLPF1 Socheretal. ( 2013 ) 90.4 DurrettandKlein ( 2015 ) 91.1 CrossandHuang ( 2016 ) 90.592.191.3 LiuandZhang ( 2016 ) 91.392.191.7 Dyeretal. ( 2016 )(discrim.) 91.7 Sternetal. ( 2017a ) 90.6392.9891.79 Sternetal. ( 2017a )+cross-span 91.6791.9491.81 Sternetal. ( 2017b ) 91.3592.3891.86 ThisWorkBeam10 91.4491.9191.67 ThisWorkBeam15 91.6492.0491.84 ThisWorkBeam20 91.4992.45 91.97 Reranking/Ensemble/SeparateDecoding Vinyalsetal. ( 2015 )(ensem) 90.5 Dyeretal. ( 2016 )(gen.,rerank) 93.3 ChoeandCharniak ( 2016 )(rerank) 93.8 Sternetal. ( 2017c )(sep.decoding) 92.5792.5692.56 Friedetal. ( 2017 )(ensem/rerank) 94.25 Table2:FinalPTBTestResults.Wecompareour modelswithother(neural)single-modelend-to- endtrainedsystems. occurrencesofword w inthetrainingcorpus.Our systemisimplementedinPythonusingtheDyNet neuralnetworklibrary( Neubigetal. , 2017 ). 5.1PennTreebank WeusetheWallStreetJournalportionofthePenn Treebank,withthestandardsplitofsections2-21 fortraining,22fordevelopment,and23fortest- ing.TagsareprovidedusingtheStanfordtagger with10-way Table 1 showsourdevelopmentresultsand overallspeeds,whileTable 2 comparesourtestre- sults.Weshowthatabeamsizeof20canbefast whilestillachievingstate-of-the-artperformances. 5.2DiscourseParsing Tomeasurethetractabilityofparsingonlonger sequences,wealsoconsiderexperimentsonthe LRLPF1 ZhaoandHuang ( 2017 ) 81.683.582.5 ThisWorkBeam10 80.4780.6180.54 ThisWorkBeam20 80.8680.7380.79 ThisWorkBeam200 81.5180.8481.18 ThisWorkBeam500 ? 81.5080.8181.16 ThisWorkBeam1000 ? 81.5580.8581.20 Table3:OveralltestaccuraciesforPTB-RSTdis- coursetreebank.Starred ? rowsindicatearunthat wasdecodedfromthebeam200model. segmentstructure+nuclearity+relation Bachetal. ( 2012 ) 95.1--- Hernaultetal. ( 2010 ) 94.072.359.147.3 ZhaoandHuang ( 2017 ) 95.478.865.052.2 ThisWorkBeam200 91.2073.3658.8746.38 ThisWorkBeam500 ? 93.5274.9360.1647.03 ThisWorkBeam1000 ? 94.0675.6060.6147.37 Table4:F1scorescomparingdiscoursesystems. ResultscorrespondtotheaccuraciesinTable 3 , brokendowntofocusonthediscourselabels. PTB-RSTdiscourseTreebank,ajointdiscourse andconstituencydatasetwithacombinedrep- resentation,allowingforparsingateitherlevel ( ZhaoandHuang , 2017 ).Wecompareourrun- timesout-of-the-boxinFigure 3 .Withoutany pre-processing,andbytreatingdiscourseexam- plesasconstituencytreeswiththousandsofwords, ourtrainedmodelsrepresentend-to-enddiscourse parsingsystems. ForouroverallconstituencyresultsinTable 3 , andfordiscourseresultsinTable 4 ,weadaptthe split-pointfeaturedescribedin( ZhaoandHuang , 2017 )inadditiontothebaseparser.Wethat largerbeamsizesarerequiredtoachievegooddis- coursescores. 6Conclusions Wehavedevelopedanewneuralparserthatmain- tainslineartime,whilestillsearchingoveranex- ponentiallylargespace.Wealsousecubeprun- ingtofurtherimprovetheruntimeto O ( nb log b ) . Fortraining,weintroduceanewlossfunction, andachievestate-of-the-artresultsamongsingle- modelend-to-endsystems. Acknowledgments WethankDezhongDengwhocontributedgreatly toSecs.3.2and4(hedeservesco-authorship),and MitchellSternforreleasinghiscodeandandsug- gestions.ThisworkwassupportedinpartbyNSF IIS-1656051andDARPAN66001-17-2-4030. References NgoXuanBach,NguyenLeMinh,andAkiraShimazu. 2012.Arerankingmodelfordiscoursesegmenta- tionusingsubtreefeatures.In Proceedingsofthe 13thAnnualMeetingoftheSpecialInterestGroup onDiscourseandDialogue .AssociationforCom- putationalLinguistics,pages160Œ168. AndersBj ¨ orkelund,AgnieszkaFale ´ nska,Wolfgang Seeker,andJonasKuhn.2016.Howtotraindepen- dencyparserswithinexactsearchforjointsentence boundarydetectionandparsingofentiredocuments. In Proceedingsofthe54thAnnualMeetingoftheAs- sociationforComputationalLinguistics(Volume1: LongPapers) .volume1,pages1924Œ1934. DavidChiang.2007.Hierarchicalphrase-basedtrans- lation. ComputationalLinguistics 33(2):201Œ208. DoKookChoeandEugeneCharniak.2016.Parsing aslanguagemodeling.In ProceedingsoftheCon- ferenceonEmpiricalMethodsinNaturalLanguage Processing .pages2331Œ2336. JamesCrossandLiangHuang.2016. Span-based constituencyparsingwithastructure-labelsystem andprovablyoptimaldynamicoracles .In Proceed- ingsofthe2016ConferenceonEmpiricalMeth- odsinNaturalLanguageProcessing .Association forComputationalLinguistics,Austin,Texas,pages 1Œ11. https://aclweb.org/anthology/D16-1001 . GregDurrettandDanKlein.2015.NeuralCRFpars- ing. arXivpreprintarXiv:1507.03641 . ChrisDyer,AdhigunaKuncoro,MiguelBallesteros, andNoahASmith.2016.Recurrentneuralnetwork grammars. arXivpreprintarXiv:1602.07776 . DanielFried,MitchellStern,andDanKlein.2017.Im- provingneuralparsingbydisentanglingmodelcom- binationandrerankingeffects.In Proceedingsofthe AssociationforComputationalLinguistics . HugoHernault,HelmutPrendinger,DavidADuVerle, MitsuruIshizuka,andTimPaek.2010.Hilda:adis- courseparserusingsupportvectormachine cation. DialogueandDiscourse 1(3):1Œ33. GeoffreyEHinton,NitishSrivastava,AlexKrizhevsky, IlyaSutskever,andRuslanRSalakhutdinov.2012. Improvingneuralnetworksbypreventingco- adaptationoffeaturedetectors. arXivpreprint arXiv:1207.0580 . LiangHuangandDavidChiang.2007.Forestrescor- ing:Fastdecodingwithintegratedlanguagemodels. In ProceedingsofACL2007 .Prague,CzechRep. LiangHuang,SuphanFayong,andYangGuo. 2012. Structuredperceptronwithinex- actsearch .In ProceedingsofNAACL . http://www.isi.edu/lhuang/perc-inexact.pdf . LiangHuangandKenjiSagae.2010.Dynamicpro- grammingforlinear-timeincrementalparsing.In ProceedingsofACL2010 .Uppsala,Sweden. DiederikPKingmaandJimmyBa.2014.Adam:A methodforstochasticoptimization. arXivpreprint arXiv:1412.6980 . EliyahuKiperwasserandYoavGoldberg.2016. Sim- pleandaccuratedependencyparsingusingbidi- rectionalLSTMfeaturerepresentations . CoRR abs/1603.04351. http://arxiv.org/abs/1603.04351 . JiangmingLiuandYueZhang.2016.Shift-reduce constituentparsingwithneurallookaheadfeatures. arXivpreprintarXiv:1612.00567 . MitchellPMarcus,MaryAnnMarcinkiewicz,and BeatriceSantorini.1993.Buildingalargeannotated corpusofenglish:Thepenntreebank. Computa- tionallinguistics 19(2):313Œ330. HaitaoMiandLiangHuang.2015.Shift-reducecon- stituencyparsingwithdynamicprogrammingand postaglattice.In ProceedingsofNAACL2015 . GrahamNeubig,ChrisDyer,YoavGoldberg,Austin Matthews,WaleedAmmar,AntoniosAnastasopou- los,MiguelBallesteros,DavidChiang,Daniel Clothiaux,TrevorCohn,KevinDuh,Manaal Faruqui,CynthiaGan,DanGarrette,YangfengJi, LingpengKong,AdhigunaKuncoro,GauravKu- mar,ChaitanyaMalaviya,PaulMichel,Yusuke Oda,MatthewRichardson,NaomiSaphra,Swabha Swayamdipta,andPengchengYin.2017.Dynet: Thedynamicneuralnetworktoolkit. arXivpreprint arXiv:1701.03980 . TianzeShi,LiangHuang,andLillianLee.2017. Fast(er)exactdecodingandglobaltrainingfor transition-baseddependencyparsingviaaminimal featureset.In ProceedingsofEMNLP2017(toap- pear) . RichardSocher,JohnBauer,ChristopherDManning, andAndrewYNg.2013.Parsingwithcomposi- tionalvectorgrammars.In ProceedingsoftheAs- sociationforComputationalLinguistics .Associa- tionforComputationalLinguistics,volume1,pages 455Œ465. MitchellStern,JacobAndreas,andDanKlein.2017a. Aminimalspan-basedneuralconstituencyparser. In ProceedingsoftheAssociationforComputational Linguistics . MitchellStern,JacobAndreas,andDanKlein. 2017b.Aminimalspan-basedneuralconstituency parser(codebase). https://github.com/ mitchellstern/minimal-span-parser . MitchellStern,DanielFried,andDanKlein.2017c. Effectiveinferenceforgenerativeneuralparsing.In ProceedingsofEmpiricalMethodsinNaturalLan- guageProcessing .pages1695Œ1700. MasaruTomita,editor.1991. GeneralizedLRParsing . KluwerAcademicPublishers. OriolVinyals,Kaiser,TerryKoo,SlavPetrov, IlyaSutskever,andGeoffreyHinton.2015.Gram- marasaforeignlanguage.In AdvancesinNeural InformationProcessingSystems .pages2773Œ2781. HengYu,LiangHuang,HaitaoMi,andKaiZhao. 2013.Max-violationperceptronandforceddecod- ingforscalablemttraining.In Proceedingsof EMNLP2013 . KaiZhaoandLiangHuang.2017.Jointsyntacto- discourseparsingandthesyntacto-discoursetree- bank.In Proceedingsofthe2017Conferenceon EmpiricalMethodsinNaturalLanguageProcess- ing .pages2117Œ2123.  
MultimodalKeylessAttentionFusionforVideo XiangLong 1 ,ChuangGan 1  ,GerarddeMelo 2 ,XiaoLiu 3 ,YandongLi 3 ,FuLi 3 ,ShileiWen 3 1 TsinghuaUniversity, 2 RutgersUniversity, 3 BaiduIDL f longx13,ganc13 g @mails.tsinghua.edu.cn,gdm@demelo.org, f liuxiao12,liyandong,lifu,wenshilei g @baidu.com Abstract Theproblemofvideotionisinherentlysequential andmultimodal,anddeepneuralmodelshenceneedtocap- tureandaggregatethemostpertinentsignalsforagivenin- putvideo.WeproposeKeylessAttentionasanelegantand efmeanstomoreeffectivelyaccountforthesequential natureofthedata.Moreover,comparingavarietyofmul- timodalfusionmethods,wethatMultimodalKeyless AttentionFusionisthemostsuccessfulatdiscerninginter- actionsbetweenmodalities.Weexperimentonfourhighly heterogeneousdatasets,UCF101,ActivityNet,Kinetics,and YouTube-8Mtovalidateourconclusion,andshowthatour approachachieveshighlycompetitiveresults.Especiallyon large-scaledata,ourmethodhasgreatadvantagesinef ciencyandperformance.Mostremarkably,ourbestsingle modelcanachieve77.0%intermsofthetop-1accuracyand 93.2%intermsofthetop-5accuracyontheKineticsvali- dationset,andachieve82.2%intermsofGAP@20onthe ofYouTube-8Mtestset. Introduction Videounderstandingisoneofthemostnaturalfundamen- talhumanabilities.Fromanearlyage,infantsbegintorec- ognizeandunderstandeventsbasedonstaticimagery,mo- tion,aswellassound.Althoughvideoisavery activeresearchareaincomputervisionaswellasmachine learning,thecurrentstate-of-the-artremainssubparincom- parisonwithhumanperformance. Unlikeimagetion,whichtakesstillpicturesas input,videoisinherentlymultimodal,andthus image,motion,aswellassoundcuesmaybenecessaryto makeacomprehensivejudgment.Forinstance,twomusi- calinstrumentsmaybediftodistinguishbasedontheir mereappearanceinavideo,butmightproduceratherdis- tinctsounds.Inthiscase,acousticfeaturescangreatlyim- provetheaccuracyofavideomodel. Asinotherareasofcomputervision,approachesbasedon deepconvolutionalneuralnetworks(CNNs)haveachieved state-of-the-artresults.However,theimprovementsbrought byCNNs,giventheirfocusonlocalpatterns,havenotbeen aspronouncedasforimages.Duetothetemporalsequential  Correspondingauthor. Copyright c  2018,AssociationfortheAdvancementofial Intelligence(www.aaai.org).Allrightsreserved. Figure1:Examplesofattentionweightsandcorresponding temporalsegments,wheredifferentmodalitiesmayfocuson differenttimeperiods. natureofvideos,whichcanbeverylong,recurrentnetworks (RNNs)maybeinvokedtobettercapturelonger-rangetem- poralpatternsandrelationships.However,existingend-to- endapproachesarerestrictedtosmall-scaledatasets.Itre- mainsverydiftocombineCNNandRNNmodeling forjointend-to-endtrainingdirectlyonlarge-scaledatasets suchasKineticsandYouTube-8M. Asimplerapproachtoaddressthisproblemistousea pre-trainedmodel,ortotrainsingle-modalityCNNmod- elsseparately.Multimodalfeaturescanthenmoreeasilybe extractedfromanewvideousingthetrainedmodels.This hasseveralcrucialadvantages:thisfacilitatestrans- ferlearning,suchasfromimageandaudio tovideoorfromonedataset toanother.Additionally,theextractedfeaturesare cantlysmallerinsizethantherawRGBframeandaudio data.Finally,large-scaledatasets,suchasYouTube-8M,of- tenalreadyprovidepre-extractedfeaturesinadvance,which greatlyfacilitatesandacceleratesresearchonsuchdata. Generally,eachfeaturethusextractedstemsfromapar- ticularlocaltimesegmentinthevideo.Thelocalfeaturesin chronologicalorderconstituteacompletefeaturesequence describingthevideo.Basedonthese,wecanusearecur- rentmodeltopredicttheoutputclassdistribution.Wepro- poseKeylessAttentionasanelegantandefattention mechanismtoachievethismoreeffectivelyandef. Asecondimportantobservationisthatwecanrepresent avideomoreadequatelybyextracting several multimodal featuresequences.Whilewecouldrelyonmultipleseparate RNNmodelsforthesedifferentfeaturesequences,itisnon- trivialtoconnectthem,asonecannotsimplypiecetogether thefeaturesoruseasimpleensemble. Figure1providesexamplesofimagesfordifferenttime segmentswithintwovideos,showinghowtheimportance ofeachmodalityvariesacrosstime.Forthetopexample, duringthebrushingprocess,theimageandmotionisthe clearest,andtheRGBandwfeaturesaremost Intermsofaudio,thesoundofbrushingisapparentlyinsuf- obvious,andthecontributionoftheaudiosignalis insteadgreatestwhenwashingthetoothbrush.Forthebot- tomexample,weencounterthegreatestweightofmotion featureswhilethepersonisrunning,whereasthegreatest weightsforRGBandaudiofeaturesareobservedduringthe jumpandlandingphase.Hence,weconcludethatdifferent modalitiesmaybepertinentatdifferenttimeperiods.Yet, therearealsocorrelationsbetweenimageandac- tion(top),orimageandsound(bottom),suchthatdifferent modalitiescannotbeconsideredindependently.Therefore, whereandhowtofusemultimodalRNNnetworksshallbe examinedingreatdetailinthispaper. Overall,wecansummarizethemaincontributionsofthis paperasfollows:  Weproposeasimpleandefattentionmechanism thateffectivelyaidsthetrainingoftheRNNmodel.  Weanalyzeandstudyavarietyoffusionmethodsformul- timodalRNN-basedarchitectures,andthatourpro- posedattention-basedfusionrobustlyachievesthebest results.  Weshowthatourproposedarchitectureperformsro- bustlyacrossfourhighlyheterogeneousvideo tiondatasets,includingdatasetswithbothtrimmedand longuntrimmedvideos,andsingle-labelaswellasmulti- labelsettings.Weachievehighlycompet- itiveresultsonboththestandardUCF-101andActivi- tyNetdatasets,aswellasthechallengingnewKinetics andYouTube-8Mcompetitions,forwhichthereleaseof theofresultsisstillpending. RelatedWork Video GiventhesuccessthatCNNshaveenjoyedforimageclassi- (Krizhevsky,Sutskever,andHinton2012;Szegedy etal.2015;SimonyanandZisserman2014a;Heetal.2016), theyhavealsobeenappliedtothetaskofvideo cation(Karpathyetal.2014;Ganetal.2015;Simonyan andZisserman2014b;Ganetal.2016b;Tranetal.2015; Ganetal.2016a;Varol,Laptev,andSchmid2017;Car- reiraandZisserman2017).Initially,2DCNNsweredirectly appliedtoRGBframesofvideos.Karpathyetal.studied multipleapproachestoextendtheconnectivityofaCNN acrossthetimedimensiontotakeadvantageoflocalspatio- temporalinformationbypoolingusingsingle,late,early,or slowfusion(Karpathyetal.2014).However,simplepool- ingdoesnotbringgainscomparedtothesingle framebaseline. Toovercometheshortcomingsof2DCNNsandbetter accountforspatio-temporalinformation,theopticalw method(Zach,Pock,andBischof2007)wasproposedto considerthevariationinthesurroundingframes.Simonyan etal.proposedamethodthatusesRGBandstackedopti- calwframesasappearanceandmotionsignals,respec- tively(SimonyanandZisserman2014b).Theyshowthatthe accuracyofactionrecognitionisyboostedeven bysimplyaggregatingprobabilityscores,whichindicates thatopticalwprovideshigh-qualitymotioninformation. However,duetotheinherentlimitationsoftheopticalw method,itcanonlycapturetemporallylocalinformation. AnothermethodtoobtainmotioninformationisC3D (Tranetal.2015),whichisanaturalextensionof2DCNNs. C3Drelieson3Dconvolutionkernelstocapturespatio- temporalinformation.Varoletal.foundthatexpandingthe temporallengthofinputsfor3DCNNscanachievebetter resultsandthatusingopticalwsasinputscanoutperform RGBinputs(Varol,Laptev,andSchmid2017).Carreiraet al.incorporatedtheInceptionarchitecture(Szegedyetal. 2015)into3DCNNs(CarreiraandZisserman2017). Manymethodsbasedontwo-streamCNNshavebeenpro- posedtoimprovetheaccuracyofactionrecognition.Feicht- enhoferetal.studiedanumberofwaysoffusingCNNsboth spatiallyandtemporallyinordertobesttakeadvantageof thisspatio-temporalinformation(Feichtenhofer,Pinz,and Zisserman2016).Theyalsocombinedtwo-streamCNNs withResNets(Heetal.2016)toshowthattheResNetar- chitectureiseffectiveforactionrecognitionwith2DCNNs (Feichtenhofer,Pinz,andWildes2016). CNNmethodsexcelatcapturingshort-termpatternsin short,ed-lengthvideos,butitremainsdiftodi- rectlycapturelong-terminteractionsinlongvariable-length videos.Recurrentneuralnetworks,particularlylongshort- termmemory(LSTM)(HochreiterandSchmidhuber1997) ones,havebeenconsideredtomodellong-termtemporalin- teractionsinvideoNgetal.proposedtwo- streamLSTMsforhigher-accuracyvideoover longertimeperiods(Ngetal.2015).Donahueetal.devised anend-to-endarchitecturebasedonLSTMsforvideoclassi- andcaptioning(Donahueetal.2015).Srivastavaet al.anLSTMinanunsupervisedmannerby usinganencoderLSTMtomapaninputsequencetoaed- lengthrepresentationandthendecodingittoreconstructthe inputsequenceortopredictthefuturesequence(Srivas- tava,Mansimov,andSalakhutdinov2015).Subsequently, theyadaptedthispre-trainedLSTMtovideo tasks.However,theaccuracyandthetrainingefyof RNN-basedmethodsforvideohasbeenunsat- isfactory,andhowtofusemultimodalRNNshasnotbeen studiedinsufdepth. AttentionMechanisms Humansrecognizeobjectsandeventsnotbyprocessingan entirevisualscenesimultaneously,butbyselectivelyfocus- ingonpartsofthescenethatprovidethemostpertinentin- formation.Attentionmodelswereproposedforobject recognitionwithrecurrentneuralnetworks,drawingonthe REINFORCEalgorithm.Inparticular,Mnihetal.applied attentiontoextractinformationfromanimagebyadaptively selectingasequenceofregionsandsequentiallyconsidering theselectedregionsatahigherresolution(Mnihetal.2014). Baetal.presentedanattention-basedmodeltothemost relevantregionsforrecognizingmultipleobjectswithinim- ages(Ba,Mnih,andKavukcuoglu2014). Softattentionmodelswereproposedtocopewithtasksin naturallanguageprocessingandvisualcaptioning.Inpartic- ular,Bahdanauetal.aimedatautomaticallycapturingsoft alignmentsbetweensourcewordsandtargetwordsinma- chinetranslation(Bahdanau,Cho,andBengio2014). Subsequently,thissoftattentionmodelwasappliedto videotasks.Sharmaetal.proposedaSoft- AttentionLSTMmodelbasedonmulti-layeredRNNsto selectivelyfocusonpartsofthevideoframesandclas- sifyvideosaftertakingafewglimpses(Sharma,Kiros,and Salakhutdinov2015).Lietal.proposedanend-to-endse- quencelearningmodelcalledVideoLSTM(Lietal.2016), whichhardwiresconvolutionsintheSoft-AttentionLSTM. ItstacksanotherRNNformotionmodelingtobetterguide theattentiontowardstherelevantspatial-temporalregions. However,thesecomplexattentionarchitecturesarehighly integratedwithRNNs,andneedtorecalculatetheattention weightmapateveryiteration.Theattentionmodelingthus addsaburdentothecomputationanddoesnot bringsufimprovementsinaccuracy. MultimodalRepresentation Videoisaninherentlymultimodalmedium,withbothvisual andsoundmodalities.Thevideosignal,moreover,canfur- therbedecomposedintostaticframesontheonehand,and signalscapturingcontinuousmotionontheother.Hence,us- ingfeaturesofasinglemodalityisclearlyinadequate.In thispaper,weusethefollowingmultimodalfeaturestomore thoroughlyrepresentthecontentsofavideo. VisualFeatures Werelyontwokindsofvisualfeatures,RGBandwfea- tures.RGBfeaturesareextracteddirectlyfromRGBim- ages,whilewfeaturesarebasedonopticalwimages createdbystackingthetwochannelsforthehorizontaland verticalvector(SimonyanandZisserman2014b).We relyondeepconvolutionnetworkstoextractthesefeatures. Themodelsareinitializedwithpre-trainedmodelfromIm- ageNetandusingthetrainingsplitofthecorre- spondingtargetdatasetbasedonthetemporalsegmentnet- workframework(Wangetal.2016a).Aftertraining,wecan Figure2:Ourvideomodelarchitecturebased onKeylessAttention,wherethereddottedlinesrepre- sentfourintegrationpointscorrespondingtodifferentmulti- modalfusionmethodsexaminedinthispaper.Fora integrationpoint,weduplicatethenetworkbeforetheinte- grationpoint K timesfor K differentmodalities,concate- natethevariablesattheintegrationpoint,andthenetwork aftertheintegrationpointremainsunchanged. extractframe-levelRGBandwfeaturesforeveryframe inthevideo. AcousticFeatures Wealsousedeepconvolutionalnetworkstoextractacoustic featuresbypreprocessingtherawaudiotoemitasequence ofmatrices.Theaudioisdividedintonon-overlapping 960msframes.Theframesarethendecomposedwitha short-timeFouriertransformevery10msandthenaggre- gated,logarithm-transformed,into64mel-spacedfrequency binsfollowing(Hersheyetal.2016).Thisyieldslog-mel 96  64 spectrogrampatches,whichcanberegardedasim- ages.Afterobtainingspectrogrampatches,wecanextract frame-levelacousticfeaturesjustasforvisualfeatures. Segment-LevelFeatures Despitehavingobtainedframe-levelvisualandacousticfea- tures,wedonotsimplyfeedtheseintoarecurrentnetwork. First,thenumberofframesinavideocanrangetoseveral thousands,whichmakesadirectapplicationofLSTMsvery challenging,asevenLSTMsoftenfailtocaptureparticu- larlylong-rangedependencies.Second,thefeaturesofsuc- cessiveframestendtobeverysimilar,anditisnotnecessary toinputallofthemintothenetwork.Third,alargenumber offrame-levelfeaturescanrequiretoomuchmemoryand makethetrainingprocessbothslowerandmoredif Wehenceusetemporaladaptivepoolingtoobtain segment-levelfeaturesforbothvisualandacousticsignals. ,weuse1Dadaptationmax-poolingtotrans- formalltheframe-levelfeaturesintoanequalnumberof segment-levelfeatures,suchthateachsegment-levelfeature correspondstoroughlyequal-lengthtemporalsegmentsof thevideo,andeachmodalityofagivenvideohasthesame numberoffeatures. ProposedNetworkArchitecture Inthissection,wedescribeourproposedapproachofMul- timodalKeylessAttentionFusionforvideo Figure2illustratesthearchitectureofourmodel.We introduceanovelkeylessattentionmechanism,andthende- scribeattention-basedmultimodalfusionindetail. KeylessAttention Ourcontributionisasimple,efandeffective attentionmechanismforvideoGivenase- quenceofinputvectors f a 1 ;a 2 ;:::;a n g ,whichwecall an- notationvectors ,thisattentionmechanismseekstocompute anoutputvector c givenbytakingtheexpectationoverthe annotationvectors: c = n X i =1  i a i (1) Theweightofeach a i iscomputedby: e i = w T a i (2)  i = exp( e i ) P n j =1 exp( e j ) (3) where w isalearnableparameterofthesamedimensionality astheannotationvectors.Forconvenience,wedenotethe keylessattentionmodel'soutputas c =KeylessAtt( f a i g ) . Existingsoftattentionmechanisms(Bahdanau,Cho,and Bengio2014)computetheweightnotonlybasedonthean- notationvectors,butrelyonanadditionalinput,suchasthe previoushiddenstatevectoroftheLSTM,oravectorrepre- sentingsometargetentity(Wangetal.2016b).Wereferto suchvectorsas keyvectors ,becausesoftattentionessentially seekstothemostrelatedweightedaverageofannota- tionsaccordingtothiskeyvector,anddifferentkeyvectors willresultindifferentweightedaverages.Unlikesoftatten- tionmechanisms,theweightsinourapproachonlydepend ontheannotations(whichwillbetheoutputsofthebidirec- tionalLSTMinthispaper).Becauseourattentionmecha- nismdoesnotrelyonkeyvectorsasinput,werefertoitas KeylessAttention. Therefore,theattentionvectoronlyneedstobecomputed onceforeachvideo,ratherthanateverytimestepinthe RNN.Theoperationtocomputetheattentionweightsinthis keylessattentionmodelisessentiallya1D-convolutionalop- erationwith1,whichisveryefbothintimeand space.Wehavealsoevaluatedothermethodstocomputethe attentionweights,butfoundthatmorecomplexarchitectures donotleadtoimprovements.Hence,wepickthissimpleand elegantmodelstructure. RecurrentVideoModelbasedon KeylessAttention OurarchitecturebuilduponabidirectionalLSTM(Hochre- iterandSchmidhuber1997)asshowninFigure2.Given aninputfeaturesequence ( x 1 ;x 2 ;:::;x T ) ,anLSTMunit computesthehiddenstate ( h 1 ;h 2 ;:::;h T ) viarepeatedap- plicationofthefollowingequations: i t = ˙ ( W i x t + U i h t  1 + b i ) (4) f t = ˙ ( W f x t + U f h t  1 + b f ) (5) o t = ˙ ( W o x t + U o h t  1 + b o ) (6) g t =tanh( W g x t + U g h t  1 + b g ) (7) c t = f t  c t  1 + i t  g t (8) h t = o t  c t ; (9) where ˙ (  ) isthesigmoidfunctionand  denoteselement- wisemultiplication.BidirectionalLSTMscomputetwosep- arateforwardandbackwardpassesyieldingtwosequences ofhiddenstates,whichwedenoteas h f t and h b t ,respectively, ateachtimestep t .Weobtaintheoutputateachtimestepof abidirectionalLSTMas h B t =[ h f t ;h b t ] ,where [  ] denotes theconcatenationofthestatevectors. Itischallengingtodirectlyfeedallthesestates ( h B 1 ;h B 2 ;:::;h B T ) tothenetworkforThisis becauseeachvideomaybeofadifferentlength,resulting inheterogeneousdimensionalities,andbecausethelength ofthevideomaybelarge,leadingtooverlyhighinputdi- mensionalities.Wethuscomputeaed-dimensionalglobal representation g .Forthis,werelyonourKeylessAttention mechanism: g =KeylessAtt( h B 1 ;h B 2 ;:::;h B T ) (10) Intuitively,thismechanismcanbeviewedastakinga globallookatthevideoandquicklyidentifyingthemost- contributingfeaturesfortheWethatthis leadstogainscomparedtousingtheaverageof statesorthestateofbidirectionalLSTMs. Afterobtainingtheglobalrepresentation g ,weapplysev- eralfullyconnectedlayers(FC)orabatchnormalization layer(BN)(IoffeandSzegedy2015)tocomputetheproba- bilitiesforclasses y i forthe i -thvideo. Forthesingle-labeldatasets(e.g.,UCF101, ActivityNet,andKinetics),weapplyaBNlayerfollowed byoneFClayerandasoftmax,andformulti- label(e.g.,onYouTube-8M),weapplytwose- quentialFClayerswith8192/4096hiddenunits,respec- tively,and tanh activation,followedbyaFC layerwithsigmoidactivation.Becausethepre-extractedfea- turesprovidedforYouTube-8Mhavebeenpreprocessedvia PCAfordimensionalityreductionfollowedbyquantization, thevariancebetweenfeaturesisnotlarge.Wehencedonot applybatchnormalizationtoYouTube-8M. Werepresentthegroundtruthclassesforthe i -thvideoas aone-hotvector ^ y i andthenumberofvideosinthetraining setisdenotedas N .Forsingle-labelwecan trainthemodelbyminimizingthelossfunction: 1 N N X i =1 y T i log(^ y i ) ; (11) whileformulti-labelweminimizethefollow- inglossfunction: 1 N N X i =1 [ y T i log(^ y i )+(1  y i ) T log(1  ^ y i )] (12) MultimodalFusion Previously,wehavedescribeavideomodelfor asinglesequenceoffeatures.However,inordertofullyex- ploitthemultimodalnatureofvideos,weneedtoaccount formultiplemodalities.Givenmultiplefeaturesequences asinput,howshallweperformmultimodalfusiontoobtain thebest-possibleresults?Weassumewearegiven K differ- entfeaturesequences ( x (1) 1 ;:::;x (1) T ) ;:::; ( x ( K ) 1 ;:::;x ( K ) T ) . Notethatthesearesegment-levelfeaturesasdescribedear- lier,andthateachfeaturesequenceisguaranteedtobeofthe samelength T . Weconsiderfourdifferentmultimodalfusionmethods correspondingtofourdifferentintegrationpoints(reddot- tedline)inFigure2. FeatureFusion :Asimplefeature-levelfusionisoneof themostintuitivemethods.Eachfeaturerepresentsapar- ticulartemporalsegmentinthevideo.Stitchingtogether thefeaturesofthesamesegmentleadstoamoredetailed representationofthattemporalsegment.Hence,wecan fusethesefeaturesequencesintoasinglefeaturesequence ( x 1 ;:::;x T ) byapplying x t =[ x (1) t ;:::;x ( K ) t ] andwecan performvideojustasforasinglefeaturese- quence. LSTMFusion :Thismethodissimilartofeature-levelfu- sioninthatitalsooperatesatthesegmentlevel.Eachin- putfeaturesequence ( x ( i ) 1 ;:::;x ( i ) T ) obtainsitsownhidden states ( h B ( i ) 1 ;:::;h B ( i ) T ) viaaseparatebidirectionalLSTM. Thenweobtainthefusedhiddenstates ( h B 1 ;h B 2 ;:::;h B T ) byapplying h B t =[ h B (1) t ;:::;h B ( K ) t ] .Afterthis,wecan computetheglobalrepresentationandinvokethesubsequent layersjustasforasinglefeaturesequence. AttentionFusion :Unliketheprevioustwofusionmeth- ods,attentionfusionoperatesatthelevelofvideos.First, weobtaintheglobalrepresentation g ( i ) foreachinputse- quencethroughabidirectionalLSTMandseparateKeyless Attentionmodels.Then,wefusetheglobalrepresentations as g =[ g (1) ;:::;g ( K ) ] andpredicttheclassesbyinvoking thesubsequentlayers.Notethatthisfusionmethoddoesnot requirethateachfeaturesequencebeofthesamelength. ProbabilityFusion :Finally,ProbabilityFusionisessen- tiallyanensemblemethod.Wetrainaseparatemodel foreachofthefeaturesequencesjustaswhenweonlyhave asinglefeaturesequence.Wethenaveragetheoutputsof K independentmodelsastheoutput.Inthisapproach, eachmodelcanonlyaccessfeaturesofasinglemodality, andthusinteractionsacrossmodalitiescannotbelearned. ExperimentalResults Inthissection,weproceedtoevaluateandcompareourpro- posedmethodsintermsoftheiraccuracy. Datasets Weevaluateourapproachonfourpopularvideo tiondatasets. UCF101 (Soomro,RoshanZamir,andShah2012)isa trimmedvideodataset,consistingofrealisticwebvideos withdiverseformsofcameramotionandillumination.It contains13,320videoclipswithanaveragelengthof180 framesperclip.Thesearelabeledwith101actionclasses, rangingfromdailylifeactivitiestounusualsports.Each videoclipisassignedjustasingleclasslabel.Followingthe originalevaluationscheme,wereporttheaverageaccuracy overthreetraining/testingsplits. ActivityNet (Heilbronetal.2015)isanuntrimmedvideo dataset.WeusetheActivityNetv1.3release,whichconsists ofmorethan648hoursofuntrimmedvideosfromatotalof around20Kvideoswith1.5annotationspervideo,selected from200classes.Videoscancontainmorethanoneactivity, and,typically,largetimesegmentsofavideoarenotrelated toanyactivityofinterest.Intheofsplit,thedistribu- tionamongtraining,validation,andtestdataisabout50%, 25%,and25%ofthetotalvideos,respectively.Becausethe annotationsforthetestingsplithavenotyetbeenpublished, wereportexperimentalresultsonthevalidationsplit. Kinetics (CarreiraandZisserman2017)isatrimmed videodataset.Thedatasetcontains246,535trainingvideos, 19,907validationvideos,and38,685testvideos,covering 400humanactionclasses.Eachcliplastsaround10sandis labeledwithasingleclass.Theannotationsforthetestsplit havenotyetbeenreleased,sowereportexperimentalresults onthevalidationsplit. YouTube-8M (Abu-El-Haijaetal.2016)ismassivelylarge untrimmedvideodataset.Itcontainsover1.9billionvideo framesand8millionvideos.Eachvideocanbeannotated withmultipletags.Visualandaudiofeatureshavebeenpre- extractedandareprovidedwiththedatasetforeachsec- ondofthevideo.Thevisualfeatureswereobtainedviaa GoogleInceptionCNNpre-trainedonImageNet(Denget al.2009),followedbyPCA-basedcompressionintoa1024- dimensionalvector.Theaudiofeatureswereextractedviaa pre-trainedVGG-inspired(SimonyanandZisserman2014a) network.Intheofsplit,thedistributionamongtraining, validation,andtestdataisabout70%,20%,and10%,re- spectively.Astheannotationsofthetestsplithavenotbeen releasedtothepublicandthenumberofvideosinthevalida- tionsetisoverlylarge,wemaintain60Kvideosfromtheof- validationsettovalidatetheparameters.Othervideos inthevalidationsetareincludedintothetrainingset.We reportexperimentalresultsonthis60Kvalidationsetandon theoftestset. ImplementationDetails ForUCF101andActivityNet,weextractbothRGBandw featuresusingaResNet-152(Heetal.2016)model.ForKi- netics,weextractRGBandwfeaturesusingInception- ResNet-v2(Szegedyetal.2016)andextractaudiofeatures withaVGG-16(SimonyanandZisserman2014a).Thenum- berofsegmentsweusedforis3forUCF101, and7forActivityNetandKineticstostrikeabalancebe- tweenrecognitionperformanceandcomputationalburden. Wemax-pooltheframe-levelfeaturesto5segment-level featuresforUCF101andKinetics,wherethelengthsof Method Accuracy(%) iDT+FV(WangandSchmid2013) 85.9 iDT+HSV(Pengetal.2016) 87.9 EMV-CNN(Zhangetal.2016) 86.4 TwoStream(SimonyanandZisserman2014b) 88.0 FSTCN(Sunetal.2015) 88.1 VideoLSTM(Lietal.2016) 89.2 TDD+FV(Wang,Qiao,andTang2015) 90.3 Fusion(Feichtenhofer,Pinz,andZisserman2016) 92.5 TSN(3seg)(Wangetal.2016a) 94.2 ST-ResNet+iDT(Feichtenhofer,Pinz,andWildes2016) 94.6 ActionVLAD(Girdharetal.2017) 93.6 Ours RGBCNN 85.4 RGBAverage 85.9 RGBLast 85.8 RGBAttention 86.2 FlowCNN 86.2 FlowAttention 87.0 FeatureFusion 94.1 LSTMFusion 94.2 ProbilityFusion 93.5 AttentionFusion 94.8 Table1:MeanaccuracyonUCF-101. videosareafewseconds,and20forActivityNet,where thevideolengthisrelativelylarge.Differentfrom(Wang etal.2016a),whichusedthetrimmedvideosfortraining, wetrainourmodelusingthewholeuntrimmedvideofrom ActivityNettoconsidermorerealisticinputconditions.For YouTube-8M,wedirectlyusetheprovidedpre-extracted RGBandaudiofeaturesassegment-levelfeatures,wherethe maximumnumberofsegmentis300. ThenumberofhiddenunitsfortheLSTMonUCF101, ActivityNet,andKineticsis512,whichisastandardchoice, whileforYouTube-8M,weuse1024tohandleitslonger videos. WerelyontheRMSPROPalgorithm(TielemanandHin- ton2012)toupdateparametersforbetterconvergence,with alearningrateof 0 : 0001 . ResultsforKeylessAttention Inthissection,weverifytheeffectivenessoftheproposed KeylessAttentionmechanism.Weperformallexperiments onasingleRGBfeaturesequence,inordertoeliminateany interferencebetweenmodalities.Ourproposedmodelwith KeylessAttention(RGBAttention)ishencethemodelde- scribedinSection. TheresultsoftheCNNmodelarereportedasRGBCNN, FlowCNN,andAudioCNNinthetables.Wealsocompare ourmethodwithtwoRNN-basedmethods: Themethod(RGBLast)istoconcatenatethetwo nalhiddenstatesofabidirectionalLSTM,andthenapplythe subsequentlayersforjustasfortheattention- basedrecurrentmodel.Thesecondmethod(RGBAverage) averagestheoutputsofthebidirectionalLSTMandthen theaveragedstatevector. TheexperimentalresultsoneachdatasetinTables1Œ3 showthatKeylessAttentionoutperformsbothRGBLastand RGBAverage.Moreover,duringtraining,wealsothat theuseofKeylessAttentionleadstoafasterconvergence speed,andvirtuallynoadditionalcomputetimeperbatch, suchthatwecantraininginashorterperiodoftime. WeconcludethattheproposedRNNmodelbasedonKey- Method mAP(%) iDT+FV(Pengetal.2016) 66.5 Depth2Action(ZhuandNewsam2016) 78.1 TwoStream(SimonyanandZisserman2014b) 71.9 C3D(Tranetal.2015) 74.1 Ours RGBCNN 70.9 RGBAverage 71.3 RGBLast 71.4 RGBAttention 72.0 FlowCNN 64.0 FlowAttention 64.8 FeatureFusion 77.5 LSTMFusion 77.6 ProbilityFusion 77.2 AttentionFusion 78.5 Table2:ActivityNetresultsonthevalidationset. lessAttentionisnotonlysimple,butalsoefandef- fective. ResultsforMultimodalFusion Inthissection,weanalyzetheexperimentalresultsanddis- cussthecharacteristicsofthefourproposedmultimodalfu- sionmethods: ProbabilityFusion: Asthisisafusionmethodthatcanbe regardedasaformofensembling,eachmodalityiscom- pletelyindependentofothers,andtheindependentlyob- tainedresultsarefusedinastep.Sincethedifferent modalitiesareonlycombinedatthestepofaverag- ingtheoutputprobabilitiesfromindependentmodels,we areunabletolearnanyinteractionsbetweenmodalities.This explainsthepoorresultsachievedbythisapproach. Fromtheexperimentalresults,asshowninTables1Œ4, wethatacrossallfourdatasets,theProbabilityFusion approachperformsworstamongthefourfusionmethods. Thisdifferenceisparticularlypronouncedonthetwolarge- scaledatasetsKineticsandYouTube-8M.OnKinetics,the differenceinaccuracybetweenProbabilityFusionandthe bestresultis2.1%,andonYouTube-8M,theGAP@20for ProbabilityFusionis1.8%lowerthanforthebestresult. Wehypothesizethat,onlargedatasets,wehavemoredata fromwhichonecanlearnmultimodalassociations,leading tomorepronouncedgaps. FeatureFusion: Thisisthemostintuitiveandeasytoimple- mentmethod.Itdirectlyconnectsmultiplemodalitieswithin eachlocaltimeinterval.However,inthiscase,theattention willselectperiodsoftimeglobally,neglectingthattheremay beaneedtofocusondivergingperiodsoftimefordiffer- entmodalities.Hence,thebidirectionalLSTMmayneedto learntoalignallthesalientsignalsacrossmodalities,while simultaneouslyalsoneedingtolearninteractionsbetween differentmodalitiesandtemporalrelationships.Thisburden maybetooheavy. Wecanobservethattheresultsobtainedbythisapproach onthefourdatasetsarebetterthanthoseofProbabilityFu- sion,butworsethanthebestresult.Thisillustratesthatthe FeatureFusionmethodcanlearnpartoftheassociationbe- tweenmodalities.Wealsoobservethatthelongerthese- quenceoffeatures(suchasforYouTube-8M,inwhichthe lengthofthevideocanbe300segments),themoreobvi- Method Top-1(%) Top-5(%) C3D(Tranetal.2015) 55.6 79.1 3DResNet(Hara,Kataoka,andSatoh2017) 58.0 81.3 Two-StreamI3D*(CarreiraandZisserman2017) 74.2 91.3 Ours RGBCNN 73.0 90.9 RGBAverage 73.2 91.1 RGBLast 73.0 91.0 RGBAttention 73.8 91.3 FlowCNN 54.5 75.9 FlowAttention 54.9 76.4 AudioCNN 21.6 39.4 AudioAttention 22.0 40.1 FeatureFusion 76.1 92.6 LSTMFusion 76.2 92.6 ProbilityFusion 74.9 91.6 AttentionFusion 77.0 93.2 Table3:Kineticsresultsonthevalidationset,exceptfor thosemarkedwith`  ',whicharebasedonthetestset. oustheperformancedrop.Weconjecturethatonlongerse- quences,theLSTMwillneedtoexpendmoreeffortinlearn- ingtemporalconnectionsratherthanfocusingonmodalin- teractions. LSTMFusion: ThismethodissimilartoFeatureFusion inthattheburdenoflearningassociationsacrossmodali- tiesandoftemporalconnectionswithinindividualmodali- tiesstillfallsontheLSTM. Theexperimentalresultsobtainedbythismethodareal- mostthesameasforFeatureFusion,orinsomecases slightlybetter,whichisinlinewithourexpectations. AttentionFusion: UnlikeFeatureFusionandLSTMFu- sion,thisapproachdoesnotforcethefeaturesofdifferent modalitieswithinthesametimeperiodtobelinked.Hence, theLSTMcanfocusonuncoveringtemporalpatternswithin individualmodalitiesandforeachmodalitycaneffortlessly payattentiontodifferenttemporalsegments.However,the AttentionFusionmethodisdifferentfromProbabilityFu- sioninthatthenetworkstillhastheopportunitytocapture interactionsbetweenmodalities. Figure1providesanexampleofattentionweightsforim- agesinthecorrespondingtemporalsegment.Weobserve thatRGB,opticalw,andaudiosignalscanindependently attendtodifferentareasofinterest. Consideringtheexperimentalresults,wethattheAt- tentionFusionmethodisthemosteffectiveacrossallof thefourdatasets.Thisfusionmethodcantakeintoaccount boththetemporalprogressionandmultimodalinteractions, whichshowstobethebestwayoffusingattention-basedre- currentcomponents.WerefertothismethodasMultimodal KeylessAttentionFusion. ComparisonwithState-of-the-Art Finally,wecompareourmethodagainstthestate-of-the-art methods.OnUCF101andActivityNet,wecompareitwith someoftheexistingpublishedtraditionalmethods(Wang andSchmid2013;Pengetal.2016)aswellasdeeplearn- ingapproaches(Zhangetal.2016;SimonyanandZisser- man2014b;Sunetal.2015;Lietal.2016;Wang,Qiao, andTang2015;Feichtenhofer,Pinz,andZisserman2016; Tranetal.2015;Wangetal.2016a;Feichtenhofer,Pinz,and Wildes2016;Girdharetal.2017).Ourapproachcanachieve Method 60KValid(%) Test(%) VLAD(Xu,Yang,andHauptmann2015) - 80.4 VideoLevel(Zhongetal.2017) - 78.6 LSTM+MoE(Wangetal.2017) - 80.2 Ours RGBAttention 76.6 77.3 AudioAttention 54.0 - FeatureFusion 80.5 81.5 ProbilityFusion 79.1 - AttentionFusion 80.9 82.2 Table4:YouTube-8MGAP@20onthe60Kvalidationand testset. competitiveresults. OnKinetics,wecompareitwithrecentlypublishedre- sults(Tranetal.2015;Hara,Kataoka,andSatoh2017; CarreiraandZisserman2017).Notethatresultof(Carreira andZisserman2017)wasreportedbasedonthetestset,for whichthetop-1resultsempiricallyare1.5%lowerthanthe top-1onthevalidationset.OnYouTube-8M,wecompareit withVLAD(Jegouetal.2010;Xu,Yang,andHauptmann 2015)andrecentlypublishedresults(Zhongetal.2017; Wangetal.2017).Ourapproachenablesourteamtoachieve verystrongresultsinboththeKineticsandYouTube-8M competitions.Tofacilitatefurthercomparison,Table4also providesthesinglemodelresultsontheofYouTube- 8Mtestset,ascomputedbythecompetitionorganizers. Unliketheserecentlypublishedworks,ourmodelro- bustlyachievescompetitiveresultsacrossarangeofdiffer- entdatasets,includingsmallerandlargertrainingsets,and trimmedanduntrimmedvideos. Conclusion Tobettercopewiththesequentialandmultimodalnature ofvideos,wehaveproposedakeylessattentionmecha- nism,whichallowsforfastandeffectivelearningofRNN models.Wefurtherassessseveralalternativesforachiev- ingmultimodalfusionwithrecurrentneuralnetworks,and thattheproposedattention-basedfusionachievesthe bestresults.Wehaveconductedexperimentsonfourwell- knowndatasets,includingbothuntrimmedandtrimmed videos,single-labelandmulti-labelsettings, andsmall-scaleaswellasverylarge-scaledatasets.Our highlycompetitiveresultsinallofthesesettingsdemon- stratethatourproposedMultimodalKeylessAttentionFu- sionframeworkisrobustacrossalargerangeofvideoclas- tasks. Intermsoffuturework,wehopetoapplythisfusionap- proachinend-to-end-trainedCNNandRNNarchitectures forfurthergains. Acknowledgments GerarddeMelo'sresearchissupportedinpartbythe DARPASocialSimprogram. References Abu-El-Haija,S.;Kothari,N.;Lee,J.;Natsev,P.;Toderici,G.; Varadarajan,B.;andVijayanarasimhan,S.2016.YouTube-8M: ALarge-ScaleVideoBenchmark. ArXiv . Ba,J.;Mnih,V.;andKavukcuoglu,K.2014.Multipleobjectrecog- nitionwithvisualattention. arXiv:1412.7755 . Bahdanau,D.;Cho,K.;andBengio,Y.2014.Neural machinetranslationbyjointlylearningtoalignandtranslate. arXiv:1409.0473 . Carreira,J.,andZisserman,A.2017.QuoVadis,ActionRecogni- tion?ANewModelandtheKineticsDataset. ArXiv . Deng,J.;Dong,W.;Socher,R.;andLi,L.J.2009.Imagenet:A large-scalehierarchicalimagedatabase.In CVPR ,248Œ255. Donahue,J.;Hendricks,L.A.;Rohrbach,M.;Venugopalan,S.; Guadarrama,S.;Saenko,K.;andDarrell,T.2015.Long-termre- currentconvolutionalnetworksforvisualrecognitionanddescrip- tion.In CVPR . Feichtenhofer,C.;Pinz,A.;andWildes,R.P.2016.Spatiotemporal residualnetworksforvideoactionrecognition.In NIPS . Feichtenhofer,C.;Pinz,A.;andZisserman,A.2016.Convolutional two-streamnetworkfusionforvideoactionrecognition.In CVPR . Gan,C.;Wang,N.;Yang,Y.;Yeung,D.-Y.;andHauptmann,A.G. 2015.Devnet:Adeepeventnetworkformultimediaeventdetec- tionandevidencerecounting.In CVPR ,2568Œ2577. Gan,C.;Sun,C.;Duan,L.;andGong,B.2016a.Webly-supervised videorecognitionbymutuallyvotingforrelevantwebimagesand webvideoframes.In ECCV ,849Œ866. Gan,C.;Yao,T.;Yang,K.;Yang,Y.;andMei,T.2016b.Youlead, weexceed:Labor-freevideoconceptlearningbyjointlyexploiting webvideosandimages.In CVPR ,923Œ932. Girdhar,R.;Ramanan,D.;Gupta,A.;Sivic,J.;andRussell,B. 2017.ActionVLAD:Learningspatio-temporalaggregationforac- tionIn CVPR . Hara,K.;Kataoka,H.;andSatoh,Y.2017.LearningSpatio- TemporalFeatureswith3DResidualNetworksforActionRecog- nition. ArXiv . He,K.;Zhang,X.;Ren,S.;andSun,J.2016.Deepresiduallearn- ingforimagerecognition.In CVPR . Heilbron,F.C.;Escorcia,V.;Ghanem,B.;andNiebles,J.C.2015. Activitynet:Alarge-scalevideobenchmarkforhumanactivityun- derstanding.In CVPR . Hershey,S.;Chaudhuri,S.;Ellis,D.P.W.;Gemmeke,J.F.;Jansen, A.;ChanningMoore,R.;Plakal,M.;Platt,D.;Saurous,R.A.;Sey- bold,B.;Slaney,M.;Weiss,R.J.;andWilson,K.2016.CNN ArchitecturesforLarge-ScaleAudio ArXiv . Hochreiter,S.,andSchmidhuber,J.1997.Longshort-termmem- ory. NeuralComputation 9(8):1735Œ1780. Ioffe,S.,andSzegedy,C.2015.BatchNormalization:Accelerat- ingDeepNetworkTrainingbyReducingInternalCovariateShift. ArXiv . Jegou,H.;Douze,M.;Schmid,C.;andPerez,P.2010.Aggregating localdescriptorsintoacompactimagerepresentation.In CVPR . Karpathy,A.;Toderici,G.;Shetty,S.;Leung,T.;Sukthankar,R.; andLi,F.F.2014.Large-scalevideowithconvolu- tionalneuralnetworks.In CVPR ,1725Œ1732. Krizhevsky,A.;Sutskever,I.;andHinton,G.E.2012.ImageNet withdeepconvolutionalneuralnetworks.In NIPS . Li,Z.;Gavves,E.;Jain,M.;andSnoek,C.G.M.2016.Video- LSTMConvolves,AttendsandFlowsforActionRecognition. ArXiv . Mnih,V.;Heess,N.;Graves,A.;etal.2014.Recurrentmodelsof visualattention.In NIPS ,2204Œ2212. Ng,Y.H.;Hausknecht,M.;Vijayanarasimhan,S.;Vinyals,O.; Monga,R.;andToderici,G.2015.Beyondshortsnippets:Deep networksforvideoIn CVPR . Peng,X.;Wang,L.;Wang,X.;andQiao,Y.2016.Bagofvisual wordsandfusionmethodsforactionrecognition:Comprehensive studyandgoodpractice. CVIU 150(C):109Œ125. Sharma,S.;Kiros,R.;andSalakhutdinov,R.2015.ActionRecog- nitionusingVisualAttention. ArXiv . Simonyan,K.,andZisserman,A.2014a.VeryDeepConvolutional NetworksforLarge-ScaleImageRecognition. ArXiv . Simonyan,K.,andZisserman,A.2014b.Two-streamconvolu- tionalnetworksforactionrecognitioninvideos. NIPS . Soomro,K.;RoshanZamir,A.;andShah,M.2012.UCF101:A Datasetof101HumanActionsClassesFromVideosinTheWild. ArXiv . Srivastava,N.;Mansimov,E.;andSalakhutdinov,R.2015.Unsu- pervisedlearningofvideorepresentationsusingLSTMs.In ICML . Sun,L.;Jia,K.;Yeung,D.Y.;andShi,B.E.2015.Human actionrecognitionusingfactorizedspatio-temporalconvolutional networks.In ICCV . Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;Anguelov,D.; Erhan,D.;Vanhoucke,V.;andRabinovich,A.2015.Goingdeeper withconvolutions.In CVPR ,1Œ9. Szegedy,C.;Ioffe,S.;Vanhoucke,V.;andAlemi,A.2016. Inception-v4,Inception-ResNetandtheImpactofResidualCon- nectionsonLearning. ArXiv . Tieleman,T.,andHinton,G.2012.Lecture6.5:RMSprop. Cours- era:NeuralNetworksforMachineLearning . Tran,D.;Bourdev,L.;Fergus,R.;Torresani,L.;andPaluri,M. 2015.Learningspatiotemporalfeatureswith3dconvolutionalnet- works.In ICCV . Varol,G.;Laptev,I.;andSchmid,C.2017.Long-termtemporal convolutionsforactionrecognition. IEEETransactionsonPattern AnalysisandMachineIntelligence PP(99):1Œ1. Wang,H.,andSchmid,C.2013.Actionrecognitionwithimproved trajectories.In ICCV . Wang,L.;Xiong,Y.;Wang,Z.;Qiao,Y.;Lin,D.;Tang,X.;and Gool,L.V.2016a.Temporalsegmentnetworks:Towardsgood practicesfordeepactionrecognition.In ECCV . Wang,L.;Cao,Z.;deMelo,G.;andLiu,Z.2016b.Relationclas- viamulti-levelattentionCNNs.In ACL . Wang,Z.;Kuan,K.;Ravaut,M.;Manek,G.;Song,S.;Fang,Y.; Kim,S.;Chen,N.;D'Haro,L.F.;Tuan,L.A.;Zhu,H.;Zeng,Z.; Cheung,N.M.;Piliouras,G.;Lin,J.;andChandrasekhar,V.2017. TrulyMulti-modalYouTube-8MVideowithVideo, Audio,andText. ArXiv . Wang,L.;Qiao,Y.;andTang,X.2015.Actionrecognitionwith trajectory-pooleddeep-convolutionaldescriptors.In CVPR . Xu,Z.;Yang,Y.;andHauptmann,A.G.2015.Adiscriminative CNNvideorepresentationforeventdetection.In CVPR . Zach,C.;Pock,T.;andBischof,H.2007.Adualitybasedapproach forrealtimetv-l1opticalw. DAGM 4713(5):214Œ223. Zhang,B.;Wang,L.;Wang,Z.;Qiao,Y.;andWang,H.2016. Real-timeactionrecognitionwithenhancedmotionvectorCNNs. In CVPR . Zhong,Z.;Huang,S.;Zhan,C.;Zhang,L.;Xiao,Z.;Wang,C.-C.; andYang,P.2017.AnEffectiveWaytoImproveYouTube-8M AccuracyinGoogleCloudPlatform. ArXiv . Zhu,Y.,andNewsam,S.2016.Depth2action:Exploringembedded depthforlarge-scaleactionrecognition.In ECCV ,668Œ684.  
ProceedingsofCOLING2012:TechnicalPapers ,pages3137Œ3152, COLING2012,Mumbai,December2012. 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151  
TacklingtheAchillesHeelofSocialNetworks: PropagationbasedLanguageModelSmoothing RuiYan BaiduInc. No.10,Shangdi10thStreet, Beijing100085,China yanrui02@baidu.com IanE.H.Yen Dept.ofComputerScience UniversityofTexasatAustin Austin,TX78712,USA ianyen@cs.utexas.edu Cheng-TeLi AcademiaSinica No128,AcademiaRoad, Taipei11529,Taiwan ctli@citi.sinica.edu.twShiqiZhao BaiduInc. No.10,Shangdi10thStreet, Beijing100085,China zhaoshiqi@baidu.comXiaohuaHu CollegeofInfo.Sci.andTech. DrexelUniversity Philadelphia,PA19104,USA xh29@drexel.edu ABSTRACT Onlinesocialnetworksnowadaysenjoytheirworldwideprosperity, astheyhaverevolutionizedthewayforpeopletodiscover,toshare, andtodistributeinformation.Withmillionsofregisteredusersand  theproliferationofuser-generatedcontents,thesocialnetworksbe- comeﬁgiantsﬂ,likelyeligibletocarryonanyresearchtasks.How- ever,thegiantsdohavetheirAchillesHeel:extremedatasparsity. Comparedwiththemassivedataoverthewholecollection,individ- ualpostingdocuments,(e.g.,amicrobloglessthan140characters),  seemtobetoosparsetomakeadifferenceundervariousresearch scenarios,whileactuallytheyaredifferent.Inthispaperwepro- posetotackletheAchillesHeelofsocialnetworksbysmoothing thelanguagemodelviapropagation.Weformulateaso- cializedfactorgraphmodel,whichutilizesboththetextualcorre-  lationsbetweendocumentpairsandthesocializedaugmentation networksbehindthedocuments,suchasuserrelationshipsandso- cialinteractions.Thesefactorsaremodeledasattributesandde-  pendenciesamongdocumentsandtheircorrespondingusers.An efalgorithmisdesignedtolearntheproposedfactorgraph model.Finallywepropagatetermcountstosmoothdocuments basedontheestimatedExperimentalresultson Twitter andWeibo datasetsvalidatetheeffectivenessoftheproposedmod- el.Byleveragingthesmoothedlanguagemodelwithsocialfactors,  ourapproachobtainsimprovementoverseveralalterna- tivemethodsonbothintrinsicandextrinsicevaluationsmeasured  intermsofperplexity,nDCGandMAPresults. CategoriesandSubjectDescriptors H.3.1[ InformationStorageandRetrieval ]:ContentAnalysis andIndexingŠ Textmining ;J.4[ SocialandBehavioralSciences ]:MiscellaneousCopyrightisheldbytheInternationalWorldWideWebConferenceCom- mittee(IW3C2).IW3C2reservestherighttoprovideahyperlinktothe author'ssiteiftheMaterialisusedinelectronicmedia. WWW2015, May18Œ22,2015,Florence,Italy. ACM978-1-4503-3469-3/15/05. http://dx.doi.org/10.1145/2736277.2741673. GeneralTerms Algorithms,Experimentation,Performance Keywords Languagemodelsmoothing;propagation;socialnetwork 1.INTRODUCTION AlongwiththeprosperityofWeb2.0,onlinesocialnetworks haveemergedasabrandnewmediuminspotlightandrevolution- izedthewayforpeopletodiscover,toshareandtopropagateinfor-  mationviapeer-to-peerinteractions[13].Amongthepopularweb- siteswhichprovidesocialservices,bignamescovernearlyevery cornerofonlinesociallifeandevents: Facebook 1encouragespho- to/video/blogsharingandleavingcommentsamongfriends, Twit- ter2providessuccinct,fastspreadingmicroblogsfornewsfeeds, Foursquare 3isalocationbasedmobilesocialnetworkwhichpro- videscommentsgenerallyaboutplacesofinterest. Inthissense,wearelivinginanerawithﬁgiantsﬂ:withincred- iblylargenumber(inbillions[1])ofafusersandtheprolif- erationofuser-generatedcontents,theonlinesocialnetworksare themightygiantsofourtimes,plausiblyinvincibleandeligibleto carryonanykindofresearchtasks.Infact,peoplehaveconducted aseriesofstudiesonsocialnetworks,suchassearchandretrieval  [31],informationrecommendation[36,5],linkprediction[12,7] andsummarization[16,38]. Althoughpowerfulasgiants,socialnetworksstillsufferfrom theirweakness:extremesparsity.Formanytextualrelatedre- searchessuchasretrieval,miningorrecommendationetc.,accurate  estimationofdocumentlanguagemodelsarelikelytobeanessen- tialparttothesuccessofnewmethodsandmodels.Moreobserved datagenerallyallowpeopletoestablishamoreaccuratestatistical  model.However,duetothereal-timepropagationonsocialnet- works,theposteddocumentsaresometimesoflimitedwith- inarestrictedlengthtofacilitatefastspreading(e.g.,140characters pertweetonTwitter).Inthiscase,wehavetoestimatethelanguage modelbasedonasmallpieceoftexts(e.g.,atweet),whilegiven 1https://www.facebook.com 2https://www.twitter.com 3https://www.foursquare.com limiteddatasampling,thelanguagemodelestimationusuallyen- counterswithseverezerocountproblem:themaximumlikelihood estimatorwouldgiveunseentermsazeroprobabilityratherthan areliableapproximation.Therefore,sparsityactuallybecomesthe AchillesHeelofsocialnetworksfortextsrelatedstudies,andwe aimattacklingthebottleneck. Statisticallanguagemodelshaveattractedmuchattentioninthe informationretrievalcommunityduetotheirsolidtheoreticalback-  groundaswellastheirsuccessinavarietyoftasks[25,14].Lan- guagemodelsmoothingisproposedtoaddressthesparsityprob- lem,andhasbeendemonstratedtoaffectretrievalperformancesig-  [40].Thequalityoftextualrelatedtasksheavilyrelies onpropersmoothingofthedocumentlanguagemodel.Although muchworkonlanguagemodelsmoothinghasbeeninvestigated,  theintuitionsbehindaregenerallyrelatedtotwomajorconcerns:1) semanticassociation[34,18,30]and2)positionalproximity[41, 19,34].However,forsocialnetworks,thereismuchmoreinfor- mationhiddeninsocialfactors,whichcouldbeutilizedtomeasure languagemodelsmoothing.Therefore,socialinteractioncouldbe  athirddirectiontoinvestigatebetterlanguagemodeling. Thekeyideaforlanguagemodelsmoothingistopropagateter- mcountsviacertainwaysofprojectiontootherplaceswherethey originallydonotreallyexist.Henceweproposeasocializedfactor graphmodeltoinvestigatevariousfactorswhichcouldhaveim-  pactsonlanguagemodels,andmeasurethepropagated alongthefactorgraph.Accordingtotheestimated,we propagatethetermoccurrenceindiscountedcountsandhences-  mooththeoriginallanguagemodels.Tothebestofourknowledge, wearetheonetomappingsocialontothetextualdi- mensiontofacilitatesocializedlanguagemodelsmoothing.Our1st contributionistofuseaseriesofsocialattributeswithtextualinfor- mationandformanintegratedobjectivefunctiontobalanceboth.  Intuitively,asmoothedlanguagemodelshouldenhancethecoher- encebetweentermswithsemanticassociation,andanalogouslyfor thosethroughsocialinteractions.Inotherwords,thetermsthat areassociatedviaclosefriendsbasedonthesociallinkagecould havesimilar(orsmoothed)probabilisticdistributions:strongerso-  cialtiescouldalsoleadtomoresimilarsmoothedlanguagemodels. Anothermaintechnicalchallengeliesinhowtotheat- tributes,factorsandformulatethepropagationfunctionstomodel  thejointprobabilisticfactorgraph.Weexploreseveraldifferent factorscapturedfrompostingdocumentpairsanduserpairs,and evaluatetheirdependenciesoneachother.Tobemorewe haveexaminedfeaturessuchastextsimilarity,textquality,social statusandsocialinteractionsandsoon,andthengroupedthemas  factorfunctions.Factorfunctionsareformulatedintoone objectivefunctiontocalculatetheestimatedandhenceto smooththelanguagemodelaccordingly. Inthispaper,wetheproblemofSocializedLanguage ModelSmoothing(SLMS)basedonthefactorgraphs.Weeval-  uatetheeffectivenessofourproposedlanguagemodelsmoothing methodusingtwodifferentsocialnetworkdatasetsfrom Twitter andWeibo 4.Bothofthemaremainstreammicroblogs,oneinEn- glishandtheotheroneinChinese.Weapplyintrinsicevaluation  measuredinperplexityandextrinsicevaluationmeasuredinnDCG andMAPinourexperimentalsetups.Experimentalresultsshow thatourproposedpropagationbasedlanguagemodels- moothingonfactorgraphsconsistentlyoutperformsthebaseline 4https://www.weibo.comsmoothingmethods,andtheimprovementiswhichin-  dicatestheeffectivenessofourapproach.Inotherwords,thesocial factorsalongwiththetextualinformationincombinationcouldto someextenttackletheAchillesHeelofsocialnetworks. Therestofthepaperisorganizedasfollows.Westartbyintro- ducingtheproblemandthenthepropagation  basedlanguagemodelsmoothingonfactorgraphs,usingtextual andsocialinformationincombination.Wedescribetheexperi- mentsandevaluationinSection4,reviewpreviousworksinSec- tion5anddrawtheconclusions. 2.PROBLEMFORMULATION Inthissection,weintroducesomenotationsandrepresentations necessaryforlanguagemodelingandsmoothingonsocialnetwork- s,andthenelaboratetheproblemofSocializedLanguageModel Smoothing(SLMS)viafactorgraphmodel. 1.(DocumentandCollection.) Givenaposting documentd0tosmooth,wehaveawholedocumentcollection D=fd1;d 2;:::;d ngasthebackgroundsettosmooth d0.Inthecontextofwebdocumentsonsocialnetworks,e.g.Twit- terorWeibo,etc.,eachpostingdocumentiswrittenbyapartic-  ularuser.Comparedwithtraditionaldocumentsthatare merelybasedonplaintexts,thesepostingdocumentsareassociated withmoreinterestingelements.Forinstance,socialnetworkssup- portparticular relationships establishedamongusers,i.e., follower- followeeonTwitter.Also,therearesocialinteractionsspreadbe- tweenthepostingdocuments,e.g., replying ,sharingandreposting documents.Wehencehaveahiddennetworkstructurebehindthe documentcollection.Useractivitiesimplicitlymoreinfor- mationbehindthedocumentstomodelthetextualproperties.We believethatthesocialrelationshipswouldhelptobetterdescribe  thetextinformation.Thus,integratingthedocumentcontentsand socialinformationcandiscloseamoreaccurateestimationofthe documentlanguagemodeltosmooth.Inthispaper,wemainlyem-  ploythemicrobloggingserviceasthebasisforourstudyandhence makeagoodutilizationofitscharacteristicsforillustration. ,givenapostingdocument di,anditsassociateduser ui,togetherwiththeassociatedusernetworks,wegivethefollow- ingofsocializedaugmentationnetwork. 2.(SocializedAugmentationNetwork.) Wehave theheterogeneousgraphofpostingdocumentsandtheircorrespond- ingusers.Wedenotethewholegraph Gasacollectionofnodes Vandedges E,andhave G=(V;E )=(Vu;V d;E u;E d;E u;d).It isobvioustoseethattherearethreekindsofrelationshipsasso-  ciated:1) (Vd;E d)isaweighteddirectedgraphbetweenposting documentpairs,where Vd=fdijdi2Dgisthepostingcollec- tionwithasizeof jDj,and Edisthesetofrelationships,indicating thefromsourcepostingstothetargetpostings,whichis ourgoaltoestimate;2) (Vu;E u)isalsoaweighteddirectedgraph indicatingthesocialtiesbetweenusers. Vu=fuijui2Vugisthesetofuserswithasizeof jVuj.Euisestablishedbytheso- cialbehavioramongusers,whichwillbedescribedinSection3.2; 3)(Vu;d;E u;d)istheunweightedbipartitegraphrepresentingau- thorshipofpostingdocumentsandusers. Vu;d=Vu[Vd.Edges inEu;dconnecteachpostingdocumentwithallofitsauthorsand helpmappingfromsocialdimensiontotextualdimension.Usually apostingdocument diswrittenbyonlyoneuser u.NowwegiveaformalofourproposedSLMSproblem:  Input:Giventheentiredocumentset D,andthesocialization augmentationnetworks,weaimtosmooththelanguagemodelof thetargetdocument,denotedas P(wjd0),basedonthe fromallotherdocuments diwherefdijdi2Dg.Output:Thesmoothedlanguagemodelof P(wjd+0)forevery originaldocument d0.Withthesepreliminaries,weshowthatrelationshipsfromdoc- umentpairs,userpairsanduser-documentpairscanbeallformu- latedandhencemappedastheinstancesonthefactorgraphusing potentialfunctionstoformacombinedobjectivefunction. 3.METHODOLOGY Inthissection,weproposeasocializedfactorgraphtocom- putepropagation,andformulatethesocializedlanguage modelsmoothingproblemintoalearningframework.The modelsimultaneouslyincorporatesallresources(i.e.,textsandso- cialinformation)intotheaugmentationcontextstogeneratehigh- qualityestimationfordocumentlanguagemodelsaftersmoothing. Thesocializedlanguagemodelsmoothingproblemcontainssev- eralsub-problems:1)documentpairmeasurement,2)us-  erpairmeasurement,and3)variablepairmea-  surement.Weaimtoquantifythecorrelationbetweendocument pairsbasedonsemanticassociationderivedfromcontents,while alsoweintendtoaugmentthepairwiserelationshipbetweendocu- mentsfromtheinteractionsofusersonsociallevel.Socialcontexts containrichinformationaboutthedocumentpairs.Wealsoanalyze  thedependencyofvariablesoneachotherbasedonthesameau- thorship.Finally,weusetheestimatedpropagationonthe documentstosmooththeoriginallanguagemodel,andthenapply toaseriesofrelatedresearchtasks.Theframeworkisillustratedin Figure1,andthedetailsareexplainedlater. 3.1ProposedModel Factorgraphassumesobservationdataarecohesiveonbothfea- turesandrelationshipsbetweeneachother[11].Ithasbeensuc- cessfullyappliedinmanyapplications,suchassocial  analysis[28,27],socialrelationshipmininig[7,33],andlinked datadisambiguation[12,32].Inthiswork,weformulatetheso- cialfeaturesandassociatednetworksintothefactorgraphmodel, whichisshowninFigure1.Giventhedocumentpairs,let Edbethepairwiselinksbetweentwopostingdocuments,and Eubethe usersocialties.Theinputofthefactormodelisthewholedoc-  umentcollectionandthesocializedaugmentationcontexts,anda targetdocument d0tosmooth.Bothpairsaredigestedintotheat- tributefactors,whichareobservable.Thereisalsoasetofhidden variables Y=fyigni=1,representingtheinferredfromthe observedpairsandcoordinationamongthehiddenvariables. Wetwofeaturefunctionsintheproposedfactormodel: attributefactor anddependencyfactor .AttributeFactor .Thefromapostingdocumentto anothercouldbeestimatedbysomeattributes(representedas x),whichrefertofeaturesthatareinherenttothedocumentsandtheir authors.Ingeneral,weaseriesoffeaturesforthedocument pairsanduserpairs.Thesefeaturesincludethetextualbasedcon-  tentssuchastextquality,similarityandpopularity,aswellasthe socialtiessuchasuserrelationships,interactions,authoritativeness andsoon.Detailsofthefeaturesaregiveninthenex-  tsection.Weusethefeaturefunction f(yi;xi)torepresentthe posteriorprobabilityoflabel yigiven xicontainedinthepairwise informationamongtheheterogenousnodes. Wethistypeofpotentialfunctionasalinearexponential functionandtoestimatetheofeachfeature,weintro- Figure1:Graphicalrepresentationofthesocializedaugmenta- tionfactorgraph.Theleftpartshowstheheterogeneousgraph, consistingofthedocumentcollectionanduserswithsocialties. Therightpartindicatesthedecomposablefactorgraph.The  toplayershowstheuserpairs,whichcouldbeinstantiatedinto severaluserpairsbydifferentdocumentpairsonthe middlelayer.Thelowerlayerindicatesthetoestimate betweendocumentpairs.Therearefactorfunctionswithinthe samelayer( g(:))andfactorfunctionsacrosslayers( f(:)).In thestep,Thelanguagemodelsaresmoothedbasedonthe estimatedonthelowerlayer.Somelinksbetweendoc- umentsandusersareomittedtokeepconcise. duceavectorofweightvariable foreachfeature c,andformally wecouldtheattributefactorsasthelocalentropyasfollows: fi(yi;xi)= 1ZexpfXccfi;c(yi;xi;c)g(1)wherexi;cisthe c-thattributetocalculatethe fc(:)isthefunctiontocalculatetheresultfromthe c-thfeatureand cisthecorrespondingweight. Zisanormalizationfactor. DependencyFactor .Asproposed,weintroducefactorsthat arecapableofhandlingmultiplehiddenvariablesonthevariable layer,tocharacterizethedependenciesamongthepostingdocu- mentsgeneratedbythesameuser.Thedependencyfactoristo propagatethesocialamongallpostingdocumentsfrom thesameuser.Theheuristicsisthatifdocument d0isby documentdi.Itishighlypossibletobebythedocument dkfromthesameuser,whichisactuallyabeliefpropagation. Tocapturethisintuition,wethepotentialfunctiontomod- elthecorrelationofacandidatevariable yiwithanothercandidate variable ykinthefactorgraph.Thefunctionisas: g(yi;y k)= 1Zikexpfkgk(yi;y k)g(2)whereg(:)isafunctionindicatingwhethertwovariablesarecorre- latedornot.Notethatif g(yi;y k)=0,therewillbenodependency betweenthetwovariables.Inotherwords,thetwovariablesare  notcorrelated.Actuallywecangroupthedocumentsetintoclus- tersandeachclusterisassociatedwithoneuser,andweuse Yitodenotetheclusterwhere yiisin.Hence,for 8yk2Yi,yihasdependencyon yk.WefurtherreviseEquation(2)as: gi(yi;Y i)= Yyk2Yig(yi;y k)= 1ZexpfXyk2Yikgk(yi;yk)g(3)Again, Zisthenormalizationfactor. Inthisway,theestimationcouldbeviewedasase- quencelabelingprocess[38],i.e.,thejudgmentonacertainpairis affectedbytheﬁsimilarﬂpairs(i.e.,thedocumentswrittenbythe  sameuserinthiswork),whichisexactlytheintuitionforlanguage modelsmoothing[21]. ObjectiveFunction .Ingeneral,theattributefactorscapture thepotentialfromdocument/userpairsandthedependen- cyfactorcapturescorrelationsbetweenvariables.InEquation(1)  wethefeatures fc(:)forallattributes,where cisthecorre- spondingweight.InEquation(3),wethecorrelationwhere  kindicatestheweights.Let YandXbethesetsofcandidatevari- ablesandattributevariablesrespectively,weaconditional probabilityencodedwithinthefactorgraphmodelbymultiplying allpotentialfunctionsandcanbewrittenas P(YjX)= Yifi(yi;xi)gi(yi;Y i)(4)Hence,byintegratingthefactorfunctions,andalsofol- lowingthelabelingassumption[10],wecanthefollowing log-likelihoodoveralltheundeterminedlabelsofallinstances,i.e., Y=fyigni=1,wheretheobjectivefunctionsumminguplikelihood: O()=log P(YjX)=XiXccfi;c(yi;x i;c)+ Xyk2Yikgk(yi;y k)logZ(5)Z=ZZisthenormalizationfactor,whichsumsupthecon- ditionallikelihoodof Poverallinstances. isthecollectionof parametersindicatingweights,i.e., =fg[f g.Calculatingthemarginaldistributionforeachfactor(inderiving thelog-gradientoftheobjectivefunction)requiresaloopysum- productinferencealgorithm.Withthelearnedparameters,wemay estimateanundeterminedbetweendocumentpairsinthe  testsetbyinferringthepropagatedandthensmoothlan- guagemodelsaccordingly.Theinferencealgorithmisintroduced insection3.3. 3.2Function Manyfeatureshavebeendesignedfordocumentcorrelationand socialassociationsamongusersinpreviousliterature.Inthispa- per,weinvestigate8featuresorfactors.Besides,somefeaturesthat arewidelyusedintraditionaltextanalysis,wealsoutilizeseveral featuresextractedfromusers'socialinteractions,forinstance,fol-  lowingbehaviors,socialstatus,andotherstatistics.Westartfrom thefeature 3.2.1Attributes Weusethepotentialfunctionsinthefactorgraphmodeltolearn thepotentialforadocument ditoshadeon d0.Referring toEquation(1),wetheattributefunctionsasfollows: TextSimilarity. Itisintuitivethatthetextualsimilaritybetween twodocumentsplayanimportantroleinlanguagemodelsmooth- ing[34,18,30].Similardocumentsshouldbesmoothedwithhigh- erweightssinceitismoreconsistentwiththeirexistingmodels.  Weusethecosinesimilaritybetweentwounigrammodels: fsim=d0dijjd0jjjjdijj(6)TextQuality. Wealsomeasurethetextqualityofdocumen- tdi.Itisnotagoodideatosmooththetargetlanguagemodel usingapieceoftextwithlowquality.HerebyweusetheOut-Of-  Vocabulary(OOV)ratiotomeasurethetextualquality.Thelower  OOVratio,thehigherqualitywouldbe.Againstthevocabulary fromtheofnewscorpora[42],OOVinmicroblogsoftenrefers toasetofmisspellings,informalterminologies,andirregularsym- bols.foov=1 jfwjw2(di\OOV )gjjdij(7)Technically,themeasurementof textquality isnotapairwise functionstrictlybetween d0anddi,butthecriteriaisindeedaprac- ticalindicatortodecidewhetherornottopropagatethe fromditod0.Wealsoincludesimilarcriteriaforuserpairs. PostingPopularity. Itisintuitivethatapopularpostingdocu- mentismorelikelytoonmanyotherpostingdocuments.  Weusetheaggregatednumbersofsocialinteraction(i.e.,replies andretweets)astheapproximationofpopularityfor di.SocialStatus. Sincemicro-bloggingservicerequiresnorecip- rocallinkageamongusers,itisnaturaltoassumethatthesocial  statusisasymmetricbetweentwousers.Afolloweeismorelikely tothefollowers.Thisfeatureisrepresentedbynominal  values,e.g.,`1'-theuserof d0followstheuserwhowrites di;`-1' -theuserof d0isfollowedbytheuserwhowrites di;`0'-thetwo usershavenodirectfollowbehaviors. UserSimilarity. Wepresumethatpeoplewithinthesamesocial circlewillhavealargerprobabilitytoeachother.Stil-  l,duetotheasymmetry,wemeasuretheJaccarddistanceofthe commonfolloweesoftwousersastheirsimilarity.Weusefunc- tionF(u)todenotethesocialcirclesetfortheuser u.The F(:)ofﬁfolloweeﬂcanbereplacedbyﬁfollowersﬂorextendedtoﬁfriendsﬂ. fusim=jF(author(d0))\F (author(di))jjF(author(d0))[F(author(di))j(8)InteractionStrength. Wealsoincludethestrengthofinterac- tionsbetweentheuserpairs.Itispossiblethatiftwousershave frequentsocialinteractions,theyarelikelytosharethewriting preferenceonthetermusage.Duetotheasymmetricalsocialre-  lationship,weonlycountthetimesfor author(d0)torepostfrom author(di)tomeasurehowlikelyfortheusertobe UserImpacts. Onsocialnetworks,someusersareintrinsically havemuchlargerontheothers,e.g.,sportsstars,polit- icalcelebrities,etc.Theirwordsareusuallycopied,quotedand spreaded.Therearemanydifferentwaystoevaluatetheuserim- pacts,whileweusetheclassicPageRank[23]scorestodenoteuser impacts.ThelinkagebasedPageRankalgorithmisquitesuitedto  thescenarioofuserimpactmeasurement.Withalargenumberof in-links,theuserisalmostguaranteedtohavehighsocialimpacts. 3.2.2Dependency Asforthedependencyfunctionbetweencandidatevariables,re- ferringtoEquation(3),wethefunction g(:)fortwocandi- datevariablesassociatedbythesameuserauthorshipin Yi,andlet thecorrespondingvariablesbe yiandyk,respectively.Thedepen- dencyfunctionaimsatencodingthepropagationbetween postingdocumentsfromthesameuser,asfollows. gk(yi;y k)= If author(di)= author(dk)g fsim(di;d k)=If author(di)= author(dk)g didkjjdijjjjdkjj(9)Thebooleanindicator I(:)is1when author(di)=author(dk)and0otherwise.Weassumethattwocandidatevariableswillhave highercorrelationiftheyrepresentdocumentswrittenbythesame  user.Thecorrelationshouldalsobeaffectedbytextsimilarity. 3.3ModelInference Therearetwoscenariosforparameterestimation.Ifwedonot haveanylabelsofvaluesamongthecandidaterandom variables,wecanonlyproposetooptimizethefactorgraphmodel viaatwo-stepiterativeprocedure.Atallofthemodelparam- eters(i.e., )arerandomlyinitialized.Giventheparameters,we inferthemarginalprobabilitiesofcandidatevariables.Then,given  themarginalprobabilities,weareabletoevaluateexperimentalper-  formance(Section4.3)withrespecttoasetofheld-outvalidation data,andsearchforbettermodelparametersuntilconvergence. Unfortunately,thisinferencealgorithmiswaytooindirect,de- pendingonexternalevaluationmetrics.Besides,ittakesalonger timetoconverge.Itisimpossibletomanuallylabelval-  ues,butwedohavepartiallabelsavailableduetothespecialphe- nomenonof repostings onTwitterandWeibo!Foreachretweeted pairs,theyhaveexactlythesamecontentsandhencewelabelthe as1.Withthepartiallylabeledvariables,wecannow estimatetheparameter =f gtomaximizethe log-likelihoodobjectivefunction O()=log P(YjX)approxi-matelywithgradientbasedmethods,e.g.,stochasticgradientde- scent[2]. =argmax O()(10)Algorithm1:Learningalgorithmonthefactorgraph Input:afactorgraph G,andthelearningrate .Output:Estimatedparameters =f g.Initializeallelementsin  1beginrepeatComputepotentialfunctionvalues Saccordingto Equations(1)-(9) Runinferencemethodusingcurrent toget P(YjX)for f g2 doComputegradientof @O()@usingSaccordingto Equation(13). @O()@=EP(YjYL)SEP(Y)SUpdateparameter withthelearningrate :=+@O()@untilConvergence ;BasedonEquations(5),theprobabilitydistribution P(YjX)canbewritteninamorebriefformatasfollows: P(YjX)= 1ZexpfXis(yi)g=1ZexpfSg(11)wheres(yi)=( fi(yi;xi);g i(yi;Y i))andS=Pis(yi).Inthisway,theobjectivefunctioncouldbewrittenas O()=log P(YLjX)=log XYjYL1ZexpfSg=log XYjYLexpfS g logXYexpfS g(12)whereYLdenotestheknownlabelsand YjYLisalabeling urationof Yinferredfrom YL.Inordertomaximizetheobjective function,weadoptthegradientdecentmethod.Wecalculatethe  gradientforeachparameter .@O()@=@(logPYjYLexpfS g logPYexpfS g)@=PYjYLexpS SPYjYLexpSPYexpS SPYexpS=EP(YjYL)SEP(Y)S(13)EP(YjYL)SandEP(Y)Saretwoexpectationsof S.Thevalue ofScanbeobtainednaturallyusingapproximatedinferencealgo- rithms,suchasLoopyBeliefPropagation(LBP)algorithm[39],  GibbsSampling[4]orContrastiveDivergence[3].Onechallenge hereisthatthegraphicalstructureinthemodelcanbearbitraryand  maycontaincycles,whichmakesitintractabletodirectlycalculate themarginaldistribution.WechoseLoopyBeliefPropagationdue toitseaseofimplementationandeffectiveness.Itshouldbenoted thattheproposedranked-marginalgorithmcanbeexploitednotjust forgraphicalmodels,butalsoforotherlearningmodelsaslongas  thegradientoftheexpecteddifferencecanbecalculated.Thegen- eralideaistousetwosteps,onestepforcalculating EP(YjYL)Sandtheotherstepforcalculating EP(Y)S,toestimatethegradient ofaparameter w.r.t.theobjectivefunction. Withthemarginalprobabilities,thegradientcanbeobtainedby summingoverallundeterminedlabels.Itisworthnotingthatwe  needtoperformtheLBPprocesstwiceineachiteration,onetime forestimatingthemarginaldistributionofunknownvariables yi=?,andtheothertimeformarginaldistributionoverallvariables. Finallywiththegradient,weupdateeachparameterwithalearning rate.ThelearningalgorithmissummarizedinAlgorithm1.After learningtheoptimalparameters ,weneedtoapplytheinference algorithmagaintocomputethemarginalprobabilitytoinferthe  unknownlabelsbyalabelwhichmaximizes theprobability P(YjX).Finally,eachnodeinthefactorgraphis assignedwithlabelthatmaximizesthemarginalprobability. Y=argmax YjYLP(YjX)(14)3.4DistributedLearning Asasocialnetworkmaycontainmillionsofusersandhundreds ofmillionsofpostingdocumentsgeneratedbyusers,itis  cialtolearnthefactorgraphfromthefullsocialnetworkdatausing multiplemachinesratherthanasingleone.Toaddressthischal-  lenge,wedeploythelearningtaskonadistributedsystemunder themap-reduceprogrammingmodel[6].Withineveryindividu- alframework,ourtargetistosmooththedocument d0basedon thesubsetofvariables(e.g.,groupofvariablesas Yi),whichmakesthegrapheasytodivide:itisnaturaltoextendthe  individual-centricmethodtoadistributedenvironment. Map-Reduceisaprogrammingmodelfordistributedprocessing oflargedatasets.Inthemapstage,eachmachine(calledaprocess Figure2:Distributedexperimentalenvironmentsetups.The wholegraphisdistributedtoseveralcomputingnodesandthe shadednodesindicateinactivedocumentpairsonthatnode. node)receivesasubsetofdataasinputandproducesasetofinter- mediatevalues.Inthereducestage,eachprocessnodemergesall intermediatevaluesassociatedwiththesametargetdocumentsand outputsthecomputationresults.Usersspecifyamapfunction thatgeneratesasetofintermediatevalues,andareducefunction thatmergesallintermediatevalues. Intheprocessofdistributedlearning,thewholegraphisdi- videdintoseveralsubgraphsthatareassignedtoslavenodes.Then  LBPisperformedoneachslavetocomputethemarginalprobabili-  tiesandtheparametergradient.Thereisamasternodecollectsand sumsupallgradientsfromsubgraphs,andupdatesparametersby gradientdescentmethod.Fordetails,pleasereferto[29,28]. 3.5LanguageModelSmoothing Giventheestimatedfromallotherpostingdocuments, wenowproposeaterm-levellanguagemodelsmoothingapproach basedonpropagation.Eachwordpropagatestheevi-  denceofitsoccurrencetootherdocumentsbasedontheestimat- edTocapturethe proximity heuristicsusedinlanguage modelsmoothing[21,19,30,34],weassignﬁclose-byﬂwordswith higherpropagatedcountsthanthoseoneswhichareﬁfarawayﬂ fromeachother.Inotherwords,mostpropagatedcountscome fromﬁnearbyﬂterms,whilehigherindicatescloserprox- imity[37,35,43].Theoretically,allpositionscanhaveafullvo- cabularywithdifferenttermdistributions:eachwordhasacertain  non-zeroprobabilitytooccurineachofthepostingdocument,asif allwordshadappearedwithavarietyofdiscountedcounts,while inpracticalscenarios,westillchoosetonoisytermswithex- tremelylowcounts. Ingeneral,wesmoothapostingdocumentusingthe backgroundinformationfromthedocumentcollection.Consider  thetraditionalwayofconcatenatingthedocumentlanguagemod- elP(w)andthebackground PB(w)inaweightedlinearcombi- nation,i.e., P0(w)=(1)P(w)+B(w).Thetradeoffisbal- ancedbythedampingfactor .Comparedwiththisarbitraryand equalweightingforalldocuments,oursocializedlanguagemodel smoothingbasedonpropagatedisina-granularity. Theideafortermprojectionisthatifaword woccursatanin- postingdocument,wewouldliketoassumethatthehighly documentwillalsohavethewordsoccurred,withadis- countedcount.Thelargertheestimated,thelargerthe propagatedtermcountstherewillbe.Notethateachpropagated counthasavaluelessthan1. Letd0=fw1;w 2;:::;w jd0jgwherejd0jisthelengthofthe document.Weuse c(w;d 0)todenotetheoriginaltermcountwith- indocument dbeforesmoothing.If wdoesnotoccurin d0,c(w;d 0)willbe0,whichisazerocountproblem.Wehavecalculatedthe  valuesof Y=fyigni=1fromthelastsection,indicatingthe fromthe ditothedocument d0tosmooth.Thefunctionactually servesasadiscountingfactorfortermsmeasuredinpairwise.We usec0(w;d 0)todenotethetotalpropagatedcountofterm wfromitsoccurrencesinallotherdocuments,i.e., c0(w;d 0)=(1 )c(w;d 0)+ Xyi2Yyic(w;d i)(15)Evenif c(w;d 0)is0, c0(w;d 0)maybegreaterthan0. Basedontermpropagation,wehaveatermfrequencyvector fc0(w1;d 0);:::;c 0(wv;d 0)gforthevirtualdocument d+0extend- edfromdocument d0.Wecanseethatterminformationwithcal- culatedhasbeenstoredinthisvector.Thusthelanguage modelofthisnew smoothedvirtualdocumentcanbeestimatedas P(wjd+0)= c0(w;d +0)Pw02Vc0(w0;d+0)(16)whereVisthevocabularysetand Pw02Vc0(w0;d +0)isthelength ofthevirtualdocumentaftersmoothing. 4.EXPERIMENTSANDEVALUATION 4.1DatasetsandExperimentalSetups Utilizingthedatacrawledfromtheonlinesocialnetworksthrough thefollowinglinkage,weconstruct2datasetsofmicroblogsand thecorrespondingusers,whichformtheheterogeneousnetwork.  ThecrawlermonitoredTwitterdatafrom3/25/2011to5/30/2011, andWeibodatafrom9/29/2012to11/30/2012.Weuseroughly onemonthasthetrainingsetandtherestastestingset.Thedetails ofthedataarelistedinTable1and#Repostdenotesthenumber ofreposting(namelyﬁretweetingﬂinTwitterorﬁ E¢ ﬂinWeibo) whichweusedaslabelsinourtraining. Table1:Statisticsofthesocialnetworkdatasets. #User#Document.#Link#RepostLangTwitter9,449,542364,287,744596,777,49155,526,494ENWeibo3,923,021216,302,309258,543,931101,024,128CNPre-processing. Basically,thesocialnetworkfactorgraphcan beestablishedfromallpostingdocumentsandallusers,however,  thedataisnoisy.Wethepointlessbabbles[1]byap- plyingthethelinguisticqualityjudgements(e.g.,OOVratio)[24], andthenremoveinactiveusersthathavelessthanonefolloweror  followeeandremovetheuserswithoutanylinkagetotheremain- ingpostingdocuments.WeremovestopwordsandURLs,perform stemmingandsegmentation(forChinesetexts),andbuildthegraph afterandrunLBPtoobtainthemarginalprobabilities.We establishthelanguagemodelsmoothedbytheestimated HashtagClustersNumbersNotes1.apple42,528Tech:appleproducts 2. 40,340Sport:Americanfootball 3.travel 38,345Generalinterst4.mlb 38,261Sport:baseball5.fashion 30,053Generalinterest1.72,184TVshow:voiceofChina 2.71,169Food:Chinesefoods 3.63,154Tech:Microblogservice 4.57,783TVdrama:culture 5.49,428Tech:smartphone Table2:Clustersofhashtagtopicsexploredinourstudy. 4.2AlgorithmsforComparison Toillustratetheperformanceofourapproach,weimplemen- tseveralalternativealgorithmsasbaselinestocomparewithour method.Thebaselinesincludenaivesmoothing,smoothingbyse- mantics,andpositionalsmoothingfromveryrecentstudies.For fairnessweconductthesamepre-processingproceduresforallal- gorithms.Thebaselineisbasedonthetraditionallanguagemodel: LMisthelanguagemodelwithoutsmoothingatall.Weinclude theplainsmoothingof Additive (alsoknownasAdd- )smoothing andAbsoluteDiscouting whichdecreasestheprobabilityofseen wordsbysubtractingaconstant[22].Wealsoimplementsever- alclassicstrategiessmoothedfromthewholecollectionasback-  groundinformation: Jelinek-Mercer appliesalinearinterpolation, andDirichletemploysaprioroncollection[40,14]. Beyondthesesimpleheuristics,wealsoexamineaseriesofse- manticbasedlanguagemodelsmoothing.Themostrepresentative twosemanticsmoothingmethodsaretheCluster-BasedDocument Model( CBDM)proposedin[18],andtheDocumentExpansion LanguageModel( DELM)in[30].Bothmethodsusesemantically similardocumentsasasmoothingcorpusforaparticulardocumen-  t:CBDMclustersdocumentsbeforehandandsmoothsadocument withtheclusterwhereitbelongsto,whileDELMnearest neighborsdynamicallyforthedocumentasthesmoothingcluster.  However,bothmethodsareonlybasedondocument-levelsemantic similarity.WealsoincludePositionalLanguageModel( PLM)pro- posedin[19],whichisthestate-of-artpositionalproximitybased  languagesmoothing.PLMmainlyutilizespositionalinformation withoutsemanticinformation.Weimplementedthebestreport-  edPLMWecompareourproposedsociallanguage modelsmoothing( SLMS)againstthesebaselines. 4.3EvaluationMetric Itisgenerallydiftoexaminetheeffectoflanguagemodel directly.Formostofthelanguagemodelsmoothingresearch,the performanceismeasuredbasedonextrinsicevaluations(e.g.,re- trieval)[34,30,19].Weincludetwoextrinsicevaluationsinthis study,i.e.,standardpostingdocumentretrievalandrecommenda-  tion,butweaimtoevaluatetheinformationcontainedinthe languageitself.Henceweuselanguage perplexity toevaluatethe smoothedlanguagemodel. 4.3.1IntrinsicEvaluation Oursetofexperimentsinvolvedintrinsicevaluationofthe ﬁperplexityﬂapproachbasedonaclusteringscenario.Theexperi- mentalprocedureisasfollows:wemanuallyselected10topics(5 foreachdataset)basedonpopularity(measuredinthenumberof postings)andtoobtainbroadcoverageofdifferenttypes:sports, technology,cultures,andgeneralinterests.Thesetopicsareshown inTable1.Wegroupthepostingdocumentswiththesamehashtag `#'intoclusters,andthenweremovethehashtagsandcomputeits  perplexity withrespecttothecurrenttopic,as pow 2;1NXwi2VlogP(wi)Perplexityisactuallyanentropybasedevaluation.Inthissense, thelowerperplexitywithinthesametopiccluster,thebetterper-  formanceinpuritythetopicclusterwouldhave. 4.3.2ExtrinsicEvaluation Inadditiontotheintrinsicperplexity-basedmeasurementson hashtagclusters,wealsoevaluatetheeffectivenessofoursmoothed languagemodelsonthetasksofmicroblogsearch,andinformation recommendation.Hereareafewmoredetailsaboutourexperi- mentalsetups.Fortheretrievaltask,toavoidthelaboriouswork ofbuildingatestcollectionbyhand,wefocusourevaluationef-  fortsondocumentsthatcontainedatleastonehashtag.Giventhe 10topicsmentionedabove,weprocessalldocumentswithhash- tagsasfollows:thegroundtruthlabels(i.e.,thehashtags)are  removedfromthedocuments.Wethenusethehashtagtermsas queriestosearchforrelevantpostingdocuments.Theonesorig- inallywiththehashtagareregardedasrelevantwhileothersnot. Notethat,theretrievalperformanceunderthisexperimentalsetting istosomeextentalowerbound,sincesomeoftheretrieveddocu-  mentscouldbefalsenegative:theydonotcontainthehashtagbut theyareindeedrelevant. Forthemicroblogrecommendationtask,weapplytherecom- mendationframeworkdescribedin[36]usingthesameexperimen- talsetups.Forauser,werecommendpostingdocuments  basedontheirpreviouspostingdocumentsandtheirsocialbehav- iorsusingthegraphco-rankingalgorithm[36].Thelanguagemod- elsforthedocumentsareestablishedaftersmoothing,andtherec-  ommendationlistpredictswhichdocumentswillberepostedinthe testdata.Again,tosavetheeffortsbyhumanevaluators,weuse theautomaticevaluationofmicroblogrecommendation.Theper- formanceisevaluatedbycomparingwiththegroundtruth,which indicateswhetherthepostinghasbeenretweetedornot.Alsothe  automaticevaluationsketchedabovedoesnotassessthefullpo- tentialoftherecommendationsystem.Forinstance,itispossible forthealgorithmtorecommenddocumentsofinteresttousersbut  withoutrecommendation,thedocumentsareoutsideoftheirscope. Forboththeretrievalandtherecommendationtask,wereturnthe resultsasarankinglistgivenasearch queryoradesignated user,andtherankinglistischeckbyexaminingthe relevant documentsorthe reposted documents.Wemeasuredrankingperformanceus- ingthenormalizedDiscountedCumulativeGain(nDCG)[9]. nDCG (k)= 1NXjj1ZkXi=12ri1log(1+ i)whereNdenotesthetotalnumbersofqueriesorusers( =qforqueriesand =uforusers), kindicatesthetop- kpositionsina rankedlist,and Zisanormalizationfactorobtainedfromaper- fectrankingforaparticularquery/user. riisthejudgescore(i.e., 1:relevant/reposted,0:irrelevant/unretweeted)forthe i-thposting documentintherankinglistforthequery/user. TopicEN-1EN-2EN-3EN-4EN-5 CN-1CN-2CN-3CN-4CN-5 LM15851113561067675848257 223061744110204168879237 Additive 15195100351034271987924 191391622110108163429003 Absolute15323101231037972308093 19403169329984166819111 Jelinek-Mercer14115100111018598188003 200251620110049160018728 Dirichlet1389295161013871247345 19712163619119158868550 PLM1373099251042669137512 19965152309865142198981 CBDM129319845931168937510 19129151949323151137906 DELM118539820951371337348 18809141659510139857621 SLMS10788?9539?8408?5817?7109?18169?15375?9194?13212?6919?Table3:Perplexityoflanguagemodelsunderdifferenthashtagtopicclusters.` ?'indicatesthatweaccepttheimprovementhypoth- esisofSLMSoverthebestrivalbaselinebyWilcoxontestatalevelof0.01. WealsoevaluatethesystemintermsofMeanAveragePrecision (MAP)[20]underasimilarjudgeassumptionasabove: MAP =1NXjj1ZkXi=1PiriHereNisthenumberofdocumentsassociatedwiththequery oruser, Zisthenumberofrelevantdocumentsretrievedorrec- ommended,and Piistheprecisionat i-thpositionforthequery/user. 4.4OverallPerformance Wecomparetheperformanceofallmethodsoflanguagemodel smoothinginthetwodatasets,measuredintheintrinsicevaluation ofperplexity,aswellastheextrinsicevaluationofretrievalandrec- ommendation.Table3-5listtheoverallresultsagainstallbaseline methods.OurproposedmethodSLMSshowsclearlybetterper- formancethanthebaselinemethods.Onaverage,SLMSachieves  anaverage+23.8%improvementcomparedwithothermethodsin termsofnDCGandMAP,andanaverage-18.6%improvementin termsoflanguageperplexityinhashtagtopicclustering.Thead-  vantageofourproposedmethodmainlycomesfromthetwodimen- sionsoftextualandsocialpropagatedthrough thedocumentsonsocialnetworks.Weuseamyriadofattribute factorsanddependenciestocontrolthepropagationon thefactorgraphtomakeamorereliableestimation. Languagemodelwithoutanysmoothingperformsworstasex- pected,andonceagaindemonstratesthesevereweaknessofdata  sparsityonsocialnetworks-the AchillesHeel !Simpleintuition basedmethodssuchasadditivesmoothingdoesnothelpalot,since itonlyarbitrarilythegiventermcountsstraightforwardto avoidzerooccurrence,whichisprovedtobeinsufAbso- lutesmoothinghasacomparableperformanceasadditive,dueto thesimilarideatoreducetermcountsnaively.Jelinek-Mercerand  Dirichletmethodsaremoreusefulsincetheyincludetheinforma- tionfromthewholecollectionasbackgroundlanguagemodels,but theyfailtodistinguishdocumentsfromdocumentsanduseallof themequallyintosmoothing.PLMoffersastrengthenedlanguage modelsmoothingstrategywithineachpostingdocumentbasedon  positions,andsmooththetermsoutsideofthepostingdocument formulatingthebackgroundcollectionintoaDirichletprior.The performanceofCBDMandDELMindicatesaprominentimprove-  ment,andprovesthatsemanticattributesincludedintothesmooth- ingprocessreallymakeadifference.Bothofthesmoothingmeth- odsclusterdocuments,andusetheclustereddocumentsasabet- terbackground.However,noneofthesemethodshasmadeuseof thesocialfactorsduringthelanguagemodelsmoothing,whileour nDCG@5nDCG@25nDCG @50MAPLM0.2710.2980.319 0.328Additive 0.2950.3200.331 0.385Absolute0.2830.3280.378 0.367Jelinek-Mercer0.3310.3760.361 0.503Dirichlet0.3650.3870.408 0.555PLM0.3920.4130.399 0.532CBDM0.3880.3970.426 0.546DELM0.4040.4380.489 0.566SLMS0.463?0.492?0.503?0.600?Table4:Retrievalperformanceagainstbaselines.` ?'indicates thatweaccepttheimprovementhypothesisofSLMSoverthe bestbaselinebyWilcoxontestatalevelof0.01. nDCG@5nDCG@25nDCG @50MAPLM0.5060.5340.570 0.603Additive 0.5210.5720.583 0.632Absolute0.5250.5810.585 0.635Jelinek-Mercer0.5430.5830.591 0.646Dirichlet0.5620.6090.621 0.639PLM0.5580.6020.619 0.653CBDM0.5380.6160.647 0.651DELM0.5570.6450.669 0.660SLMS0.617?0.662?0.689?0.673?Table5:Recommendationperformance.` ?'indicatesthatwe accepttheimprovementhypothesisofSLMSoverthebestrival  baselinebyWilcoxontestatalevelof0.01. methodsuggestssocialattributes,suchasinteractionsandrelation- ships,dohaveanimpactontextsthroughpropagation. Havingdemonstratedtheeffectivenessofourproposedmethods, wecarrythenextmovetoinvestigatemoreanalysisonparameter settings,factorcontributions,convergenceanddistributedlearning  analysis,whichleadtosomeinterestingobservations. 4.5AnalysisandDiscussions 4.5.1ParameterSettings Intheexperiments,aswecrawleddatafromtwoconsecutive months,welearnparameters =f gonthedatafromthe month,usingthelabelsbytherepostingbehavior,andexamine theperformanceonthetestingdatafromthenextmonth.There isanotherfreeparameter inEquation(15)tobalancetheorigi- Figure3:Performancecomparisonmeasuredinperplexity,nDCG @25,andMAPinhashtagtopicclustering,retrievalandrecom- mendationtasksforfeatureanalysis.ﬁ+factor(s)ﬂmeanstheperformanceofindividualfactor(group)inisolation. Figure4:Performancecomparisonmeasuredinperplexity,nDCG @25,andMAPinhashtagtopicclustering,retrievalandrecom- mendationtasks.ﬁ-factor(s)ﬂmeanstheperformanceofindividualfactor(group)whendroppedoutfromtheall-featuremodel. nallanguagemodelandthesmoothinglanguagemodel.Asweopt formoreorlessgenericparametervalueaswedonotwanttotune ourmethodtoomuchtosuitthedatasetsathand,weex-  perimentwithvaluerangingfrom0to0.9,withastepsizeof0.1. Byexaminingtheperformanceofperplexity,we 2[0.3,0.4] yieldsthebestresultsforthetwodatasets,henceweset =0.35.4.5.2FactorContributions Wefurtheranalyzethecontributionorallfactors.Weconduct toadetailedexperimentonallseparatefactorsandvisualizethe  resultinFigure3-4.Inthefactorgraphforsocializedlanguage modelsmoothing,weconsider8differentattributesandfactors:(i)  textsimilarity,(ii)textquality,(iii)postingpopularity,(iv)social status,(v)usersimilarity,(vi)socialinteractions,(vii)userimpacts and(viii)variabledependency.Besides,wecombinefactors(i)- (iii)as textrelated onesand(iv)-(vii)as socialrelated ones.We alsolisttheperformanceofSLMSwhichemploysallcomponents hereforcomparison.Hereweexaminethecontributionofthed- ifferentfactorsinourmodel.Tobeweshowthe performanceofallthefactorsinisolationandthenleave-one-out fromthefullcombinationofallfeatures,oneatatime. FromFigure3and4,weseethatalloftheindividualfactors havepositivecontributionstoourevaluationtasks.There-  sultinFigure3isperformedusingthecorrespondcomponentonly andthesecondgroupofresultsinFigure4isperformedusingthe  fullfactorcombinationexemptingthecorrespondingcomponent, usingaleave-one-outmanner.Fortheindividualfactoranalysis, wecouldseethatonaverage textsimilarity stillcontributesmost inisolationanditsabsenceleadstounfavorabledecrease.Asto  thesocialrelatedfeatures, interaction isthemostimportantsocial factorformeasuringthepropagatedandgetsacleardrop ontheperformancewhenleftoutfromfullfactorcombination.It isnaturaltoseethroughtherepostingbehavior,thelanguagemod-  elforaparticularuserisbyothers.Wealsoexamine thethreeaspectsoffeaturegroups,i.e.,textrelatedfactors,social relatedfactorsandvariabledependencies.Textrelatedfactorsare provedtobemoreusefulwhilethesocialgroupyieldsbetterper- formancewhenintegratethefactorstogether.Dependencyfactor  seemstobetheleastpowerfulpredictor.Itisunderstandablethat dependencyfactorisnotdeterministicbutjusttobalancethelabel values.Ingeneral,thecombinationofallfactorswillbe toimprovetheperformance,asdirectlycomparedinFigure3and 4,whichindicatesthatourmethodworkswellbycombiningthe  differentfactorfunctionsandeachfactorinourmethodcontributes totheoverallimprovements. 4.5.3ConvergenceProperty Weconductanexperimenttoseetheeffectofthenumberof theloopybeliefpropagationiterations.Figure5(a)illustratesthe convergenceanalysisresultsofthelearningalgorithm.Weseeon bothtestcases,theLBP-basedlearningalgorithmcanconverges inlessthan10iterations.Afteronly6-7learningiterations,the performanceintermsofperplexityonbothtestcasesbecomessta- ble.Thissuggeststhatlearningalgorithmisveryefandhas  agoodconvergenceproperty. 4.5.4DistributedScalability Thedistributedlearningenvironmentcantypicallyachieveasig- reductionoftheCPUtimeonthelarge-scalesocialnet-  works.Onthemoderatescalednetworkafterandpre- (a).Convergenceperformance. (b).Distributedcomputingspeedups. Figure5:Experimentalanalysisillustrations. processing,thespeedupofthedistributedlearningisabout 3-4times.However,underthescenarioofrealworldapplications withouthumansupervisedorpre-processing,thedistribut- edlearningframeworkcangenerallyscaleupwithaspeedupof 10-15timesormore[28]. Inparticular,wefurtherconductascalabilityexperimentwith ourdistributedlearningenvironment.Weevaluatethespeedupof  thedistributedlearningalgorithmonthe5computernodesusing  thetwodatasets.ItcanbeseenfromFigure5(b)thatwhenthesize ofthedatasetissuflarge,thedistributedlearningshows agoodparallelefy(speedup >3).Inthethedashed linedenotestheperfectspeedupideally.Theresultthat  thetoapplythedistributedalgorithmonthedivisiblesub-  graphsandthatthedistributedlearninglikemanydistributedlearn- ingalgorithmsisgoodonlarge-scaledatasets. 5.RELATEDWORK Languagemodelshavebeenpaidhighattentiontoduringre- centyears[25].Manydifferentwaysoflanguagemodelinghave beenproposedtosolvedifferentresearchtasks.Betterestimation ofquerylanguagemodels[14,15]andmoreaccurateestimationof  documentlanguagemodels[18,30]havelongbeenprovedtobe ofgreatininformationretrievalandtextmining,etc. Languagemodelsaretypicallyimplementedbasedontraditional retrievalmodels,suchastextweightingandnormalization[40],but withmoreelegantmathematicalandstatisticalfoundations[26]. Thereisoneproblemforlanguagemodels.Givenlimiteddata sampling,alanguagemodelestimationsometimesencounterswith  thezerocountproblem:themaximumlikelihoodestimatorwould  giveunseentermsazeroprobability,whichisnotreliable.Lan- guagemodelsmoothingisproposedtoaddressthisproblem,and hasbeendemonstratedtoaffectperformance[40,14]. Manyapproacheshavebeenproposedandtested.Therearesev- eralwaysoftosmooththeoriginallanguagemodel.Theinforma-  tionofbackgroundcorpushasbeenincorporatedusinglinearcom- bination[25,40].Incontrasttothesimplestrategywhichsmoothes alldocumentswiththesamebackground,recentlycorpuscontents havebeenexploitedformoreaccuratesmoothing.Thebasicidea istosmoothadocumentlanguagemodelwiththedocumentssimi-  lartothedocumentunderconsiderationthroughclustering[34,18, 30].Positioninformationhasalsobeenusedtoenrichlanguage modelsmoothing[41,19]andthecombinationofbothstrategies  ofpositionandsemantics[34].Intheirwork,thekeyideaisto alanguagemodelforeachpositionwithinadocument,and scoreitbasedonthelanguagemodelsonallpositions:hencethe effectofpositionaladjacencyisrevealed.Beyondthesemantic and/orpositionrelatedsmoothingintuitions,structuralbasedlan- guagemodelsmoothingisanalternativedirectiontoinvestigate.A  graphbasedlanguagemodelsmoothingmethodhasbeenproposed utilizingstructuraladjacencyonlybetweenneighboringnodes[21, 8].Thereisastudyin[17]whichsmoothesdocumentlanguage modelsoftweetsfortopictrackinginonlinetextstreams.Basi-  cally,itappliesgeneralsmoothingstrategies(e.g.,Jelinek-Mercer, Dirichlet,AbsoluteDiscounting,etc.)onthetrackingtask, whilewehaveproposedafactorgraphmodelbasedframe- workwhichincorporatesaseriesofsocialfactorsaswellasthe textinformationonlanguagemodelsmoothing,whichisanovel  insight.Tothebestofourknowledge,wearethepilotstudywhich mapsthesocialontothetextualdimensionandhenceto estimatethebetweenpostingdocuments.Languagemod-  elissmoothedbythepropagatedonthefactorgraph. 6.CONCLUSIONS Onlinesocialnetworksaremassive,usefulandofgreatpoten- tials,buthaveaseverebottleneckoftextualdatasparsity.Totackle suchanAchillesHeelofsocialnetworks,wepresentan  propagationbasedlanguagemodelsmoothingmethodtosolvethe zerocountphenomenonforonlinesocialnetworks.Thesocialin- isestimatedbasedonafactorgraphmodel,byutilizinga  seriesofattributesanddependencyfactorsfrombothtextualand socialdimensions.Inthisway,wepropagatethetermoccurrence alongthenetworkswithadiscountedcountsaccordingtotheesti- matedpairwisebetweendocuments,andsmooth thesparselanguagemodelaccordingly. Weexaminetheeffectofourproposedlanguagemodelsmooth- ingmethodonaseriesofintrinsicandextrinsicevaluationmetrics  basedontheTwitterdataset(inEnglish)andWeibodataset(inChi- nese).Ourproposedmethodconsistentlyandoutper- formsthealternativebaselines(with-18.6%improvementinterms  ofperplexityand+23.8%improvementinnDCGandMAP).Fur- thermore,wehaveinvestigatedfactorcontributionsaswellasase- riesofexperimentalanalysisforconvergenceanddistributedlearn-  ing.Ingeneral,allfeaturesfacilitatethesmoothingperformance, whiletextsimilarityandsocialinteractionareprovedtohavea strongercontribution.Inthefuture,wewillincludemorexible socialfactorsandmakeourmodeladaptivetodivonline socialnetworks,e.g.,structureswithreciprocallinkage(e.g.,Face-  book)ordocumentswithoutrepostingbehaviors(e.g.,Foursquare). 7.ACKNOWLEDGMENTS ThisworkwaspartiallydonewhentheauthorwasinUni- versityofPennsylvania.Wethankalltheanonymousreviewersfor theirvaluableandconstructivecomments. 8.REFERENCES [1]P.Analytics.TwitterstudyŒaugust2009.15,2009. [2]L.Bottou.Onlinelearningandstochasticapproximations. On-linelearninginneuralnetworks ,17:9,1998. [3]M.A.Carreira-PerpinanandG.E.Hinton.Oncontrastive divergencelearning.In IntelligenceandStatistics ,volume2005,page17,2005. [4]G.CasellaandE.I.George.Explainingthegibbssampler. TheAmericanStatistician ,46:167Œ174,1992. [5]K. Chen,T.Chen,G.Zheng,O.Jin,E.Yao,andY.Yu. Collaborativepersonalizedtweetrecommendation.In SIGIR'12,pages661Œ670,2012. [6]J.DeanandS.Ghemawat.Mapreduce:data processingonlargeclusters.pages107Œ113,2004. [7]J.Hopcroft,T.Lou,andJ.Tang.Whowillfollowyouback?: Reciprocalrelationshipprediction.In CIKM'11 ,pages 1137Œ1146,2011. [8]Y.-Y.Huang,R.Yan,T.-T.Kuo,andS.-D.Lin.Enriching coldstartpersonalizedlanguagemodelusingsocialnetwork information.In ACL'14 ,pages611Œ617,2014. [9]K.JärvelinandJ.Kekäläinen.Cumulatedgain-based evaluationofirtechniques. ACMTrans.Inf.Syst. ,20(4):422Œ446,2002. [10]R.Kindermann,J.L.Snell,etal. Markovrandomand theirapplications ,volume1.AmericanMathematical SocietyProvidence,RI,1980. [11]F.R.Kschischang,B.J.Frey,andH.-A.Loeliger.Factor graphsandthesum-productalgorithm. InformationTheory, IEEETransactionson ,47(2):498Œ519,2001. [12]T.-T.Kuo,R.Yan,Y.-Y.Huang,P.-H.Kung,andS.-D.Lin. Unsupervisedlinkpredictionusingaggregativestatisticson heterogeneoussocialnetworks.In KDD'13 ,pages775Œ783, 2013.[13]H.Kwak,C.Lee,H.Park,andS.Moon.Whatistwitter,a socialnetworkoranewsmedia?In WWW'10 ,pages 591Œ600,2010. [14]J.LaffertyandC.Zhai.Documentlanguagemodels,query models,andriskminimizationforinformationretrieval.In SIGIR'01 ,pages111Œ119,2001. [15]V.LavrenkoandW.B.Croft.Relevancebasedlanguage models.In SIGIR'01 ,pages120Œ127,2001. [16]C.Lin,C.Lin,J.Li,D.Wang,Y.Chen,andT.Li. Generatingeventstorylinesfrommicroblogs.In CIKM'12 ,pages175Œ184,2012. [17]J.Lin,R.Snow,andW.Morgan.Smoothingtechniquesfor adaptiveonlinelanguagemodels:Topictrackingintweet  streams.In KDD'11 ,pages422Œ429,2011. [18]X.LiuandW.B.Croft.Cluster-basedretrievalusing languagemodels.In SIGIR'04 ,pages186Œ193,2004. [19]Y.LvandC.Zhai.Positionallanguagemodelsfor informationretrieval.In SIGIR'09 ,pages299Œ306,2009. [20]C.D.Manning,P.Raghavan,andH.Schütze. Introductionto informationretrieval ,volume1.2008. [21]Q.Mei,D.Zhang,andC.Zhai.Ageneraloptimization frameworkforsmoothinglanguagemodelsongraph  structures.In SIGIR'08 ,pages611Œ618,2008. [22]H.Ney,U.Essen,andR.Kneser.Ontheestimation ofsmall'probabilitiesbyleaving-one-out. PatternAnalysis andMachineIntelligence,IEEETransactionson ,17(12):1202Œ1212,1995. [23]L.Page,S.Brin,R.Motwani,andT.Winograd.The pagerankcitationranking:bringingordertotheweb.1999. [24]E.Pitler,A.Louis,andA.Nenkova.Automaticevaluationof linguisticqualityinmulti-documentsummarization.In ACL '10,pages544Œ554,2010. [25]J.M.PonteandW.B.Croft.Alanguagemodelingapproach toinformationretrieval.In SIGIR'98 ,pages275Œ281,1998. [26]F.SongandW.B.Croft.Agenerallanguagemodelfor informationretrieval.In CIKM'99 ,pages316Œ321,1999. [27]J.Tang,T.Lou,andJ.Kleinberg.Inferringsocialtiesacross heterogenousnetworks.In WSDM'12 ,pages743Œ752,2012. [28]J.Tang,J.Sun,C.Wang,andZ.Yang.Social analysisinlarge-scalenetworks.In KDD'09 ,pages 807Œ816,2009. [29]W.Tang,H.Zhuang,andJ.Tang.Learningtoinfersocial tiesinlargenetworks.In ECML/PKDD'11 ,pages381Œ397. 2011.[30]T.Tao,X.Wang,Q.Mei,andC.Zhai.Languagemodel informationretrievalwithdocumentexpansion.In HLT-NAACL'06 ,pages407Œ414. [31]J.Teevan,D.Ramage,andM.R.Morris.#twittersearch:A comparisonofmicroblogsearchandwebsearch.In WSDM'11,pages35Œ44,2011. [32]Z.Wang,J.Li,Z.Wang,andJ.Tang.Cross-lingual knowledgelinkingacrosswikiknowledgebases.In WWW'12,pages459Œ468,2012. [33]S.Wu,J.Sun,andJ.Tang.Patentpartnerrecommendationin enterprisesocialnetworks.In WSDM'13 ,pages43Œ52, 2013.[34]R.Yan,H.Jiang,M.Lapata,S.-D.Lin,X.Lv,andX.Li. Semanticv.s.positions:Utilizingbalancedproximityin  languagemodelsmoothingforinformationretrieval.In IJCNLP'13,pages507Œ515,2013. [35]R.Yan,L.Kong,C.Huang,X.Wan,X.Li,andY.Zhang. Timelinegenerationthroughevolutionarytrans-temporal summarization.In EMNLP'11 ,pages433Œ443,2011. [36]R.Yan,M.Lapata,andX.Li.Tweetrecommendationwith graphco-ranking.In ACL'12 ,pages516Œ525,2012. [37]R.Yan,X.Wan,J.Otterbacher,L.Kong,X.Li,and Y.Zhang.Evolutionarytimelinesummarization:Abalanced optimizationframeworkviaiterativesubstitution.In SIGIR'11,pages745Œ754,2011. [38]Z.Yang,K.Cai,J.Tang,L.Zhang,Z.Su,andJ.Li.Social contextsummarization.In SIGIR'11 ,pages255Œ264,2011. [39]J.S.Yedidia,W.T.Freeman,Y.Weiss,etal.Generalized beliefpropagation.In NIPS,volume13,pages689Œ695, 2000.[40]C.ZhaiandJ.Lafferty.Astudyofsmoothingmethodsfor languagemodelsappliedtoadhocinformationretrieval.In  SIGIR'01 ,pages334Œ342,2001. [41]J.ZhaoandY.Yun.Aproximitylanguagemodelfor informationretrieval.In SIGIR'09 ,pages291Œ298,2009. [42]W.X.Zhao,J.Jiang,J.Weng,J.He,E.-P.Lim,H.Yan,and X.Li.Comparingtwitterandtraditionalmediausingtopic  models.In ECIR'11 ,pages338Œ349.2011. [43]X.W.Zhao,Y.Guo,R.Yan,Y.He,andX.Li.Timeline generationwithsocialattention.In SIGIR'13 ,pages 1061Œ1064,2013.  
Proceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing ,pages1299Œ1308 Copenhagen,Denmark,September7Œ11,2017. c  2017AssociationforComputationalLinguistics 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308  
Multi-channelEncoderforNeuralMachineTranslation HaoXiong,ZhongjunHe,XiaoguangHuandHuaWu BaiduInc.No.10,Shangdi10thStreet,Beijing,100085,China f xionghao05,hezhongjun,huxiaoguang,wu hua g @baidu.com Abstract Attention-basedEncoder-Decoderhastheeffectivearchitec- tureforneuralmachinetranslation(NMT),whichtypically reliesonrecurrentneuralnetworks(RNN)tobuildtheblocks thatwillbelatelycalledbyattentivereaderduringthedecod- ingprocess.Thisdesignofencoderyieldsrelativelyuniform compositiononsourcesentence,despitethegatingmecha- nismemployedinencodingRNN.Ontheotherhand,weof- tenhopethedecodertotakepiecesofsourcesentenceatvary- inglevelssuitingitsownlinguisticstructure:forexample,we maywanttotaketheentitynameinitsrawformwhiletak- inganidiomasaperfectlycomposedunit.Motivatedbythis demand,weproposeMulti-channelEncoder(MCE),which enhancesencodingcomponentswithdifferentlevelsofcom- position.More,inadditiontothehiddenstate ofencodingRNN,MCEtakes1)theoriginalwordembed- dingforrawencodingwithnocomposition,and2)apartic- ulardesignofexternalmemoryinNeuralTuringMachine (NTM)formorecomplexcomposition,whileallthreeen- codingstrategiesareproperlyblendedduringdecoding.Em- piricalstudyonChinese-Englishtranslationshowsthatour modelcanimproveby6.52BLEUpointsuponastrongopen sourceNMTsystem:DL4MT 1 .OntheWMT14English- Frenchtask,oursingleshallowsystemachievesBLEU=38.8, comparablewiththestate-of-the-artdeepmodels. Introduction Attention-basedneuralmachinetranslationhasarguably themosteffectivearchitectureforneuralmachinetransla- tion(NMT),outperformingconventionalstatisticalmachine translation(SMT)systemsonmanylanguagepairs(Sen- nrichetal.2017).Thesuperiorityofattention-basedmodel overcanonicalencoder-decodermodel(Sutskever,Vinyals, andLe2014)liesinthefactthatitcandynamicallyretrieve relevantpiecesofthesource(muchisomorphictoalignment inSMT)througharelativelysimplematchingfunction.In otherwords,attention-basedmodelfromaricher representationofsourcesentencewithitsxibilityrepre- sentinglocalstructure. Inatypicalattention-basedNMTsystem,abidirectional recurrentneuralnetworks(biRNN)(SchusterandPaliwal 1997)isusedtoencodethesource,yieldingasequenceof Copyright c  2018,AssociationfortheAdvancementof Intelligence(www.aaai.org).Allrightsreserved. 1 https://github.com/nyu-dl/dl4mt-tutorial vectorsfromtheRNN,whichcanberoughlyinterpretedas context-awareembeddingofthewordsinthesourcesen- tence.Withthisdesign,theencoderlearnsrelativelyuniform compositionofthesentence,despitetheRNNintheencoder arealreadyequippedwithsomeadvancedgatingmecha- nism,suchaslongshorttermmemory(LSTM)(Hochreiter andSchmidhuber1997)networkandgatedrecurrentunit (GRU)(Choetal.2014).Fortranslation,itiscommonthat wehopethedecodertotakepiecesofsourcesentenceat varyingcompositionlevelssuitingitsownlinguisticstruc- ture.Thisneedcanbeillustratedthroughthefollowingtwo examples  wemaywanttotaketheentitynameinthesourceinits rawformwhiletakinganidiomasadenselycomposed unit;  whenwetranslatingthesubstantive,wemayagainwant toknowthesurroundingwordsofthenounandthende- termineitssingularorpluralforms. Motivatedbythisdemand,weproposeMulti-channelEn- coder(MCE),whichtakesencodingcomponentswithdif- ferentlevelsofcomposition.More,inaddition tothehiddenstateofencodingRNN,MCEtakestheorig- inalwordembeddingforrawencodingwithnocomposi- tion,andaparticulardesignofexternalmemoryinNTM (Graves,Wayne,andDanihelka2014)formorecomplex composition,inawayanalogoustovisualchannelswithdif- ferentfrequency.Allthreeencodingstrategiesareproperly blendedduringdecodingcontrolledbyparameterscanbe learnedinanend-to-endfashion.More,wede- signagatethatcanautomaticallytunestheweightsofdif- ferentencodingchannels. Inthispaper,wealsoempiricallyinvestigateMCEon differenttranslationtasks.Wetestourmodelsonthe NISTChinese-Englishmachinetranslationtasks,achiev- inganaverageimprovementby6.52BLEUoverthe strongDL4MTsystem.Furthermore,experimentsonthe WMT2014English-Frenchmachinetranslationtaskshow thatoursingleshallowmodelobtainsaBLEUscoreof38.8, whichiscomparabletothestate-of-the-artmodels. Thereminderofthispaperisorganizedasfollows:inthe nextsection,wewillintroducethebasicsofcon- ventionalattention-basedNMT.Afterthat,wewillpresent MCEinmoredetails.Afterthat,wewillreportextensiveex- arXiv:1712.02109v1  [cs.CL]  6 Dec 2017Figure1:Leftistheconventionalattention-basedNMTwhichconsistsofencoder,decoderandanattentionmechanism.Right isournovelNMTmodelwithmultiplechannelencoder,whichconsistsofhiddenstatefrombiRNN,externalmemoryinthe NTMandembeddingsdirectlyfromtheinputsequence.Agatedannotationisdesignedtoautomaticallylearntheweightsfor differentencodingcomponents. perimentalresultsandconcludethepaperinthelastsection. Attention-basedNeuralMachineTranslation Inrecentyears,researchershaveproposedexcellentworks toimprovetheperformanceofNMT.Moststate-of-the-art NMTsystemsarebasedontheattention-basedencoder- decoderarchitecture.Figure1(a)illustratestheattention- basedencoder-decoderarchitecture,whichconsistsofthree parts:anencoder,adecoderandanattentionmechanismthat buildingrelationshipsbetweenencoderanddecoder. Inmoredetails,thestepofNMTsystemsistocon- verteachatomicsymbolintoacorrespondingcontinuous vector,namedwordembedding.Thisstepisdoneforeach sourcewordindependentlyoftheotherwordsandresultsin asourcesequenceofwordembeddings.Abovetheembed- dinglayer,abiRNNisdesignedtolearntherepresentation ofthewholesequenceafterrunningonsometimesteps.Be- tweentheencoderanddecoder,anattentionmechanismis employedtofusealltimestepsoftheinputsequenceand drawtheattentionforcurrenttimestepinthedecoder.Dur- ingthegenerationofthetargetword,thecontrollerwillcon- siderthesuggestionfromlastgeneratedword,currenthid- denstate,andthecontextcomputedbytheattentionmecha- nismtodecidethenextword. Formally,givenasourcesequence x =( x 1 ;:::;x t ) and previoustranslatedwords ( y 1 ;:::;y j  1 ) ,theprobabilityof nextword y j iscalculatedas: p ( y j j s j ;y j  1 ;c j )=softmax( t j W o ) (1) and t j =tanh( s j W t 1 + e y j  1 W t 2 + c j W t 3 ) (2) where W t 1 , W t 2 , W t 3 , W o arethetrainedmodelparameters. e x t , e y j  1 aretheembeddingrepresentationof x t and y j  1 respectively,whichisusuallyinitializedwithaone-hotem- beddingvector. s j isthehiddenstateinthedecoderattime step j ,whichiscomputedas: s j = g ( s j  1 ;e y j  1 ;c j ) (3) Here g isanonlineartransformfunction,whichcanbe implementedasLSTMorGRU,and c j isadistinctcontext vectorattimestep j ,whichcanbeobtainedbyanattention mechanism.Normally, c j iscalculatedasaweightedsumof theinputannotations h i : c i = Tx X i  ij h i (4) where h i =[ ! h T i ;   h T i ] T istheannotationof x i froma biRNNand Tx isthelengthofthesourcesequence.The normalizedweight  ij for h i iscalculatedas:  ij = exp( e ij ) P Tx k =1 exp( e kj ) (5) e ij = V T a tanh( U a s 0 j + W a h i ) (6) where V a , U a and W a arethetrainableparameters.Allofthe parametersintheNMTmodelareoptimizedtomaximize thefollowingconditionallog-likelihoodoftheMsentence alignedbilingualsamples: ` (  )= 1 M M X m =1 Ty X j =1 log p ( y j j s j ;y j  1 ;c j ) (7) Here,allthebiasesareomittedforsimplify. Multi-channelEncoder Asanimportantpartoftheattention-basedNMTmodels, theRNNencodestherepresentationofthesourcesequence whichislatelyusedbytheattentionmechanism.Neverthe- less,aswehavementionedinthesection,itisdif fortheconventionalRNNtoencodesequencewithdifferent levelsofcompositionwhichisnecessaryduringthetransla- tion.Thus,weproposetousemultiple-channeltoenhance theencoderandtheattentionmechanism.Figure1(b)illus- tratestheoverallarchitectureofourmodel,whereanexter- nalmemoryisdesignedtoco-operatewiththeRNNonlean- ingcomplexcompositions.Additionally,thehiddenstateof RNNtogetherwithexternalmemoryinNTMandsequence ofembeddingvectorsaregatheredtogeneratethegatedan- notationusedbytheattentionmechanism. Ontheotherhand,incorporatingtheembeddingsintothe attentionmechanismcanalsobeviewedasbuildingashort- cutconnectionsthathasbeenprovedtoalleviatetheunder- lyingdegradationproblem(Heetal.2016).Moreover,short- cutconnectionshaveanaddedofnotaddinganyex- traparametersorcomputationalcomplexity. ExternalMemoryinNTM IntheconventionalRNNbasedNTMsystems,aRNNis usedtolearntherepresentationofthesequence.Concretely, intheRNN,ateachtimestep t i ,thecurrentstate s i isde- pendingontheinput e i fromtheembeddinglayer,andthe laststate s i  1 .Tomeasuretheimportanceoftheinputand thehistoricalstates,anon-linearfunctionisusedtolearn theweightsofthetwoparts.InNMTtask,mostresearchers priortousetheGRUfromitssimpleform.Fol- lowingtheoriginaloftheGRU,thevalueof s i couldbecalculatedas: s i =GRU( e i ;s i  1 ) (8) =(1  z i )  s 0 i + z i  s i  1 (9) and s 0 i =tanh( We i + r i  ( Us i  1 )) (10) r i = ˙ ( W r e i + U r s i  1 ) (11) z i = ˙ ( W z e i + U z s i  1 ) (12) where W , U , W r , U r , W z , U z aretrainableparameters. However,asequation(9)and(12)indicates,thecurrent stateisdependingonthecurrentinputembeddingsandhis- toricalstate.Inthissituation,theRNNhasdifinwell Figure2:IllustrationofNTMintheencoder.TheRNNreads andwritesfromtheNTMateachtimestep. learningboththecurrentlexicalsemanticsandhistoricalde- pendentrelationships.AnotherdrawbackisthattheRNN knowingnothingaboutthefutureinformationwhengener- atingthecurrentstatethatpotentiallyaffectsthecapturing oflongdistancedependentrelationships. Toenabletheencodertorepresentthelexicalseman- ticsaswellascomplexcomposition,oursolutionistode- velopmulti-channelencoderthatconsistsofembeddings fromoriginalsentenceandanexternalmemoryinNTM aswellashiddenstatefromtheRNN.Mostinspiredby thedesignofneuralturingmachine(Graves,Wayne,and Danihelka2014)andrecentexcellentworksonexternal memoriesforNMT(Mengetal.2015;Wangetal.2016; Fengetal.2017),wepresenta Read-Write memorytoen- hancetheGRUintheRNNandintendtocapturemorecom- plexcompositionalrelationships. Inordertobetterunderstandthewholeprocess,Figure2 illustratesthedetailedstructureoftheencoder.Ateachtime step,thestateinRNNqueriestheexternalmemoryusingan attention-basedaddressingmechanismandreadsthecontext ofthememory.Insteadofdirectlyusingthelaststatetofeed theGRU,weusethelaststatetoquerytheexternalmemory andfetchthecontextualmemoryasthestateinputofthe GRU.Thisoperationguaranteesthatthecontrollerknowing morecontextualinformationbeforegeneratingthecurrent statewhichmaypotentiallyassistthedecisionofGRU. Besidesthe Read operation,wealsodesigna Write op- eration.Themotivationofaddinga Write operationisthat weexpecttheRNNandNTMtolearndifferenttypesofre- lationshipsrespectivelyviadifferentupdatingstrategies. Formally,let M t 2 R n  m representingthememoryin time t aftertheupdateofencodingstate,where n isthenum- berofmemorycellsand m isthedimensionofvectorineach cell.Weinitializethe M 0 bythesequenceofembeddingvec- tor E=[ e 1 ;e 2 ;:::;e t ] . Inspiredbytheworkofinteractiveattention(Mengetal. 2016),ateachtimestep t ,wegeneratethe Read mem- ory ~ M t asfollows, ~ M t =M t  1 ( w R t  R t ) (13) where M t  1 isthecontentofthememoryinthelasttime step,and w R t 2 R n thenormalizedweightsas- signedtothecellsin M t  1 .AsdescribedinGraves,Wayne, andDanihelka,wecanusecontext-basedoraddress-based addressingmechanismtodetermine w R t .Inpractice,we foundthataconventionalattentionmodelworkwellinour model,thuswecomputethe w R t asdescribedinequation (5)and(6).Inaddition, R t issimilartothereadgateinthe GRU,whichdeterminesthecontenttobereadfrommemory cells.More,itcanbeas, R t = ˙ ( W r s t  1 ) (14) where s t  1 isthestateofthelasttimestepintheencoder, and W r 2 R m  m istrainableparameter. Oncewehaveobtainedthe Read memory ~ M t ,weuseit tofetchthecontext c t ,similartothecomputation inequation(4). c t = T x X i w R t ~ M t (15) Afterthat, c t iscombinedwithembeddingvector e t and usedtoupdatethenewstate s t =GRU( c t ;e t ) (16) Finally,thenewstate s t isusedtoupdatetheexternal memorybywritingtoittotheroundofstate-update. M t = ~ M t + w W t  U t (17) where U t = ˙ ( W u s t ) istheupdategate,andisparameter- izedwith W u 2 R m  m .Meanwhile,inourexperiments,the weightsforreading w R t andwriting w W t attime t areshared, thatmeanswecomputeitusingtheformulasinequation(5) and(6)withthesamestate s t  1 andthememory M t  1 . Itisworthnoting,sinceweusebiRNNtorepresentthe sourcesequence,wewillobtaintwoexternalmemories: ! M and   M ,whichisoriginatedfromtheforwardRNNandthe backwardRNN,respectively.Similartothemethodpre- sentedinthetraditionalNMT,weconcatenatethetwotypes ofhiddenstates.Weequallyconcatenatetwoexternalmem- oriesanduseitasoneannotationfortheattentionmecha- nism. GatedAnnotation Asdescribedinthebeginningofthesection,wedesignmul- tipleencodingcomponentsinourencoder.Consequently,we willobtainmultipleannotationsfromtheencoder,includ- ing M t fromexternalmemoryinNTM, h t fromthehidden stateoftheRNNandthesequenceofembeddingvectors E=[ e x 1 ;:::;e x t ] fromtheoriginalsourceinputrespec- tively. Toutilizemultipleannotations,onefeasiblesolutionis summingorconcatenatingofthem.Inthispaper,motivated bythedesignofGRU,weproposeanalternativereasonable solution.Sinceinprior,wecannotdeterminewhichencod- ingcomponentisbetterforthetranslation,thebestdecision istoletthemodellearningtheweightbetweentwoannota- tionsautomatically. Formally,giventheexternalmemory M 2 R n  2  m in NTM,andthehiddenstate h 2 R n  2  d inRNN,inwhich d indicatesthehiddensize. 2 Thecomputationofannotation whichwillbefurtherutilizedbytheattentionmechanism canbeas, h rnn ntm = g 0  M+(1  g 0 )  h (18) where g 0 isthegatedunit,calculatedas, g 0 = ˙ ( W g 0 M+ U g 0 h) (19) where W g 0 and U g 0 aretrainableparameters. Intheexperimentalsection,wewillinvestigatewhich typeofencodingcomponentplaysanimportantroleinim- provingthetranslation,thuswewillvalidatevariousofcom- binationofannotations. BesidesthecombinationofexternalmemoryinNTMand hiddenstateinRNN,wealsogeneratethefollowingcombi- nations: h rnn emb = g 1  E+(1  g 1 )  h (20) h ntm emb = g 2  E+(1  g 2 )  M (21) h ntm rnn emb = g 3  E+(1  g 3 )  h rnn ntm (22) and g 1 = ˙ ( W g 1 E+ U g 1 h) (23) g 2 = ˙ ( W g 2 E+ U g 2 M) (24) g 3 = ˙ ( W g 3 E+ U g 3 h rnn ntm ) (25) where W g 1 , U g 1 , W g 2 , U g 2 , W g 3 and U g 3 aretrainablepa- rameters. Experiments WemainlyevaluateourapproachesonthewidelyusedNIST Chinese-Englishtranslationtask.Inordertocompareour modeltothepreviousworks,wealsoprovideresultsonthe WMTEnglish-Frenchtranslationtask.ForChinese-English task,weapplycase-insensitiveNISTBLEU.ForEnglish- French,wetokenizethereferenceandevaluatetheperfor- mancewithmulti-bleu.pl 3 .Themetricsareexactlythesame asinthepreviousliteratures. Datasets NISTChinese-English. Weuseasubsetofthedataavail- ableforNISTOpenMT08task 4 .Theparalleltrainingcor- puscontains1.5millionsentencepairsafterwewith somesimpleheuristicrules,suchassentencebeingtoo longorcontainingmessycodes.WechooseNIST2006 2 Sincewehaveconcatenatedthememoriesandtheannotations oftheforwardandthebackwardRNN,thusthedimensionoftwo unitsshouldbemultipliedbytwo. 3 https://github.com/moses-smt/ \mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 4 1LDC2002E18,LDC2002L27,LDC2002T01,LDC2003E07, LDC2003E14,LDC2004T07,LDC2005E83,LDC2005T06, LDC2005T10,LDC2005T34,LDC2006E24,LDC2006E26, LDC2006E34,LDC2006E86,LDC2006E92,LDC2006E93, LDC2004T08(HKNews,HKHansards) (NIST06)datasetasourdevelopmentset,andtheNIST2003 (NIST03),2004(NIST04)2005(NIST05),2008(NIST08) and2012(NIST12)datasetsasourtestsets.Weuseasource andtargetvocabularywith30Kmostfrequentwordsand terthesentenceslongerthan50. WMT'14English-French. WeusethefullWMT'14par- allelcorpusasourtrainingdata.Thedetaileddatasetsare Europarlv7,CommonCrawl,UN,NewsCommentary,Gi- gaword.Intotal,itincludes36millionsentencepairs.The news-test-2012andnews-test-2013areconcatenatedasour developmentset,andthenews-test-2014isthetestset.Our datapartitionanddatapreprocessisconsistentwithpre- viousworksonNMT(Luong,Pham,andManning2015; Jeanetal.2014)toensurefaircomparison.Asvocabulary weuse40Ksub-wordtokens(Sennrich,Haddow,andBirch 2015)basedonbyte-pairencodingandthesentences longerthan120. ModelSettings FortheChinese-Englishtask,werunwidelyusedopen sourcetoolkitDL4MTtogetherwithtworecentlypublished strongopensourcetoolkitsT2T 5 andConvS2S 6 onthe sameexperimentalsettingstovalidatetheperformanceof ourmodels 7 .Beyondthat,wealsoreimplementanattention- basedNMTwritteninw 8 asourbaselinesystem. Tomeasuretheimportanceofdifferentencodingcompo- nentonthequalityoftranslation,wevalidateourapproach withdifferentmodelimplements,whichincludes:  DL4MT:anopensourcetoolkit.  RNN:aninhouseimplementedattention-basedRNN writteninw,andtheannotationforattention mechanismconsistsofonlythehiddenstateofRNN.  NTM:usingtheexternalmemoryinNTMdirectlyasthe annotation.  EMB:usingthesequenceofembeddingvectorsasthean- notation.  NTM-EMB:usingthecombinationofexternalmemory inNTMandembeddingsdescribedintheequation(21).  NTM-RNN:usingthecombinationofexternalmemoryin NTMandhiddenstatesofRNNdescribedintheequation (18). 5 https://github.com/tensorflow/ tensor2tensor 6 https://github.com/facebookresearch/ fairseq 7 ParametersforDL4MT:`dim':1000,`optimizer':`adadelta', `dim word':620,`clip-c':1.0,`n-words':30000,`learning-rate': 0.0001,`decay-c':0.0 ParametersforT2T:`model':`transformer',`hparams set':`trans- former base single gpu' ParametersforConvS2S:`model':`fconv',`nenclayer':12, `nlayer':12,`dropout':0.2,`optim':`nag',`lr':0.25,`clip':0.1, `momentum':0.99,`bptt':0,`nembed':512,`noutembed':512, `nhid':512 8 https://www.tensorflow.org/  RNN-EMB:usingthecombinationofannotationfrom hiddenstateofRNNandembeddingsdescribedinthe equation(20).  NTM-RNN-EMB:usingthecombinationofoverallthree encodingcomponentsdescribedintheequation(22).  T2T:anopensourcetoolkit(Vaswanietal.2017).  ConvS2S:anopensourcetoolkit(Gehringetal.2017). Inthatthemainintentofourworkistothatusing multiple-channelencoderdohelponthequalityoftransla- tion,thusweutilizetheempiricalhyperparametersaccord- ingtoourpreviousresearch.Inmoredetails,weuse512 dimensionalwordembeddingsforboththesourceandtar- getlanguages.Allhiddenlayersbothintheencoderandthe decoder,have512memorycells.Theoutputlayersizeisthe sameasthehiddensize.Thedimensionof c j is1024. TrainingDetails InspiredbytheworkofGNMT(Wuetal.2016),weinitial- izealltrainableparametersuniformlybetween[-0.04,0.04]. AsiscommonwisdomintrainingRNNmodels,weapply gradientclipping:allgradientsareuniformlyscaleddown suchthatthenormofthegradientsisnolargerthan aedconstant,whichis1.0inourcase. Followingtheworkof(Vaswanietal.2017),weusethe Adamoptimizerwith  1 =0 : 9 ,  2 =0 : 98 and  =10  9 . Weusethesimilarlearningratesettingwithminormod- toadaptourparalleltrainingequipment.Overthe courseoftraining,thelearningrateiscalculatedaccording totheformula: lrate = d  0 : 5  min(   0 : 5 ;    1 : 5 ) (26)  =step num = num gpus (27) wherewesetthe  =6000 representsthewarmsteps withthesamemeaningintheoriginalwork.Sincewetrain ourmodelwithparallelizationatthedatabatchlevel,we penalizethenumberofstepsbythedivisionofthenumber ofGPUsusedinourmodel.Aswesetthebatchsizeto128, onChinese-Englishtaskittakesaround1daytotrainthe basicmodelon8NIVDIAP40GPUsandonEnglish-French taskittakesaround7days. Additionally,translationsaregeneratedbyabeamsearch andlog-likelihoodscoresarenormalizedbysentencelength. Andweuseabeamwidthof10inalltheexperiments.More- over,dropoutisalsoappliedontheoutputlayertoavoid overandwesetthedropoutrateto0.5. ResultsonChinese-to-English Table1liststheoverallresultsoneachChinese-English evaluationtasks.Tothatoursystemisstrong enough,wealsoreportperformanceofanopensourcesys- tem:DL4MT.Atwethatthereisaim- provementsofoursystemsovertheDL4MT.Comparedto theDL4MT,ourbasic RNN systemachievesanimprove- mentby4.94BLEUpoints.Althoughour RNN isabasic attention-basedNMT,weassembleitwithsomeadvanced techniques,suchasinitializationallparametersuniformly, SYSTEMS PARAMETERS NIST03 NIST04 NIST05 NIST08 NIST12 AVG DL4MT - 31.82 34.86 31.81 24.71 20.47 28.73 RNN 56,314,881 36.65 39.86 36.75 29.76 25.34 33.67(+4.94) NTM 59,988,995 37.73 40.03 36.06 29.19 25.12 33.63(+4.90) EMB 58,416,131 21.43 24.38 20.58 16.71 14.48 19.52 NTM-EMB 63,135,747 36.97 40.05 36.35 29.87 25.51 33.75(+5.02) NTM-RNN 64,184,323 37.63 40.44 37.77 30.44 25.38 34.33(+5.60) RNN-EMB 59,461,633 37.98 40.90 37.59 30.58 25.99 34.61(+5.88) NTM-RNN-EMB 67,331,075 38.56 40.79 38.49 31.51 26.90 35.25(+6.52) T2T - 38.11 41.41 38.12 31.53 25.55 34.94 ConvS2S - 38.85 40.79 37.44 30.60 26.08 34.75 Table1:PerformanceofdifferentsystemsontheNISTChinese-to-Englishtranslationtasks.Comparedtothestrongopen sourcesystemDL4MT,ourmodelsachieveimprovements.Wealsolistresultsfromanothertworecentlypublished opensourcetoolkitsT2TandConvS2Sforcomparison.RememberthatbothT2TandConvS2Saredeepmodelswithmultiple layers. SYSTEMS Voc. En-Fr ExistingRNNsystems LSTM(6layers)(Luong,Pham,andManning2015) 80K 31.50 LSTM(6layers+PosUNK)(Luong,Pham,andManning2015) 80K 33.10 Deep-Att(Zhouetal.2016) 80K 37.70 Deep-Att+PosUnk(Zhouetal.2016) 80K 39.20 GNMTWPM-32K(Wuetal.2016) 80K 38.95 DeepLAU+PosUNKtrainedon12Mdata(Wangetal.2017) 80K 35.10 GNMTWPM-32K,HyperLSTM(Ha,Dai,andLe2017) 80K 40.03 ExistingOthersystems ConvS2S(15layers)+BPE-40K(Gehringetal.2017) 40K 40.46 Transformer(base)(Vaswanietal.2017) - 38.10 Transformer(big)(Vaswanietal.2017) - 41.00 Oursystem RNN+BPE-40K 40K 38.19 MCE+BPE-40K 40K 38.80 Table2:English-to-Frenchtask:BLEUscores.The RNN isourbasicRNNmodel,andthe MCE modelcombinesthreeencoding componentsfromembeddings,hiddenstatefromRNN,externalmemoryinNTM.Notingthatourmodeldoesnotperform PosUNKandusesmallsizeofvocabulary. addingbiasesfortheembeddingvectors,usingtheoutputof forwardRNNastheinputofthebackwardRNNandtraining withdynamiclearningrate.Whatever,wegivethecompari- sonbetweenourbasic RNN systemandopensourcetoolkits istoprovethatourbaselineisstrongenough,andallim- provementsoverbaselinesystemarereliable. Unsurprisingly,the EMB modelwhichonlyusingembed- dingsastheannotationobtainsaverylowperformancedue totheencodingofnocompositionofthesentence.Morein- teresting,anycomplexencodingcomponentcombinedwith embeddings,suchas NTM-EMB and RNN-EMB receives betterperformanceagainsttheindividualone.Thereason isalthoughboththeexternalmemoryinNTMandtheRNN encodelexicalsemanticsandcomplexcompositions,how- everateachtimestep,thelexicalsemanticswillbeblended bythehistorystate,thusitisdifforthemtoencodethe sequenceatvaryinglevels.Whileunderthearchitectureof ourMCE,itispossibleforthedecodertotakethesource worddirectlyfromtheembeddingcomponent. Wealsonoticethattheperformanceof NTM isalmost equaltothe RNN whiletheperformanceof RNN-EMB isrel- ativelymuchbetterthanthe NTM-EMB .Oneexplanationis thattheexternalmemoryinNTMisinitializedwiththeem- beddings,andateachtimestepanattention-basedcontent addressingmechanismisemployedtoupdatethecontentsof thememory.Thatmeans,comparedtotheRNN,theexter- nalmemoryrecordsmorelexicalsemanticswithnocompo- sition.Thuswhencombinedwiththeembeddingsencoding component,theRNNcanfocusonlearningcompositional relationshipsandyieldingmoreimprovements. Lastly,whenusingthreeencodingcomponents,ourbest modelachieves1.58improvementsoverthestrongbase- linesystemand6.52BLEUpointsovertheDL4MT,which provesthatourmodeliseffectiveinpractice.Notingthat, theperformanceofourbestsystemisalsoslightlybetter thantheT2T. Figure3:Performanceondifferentsentencelength. ResultsonEnglish-French TheresultsonEnglish-Frenchtranslationarepresentedin table2.WecompareourNMTsystemswithvariousother systemsincludingDeepRNNmodel,DeepCNNmodeland DeepAttentionmodel.Forfaircomparison,herewejust listthesinglesystemresultsreportedintheirpapers.On theEnglish-Frenchtranslationtask,apromisingfoundingis thatoursystemachievescomparableperformanceoverthe state-of-the-artsystems,evencomparedwiththedeepmod- els.Besides,comparedtootherRNNmodels,oursystemis verycompetitive,althoughoursystemisashallowmodel. Notingthat,thebestreportedresultofsinglesystemon thisdatasetisfromtheworkof(Vaswanietal.2017),where theybuildabigattentionneuralnetworkswiththedepth of6layersand16heads.Comparedtothebasicversionof them: Transformer(base) ,theperformanceofoursystemis evenbetterdespiteourmodelisashallownetworksandtheir systemismoredeeper. Clearly,comparedtothepreviousreportedworks,our modelseemsbeingcompetitivebothonthesmallandlarge trainingdata.Moreimportant,ourmodeliseasytoimple- ment.Onethingwecouldconcludeisthatwheninstalled withadvancedtrainingtechniquesandinnovativemodelde- signs,RNNbasedmodelisstillcompetitivetoothermodels especiallyonthelargescaletrainingcorpus. Analysis OntheChinese-Englishtranslationtask,werandomlyse- lect1000sentencesfromthedifferenttestingsetsandsplit thetestingsetintothedifferentpartsaccordingtothelength ofthesentences.Inparticular,wetesttheBLEUscoreson sentenceslongerthan10;20;30;40;50;60onthetestset shownin3.Clearly,inallcurves,performancede- gradeswithincreasedsentencelength.However,ourbest modelassembledwithmultipleencodingcomponentsyield consistentlyhigherBLEUscoresthanthebaselinemodelon longersentences.Theseobservationsareconsistentwithour intuitionthattheMCEcanencodessentenceatvaryinglev- elswhichisusefulduringthetranslationoflongsentence. TranslationSample Table3showsaparticulartranslationcaseinwhichindicates thatourmodelcouldtakepiecesofsourcesentenceatvary- ingcompositionlevels.Inthistranslationcase,themodel shouldlearnthetranslationoftwonamedentities` HuaRen ZhiYe 'and` GaoShengOuZhouZongBu '.Moreover, thereisonelongdistancereorderingrelationshipbetween ` GaoJia 'and` MaiXia '.Thusitischallengingformodels totranslatethissentencecorrectly. Fromthetable3wendthatone-channelmodels RNN and NTM failedtotranslatethe` GaoJia ',whilethemulti- channelmodelsexcepts NTM-RNN successfullycapturethe reorderingrelationshipandtranslatethe` GaoJia 'incorrect form.Onereasonforthepoorperformanceof NTM-RNN is thatitlacksonecomponenttotaketheoriginalwordem- beddingforrawencodingwithnocomposition,thusaffects theNTMandRNNcomponentstolearnthelongdistance compositionalrelationship. ConclusionandFutureWork Inthispaper,wehaveproposedmultiple-channelencoder toenhancetheencoderandtheattentionmechanismof attention-basedneuralmachinetranslation.Toenabletheen- codertorepresentthesentenceatvaryinglevels,besidethe hiddenstateoftheRNN,weemploytheoriginalwordem- beddingforrawencodingwithnocomposition,anddesign aparticularexternalmemoryinNeuralTuringMachinefor morecomplexcomposition.Agatedannotationmechanism isdevelopedtolearntheweightsofdifferentencodingcom- ponentsautomatically.ExperimentsonextensiveChinese- EnglishtranslationtasksshowthatMCEimprovethequality oftranslationandourbestmodelbeattheDL4MTby6.52 BLEUpoints.AndfaircomparisononEnglish-Frenchtrans- lationindicatesthatourshallowRNNbasedmodelobtain thecomparableperformanceagainstthepreviousreported literatures. Inthefuture,wewillattempttodevelopmoreencod- ingcomponents,suchasCNN(LeCun,Bengio,andothers 1995)andself-attentivestructure(Linetal.2017). Acknowledgements ThisresearchissupportedbytheNationalBasicResearch ProgramofChina(973programNo.2014CB340505).We wouldliketothankZhengdongLu,DaqiZhengandthe anonymousreviewersfortheirinsightfulcomments. References [Choetal.2014] Cho,K.;VanMerri ¨ enboer,B.;Gulcehre, C.;Bahdanau,D.;Bougares,F.;Schwenk,H.;andBengio, Y.2014.Learningphraserepresentationsusingrnnencoder- decoderforstatisticalmachinetranslation. arXivpreprint arXiv:1406.1078 . [Fengetal.2017] Feng,Y.;Zhang,S.;Zhang,A.;Wang,D.; andAbel,A.2017.Memory-augmentedneuralmachine translation. arXivpreprintarXiv:1708.02005 . [Gehringetal.2017] Gehring,J.;Auli,M.;Grangier,D.; Yarats,D.;andDauphin,Y.N.2017.Convolu- Source: HuaRenZhiYe GaoJia MaiXia GaoShengOuZhouZongBu. Reference: ChineseEstates buys Goldman'EuropeanHeadquarters athighprice . RNN: ChineseHomeBuyers purchase highpricesof OOVeuropeanheadquarters. NTM: ChineseHomeBuyers buy OOVeuropeanheadquarters. NTM-RNN: Chinese purchase pricesathighprice tobuy OOVheadquartersineurope. RNN-EMB: China'sHomeBuyers purchased OOVeuropeanheadquarters athighprice . NTM-EMB: Chinese bought OOVeuropeanheadquarters athighprice . NTM-RNN-EMB: ChineseHomeBuyers purchase OOVeuropeanheadquarters athighprice . Table3:Translationresultsforeachmodels.OOVrepresentsthewordwhichisoutofvocabulary. tionalsequencetosequencelearning. arXivpreprint arXiv:1705.03122 . [Graves,Wayne,andDanihelka2014] Graves,A.;Wayne, G.;andDanihelka,I.2014.Neuralturingmachines. arXiv preprintarXiv:1410.5401 . [Ha,Dai,andLe2017] Ha,D.;Dai,A.M.;andLe,Q.V. 2017.Hypernetworks. CoRR abs/1609.09106. [Heetal.2016] He,K.;Zhang,X.;Ren,S.;andSun,J.2016. Deepresiduallearningforimagerecognition.In Proceed- ingsoftheIEEEconferenceoncomputervisionandpattern recognition ,770Œ778. [HochreiterandSchmidhuber1997] Hochreiter,S.,and Schmidhuber,J.1997.Longshort-termmemory. Neural computation 9(8):1735Œ1780. [Jeanetal.2014] Jean,S.;Cho,K.;Memisevic,R.;andBen- gio,Y.2014.Onusingverylargetargetvocabularyforneu- ralmachinetranslation. arXivpreprintarXiv:1412.2007 . [LeCun,Bengio,andothers1995] LeCun,Y.;Bengio,Y.; etal.1995.Convolutionalnetworksforimages,speech, andtimeseries. Thehandbookofbraintheoryandneural networks 3361(10):1995. [Linetal.2017] Lin,Z.;Feng,M.;dosSantos,C.N.;Yu, M.;Xiang,B.;Zhou,B.;andBengio,Y.2017.Astructured self-attentivesentenceembedding. CoRR abs/1703.03130. [Luong,Pham,andManning2015] Luong,M.-T.;Pham,H.; andManning,C.D.2015.Effectiveapproachesto attention-basedneuralmachinetranslation. arXivpreprint arXiv:1508.04025 . [Mengetal.2015] Meng,F.;Lu,Z.;Tu,Z.;Li,H.;andLiu, Q.2015.Adeepmemory-basedarchitectureforsequence- to-sequencelearning. arXivpreprintarXiv:1506.06442 . [Mengetal.2016] Meng,F.;Lu,Z.;Li,H.;andLiu,Q. 2016.Interactiveattentionforneuralmachinetranslation. arXivpreprintarXiv:1610.05011 . [SchusterandPaliwal1997] Schuster,M.,andPaliwal,K.K. 1997.Bidirectionalrecurrentneuralnetworks. IEEETrans- actionsonSignalProcessing 45(11):2673Œ2681. [Sennrichetal.2017] Sennrich,R.;Birch,A.;Currey,A.; Germann,U.;Haddow,B.;K.;MiceliBarone, A.V.;andWilliams,P.2017.TheUniversityofEdinburgh's NeuralMTSystemsforWMT17.In ProceedingsoftheSec- ondConferenceonMachineTranslation,Volume2:Shared TaskPapers . [Sennrich,Haddow,andBirch2015] Sennrich,R.;Haddow, B.;andBirch,A.2015.Neuralmachinetranslationofrare wordswithsubwordunits. CoRR abs/1508.07909. [Sutskever,Vinyals,andLe2014] Sutskever,I.;Vinyals,O.; andLe,Q.V.2014.Sequencetosequencelearningwithneu- ralnetworks.In Advancesinneuralinformationprocessing systems ,3104Œ3112. [Vaswanietal.2017] Vaswani,A.;Shazeer,N.;Parmar,N.; Uszkoreit,J.;Jones,L.;Gomez,A.N.;Kaiser,L.;and Polosukhin,I.2017.Attentionisallyouneed. CoRR abs/1706.03762. [Wangetal.2016] Wang,M.;Lu,Z.;Li,H.;andLiu,Q. 2016.Memory-enhanceddecoderforneuralmachinetrans- lation. arXivpreprintarXiv:1606.02003 . [Wangetal.2017] Wang,M.;Lu,Z.;Zhou,J.;andLiu,Q. 2017.Deepneuralmachinetranslationwithlinearassocia- tiveunit. arXivpreprintarXiv:1705.00861 . [Wuetal.2016] Wu,Y.;Schuster,M.;Chen,Z.;Le,Q.V.; Norouzi,M.;Macherey,W.;Krikun,M.;Cao,Y.;Gao,Q.; Macherey,K.;Klingner,J.;Shah,A.;Johnson,M.;Liu,X.; Kaiser,L.;Gouws,S.;Kato,Y.;Kudo,T.;Kazawa,H.; Stevens,K.;Kurian,G.;Patil,N.;Wang,W.;Young,C.; Smith,J.;Riesa,J.;Rudnick,A.;Vinyals,O.;Corrado,G.; Hughes,M.;andDean,J.2016.Google'sneuralmachine translationsystem:Bridgingthegapbetweenhumanand machinetranslation. CoRR abs/1609.08144. [Zhouetal.2016] Zhou,J.;Cao,Y.;Wang,X.;Li,P.;and Xu,W.2016.Deeprecurrentmodelswithfast-forward connectionsforneuralmachinetranslation. arXivpreprint arXiv:1606.04199 .  
B LOCK -S PARSE R ECURRENT N EURAL N ETWORKS SharanNarang sharan@baidu.com BaiduResearch EricUndersander undersandereric@baidu.com BaiduResearch GregoryDiamos gregdiamos@baidu.com BaiduResearch A BSTRACT RecurrentNeuralNetworks(RNNs)areusedinstate-of-the-artmodelsindomains suchasspeechrecognition,machinetranslation,andlanguagemodelling.Spar- sityisatechniquetoreducecomputeandmemoryrequirementsofdeeplearning models.SparseRNNsareeasiertodeployondevicesandhigh-endserverpro- cessors.Eventhoughsparseoperationsneedlesscomputeandmemoryrelativeto theirdensecounterparts,thespeed-upobservedbyusingsparseoperationsisless thanexpectedondifferenthardwareplatforms.Inordertoaddressthisissue,we investigatetwodifferentapproachestoinduce block sparsityinRNNs:pruning blocksofweightsinalayerandusinggrouplassoregularizationtocreateblocks ofweightswithzeros.Usingthesetechniques,wedemonstratethatwecancreate block-sparseRNNswithsparsityrangingfrom80%to90%withsmalllossinac- curacy.Thisallowsustoreducethemodelsizebyroughly10  .Additionally,we canprunealargerdensenetworktorecoverthislossinaccuracywhilemaintain- inghighblocksparsityandreducingtheoverallparametercount.Ourtechnique workswithavarietyofblocksizesupto32  32.Block-sparseRNNseliminate overheadsrelatedtodatastorageandirregularmemoryaccesseswhileincreasing hardwareefycomparedtounstructuredsparsity. 1I NTRODUCTION Improvementsinseveralapplicationssuchasspeechrecognition( Amodeietal. , 2016 ),language modeling( J ´ ozefowiczetal. , 2016 ),andmachinetranslation( Wuetal. , 2016 )arearesultoflarge RecurrentNeuralNetworks(RNNs)trainedonlargescaledatasets.Asthedatasetsavailabletotrain thesemodelshavegrown,sohavemodelsizes.Deploymentofsuchlargemodelsiscomputeand memoryintensive. Pruningdeepneuralnetworksisaneffectivestrategytoreducetheoverallmemoryandcompute requirementsofthesemodels( Narangetal. , 2017 ; Hanetal. , 2015 ).However,theseapproachesin- ducerandom,unstructuredsparsityintheweightmatrices.Speed-upobtainedwithrandomsparsity onvarioushardwareplatformsarelowerthanexpected(asshownin Narangetal. ( 2017 ); Narang &Diamos ( 2017 )).Sparseformatsdonotefutilizethehardwareresourcesduetostorage overheads,irregularmemoryaccess,andinabilitytotakeadvantageofarraydata-pathsinmodern processors. Blocksparsitycanaddresstheseissues.Savingindicesofnon-zeroblocksinsteadofindicesfor non-zeroelementsreducesthestorageoverheadbyafactorofblocksize.Block-sparseformats storeblockscontiguouslyinmemoryreducingirregularmemoryaccesses.Blocksparsityinherently allowsustotakeadvantageofarray-data-pathinmodernprocessors. InordertoinduceblocksparsityinRNNs,weproposeablockpruningapproachthatzerosout blocksofweightsinthematrixwhilethenetworkistraining.Attheendoftraining,thealgo- rithmcreatesablock-sparseRNN.Inadditiontothispruningtechnique,weexaminetheefy ofgrouplassoregularizationtoinduceblocksparsityinthenetwork.Wealsocombinegrouplasso regularizationwithblockpruning. 1 arXiv:1711.02782v1  [cs.LG]  8 Nov 2017Wedemonstratethatblockpruningandgrouplassoregularizationwithpruningaresuccessfulin creatingblock-sparseRNNs.Inducingblocksparsitywith4  4blocksinvanillaRNNsandGated RecurrentUnits(GRUs)( Choetal. , 2014 )resultsin9%to17%lossinaccuracycomparedtothe densebaseline.Modelsizereducesbynearly10  .Blocksizescanbescaledupto32  32withour approach.Largerblocksrequirelowersparsitytomaintainsimilaraccuracy.Wecanalsoreduce accuracylossbystartingwithalargerdensematrixthanthebaselineandthenpruningitdownwhile stillreducingthenumberofparameterscomparedtothebaseline. Ourapproachisagnostictotheoptimizationalgorithmanddoesnotrequireanyhyper-parameter retuning(besidespruningandregularizationhyper-parameters).Furthermore,sinceourapproach doesnotrequirere-trainingthemodel,trainingtimeremainsthesame. 2R ELATED W ORK Therehavebeenseveralapproachestoreducethenetworksizebypruningthemodel. Hanson& Pratt ( 1989 )useseveralbiastechniquestodecayweightsinanetwork. LeCunetal. ( 1989 )and Hassibietal. ( 1993 )bothuseHessian-basedapproachestopruneweightsbelowacertainthreshold. Simplerapproacheslikesortingorthresholdingcanbeusedtopruneaneuralnetwork. Hanetal. ( 2015 )and Liuetal. ( 2015 )pruneConvolutionNeuralNetworks(CNNs)whilemaintaininghigh accuracy. Yuetal. ( 2012 )useahardthresholdtoprunedeeplearningmodels. Narangetal. ( 2017 ) and Zhu&Gupta ( 2017 )prunerecurrentneuralnetworksduringtheinitialtrainingrunwithasmall accuracylossusinggradualpruning.Unlikeourtechnique,alloftheaboveapproachesinduce random,unstructuredsparsityinneuralnetworks. Severalapproachesexisttoinducestructuredsparsityinneuralnetworks. Maoetal. ( 2017 )use asimplethresholdbasedtechniquetocreatestructurallysparseCNNs. Yuetal. ( 2017 )propose ScalpelthatprunesCNNstakingintoaccounttheunderlyingtargethardwarearchitecture. Wen etal. ( 2017 )alterthestructureofLongShortTermMemory(LSTM)( Hochreiter&Schmidhuber , 1997 )tocreateLSTMswithsmallermemoryfootprint.Theydemonstratethatthistechniqueworks forlanguagemodelingonthePennTreeBankdataset.OurapproachworkswithbothvanillaRNN andGRUmodelstrainedonalarge-scaledatasetsforspeechrecognition. Grouplassoregularizationhasbeenusedasanefmethodforgeneratingsparsestructures ( Yuan&Lin , 2006b ; Kim&Xing , 2010 ). Wenetal. ( 2016 )usegrouplassoregularizationto inducestructuredsparsityinconvolutionalneuralnetworks.Regularizationisaknownmethodto inducesparsityindeepneuralnetworks( Faraoneetal. , 2017 ; Fanetal. , 2016 ).Tothebestofour knowledge,noneoftheseapproacheshavebeenusedwithRNNstrainedonlarge-scaledatasets. Otherapproachestoreducecomputeandmemoryfootprintfordeeplearningmodelsincludequan- tization( Micikeviciusetal. , 2017 ; Vanhouckeetal. , 2011 ; Rastegarietal. , 2016 ; Guptaetal. , 2015 ) andlow-rankfactorization( Deniletal. , 2013 ; Dentonetal. , 2014 ).Ourapproachisorthogonalto thesemethodsandcanbecombinedwiththem. 3 IMPLEMENTATION 3.1B LOCK P RUNING Ourapproachtopruningdeeplearningmodelsbuildsontheworkby Narangetal. ( 2017 ).They proposeaweightpruningalgorithmthatintroducesrandom,unstructuredsparsityinRNNs.Intheir work,theyproposepruningweightsbelowamonotonicallyincreasingthreshold.Theirpruning strategydoesnotimposeanystructureontheweights. Weextendthisapproachtopruneblocksofamatrixinsteadofindividualweights.Inordertoprune blocks,wepicktheweightwiththemaximummagnitudeasarepresentativefortheentireblock. Ifthemaximummagnitudeofablockisbelowthecurrentthreshold,wesetalltheweightsinthat blocktozeros.Figure 1 depictstheprocessofgeneratingablock-sparsemaskfromaweightmatrix foragiventhreshold.Theblock-sparsemaskismultipliedwiththeweightstogenerateblock-sparse weightmatrix.Themonotonicallygrowingthreshold(  )causesmoreblockstobeprunedastraining progress.Westoppruningmoreblocksafteraround40%oftraininghascompleted.Anyblocks 2 Figure1:Generatingblock-sparsemasksfromaweightmatrix Table1:Heuristicstopickhyper-parametersforblock-pruning HYPER-PARAMDESCRIPTIONHEURISTICVALUES start itr IterationtostartpruningStartofsecondepoch ramp itr Iterationtoincreasetherateof pruning Startof20%oftotalepochs end itr Iterationtostoppruningmorepa- rameters Startof40%oftotalepochs start slope (  ) Initialrateofincreasingthethresh- old SeeEquation 2 ramp slope ( ˚ ) Rateofincreasingthresholdafter rampiteration 1 : 2  to 1 : 7  freq Numberofiterationsafterwhich  isupdated 100 thathadbeenzeroedoutareheldatzeroevenafterpruninghasendedresultinginasparsemodelat theendoftraining. Narangetal. ( 2017 )usesixhyper-parameterstodeterminethethresholdatagiveniteration.Table 1 providesthedescriptionandheuristics(adaptedforblockpruning)forthesehyper-parameters.The start slope and ramp slope determinetherateatwhichthethresholdincreases.Inordertodetermine start slope ,theyrecommendusingweightsfromanexistingdensemodel.Toachieve90%sparsity, theyassign q totheweightwhichisthe90thpercentileoftheabsolutevaluesinaweightmatrix. Assuming ˚ is1.5  ,theyuseEquation 1 todetermine  .  = 2  q  freq 2  ( ramp itr  start itr )+3  ( end itr  ramp itr ) (1) Forblockpruning,weneedtomodifythe start slope totakeintoaccountthenumberofelementsin ablock( N b ).Inordertocalculatethe start slope ,wecalculate start slope forweightpruning (  w )usingtheEquation 1 .Given  w ,wesuggestusingEquation 2 todeterminetheinitialslope (  b )forblockpruning.Basedonempiricalresults,wehavefoundthatusingthisapproachallows ustoachieveblocksparsityrangingfrom85%to95%.Furthertuningofthesehyper-parametersis requiredtoachievedesiredblocksparsity.  b =  w  4 p N b (2) Weprunealltherecurrentandfullyconnectedlayersinthenetworkusingthesameblocksize.The pruninghyper-parametersaresameforeachtypeoflayerinthenetwork-recurrentweightlayer andlinear/fullyconnectedlayer. 3 3.2G ROUP L ASSO R EGULARIZATION Grouplassoisatypeofweightregularizationthatworksongroupsofweightsandcanzerooutall theweightsinagroup.Inordertoinduceblocksparsityinthenetwork,wedivideallweightsinthe modelintoblocks.Foreachblock,weaddalosstermproportionaltothe ` 2 normoftheblock. L = L training +  g G X g =1 k w ( g ) k 2 where w ( g ) isablockofweights, k w ( g ) k 2 isthe ` 2 normoftheblock,and G isthetotalnumber ofblock.Ouruseof ` 2 normisavariantofthemoregeneralgrouplassoin Yuan&Lin ( 2006a )as k n k K =( n 0 Kn ) 1 = 2 . Grouplassohasthepropertythatalargeenough  g willdriveallweightswithincertaingroupsto hardzeros.Thus,weexploregrouplassoregularizationtoproduceblock-structuredsparsity.We chooseanappropriateconstant  g forthedurationoftraining. Oneinterpretationofweightregularizationisthatlessimportantweightsaredriventowardszero andmoreimportantweightsretainlargeabsolutevalues.Thus,wecombinegrouplassowithblock pruning,suchthatgrouplassoguidestheselectionofblockstoprune.Weapplygrouplassoregular- izationtocoincidewiththepruningschedule.Weturnoffregularizationwhenthepruningschedule ends,whichistypicallyafteraround40%oftrainingepochs.AsdiscussedinSection 3.1 ,weights thatwerealreadysettozeroremainunchangedafterthispoint.Grouplassoisrelatedtothewell- known ` 1 regularization.InAppendix A ,wediscussexplorationof ` 1 regularizationcombinedwith weightpruning. 4E XPERIMENTS Werunblocksparsityexperimentsontwodifferentspeechrecognitionmodelsfrom Amodeietal. ( 2016 ).TheRNNmodelconsistsofaconvolutionallayer,followedbysevenbidirectionalrecurrent layersandaConnectionistTemporaltion(CTC)layer( Gravesetal. , 2006 ).Thebaseline RNNmodel(RNNDense1760)consistsof1760hiddenunitsineachrecurrentlayerwithnearly 67millionparameters.TheGRUmodelconsistsoftwoconvolutionallayers,threerecurrentlayers withGRUcellsandaCTClayer.ThebaselineGRUmodel(GRUDense2560)consistsof2560 hiddenunitsineachlayerwithatotalof115millionparameters.Thedatasetusedfortrainingthese modelsconsistsof2100hoursofEnglishspeech.Weuseavalidationsetconsistingof3.46hours ofdata.TheCharacterErrorRate(CER)resultsarereportedonanindependenttestset,consisting of2.9hoursofEnglishdata. InordertointroduceblocksparsityinRNNs,werunthreedifferenttypesofexperiments-Block Pruning(BP),GroupLasso(GL),andGroupLassowithblockpruning(GLP).Wepruneweights intherecurrentlayers(bothlinearandrecurrentweights)andfullyconnectedlayers.Biases,batch- normalizationparametersandweightsintheconvolutionalandCTClayersarenotprunedsincethey accountforasmallportionofthetotalweightsinthenetwork.Besidespruninghyper-parameters and  g ,nootherhyper-parameterchangeswererequiredforsparsetrainingruns.Themodelsare trainedusingNesterovStochasticGradientDescent(SGD)withmomentum.Allmodelsaretrained for25epochs.Thedensemodelsaretrainedwithoutanyregularization. InSection 4.1 ,wereportresultsfordifferentsparsemodelsprunedwith4  4blocks.Section 4.2 comparestheresultsforthetwodifferentgrouplassoexperiments.Section 4.3 discussestheimpact ofvaryingtheblocksizeontheaccuracyofthemodel. 4.1B LOCK S PARSITY WeconductthreetypesofexperimentsforbothRNNandGRUmodels:pruningthebaselinemodel, trainingsmallerdensemodels,andpruningamodellargerthanthebaselinemodel. Initially,weprunethebaselineRNNandGRUmodels.UsingBPandGLP,weareabletoreduce theparametercountforboththesemodelsbynearly10  .AsshowninTable 2 ,thesparseRNN modelwith1760hiddenunitshasanoverallblocksparsityof89%witharelativelossinaccuracy 4 Table2:GRUandbidirectionalRNNmodelresultswith4  4blocks #PARAMSRELATIVEPRUNING MODEL (inmillions) SPARSITYCERPERFALGORITHM RNNDense1760670.0%15.360.0%N/A RNNDense70411.60.0%18.95-23.4%N/A RNNSparse17607.389.2% 17.93 -16.7%BP RNNSparse256012.990.8%15.89-3.4%GLP RNNSparse307225.887.3% 15.66 -1.9%BP GRUDense25601150.0%15.420.0%N/A GRUDense70411.00.0%21.26-37.9%N/A GRUSparse256010.890.6% 16.78 -8.8%GLP GRUSparse358425.688.4% 16.23 -5.2%BP of16.7%.ThesparseGRUmodelachievesslightlyhighersparsity(90%)whilelosingonly8.8%of accuracy.Thisindicatesthattheblock-sparseGRUmodelretainsmostofthecapacityofthedense model. Secondly,wetraindensemodelswithfewerparameterstodetermineifsparsityisreducingov tinginthelargedensebaselinemodels.ForbothRNNandGRUmodels,wetrainadensemodel with704hiddenunitsineachlayer,resultinginapproximatelythesamenumberofparametersas thesparsemodels.Table 2 showsthatthesedensemodelsperformworsethanthesparsemod- elsforbothRNNandGRUmodels.Largesparsemodelsareabetterapproachtoreduceparameter countthandensesmallmodels. Finally,wetrainsparsemodelswithmorehiddenunitsineachrecurrentlayerstorecovertheaccu- racy.ForRNNmodels,weincreasethehiddenlayersizeto2560and3072.AsshowninTable 2 , theRNNsparse3072isonly1.9%worsethanthedensebaselinemodel.The2560and3072sparse RNNmodelsreducetheoverallparametercountby5  and2.5  respectively.Similarly,pruningthe GRUmodelwith3584hiddennodesreducestheaccuracylosstoabout5%whilestillshrinkingthe modelby4.5  . Ourevaluationshowthatinducingblocksparsityinthebaselinemodelallowsustoreducethemodel sizebyapproximately10  withasmalllossinaccuracy.Pruningamodellargerthanthebaseline modelallowstoreducetheaccuracylosswhilereducingmodelsizebynearly5  .Ourresultsalso indicatethatlargesparsemodelsresultinbetteraccuracythatsmalldensemodels. 4.2G ROUP L ASSO V ARIANTS Table 3 highlightstheresultsofGLandGLPexperimentsfortwodifferentmodels.ForbothRNN modelswith1760and2560hiddennodes,grouplassowithoutanypruningdoesworse thancombininggrouplassowiththeblockpruningmethodology. Table3:GrouplassoexperimentsforRNNmodelswith4  4blocks #PARAMSRELATIVEPRUNING MODEL (inmillions) SPARSITYCERPERFALGORITHM RNNSparse176010.983.3%30.14-96%GL RNNSparse17606.290.8% 19.24 -25.3%GLP RNNSparse256024.482.8%27.4-78.4%GL RNNSparse256012.990.8% 15.89 -3.4%GLP Inordertoachievehighsparsity(80%orhigher),weneedtoset  g toarelativelyhighvalue.For instance,experimentsusingGLrequireda  g ofapproximately3  largerthantheGLPexperiments. Thishighregularizationfactorhurtsthemodelaccuracy.Thedensebaselinemodelistrainedwithout 5 Table4:GRUandbidirectionalRNNresultsfordifferentblocksizesusingBP BLOCK#PARAMSRELATIVE MODELSIZE (inmillions) SPARSITYCERPERF RNNSparse1x17.389.2%17.32-12.8% RNNSparse4x47.389.2%17.93-16.7% RNNSparse12x210.884.1%16.96-9.99% RNNSparse8x810.784.1%17.66-14.9% RNNSparse16x1611.183.6%17.1-11.3% RNNSparse32x3214.179.1%16.67-8.5% GRUSparse1x113.188.7%16.55-7.3% GRUSparse4x416.286.0%16.97-10.5% GRUSparse16x1620.881.9%16.84-9.2% anyregularization.Evenwithoutregularization,thedensemodeldoesnotovthetrainingdataset. Grouplassoexperimentsthetrainingdataduetothehighvalueof  g .Grouplassocould bemoresuccessfulininducingsparsitywherethedensemodelovthetrainingdataset.Inthe GLPexperiments,wecanreducetheregularizationfactorsincepruningforcessmallermagnitude weightstozero.Thiscombinedapproachresultsinimprovedaccuracywhilemaintaininghighlevels ofsparsity. 4.3B LOCK S IZE V ARIATION Table 4 showstheresultsofvaryingblocksizeforpruningforRNNandGRUbaselinemodels. Increasingtheblocksizeto16  16and32  32requiresreducingthesparsityto83.6%and79.1% respectivelyforRNNmodelstoobtaingoodaccuracy.SimilarresultsholdtruefortheGRUmodel aswell.Largesparseblocksreducememoryoverheadforstoringnonzerovaluesandcantake advantageofarraydata-pathsinmoremodernprocessors.Therefore,eventhoughlargeblocks achievelowersparsity,theyresultinlowermemoryandcomputerequirements. 5P ERFORMANCE Theprimaryadvantageofablock-sparseformatistoincreasehardwareefybymakingthe computationmoreregular.Sparseformatsincuratleastthreetypesofoverhead:i)indexingover- head,ii)irregularmemoryaccesses,andii)incompatibilitywitharray-data-paths,allofwhichare mitigatedbyusinglargerblocksizes. IndexingOverheads .Sparseformatsuseextramemorytotrackthelocationofeachnon-zerovalue. Forexample,thecompressed-sparse-row(CSR)formatusesapproximatelytwoextraindexvalues foreachnon-zerovalue.Thesizeoftheseextraindexvaluesdependsonthemaximummatrix size.Using16-bitindicesincurs32-bitsofoverheadpernon-zerovalueandallowsupto64kx 64kmatricestobesupported.Assumingthatneuralnetworkweightsarerepresentedwith16-bits asin Micikeviciusetal. ( 2017 ),thisisa 200% overhead.Blocksparsityreducesthisoverheadby afactoroftheblocksizebecausetheindexissharedovertheentireblock.Forexample,usinga blocksizeof4x4reducesthememorybloatto 12 : 5% ,andusingablocksizeof16x16reducesthe overheadtolessthan 1% . IrregularMemoryAccesses .Cacheslines,DRAMrowbuffers,andTLBsprovidethebestperfor- mancewhenmemoryisaccessedinrelativelylargecontiguousunits(e.g.64bytesforcachelines, 4KBforaDRAMrow)asopposedtoinrandomaccesses.Block-sparseformatsstore blockscontiguouslyinmemory,resultinginlargecoalescedaccesses. ArrayData-Paths .Fine-grainedsparsitycannotdirectlytakeadvantageofarray-data-paths,such asthe16x16TensorCoreunitsintheVoltaGPUdescribedby NVIDIA ( 2017 )orthe256  256units intheGoogleTPUdescribedby Jouppietal. ( 2017 ).Thereareadvantagesofusing theseunits,forexample,ontheVoltaV100GPU,theyenableupto8xhigherthroughputthanthe 6 (a)Speed-upforRNN1760layermatrixmultiply (b)Speed-upforGRU2560layermatrixmultiply Figure2:Speed-upforsparsematrixdensematrixmultiply.BenchmarksarerunonTitanXMaxwell usingtheCuSparselibrary.SparsematricesarerepresentedintheCSRformat.RNNmatrixsizes are(1760,1760)with90%sparsityand(1760,batch size).GRUmatrixsizesare(7680,2560)with 95%sparsityand(2560,batch size).ResultsareshownformatricesfromWeightPruning(WP)and BlockPruning(BP). SIMDdata-paths.Inordertokeeptheseunitsbusy,theblocksizeshouldbeatleastaslargeasthe hardwaredata-pathsize(i.e.16  16orgreateronV100). Figure 2 showsthatblock-sparsematricesachievehigherspeed-upthanunstructuredsparsityfor largebatchsizes.Inthiscase,thespeed-upisachievedduetoreducingirregularmemoryaccesses andimprovingloadbalance.4  4blockshavehigherspeed-upthan16  16blocks.Furtherinvesti- gationisneededtounderstandthisbehavior. 6D ISCUSSION 6.1P RUNING C HARACTERISTICS InFigure 3a ,weplotthepruningscheduleofarecurrentandlinearlayerofthebidirectionalmodel trainedwithBPandWeightPruning(WP)( Narangetal. , 2017 ).Forallthreealgorithms,pruning beginsjustaftertheepochat2700iterations.TheBPandGLPmodelsresultinasharpercurve withmoreweightsbeingsettozeroinashortspanofiterations.Intheseexperiments,weusethe max functiontoreducetheblockstoasinglevaluewhichcouldbethecauseofthesharpnessin pruning.AlsotheGLPmodelreaches90%sparsityjustbefore10,000iterationswhichis cantlyearlierthantheBPmodel.GLPtrainingencouragessparsityearlyoninthetrainingrunby pushingtheblocksofweightstowardszero. Figure 3b showsthehistogramofthenumberofoutputconnectionsforalltheneuronsinanetwork fortwomodelswithdifferentsparsityprunedwithBP.The94%sparsemodeldoes worsethanthe89%sparse.Forthemodelwith89%sparsity,only180neuronshavealltheiroutput weightssettozerooutofatotalof38270.Thismodelproducedgoodaccuracyrelativetothedense baseline.However,increasingthesparsityto94%forthelayerresultsin1620neuronshavingall zerooutputweights.Additionally,alotmoreneuronshaveasmallernumberofnon-zerooutput weights. 6.2I MPACTOFSPARSITYONACCURACY UsingourbaselineRNNmodel,werunmanyweightandblockpruningexperiments,varyinghyper- parameterstoproduceaspectrumofresultsrangingfrom70%to97%sparsity.Fortheseexperi- ments,themodelsaretrainedfor20epochsandtheaccuracyismeasuredonthevalidationset insteadofthetestset.Therefore,therelativeaccuracyforthesemodelsisslightlydifferentfromthe resultsreportedinSection 4.1 .AsshowninFigure 4a ,modelsprunedusingWPwithsparsityless than95%haverelativeaccuracyrangingfrom-20%to-27%.Increasingthesparsityforthemodel beyond95%resultsin30%ormoreaccuracyloss.Thisaccuracyﬂcliffﬂisearlierformodelspruned withblocksparsity.Forblocksize4  4,modelswithsparsitygreater90%yieldarelativeaccuracy 7 (a) (b) Figure3:Figure 3a showsthepruningschedulefortwolayersinthenetworkforWP,GLPandBP models.TheGLPandBPmodelsuseblocksizeof4x4.Figure 3b plotsthehistogramofthenumber ofoutputconnectionsforallneuronsinthenetworkusingblockpruningwith4  4blocks. (a) (b) Figure4:Figure 4a showstherelativeaccuracyfordifferentblocksizes(4x4,16x16)andWPfor varyingsparsityontheRNN1760model.Anymodelswithrelativeaccuracyworsethan-75%are cappedat75%.Figure 4b showsthesparsityofdifferentrecurrentlayersinthenetworkintheRNN model,prunedusingBPandWP. lossof30%orhigher.Similarly,forblocksof16  16,modelswithsparsitygreaterthan86%have 30%ormoreaccuracyloss.Asimilartrendisobservedforblocksize32  32.Thisindicatesthat thereisatradeoffbetweensparsity,blocksizeandaccuracyofthemodel. 6.3S PARSITYVS L AYERS Figure 4b showsthesparsityofalltherecurrentlayersinthenetworkusingBPandWP.Allrecurrent layershavethesamepruninghyper-parameters.Layer1istherecurrentlayerandlayer14isthe recurrentlayerbeforetheCTCcostlayer.Forbothblockpruningandweightpruning,wesee thattheinitiallayersareprunedmoreaggressivelycomparedtothelayers.Increasingsparsity inthelayersclosertotheoutputresultsinpooraccuracy.Additionally,thevarianceinsparsity acrossthelayersincreaseswiththeblocksize.Thisincreasingvariancemakesithardertoincrease theblocksizebeyond32  32withthesamepruninghyper-parametersforallrecurrentlayers. 7C ONCLUSIONAND F UTURE W ORK Wehavedemonstratedthatusingblockpruningandgrouplassocombinedwithpruningduringtrain- ingwecanbuildblock-sparseRNNsthatareaboutasaccurateasthedensebaselinemodels.The 8 block-sparsemodelshavefewerparametersthanthedensebaselinesreducingmemory requirements.Block-sparsemodelscantakeadvantageoftheunderlyinghardwareef. Wewouldliketoinvestigateifpruningcanbeperformedevenearlierinthetraining,therebyallow- ingustotrainsparsemodels.Trainingsparsemodelswouldallowustoreaptheofsparsity duringtrainingresultinginlessercomputeandmemorydemands.Furtherworkremainstoimple- mentefblock-sparsematrixdenesematrix/vectormultipliesforGPUandARMprocessors thatwouldprovideincreasedspeed-upduringdeployment. A CKNOWLEDGEMENTS WewouldliketothankSongHan,MohammadShoeybi,andMarkusKlieglforhelpfuldiscussions relatedtothiswork.WewouldalsoliketothankVarunAroraforcreatingainthepaper. 9 R EFERENCES DarioAmodei,RishitaAnubhai,EricBattenberg,CarlCase,JaredCasper,BryanCatanzaro,JingDongChen, MikeChrzanowski,AdamCoates,GregDiamos,etal.Deepspeech2:End-to-endspeechrecognitionin englishandmandarin.In ProceedingsofThe33rdInternationalConferenceonMachineLearning ,pp. 173Œ182,2016. KyunghyunCho,BartVanMerri ¨ enboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,Holger Schwenk,andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatistical machinetranslation. arXivpreprintarXiv:1406.1078 ,2014. MishaDenil,BabakShakibi,LaurentDinh,Marc'AurelioRanzato,andNandodeFreitas.Predictingparame- tersindeeplearning. CoRR ,abs/1306.0543,2013.URL http://arxiv.org/abs/1306.0543 . EmilyDenton,WojciechZaremba,JoanBruna,YannLeCun,andRobFergus.Exploitinglinearstructurewithin convolutionalnetworksforefevaluation. CoRR ,abs/1404.0736,2014.URL http://arxiv.org/ abs/1404.0736 . QinweiFan,WeiWu,andJacekMZurada.Convergenceofbatchgradientlearningwithsmoothingregular- izationandadaptivemomentumforneuralnetworks. SpringerPlus ,5(1):295,2016. JulianFaraone,NicholasFraser,GiulioGamberdella,MichaelaBlott,andPhilipHWLeong.Compressinglow precisiondeepneuralnetworksusingsparsity-inducedregularizationinternarynetworks. arXivpreprint arXiv:1709.06262 ,2017. AlexGraves,SantiagoFern ´ andez,FaustinoGomez,andJ ¨ urgenSchmidhuber.Connectionisttemporalclassi- labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In Proceedingsofthe23rd internationalconferenceonMachinelearning ,pp.369Œ376.ACM,2006. SuyogGupta,AnkurAgrawal,KailashGopalakrishnan,andPritishNarayanan.Deeplearningwithlimited numericalprecision.In Proceedingsofthe32ndInternationalConferenceonMachineLearning(ICML- 15) ,pp.1737Œ1746,2015. SongHan,HuiziMao,andWilliamJDally.Deepcompression:Compressingdeepneuralnetworkswith pruning,trainedquantizationandhuffmancoding. arXivpreprintarXiv:1510.00149 ,2015. StephenJos ´ eHansonandLorienPratt.Advancesinneuralinformationprocessingsystems1.chapterCom- paringBiasesforMinimalNetworkConstructionwithBack-propagation,pp.177Œ185.MorganKaufmann PublishersInc.,SanFrancisco,CA,USA,1989.ISBN1-558-60015-9.URL http://dl.acm.org/ citation.cfm?id=89851.89872 . BabakHassibi,DavidGStork,andGregoryJWolff.Optimalbrainsurgeonandgeneralnetworkpruning.In NeuralNetworks,1993.,IEEEInternationalConferenceon ,pp.293Œ299.IEEE,1993. SeppHochreiterandJ ¨ urgenSchmidhuber.Longshort-termmemory. NeuralComput. ,9(8):1735Œ1780, November1997.ISSN0899-7667.doi:10.1162/neco.1997.9.8.1735.URL http://dx.doi.org/ 10.1162/neco.1997.9.8.1735 . NormanP.Jouppi,CliffYoung,NishantPatil,DavidPatterson,GauravAgrawal,RaminderBajwa,SarahBates, SureshBhatia,NanBoden,AlBorchers,RickBoyle,Pierre-lucCantin,CliffordChao,ChrisClark,Jeremy Coriell,MikeDaley,MattDau,JeffreyDean,BenGelb,TaraVazirGhaemmaghami,RajendraGottipati, WilliamGulland,RobertHagmann,RichardC.Ho,DougHogberg,JohnHu,RobertHundt,DanHurt,Julian Ibarz,AaronJaffey,AlekJaworski,AlexanderKaplan,HarshitKhaitan,AndyKoch,NaveenKumar,Steve Lacy,JamesLaudon,JamesLaw,DiemthuLe,ChrisLeary,ZhuyuanLiu,KyleLucke,AlanLundin,Gordon MacKean,AdrianaMaggiore,MaireMahony,KieranMiller,RahulNagarajan,RaviNarayanaswami,Ray Ni,KathyNix,ThomasNorrie,MarkOmernick,NarayanaPenukonda,AndyPhelps,JonathanRoss,Amir Salek,EmadSamadiani,ChrisSevern,GregorySizikov,MatthewSnelham,JedSouter,DanSteinberg,Andy Swing,MercedesTan,GregoryThorson,BoTian,HoriaToma,ErickTuttle,VijayVasudevan,Richard Walter,WalterWang,EricWilcox,andDoeHyunYoon.In-datacenterperformanceanalysisofatensor processingunit. CoRR ,abs/1704.04760,2017.URL http://arxiv.org/abs/1704.04760 . RafalJ ´ ozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.Exploringthelimitsof languagemodeling. CoRR ,abs/1602.02410,2016.URL http://arxiv.org/abs/1602.02410 . SeyoungKimandEricPXing.Tree-guidedgrouplassoformulti-taskregressionwithstructuredsparsity. 2010. YannLeCun,JohnSDenker,SaraASolla,RichardEHoward,andLawrenceDJackel.Optimalbraindamage. In NIPs ,volume2,pp.598Œ605,1989. 10 BaoyuanLiu,MinWang,HassanForoosh,MarshallTappen,andMariannaPensky.Sparseconvolutional neuralnetworks.In ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition ,pp. 806Œ814,2015. HuiziMao,SongHan,JeffPool,WenshuoLi,XingyuLiu,YuWang,andWilliamDally.Exploringthe regularityofsparsestructureinconvolutionalneuralnetworks.052017. P.Micikevicius,S.Narang,J.Alben,G.Diamos,E.Elsen,D.Garcia,B.Ginsburg,M.Houston,O.Kuchaiev, G.Venkatesh,andH.Wu.MixedPrecisionTraining. ArXive-prints ,October2017. SharanNarangandGregoryDiamos.Deepbench. https://svail.github.io/ DeepBench-update/ ,2017.Accessed:2017-06-28. SharanNarang,GregoryDiamos,ShubhoSengupta,andErichElsen.Exploringsparsityinrecurrentneural networks. arXivpreprintarXiv:1704.05119 ,2017. NVIDIA.NVIDIATeslaV100GPUArchitecture.Technicalreport,2017. MohammadRastegari,VicenteOrdonez,JosephRedmon,andAliFarhadi. XNOR-Net:ImageNetClas- UsingBinaryConvolutionalNeuralNetworks ,pp.525Œ542.SpringerInternationalPublish- ing,Cham,2016.ISBN978-3-319-46493-0.doi:10.1007/978-3-319-46493-0 32.URL https: //doi.org/10.1007/978-3-319-46493-0_32 . VincentVanhoucke,AndrewSenior,andMarkZ.Mao.Improvingthespeedofneuralnetworksoncpus.In DeepLearningandUnsupervisedFeatureLearningWorkshop,NIPS2011 ,2011. WeiWen,ChunpengWu,YandanWang,YiranChen,andHaiLi.Learningstructuredsparsityindeepneural networks.In AdvancesinNeuralInformationProcessingSystems ,pp.2074Œ2082,2016. WeiWen,YuxiongHe,SamyamRajbhandari,WenhanWang,FangLiu,BinHu,YiranChen,andHaiLi. Learningintrinsicsparsestructureswithinlongshort-termmemory. arXivpreprintarXiv:1709.05027 ,2017. YonghuiWu,MikeSchuster,ZhifengChen,QuocV.Le,MohammadNorouzi,WolfgangMacherey,Maxim Krikun,YuanCao,QinGao,KlausMacherey,JeffKlingner,ApurvaShah,MelvinJohnson,XiaobingLiu, LukaszKaiser,StephanGouws,YoshikiyoKato,TakuKudo,HidetoKazawa,KeithStevens,GeorgeKurian, NishantPatil,WeiWang,CliffYoung,JasonSmith,JasonRiesa,AlexRudnick,OriolVinyals,GregCorrado, MacduffHughes,andJeffreyDean.Google'sneuralmachinetranslationsystem:Bridgingthegapbetween humanandmachinetranslation. CoRR ,abs/1609.08144,2016.URL http://arxiv.org/abs/1609. 08144 . DongYu,FrankSeide,GangLi,andLiDeng.Exploitingsparsenessindeepneuralnetworksforlargevocabu- laryspeechrecognition.In 2012IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing (ICASSP) ,pp.4409Œ4412.IEEE,2012. JiecaoYu,AndrewLukefahr,DavidPalframan,GaneshDasika,ReetuparnaDas,andScottMahlke.Scalpel: Customizingdnnpruningtotheunderlyinghardwareparallelism.In Proceedingsofthe44thAnnualInter- nationalSymposiumonComputerArchitecture ,pp.548Œ560.ACM,2017. MingYuanandYiLin.Modelselectionandestimationinregressionwithgroupedvariables.2006a. MingYuanandYiLin.Modelselectionandestimationinregressionwithgroupedvariables. Journalofthe RoyalStatisticalSociety:SeriesB(StatisticalMethodology) ,68(1):49Œ67,2006b. MichaelZhuandSuyogGupta.Toprune,ornottoprune:exploringtheefyofpruningformodelcom- pression. arXivpreprintarXiv:1710.01878 ,2017. 11 A ` 1 AND ` 1 = 2 R EGULARIZATION Priortoourworkwithgrouplassoregularization,weconsidered ` 1 and ` 1 = 2 regularizerstoinduce sparsityinthenetwork.Theseregularizersactonindividualweightsandcouldaidininducing unstructuredsparsityinthenetwork. ` 1 regularizationisas: L = L training +  k X i =1 j w i j where j w i j istheabsolutevalueofaweightand k isthetotalnumberofweights.Notethegradient expressionforeachweight w j : @ @w j k X i =1 j w i j = sgn ( w j ) Aswiththegrouplassoexperimentsdescribedin 3.2 ,weexplore ` 1 regularizationwithandwithout pruning.Theweightpruning(WP)algorithmfrom Narangetal. ( 2017 )isusedalongwithregu- larization.Themotivationisthesameasgrouplassoblocksparsityexperiments:eithertoguide pruningortoproducesparsitydirectly. Wealsoexplore ` 1 = 2 regularizationwhichisas: L = L training +  k X i =1 j w i j 1 = 2 Fanetal. ( 2016 )uses ` 1 = 2 regularizationtoproducesparsitydirectly.Thegradientfor ` 1 = 2 regular- izationis 1 2 j w j j  1 = 2 .Thistermissmallerforweightswithlargermagnitude.Ourexpectationisthat ` 1 = 2 willdriveunimportantweightstowardszerowhileleavinglargeweightsrelativelyunaffected, thusavoidingtheaccuracylossassociatedwithexcessiveregularization. Forour ` 1 and ` 1 = 2 experiments,weusetheDeepSpeech2BidirectionalRNNbaselinemodel describedinSection 4 .Thesemodelsaretrainedfor25epochsonourinternaltrainingdatasetof 2000hours.Theresultsarereportedonaindependenttestsetconsistingof2.9hours. Table5: ` 1 and ` 1 = 2 resultswiththebidirectionalRNNmodelwith1760hiddenunits #PARAMSRELATIVEPRUNING MODEL (inmillions) SPARSITYCERPERFALGORITHM RNNDense670.0%15.360.0%N/A RNNSparse7.389.2% 17.32 -12.8%Weightpruning RNNSparse11.283.6%24.8-61.5% ` 1 RNNSparse7.489.1% 17.28 -12.5% ` 1 withpruning RNNSparse6.690.3%18.50-20.4% ` 1 = 2 withpruning Withoutpruning, ` 1 modelresultsinworseaccuracycomparedtothedensebaseline. Combining ` 1 withweightpruningallowsustorecoverthelossinaccuracywithsimilarsparsity. The ` 1 = 2 withpruningmodelperformsworsethanthe ` 1 withpruningmodel.Comparingthetwo regularizers,thisresultindicatesthat ` 1 isbetteratguidingpruningthan ` 1 = 2 ,moresuitableasa regularizer,orboth. Similartogrouplassoexperiments, ` 1 regularizationexperimentsrequireahigher  toachievehighsparsitywithoutanypruning.Wesuspectthattheseregularizerswouldbemore successfulininducingsparsityformodelsthatovthetrainingtrainingdataset. 12  
RevisitingtheEffectivenessofOff-the-shelfTemporalModeling ApproachesforLarge-scaleVideo YunlongBian,ChuangGan  ,XiaoLiu,FuLi,XiangLong YandongLi,HengQi,JieZhou,ShileiWen,YuanqingLin BaiduIDL&TsinghuaUniversity Abstract Thispaperdescribesoursolutionforthevideorecogni- tiontaskofActivityNetKineticschallengethatrankedthe 1stplace.Mostofexistingstate-of-the-artvideorecog- nitionapproachesareinfavorofanend-to-endpipeline. OneexceptionistheframeworkofDevNet[3].Themerit ofDevNetisthattheystusethevideodatatolearna network( i.e .ortrainingfromscratch).Instead ofdirectlyusingtheend-to-endscores( e.g . softmaxscores),theyextractthefeaturesfromthelearned networkandthenfedthemintotheoff-the-shelfmachine learningmodelstoconductvideoHowever, theeffectivenessofthislineworkhaslong-termbeenig- noredandunderestimated.Inthissubmission,weexten- sivelyusethisstrategy.Particularly,weinvestigatefour temporalmodelingapproachesusingthelearnedfeatures: Multi-groupShiftingAttentionNetwork,TemporalXception Network,Multi-streamsequenceModelandFast-Forward SequenceModel.Experimentresultsonthechallenging Kineticsdatasetdemonstratethatourproposedtemporal modelingapproachescanimproveexistingap- proachesinthelarge-scalevideorecognitiontasks.Most remarkably,ourbestsingleMulti-groupShiftingAttention Networkcanachieve77.7%intermoftop-1accuracyand 93.2%intermoftop-5accuracyonthevalidationset. 1.Introduction Videounderstandingisamongoneofthemostfunda- mentalresearchproblemsincomputervisionandmachine learning.Theubiquitousvideoacquisitiondevices(e.g., smartphones,surveillancecameras,etc.)havecreated videosfarsurpassingwhatwecanwatch.Ithasthere- forebeenapressingneedtodevelopautomaticvideounder- standingandanalysisalgorithmsforvariousapplications. Torecognizeactionsandeventsinvideos,recentap- proachesbasedondeepconvolutionalneuralnetworks  Correspondingauthor. (CNNs)[9,13,3,17,4]and/orrecurrentnetworks[7,15,1] haveachievedstate-of-the-artresults.However,duetothe lackofpublicavailabledatasets,existingvideorecognition approachesarerestrictedtounderstandsmall-scaledata, whilelarge-scalevideounderstandingremainsanunder- addressedproblem.Toremedythisissue,GoogleDeep- Mindreleasesanewlarge-scalevideodataset,namedas Kineticsdataset[10],whichcontains300Kvideoclipsof 400humanactionclass. Toaddressthischallenge,oursolutionfollowsthestrat- egyofDevNetframework[3].Particularly,welearn thebasicRGB,FlowandAudioneutralnetworkmodels usingthevideos.Thenweextractthemultimodalityfea- tureandfedthemintodifferentoff-shelftemporalmodels. Wealsodesignfournoveltemporalmodelingapproaches, namelyMulti-groupShiftingAttentionNetwork,Temporal XceptionNetwork,Multi-streamsequenceModelandFast- ForwardSequenceModel.Experimentresultsveritytheef- fectivenessofthefourmodelsoverthetraditionaltemporal modelingapproaches.Wealsothatthesefourtemporal modelingapproachesarecomplementarywitheachothers andleadtothestate-of-the-artsperformancesafterensem- ble. Theremainingsectionsareorganizedasfollows.Section 2presentsthebasicmultimodalfeatureextraction.Sec- tion3describeourproposedoff-shelftemporalmodeling approaches.Section4reportsempiricalresults,followed bydiscussionsandconclusionsinSection5. 2.MultimodalFeatureExtraction Videosarenaturallymultimodalbecauseavideocanbe decomposedintovisualandacousticcomponents,andthe visualcomponentcanbefurtherdividedintospatialand temporalparts.Weextractedmultimodalfeaturestobest representvideosaccordingly. 2.1.VisualFeature Asin[13],weusedRGBimagesforspatialfeatureex- tractionandstackedopticalwfortemporalfea- 1 arXiv:1708.03805v1  [cs.CV]  12 Aug 2017Figure1.Multi-groupShiftingAttentionNetwork. tureextraction.WetrieddifferentConvNetarchitectures andfoundInception-ResNet-v2[16]outperformsothersin bothspatialandtemporalcomponents.TheRGBmodelis initializedwithpre-trainedmodelfromImageNetand tunedintheKineticsdataset,whilethewmodelisinitial- izedfromtheRGBmodel.Inspiredby[19],the temporalsegmentnetworkframeworkisusedandthreeseg- mentsaresampledfromeachtrimmedvideoforvideo-level training.Duringtesting,wecandenselyextractfeaturesfor eachframesinthevideo. 2.2.AcousticFeature WeuseConvNet-basedaudiosystem[6] toextractacousticfeature.Theaudioisdividedinto960ms frames,andtheframesareprocessedwithFouriertrans- formation,histogramintegrationandlogarithmtransforma- tion.Theresultingframecanbeseenasa 96  64 image thatformtheinputofaVGG16[14]image model.Similarwiththevisualfeature,wetrainedtheacous- ticfeatureinthetemporalsegmentnetworkframework. 3.Off-shelfTemporalModelingApproaches Inthissection,wepresentabriefintroductionofourpro- posedshiftingattentionnetworkandtemporalXceptionnet- work.Moreimplementationdetailsandanalysiswillbeina followingtechniquereport.Wealsorefer[11]forthedetails ofmulti-streamsequencemodelandfast-forwardsequence model. 3.1.ShiftingAttentionNetwork Attentionmodelshaveshowngreatpotentialinsequence modeling.Forexample,numerouspureattentionarchitec- tures[18,12]havebeenproposedandachievedpromising resultsinnaturallanguageprocessingproblems.Inorderto explorethecapabilitiesofattentionmodelsinactionrecog- nition,ashiftingattentionnetworkarchitectureisproposed, whichisefelegantandsolelybasedonattention. 3.1.1ShiftingAttention Anattentionfunctioncanbeconsideredasmappingasetof inputfeaturestoasingleoutput,wheretheinputandoutput arebothmatricesthatconcatenatefeaturevectors.Theout- putoftheshiftingattentionSATT ( X ) iscalculatedthrough ashiftingoperationbasedonaweightedsumofthefea- tures: SATT ( X )=   a + b k   a + b k 2 ; (1) where  isaweightvectorcalculatedas  = softmax (   wX T ) ; (2) w islearnablevector, a and b arelearnablescalars,and  isahyper-parametertocontrolthesharpnessofthedistri- bution.Theshiftingoperationactuallyshiftstheweighted sumandatthesametimeensuresscale-invariance.Theshift operationefenablesdifferentattentioncomponents toxiblydivergefromeachotherandhavedifferentdis- tributions.ThislaysthefoundationforMulti-SATT,which wedescribenext. 3.1.2Multi-GroupShiftingAttentionNetwork Inordertocollectmultimodalinformationfromvideos,we extractavarietyofdifferentfeatures,suchasappearance 2 Figure2.TemporalXceptionNetwork. (RGB),motionw)andaudiosignals.Althoughtheat- tentionmodelfocusesonsomefeaturesandeffec- tivelyoutirrelevantnoise,itisunrealistictomerge allmultimodalfeaturesetswithinoneattentionmodel,be- causefeaturesofdifferentmodalityhavedifferentvalues, dimensionsandscales.Instead,weproposeMulti-Group ShiftingAttentionNetworksfortrainingmultiplegroupsof attentionssimultaneously.Thearchitectureoftheproposed Multi-SATTisillustratedinFigure1. First,weextractmultiplefeaturesetsfromthevideo.For eachfeatureset X i ,weapply N i differentshiftingatten- tions,whichwecalloneattentiongroup,andthenwecon- catenatetheoutputs.Next,theoutputsofdifferentatten- tiongroupsarenormalizedseparatelyandconcatenatedto formaglobalrepresentationvectorforthevideo.Finally, therepresentationvectorisusedforthrougha fully-connectedlayer. 3.2.TemporalXceptionNetwork Depthwiseseparableconvolutionarchitecture[2,20]has shownitspowerinimagebyreducingthe numberofparametersandincreasingaccu- racysimultaneously.Recently,convolutionalsequence-to- sequencenetworkshavebeensuccessfullyappliedtoma- chinetranslationtasks[5,8].Inthiscompetition,we adoptthetemporalXceptionnetworkforactionrecognition, whichapplythedepthwiseseparableconvolutionfamilies tothetemporaldimensionandachievespromisingperfor- mance.TheproposedtemporalXceptionnetworkarchitec- tureisshowninFigure2.Zero-valuedmultimodalfea- tureswerepaddedtomakeedlengthdataforeachstream. Weappliedadaptivetemporalmaxpoolingtoobtain n seg- mentsforeachvideo.Wethenfeedthevideosegmentfea- turesintoaTemporalConvolutionalblock,whichisconsist ofastackoftwoseparableconvolutionallayersfollowedby batchnormandactivationwithashortcutconnection.Fi- nally,theoutputsofthreestreamfeaturesareconcatenated andfedintothefully-connectedlayerfor 4.ExperimentResults WeconductexperimentonthechallengingKinetics datasetThedatasetcontains246,535trainingvideos, 19,907validationvideosand38,685testingvideos.Each videoisinoneof400categories. Table1summarizesourresultsontheKineticsvalida- tiondataset.FromTable1,wehavethreekeyobservations. (1)Temporalmodelingapproacheswithmultimodalfea- turesareamoreeffectiveapproachthannaivecombining thescoresofdifferentmodalitynetworksfor 3 Model Modality Top-1Accuracy(%) Top-5Accuracy(%) Inception-ResNet-v2 RGB 73.0 90.9 Inception-ResNet-v2 Flow 54.5 75.9 VGG16 Audio 21.6 39.4 Latefusion RGB+Flow+Audio 74.9 91.6 Multi-streamSequenceModel RGB+Flow+Audio 77.0 93.2 Fast-forwardLSTM RGB+Flow+Audio 77.1 93.2 TemporalXceptionNetwork RGB+Flow+Audio 77.2 93.4 ShiftingAttentionNetwork RGB+Flow+Audio 77.7 93.2 Ensemble RGB+Flow+Audio 81.5 95.6 Table1.Kineticsvalidationresults. thevideo(2)TheproposedShiftingAtten- tionNetworkandTemporalXceptionNetworkcanachieve comparableorevenbetterresultsthanthetraditionalse- quencemodels(e.g.LSTM),whichindicatestheymight serveasalternativetemporalmodelingapproachesinfu- ture.(3)Differenttemporalmodelingapproachesarecom- plementarytoeachother. 5.Conclusions Inthiswork,wehaveproposedfourtemporalmodel- ingapproachestoaddressthechallenginglarge-scalevideo recognitiontask.Experimentresultsverifythatourap- proachesachievebetterresultsthanthetradi- tionaltemporalpoolingapproaches.Theensembleofour individualmodelshasbeenshowntoimprovetheperfor- mancefurther,enablingourmethodtorankworldwide inthechallengecompetition.Allthecodeandmodelswill bereleasedsoon. References [1] K.Cho,B.VanMerri ¨ enboer,D.Bahdanau,andY.Bengio. Onthepropertiesofneuralmachinetranslation:Encoder- decoderapproaches. arXivpreprintarXiv:1409.1259 ,2014. [2] F.Chollet.Xception:Deeplearningwithdepthwisesepara- bleconvolutions. CVPR ,2017. [3] C.Gan,N.Wang,Y.Yang,D.-Y.Yeung,andA.G.Haupt- mann.Devnet:Adeepeventnetworkformultimediaevent detectionandevidencerecounting.In CVPR ,pages2568Œ 2577,2015. [4] C.Gan,T.Yao,K.Yang,Y.Yang,andT.Mei.Youlead, weexceed:Labor-freevideoconceptlearningbyjointlyex- ploitingwebvideosandimages.CVPR,2016. [5] J.Gehring,M.Auli,D.Grangier,D.Yarats,andY.N. Dauphin.Convolutionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122 ,2017. [6] S.Hershey,S.Chaudhuri,D.P.W.Ellis,J.F.Gemmeke, A.Jansen,R.C.Moore,M.Plakal,D.Platt,R.A.Saurous, B.Seybold,M.Slaney,R.J.Weiss,andK.Wilson.Cnn architecturesforlarge-scaleaudioIn arXiv preprintarXiv:1609.09430 ,2017. [7] S.HochreiterandJ.Schmidhuber.Longshort-termmemory. Neuralcomputation ,9(8):1735Œ1780,1997. [8] L.Kaiser,A.N.Gomez,andF.Chollet.Depthwisesep- arableconvolutionsforneuralmachinetranslation. arXiv preprintarXiv:1706.03059 ,2017. [9] A.Karpathy,G.Toderici,S.Shetty,T.Leung,R.Suk- thankar,andL.Fei-Fei.Large-scalevideowith convolutionalneuralnetworks.In CVPR ,2014. [10] W.Kay,J.Carreira,K.Simonyan,B.Zhang,C.Hillier, S.Vijayanarasimhan,F.Viola,T.Green,T.Back,P.Nat- sev,etal.Thekineticshumanactionvideodataset. arXiv preprintarXiv:1705.06950 ,2017. [11] F.Li,C.Gan,X.Liu,Y.Bian,X.Long,Y.Li,Z.Li,J.Zhou, andS.Wen.Temporalmodelingapproachesforlarge-scale youtube-8mvideounderstanding. arXiv:1707.04555 ,2017. [12] Z.Lin,M.Feng,C.NogueiradosSantos,M.Yu,B.Xiang, B.Zhou,andY.Bengio.AStructuredSelf-attentiveSen- tenceEmbedding. ArXive-prints ,Mar.2017. [13] K.SimonyanandA.Zisserman.Two-streamconvolutional networksforactionrecognitioninvideos.In NIPS ,2014. [14] K.SimonyanandA.Zisserman.Verydeepconvolu- tionalnetworksforlarge-scaleimagerecognition. CoRR , abs/1409.1556,2014. [15] N.Srivastava,E.Mansimov,andR.Salakhutdinov.Un- supervisedlearningofvideorepresentationsusinglstms. ICML ,2015. [16] C.Szegedy,S.Ioffe,V.Vanhoucke,andA.Alemi. Inception-v4,inception-resnetandtheimpactof residualconnectionsonlearning.In arXivpreprint arXiv:1602.07261 ,2016. [17] D.Tran,L.Bourdev,R.Fergus,L.Torresani,andM.Paluri. C3D:Genericfeaturesforvideoanalysis. ICCV ,2015. [18] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones, A.N.Gomez,L.Kaiser,andI.Polosukhin.AttentionIsAll YouNeed. ArXive-prints ,June2017. [19] L.Wang,Y.Xiong,Z.Wang,Y.Qiao,D.Lin,X.Tang, andL.V.Gool.Temporalsegmentnetworks:Towardsgood practicesfordeepactionrecognition.In ECCV ,2016. [20] S.Xie,R.Girshick,P.Doll ´ ar,Z.Tu,andK.He.Aggregated residualtransformationsfordeepneuralnetworks. CVPR , 2017. 4  
DuReader:aChineseMachineReadingComprehensionDatasetfrom Real-worldApplications WeiHe,KaiLiu,JingLiu,YajuanLyu,ShiqiZhao,XinyanXiao,YuanLiu,YizhongWang, HuaWu,QiaoqiaoShe,XuanLiu,TianWu,HaifengWang BaiduInc.,Beijing,China f hewei06,liukai20,liujing46,lvyajuan,zhaoshiqi,xiaoxinyan,liuyuan04,wangyizhong01, wu hua,sheqiaoqiao,liuxuan,wutian,wanghaifeng g @baidu.com Abstract ThispaperintroducesDuReader,anew large-scale,open-domainChinesema- chinereadingcomprehension(MRC) dataset,designedtoaddressreal-world MRC.DuReaderhasthreeadvantages overpreviousMRCdatasets:(1) data sources: questionsanddocumentsare basedonBaiduSearchandBaiduZhi- dao 1 ;answersaremanuallygenerated. (2) questiontypes: itprovidesrich annotationsformorequestiontypes, especiallyyes-noandopinionquestions, thatleavesmoreopportunityforthe researchcommunity.(3) scale: itcontains 200Kquestions,420Kanswersand1M documents;itisthelargestChinese MRCdatasetsofar.Experimentsshow thathumanperformanceiswellabove currentstate-of-the-artbaselinesystems, leavingplentyofroomforthecommunity tomakeimprovements.Tohelpthe communitymaketheseimprovements, bothDuReader 2 andbaselinesystems 3 havebeenpostedonline.Wealsoorganize asharedcompetitiontoencouragethe explorationofmoremodels.Sincethe releaseofthetask,thereare improvementsoverthebaselines. 1Introduction Thetaskofmachinereadingcomprehension (MRC)aimstoempowermachinestoanswer questionsafterreadingarticles( Rajpurkaretal. , 1 Zhidao( https://zhidao.baidu.com )isthe largestChinesecommunity-basedquestionanswering (CQA)siteintheworld. 2 http://ai.baidu.com/broad/download? dataset=dureader 3 https://github.com/baidu/DuReader 2016 ; Nguyenetal. , 2016 ).Inrecentyears,a numberofdatasetshavebeendevelopedforMRC, asshowninTable 1 .Thesedatasetshaveledto advancessuchasMatch-LSTM( WangandJiang , 2017 ),BiDAF( Seoetal. , 2016 ),AoAReader( Cui etal. , 2017 ),DCN( Xiongetal. , 2017 )andR- Net( Wangetal. , 2017 ).Thispaperhopesto advanceMRCevenfurtherwiththereleaseof DuReader,challengingthecommunitytodeal withmorerealisticdatasources,moretypesof questionsandmorescale,asillustratedinTables 1-4.Table 1 highlightsDuReader'sadvantages overpreviousdatasetsintermsofdatasourcesand scale.Tables2-4highlightDuReader'sadvantages intherangeofquestions. Ideally,agooddatasetshouldbebasedonques- tionsfromrealapplications.However,manyex- istingdatasetshavebeenforcedtomakevari- ouscompromisessuchas:(1) clozetask: Data issynthesizedmissingakeyword.Thetaskis tointhemissingkeyword( Hermannetal. , 2015 ; Cuietal. , 2016 ; Hilletal. , 2015 ).(2) multiple-choiceexams: Richardsonetal. ( 2013 ) collectbothstoriesandthecorresponding multiple-choicequestionsbycrowdsourcing. Lai etal. ( 2017 )collectthemultiple-choicequestions fromEnglishexams.(3) crowdsourcing: Turkers aregivendocuments(e.g.,articlesfromthenews and/orWikipedia)andareaskedtoconstructques- tionsafterreadingthedocuments( Trischleretal. , 2017 ; Rajpurkaretal. , 2016 ; Ko  cisk ˚ yetal. , 2017 ). Thelimitationsofthedatasetsleadtobuild datasetsbasedonqueriesthatrealuserssubmit- tedtorealsearchengines.MS-MARCO( Nguyen etal. , 2016 )isbasedonBinglogs(inEnglish), andDuReader(thispaper)isbasedonthelogs ofBaiduSearch(inChinese).Besides question sources ,DuReadercomplementsMS-MARCO andotherdatasetsinthefollowingways: questiontypes: DuReadercontainsaricherin- arXiv:1711.05073v4  [cs.CL]  11 Jun 2018DatasetLang#Que.#DocsSourceofQue.SourceofDocsAnswerType CNN/DM( Hermannetal. , 2015 )EN1.4M300KSyntheticclozeNewsFillinentity HLF-RC( Cuietal. , 2016 )ZH100K28KSyntheticclozeFairy/NewsFillinword CBT( Hilletal. , 2015 )EN688K108SyntheticclozeChildren'sbooksMulti.choices RACE( Laietal. , 2017 )EN870K50KEnglishexamEnglishexamMulti.choices MCTest( Richardsonetal. , 2013 )EN2K500CrowdsourcedFictionalstoriesMulti.choices NewsQA( Trischleretal. , 2017 )EN100K10KCrowdsourcedCNNSpanofwords SQuAD( Rajpurkaretal. , 2016 )EN100K536CrowdsourcedWiki.Spanofwords SearchQA( Dunnetal. , 2017 )EN140K6.9MQAsiteWebdoc.Spanofwords TrivaQA( Joshietal. , 2017 )EN40K660KTriviawebsitesWiki./Webdoc.Span/substringofwords NarrativeQA( Ko  cisk ˚ yetal. , 2017 )EN46K1.5KCrowdsourcedBook&movieManualsummary MS-MARCO( Nguyenetal. , 2016 )EN100K200K 1 UserlogsWebdoc.Manualsummary DuReader(thispaper)ZH200k1MUserlogsWebdoc./CQAManualsummary Table1:DuReaderhasthreeadvantagesoverpreviousMRCdatasets:(1) datasources :questionsanddocumentsarebasedonBaidu Search&BaiduZhidao;answersaremanuallygenerated,(2) questiontypes ,and(3) scale :200kquestions,420kanswersand1M documents(largestChineseMRCdatasetsofar).Thenextthreetablesaddressadvantage(2). 1 Numberofuniquedocuments ventoryofquestionsthanpreviousdatasets.Each questionwasmanuallyannotatedaseitherEntity, DescriptionorYesNoandoneofFactorOpin- ion.Inparticular,itannotatesyes-noandopin- ionquestionsthattakealargeproportioninreal user'squestions.Priorworkhaslargelyempha- sizedfacts,butDuReaderarefullofopinionsas wellasfacts.Muchoftheworkonquestionan- sweringinvolvesspanselection,methodsthatan- swerquestionsbyreturningasinglesubstringex- tractedfromasingledocument.Spanselection mayworkwellforfactoids(entities),butitisless appropriateforyes-noquestionsandopinionques- tions(especiallywhentheanswerinvolvesasum- marycomputedoverseveraldifferentdocuments). documentsources: DuReadercollectsdocu- mentsfromthesearchresultsofBaiduSearchas wellasBaiduZhidao.AllthecontentinBaidu Zhidaoisgeneratedbyusers,makingitdifferent fromthecommonwebpages.Itisinterestingto seeifsolutionsdesignedforonescenario(search) transfereasilytoanotherscenario(questionan- sweringcommunity).Additionally,previouswork providesonlyasingleparagraph( Rajpurkaretal. , 2016 )orafewpassages( Nguyenetal. , 2016 )to extractorgenerateanswers,whileDuReaderpro- videsmultiplefulldocuments(thatcontainsalot ofparagraphsorpassages)foreachquestionto generateanswers.Thiswillraiseparagraphselec- tion(i.e.selecttheparagraphslikelycontaining answers)animportantchallengeasshowninSec- tion4. datascale: ThereleaseofDuReadercon- tains200Kquestions,1Mdocumentsandmore than420Khuman-summarizedanswers.Tothe bestofourknowledge,DuReaderisthelargest ChineseMRCdatasetsofar. 2PilotStudy Whattypesofquestionqueriesdoweinthe logsofasearchengine?Apilotstudywasper- formedtocreateataxonomyofquestiontypes. Westartedwitharelativelysmallsampleof1000 questionqueries,selectedfromasingledayof BaiduSearchlogs. Thepilothelpedustoagreeonthefollowing taxonomyofquestiontypes.Eachquestionwas manuallyannotatedas:  either Fact or Opinion ,and  oneof: Entity , Description or YesNo Regardingto Entity questions,theanswersare expectedtobeasingleentityoralistofentities. Whiletheanswersto Description questionsare usuallymulti-sentencesummaries.The Descrip- tion questionscontainhow/whyquestions,com- parativequestionsthatcomparingtwoormoreob- jects,andthequestionsthatinquiringthemer- its/demeritsofgoods,etc.Asfor YesNo questions, theanswersareexpectedtobeanafveor negativeanswerswithsupportingevidences.Af- terthedeepanalysisofthesampledquestions,we thatwhichevertheexpectedanswertypeis, aquestioncanbefurtherinto Fact or Opinion ,dependingonwhetheritisaboutasking afactoranopinion.Table 2 givestheexamplesof thesixtypesofquestions. Thepilotstudyhelpedusidentifyanumberof importantissues.Table 3 showsthatallsixtypes ofquestionqueriesarecommoninthelogsof BaiduSearch,whilepreviousworkhastendedto focusonfact-entityandfact-descriptionquestions. AsshowninTable 3 ,fact-entityquestionsaccount FactOpinion Entity iphone ê ) Ñ  2017  }  — A è 5 q OnwhichdaywilliphonebereleasedTop10moviesof2017 Description ‹ 2 f :ÀH / ¢ — 0 0 a W É  H 7 WhyareredHowisToyotaCarola YesNo 39.5 ¦ Š Ø ç  f ô Ë ý  Ñ z ł  Is39.5degreeahighfeverDoeslearningtoplaygoimproveintelligence Table2:ExamplesofthesixtypesofquestionsinChinese(withglossesinEnglish).Previousdatasets havefocusedonfact-entityandfact-description,thoughallsixtypesarecommoninsearchlogs. FactOpinionTotal Entity 23.4%8.5%31.9% Description 34.6%17.8%52.5% YesNo 8.2%7.5%15.6% Total 66.2%33.8%100.0% Table3:PilotStudyfoundthatallsixtypes ofquestionqueriesarecommoninsearchlogs. PreviousMRCdatasetshaveemphasizedspan- selectionmethods.Suchmethodsareappropriate forfact-entityandfact-description.Opinionsand yes-noleavebigopportunities(about33.8%and 15.6%ofthesample,respectively). forarelativelysmallfraction(23.4%)ofthesam- ple.Fact-descriptionsaccountforalargerfraction ofthesample(34.6%).FromthisTable,wecan seethatopinions(33.8%)arecommoninsearch logs.Yes-Noquestionsaccountfor15.6%,with onehalfaboutfact,anotherhalfaboutopinion. PreviousMRCdatasetshaveemphasizedspan- selectionmethods.Suchmethodsareappropriate forfact-entityandfact-description,butitisprob- lematicwhentheanswerinvolvesasummaryof multiplesentencesfrommultipledocuments,es- peciallyforYes-noandopinionquestions.This requiresmethodsthatgobeyondcurrentlypopu- larmethodssuchasspanselection,andleavelarge opportunityforthecommunity. 3ScalingupfromthePilottoDuReader 3.1DataCollectionandAnnotation 3.1.1DataCollection Afterthesuccessfulcompletionofthepilotstudy, webeganworkonscalinguptherelativelysmall sampleof1kquestionstoamoreambitiouscol- lectionof200kquestions. TheDuReaderisasequenceof4-tuples: f q , t , D , A g ,where q isaquestion, t isaquestiontype, D isasetofrelevantdocuments,and A isananswer setproducedbyhumanannotators. Beforelabelingquestiontypes,weneedtocol- lectasetofquestions q fromsearchlogs.Accord- ingtoourestimation,thereareabout 21% ques- tionqueriesinsearchlogs.Itwouldtaketoomuch time,ifhumanannotatorsmanuallylabeleach queryinsearchlogs.Hence,werandomly samplethemostfrequentqueriesfromsearchlogs, anduseapre-trained(withrecallhigher than 90% )toautomaticallyselectquestionqueries fromsearchlogs.Then,workerswillannotatethe questionqueriesselectedbythe.Since thisannotationtaskisrelativelyeasy,eachquery wasannotatedbyoneworker.Theexpertswill furtherreviewalltheannotationsbyworkersand correctthemiftheannotationiswrong.Theaccu- racyofworkers'annotation(judgedbyexperts)is higherthan 98% . Initially,wehave1Mfrequentqueriessam- pledfromsearchlogs.Theautomati- callyselected280Kquestionqueries.Afterhuman annotation,thereare210Kquestionqueriesleft. Eventually,weuniformlysampled200Kquestions fromthe210Kquestionqueries. Wethencollecttherelevantdocuments, D ,by submittingquestionstotwosources,BaiduSearch andBaiduZhidao.Notethatthetwosourcesare verydifferentfromoneanother;Zhidaocontains user-generatedcontentandtendstohavemore documentsrelevanttoopinions.Sincethetwo sourcesaresodifferentfromeachanother,wede- cidedtorandomlysplitthe200kuniquequestions intotwosubsets.Thesubsetwasusedtopro- ducethetop5rankeddocumentsfromonesource, andthesecondsubsetwasusedtoproducethetop 5rankeddocumentsfromtheothersource. Wealsobelievethatitisimportanttokeepthe entiredocumentunlikepreviousworkwhichkept asingleparagraph( Rajpurkaretal. , 2016 )orafew FactOpinionTotal Entity 14.4%13.8%28.2% Description 42.8%21.0%63.8% YesNo 2.9%5.1%8.0% Total 60.1%39.9%100.0% Table4:Thedistributionofquestiontypesin DuReaderissimilarto(butdifferentfrom)thePi- lotStudy(Table3),largelybecauseofduplicates. TheduplicateswereremovedfromDuReader(but notfromthePilotStudy)toreducetheburdenon theannotators. passages( Nguyenetal. , 2016 ).Inthiscase,para- graphselection(i.e.selecttheparagraphslikely containinganswers)becomescriticaltotheMRC systemsaswewillshowinSection4. Documentsareparsedintoafewinclud- ingtitleandmaincontent.Texthasbeentokenized intowordsusingastandardAPI. 4 3.1.2QuestionTypeAnnotation Asmentionedabove,annotatorslabeledeach questionintwopasses.Thepass questionsintooneofthreetypes: Entity , Descrip- tion and YesNo questions.Thesecondpassclassi- questionsaseither Fact or Opinion . Statisticsonthesearereportedin Table4.Notethatthesestatisticsaresimilarto thosereportedforthepilotstudy(Table3),butdif- ferentbecauseduplicateswereremovedfromTa- ble4(butnotfromTable3).Wedon'twanttobur- dentheannotatorswithlotsofcopiesofthemost frequentquestions,hencewekeptuniqueques- tionsinDuReader.Thatsaid,bothtablesagree onanumberofimportantpoints.Aspointedout above,previousworkhastendedtofocusonfact- entityandfact-description,whileleaveslargeop- portunityonyes-noandopinionquestions. 3.1.3AnswerAnnotation Crowd-sourcingwasusedtogenerateanswers. Turkersweregivenaquestionandasetofrelevant documents.He/shewasthenaskedtowritedown answersinhis/herownwordsbyreadingandsum- marizingthedocuments.Ifnoanswerscanbe foundintherelevantdocuments,theannotatorwas askedtogiveanemptyanswer.Ifmorethanone answercanbefoundintherelevantdocuments, theannotatorwasaskedtowritethemalldown. 4 http://ai.baidu.com/tech/nlp/lexical Insomecases,multipleanswersweremergedinto asingleanswer,whenitwasdeterminedthatthe multipleanswerswereverysimilartooneanother. Notethattheanswersto Entity questionsand YesNo questionsaremorediverse.Theanswersto the Entity questionsincludeboththeentitiesand thesentencescontainingthem.Seetheex- ampleinTable 5 .Theboldwords(i.e.green, gray,yellow,pink)aretheentityanswerstothe question,andthesentencesaftertheentitiesare thesentencecontainingthem.Theanswerstothe YesNo questionsincludetheopiniontypes( Yes , No or Depend )aswellasthesupportingsentences. SeethelastexampleinTable 5 .Theboldwords (i.e.YesandDepend)aretheopiniontypesby followingthesupportingsentences.Thesecond exampleshowsthatasimpleyes-noquestionisn't sosimple.Theanswercanbealmostanything, includingnotonly Yes and No ,butalso Depends , dependingoncontext(supportingsentences). 3.1.4QualityControl Qualitycontrolisimportantbecauseofthesize ofthisproject: 51 ; 408 man-hoursdistributedover about 800 workersand 52 experts. Wehaveaninternalcrowdsourcingplatform andannotationguidelinestoannotatedata.When annotatinganswers,workersarehiredtocreatethe answersandexpertsarehiredtovalidatethean- swerquality.Theworkerswillbehiredifthey passanexamineonasmalldataset.Theaccuracy ofworkers'annotationshouldbehigherthan 95% (judgedbytheexperts).Basically,therearethree roundsforanswerannotations:(1)theworkers willgivetheanswerstothequestionsafterread- ingtherelevantdocuments.(2)theexpertswillre- viewallanswerscreatedbytheworkers,andthey willcorrecttheanswersiftheyconsiderthatthe answersarewrong.Theaccuracy(judgedbythe experts)ofanswersbytheworkersisaround 90% . (3)Thedatasetisdividedinto 20 groupsaccording totheworkersandexpertswhoannotatethedata. 5% ofdatawillbesampledfromeachgroup.The sampleddataineachgroupwillbefurtherchecked againbyotherexperts.Iftheaccuracyislower than 95% ,thecorrespondingworkersandtheex- pertsneedtorevisetheanswersagain.Theloop willenduntiltheoverallaccuracyreaches 95% . 3.1.5Training,DevelopmentandTestSets Inordertomaximizethereusabilityofthedataset, weprovideasplitofthedatasetinto Question f ë  œ r /Whatarethecolorsofacademicdresses? QuestionType Entity-Fact Answer1[ ÿÿÿ rrr , ppp rrr , ÄÄÄ rrr , ››› rrr ] ˆ œ ff ë  ÿ r   ff ë  p r  å ff ë  Ä r  ¡  ff ë  p r  Õ ff ë  › r  ⁄ ff ë  › r  Ï N ff ë  p r  / [green,gray,yellow,pink] GreenforBachelorofAgriculture,grayforBachelorofScience, yellowforBachelorofEngineering,grayforBachelorofManagement,pinkforBachelor ofLaw,pinkforBachelorofArt,grayforBachelorofEconomics Document1 œ ff ë  ÿ r   ff ë  p r  ...  n ı : ⁄    å  œ  ;  ł ‰ m ' {   d ø ﬂ — p ¹ œ r : ›  p  Ä  ÿ  }  ¢ m Í œ r  ... Document5 f ë  / f ë f M · Š  ( f M ‹ ‹ê    4 — h : f M — c  <   ...  7 s ˜ ý ﬂ @ ñ r ® ‰  Question z g Y  ı † Ô  /DoIhavetohavemywisdomteethremoved QuestionType YesNo-Opinion Answer1[Yes] à : z  ‹ ¾   —  à  Ô  , — Y  ¹  ú ° ã T î Ÿ  @ å ; ˜ ˆ ú ® Ô › / [Yes] Thewisdomteetharediftoclean,andcausemoredentalproblemsthannormal teethdo,sodoctorsusuallysuggesttoremovethem Answer2[Depend] z   ı ^ Š Ô ›   , ê Ô ú  Ç ¶ h ° — z   Ô ‡ ô Ï 8  w Ñ ” .../ [Depend] Notalways,onlythebadwisdomteethneedtoberemoved,forexample,theone oftencauses... Document1 :ÀH † Ô z  ? z  }} — ; ˜ :ÀH † ú ® Ô › ? ; † Ø / à : z  ‹ ¾   ... ... Document5 9 n  ˆ t — 4 − Ï „ e ô , z   ı ^ Š Ô › . z  ; ˜  }ˆ Í ... Table5:ExamplesfromDuReader.Annotationsforthesequestionsincludeboththeanswers,aswellas supportingsentences. training,developmentandtestsets.Thetraining, developmentandtestsetsconsistof 181 K , 10 K and 10 K questions, 855 K , 45 K and 46 K doc- uments, 376 K , 20 K and 21 K answers,respec- tively. 3.2DuReaderis(Relatively)Challenging Figures1-2illustratesomeofthechallengesof DuReader. Thenumberofanswers. Onemightthinkthat mostquestionswouldhaveone(andonlyone)an- swer,butFigure 1 showsthatthisisnotthecase, especiallyforBaiduZhidao(70.8%questionsin BaiduZhidaohavemultipleanswers,whilethe numberinBaiduSearchis62.2%),wherethereis moreroomforopinionsandsubjectivity,andcon- sequently,thereismoreroomfordiversityinthe answerset.Meanwhile,wecanseethat1.5%of questionshavezeroanswersinBaiduSearch,but thisnumberincreasesto9.7%inBaiduZhidao.In thelatercase,noanswerdetectionisanewchal- lenge. Theeditdistance. Onemightalsohavebeen tempted,basedonpriorwork,tostartwithaspan selectionmethod,basedonthesuccessofsuch methodswithpreviousdatasets,manyofwhich weredesignedforspanselection,suchas:SQuAD ( Rajpurkaretal. , 2016 ),NewsQA( Trischleretal. , Figure1:Afewquestionshaveone(andonlyone) answer,especiallyforZhidao. 2017 )andTriviaQA( Joshietal. , 2017 ).How- ever,thismaynotworkwellonDuReader,since thedifferencebetweenthehumangeneratedan- swersandthesourcedocumentsislarge.Tomea- surethedifference,weuseasanapproximatemea- surementtheminimumeditdistance(MED)be- tweentheanswersgeneratedbyhumanandthe sourcedocuments 5 .AlargeMEDmeansthatan annotatorneedstomakemoreeffortsonsumma- 5 HereMEDistheminimumeditdistancebetweenthean- swerandanyconsecutivespaninthesourcedocument. Figure2:Spanselectionisunlikelytoworkwell forDuReaderbecausemanyoftheanswersare relativelyfar(ineditdistance)fromsourcedoc- uments(comparedtoMSMARCO). rizingandparaphrasingthesourcedocumentsto generateananswer,insteadofjustcopyingwords fromthesourcedocuments.Figure2compares DuReaderandMS-MARCOintermsofMED, andsuggeststhatspanselectionisunlikelytowork wellforDuReaderwheremanyoftheanswersare relativelyfarfromsourcedocumentscompared toMSMARCO.NotethattheMEDofSQuAD, NewsQAandTriviaQAshouldbezero. Thedocumentlength. InDuReader,ques- tionstendtobeshort(4.8wordsonaverage)com- paredtoanswers(69.6words),andanswerstend tobeshortcomparedtodocuments(396words onaverage).ThedocumentsinDuReaderare5x longerthandocumentsinMS-MARCO( Nguyen etal. , 2016 ).Thedifferenceisduetoadesign decisiontoprovideunabridgeddocuments(asop- posedtoparagraphs).Webelieveunabridgeddoc- umentsmaybehelpfulbecausetheremaybeuse- fulcluesthroughoutthedocumentwellbeyonda singleparagraphorafewpassages. 4Experiments Inthissection,weimplementandevaluatethe baselinesystemswithtwostate-of-the-artmod- els.Furthermore,withtherichannotationsin ourdataset,weconductcomprehensiveevalua- tionsfromdifferentperspectives. 4.1BaselineSystems Aswediscussedinprevioussection,DuReader provideseachquestionthefulldocumentsthat containmulti-paragraphsormulti-passages,while previousworkprovidesonlyasinglepara- graph( Rajpurkaretal. , 2016 )orafewpas- sages( Nguyenetal. , 2016 )toextractorgenerate answers.Theaveragelengthofeachdocumentis muchlongerthanpreviousones( Nguyenetal. , 2016 ).Ifwedirectlyapplythestate-of-the-art MRCmodelsthatwasdesignedforanswerspan selction,therewillbeefyissues.Toim- proveboththeefyoftrainingandtesting, ourdesignedsystemshavetwosteps:(1)select onemostrelatedparagraphfromeachdocument, and(2)applythestate-of-the-artMRCmodelson theselectedparagraphs. 4.1.1ParagraphSelection Inthispaper,weapplysimplestrategiestoselect themostrelevantparagraphfromeachdocument. Intrainingstage,weselectoneparagraphfroma documentasthemostrelevantone,ifthepara- graphhasthelargestoverlapwithhumangener- atedanswer.Weselectonemostrelevantpara- graphforeachdocument.Then,MRCmodelsde- signedforanswerspanselectionwillbetrainedon theseselectedparagraphs. Intestingstage,sincewehavenohumangener- atedanswer,weselectthemostrelevantparagraph thathasthelargestoverlapwiththecorresponding question.Then,thetrainedMRCmodelsdesigned foranswerspanselectionwillbeappliedonthe theseselectedparagraphs. 4.1.2AnswerSpanSelection Weimplementtwotypicalstate-of-the-artmodels designedforanswerspanselectionasbaselines. Match-LSTM Match-LSTMisawidelyused MRCmodelandhasbeenwellexploredinrecent studies( WangandJiang , 2017 ).Toananswer inaparagraph,itgoesthroughtheparagraphse- quentiallyanddynamicallyaggregatesthematch- ingofanattention-weightedquestionrepresenta- tiontoeachtokenoftheparagraph.Finally,anan- swerpointerlayerisusedtoananswerspan intheparagraph. BiDAF BiDAFisapromisingMRCmodel,and itsimprovedversionhasachievedthebestsingle modelperformanceonSQuADdataset( Seoetal. , 2016 ).Itusesbothcontext-to-questionattention andquestion-to-contextattentioninordertohigh- lighttheimportantpartsinbothquestionandcon- text.Afterthat,theso-calledattentionwlayer isusedtofuseallusefulinformationinorderto Systems BaiduSearchBaiduZhidaoAll BLEU-4%Rouge-L%BLEU-4%Rouge-L%BLEU-4%Rouge-L% SelectedParagraph 15.822.616.538.316.430.2 Match-LSTM 23.131.242.548.031.939.2 BiDAF 23.131.142.247.531.839.0 Human 55.154.457.160.756.157.4 Table6:PerformanceoftypicalMRCsystemsontheDuReader. BLEU-4%Rouge-L% GoldParagraph 31.761.3 Match-LSTM 46.352.4 BiDAF 46.351.8 Table7:Modelperformancewithgoldparagraph. Theuseofgoldparagraphscould booststheoverallperformance. getavectorrepresentationforeachposition. ImplementationDetails Werandomlyinitial- izethewordembeddingswithadimensionof 300 andsetthehiddenvectorsizeas 150 foralllay- ers.WeusetheAdamalgorithm( KingmaandBa , 2014 )totrainbothMRCmodelswithaninitial learningrateof 0 : 001 andabatchsizeof 32 . 4.2ResultsandAnalysis Weevaluatethereadingcomprehensiontaskvia character-levelBLEU-4( Papinenietal. , 2002 )and Rouge-L( Lin , 2004 ),whicharewidelyusedfor evaluatingthequalityoflanguagegeneration.The experimentalresultsontestsetareshowninTa- ble 6 .Forcomparison,wealsoevaluatetheSe- lectedParagraphthathasthelargestoverlapwith thequestionamongalldocuments.Wealsoassess humanperformancebyinvolvinganewannotator toannotateonthetestdataandtreathisan- swerastheprediction. Theresultsdemonstratethatcurrentreading comprehensionmodelscanachieveanimpressive improvementcomparedwiththeselectedpara- graphbaseline,whichapprovestheeffectiveness ofthesemodels.However,thereisstillalargeper- formancegapbetweenthesemodelsandhuman. Aninterestingdiscoverycomesfromthecompar- isonbetweenresultsonBaiduSearchandBaidu Zhidaodata.Wethatthereadingcomprehen- sionmodelsgetmuchhigherscoreonZhidaodata. Thisshowsthatitismuchharderforthemodelsto comprehendopen-domainwebarticlesthanto answersinpassagesfromaquestionanswering community.Incontrast,theperformanceofhu- manbeingsonthesetwodatasetsshowslittledif- ference,whichsuggeststhathuman'sreadingskill ismorestableondifferenttypesofdocuments. AsdescribedinSection 4.1 ,themostrelevant paragraphofeachdocumentisselectedbasedon itsoverlapwiththecorrespondingquestionduring testingstage.Toanalyzetheeffectofparagraph selectionandobtainanupperboundofthebase- lineMRCmodels,were-evaluateoursystemson thegoldparagraphs,eachofwhichisselectedif ithasthelargestoverlapwiththehumangener- atedanswersinadocument.Theexperimentre- sultshavebeenshowninTable 7 .ComparingTa- ble 7 withTable 6 ,wecanseethattheuseofgold paragraphscouldlybooststheoverall performance.Moreover,directlyusingthegold paragraphcanobtainaveryhighRouge-Lscore. Itmeetstheexception,becauseeachgoldpara- graphisselectedbasedonrecallthatisrelevantto Rouge-L.Though,wethatthebaselinemod- elscangetmuchbetterperformancewithrespect toBLEU,whichmeansthemodelshavelearnedto selecttheanswers.Theseresultsshowthatpara- graphselectionisacrucialproblemtosolveinreal applications,whilemostcurrentMRCdatasets supposetotheanswerinasmallparagraph orpassage.Incontrast,DuReaderprovidesthe fullbodytextofeachdocumenttostimulatethe researchinareal-worldsetting. Togainmoreinsightintothecharacteristicsof ourdataset,wereporttheperformanceacrossdif- ferentquestiontypesinTable 8 .Wecansee thatboththemodelsandhumanachieverelatively goodperformanceondescriptionquestions,while YesNo questionsseemtobethehardesttomodel. Weconsiderthatdescriptionquestionsareusually answeredwithlongtextonthesametopic.This ispreferredbyBLEUorRouge.However,the answersto YesNo questionsarerelativelyshort, whichcouldbeasimple Yes or No insomecases. Questiontype DescriptionEntityYesNo BLEU-4%Rouge-L%BLEU-4%Rouge-L%BLEU-4%Rouge-L% Match-LSTM 32.840.029.538.55.97.2 BiDAF 32.639.729.838.45.57.5 Human 58.158.044.652.056.257.4 Table8:Performanceonvariousquestiontypes.CurrentMRCmodelsachieveimpressiveimprovements comparedwiththeselectedparagraphbaseline.However,thereisalargegapbetweenthesemodelsand human. FactOpinion BLEU-4%Rouge-L%BLEU-4%Rouge-L% Opinion-unaware 6.38.35.07.1 Opinion-aware 12.013.98.08.9 Table9:Performanceofopinion-awaremodelon YesNo questions. 4.3Opinion-awareEvaluation Consideringthecharacteristicsof YesNo ques- tions,wefoundthatit'snotsuitabletodirectlyuse BLEUorRougetoevaluatetheperformanceon thesequestions,becausethesemetricscouldnot theagreementbetweenanswers.Forexam- ple,twocontradictoryanswerslikeﬂYoucando itﬂandﬂYoucan'tdoitﬂgethighagreementscores withthesemetrics.Anaturalideaistoformulate thissubtaskasaproblem.However, asdescribedinSection 3 ,multipledifferentjudg- mentscouldbemadebasedontheevidencecol- lectedfromdifferentdocuments,especiallywhen thequestionisofopiniontype.Inreal-worldset- tings,wedon'twantasmartmodeltogiveanar- bitraryanswerforsuchquestionsas Yes or No . Totacklethis,weproposeanovelopinion- awareevaluationmethodthatrequirestheevalu- atedsystemtonotonlyoutputananswerinnatu- rallanguage,butalsogiveitanopinionlabel.We alsohavetheannotatorsprovidetheopinionlabel foreachanswertheygenerated.Insuchcases,ev- eryanswerispairedwithanopinionlabel( Yes , No or Depend )sothatwecancategorizetheanswers bytheirlabels.Finally,thepredictedanswersare evaluatedviaBlueorRougeagainstonlytherefer- enceanswerswiththesameopinionlabel.Byus- ingthisopinion-awareevaluationmethod,amodel thatcanpredictagoodanswerinnaturallanguage andgiveitanopinionlabelcorrectlywillgeta higherscore. Inordertoclassifytheanswersintodifferent opinionpolarities,weadda.Weslightly changetheMatch-LSTMmodel,inwhichthe nalpointernetworklayerisreplacedwithafully connectedlayer.Thisistrainedwiththe goldanswersandtheircorrespondingopinionla- bels.Wecompareareadingcomprehensionsys- temequippedwithsuchanopinionwith apurereadingcomprehensionsystemwithoutit, andtheresultsaredemonstratedinTable 9 .We canseethatdoingopiniondoeshelp underourevaluationmethod.Also,classifyingthe answerscorrectlyismuchharderforthequestions ofopiniontypethanforthoseoffacttype. 4.4Discussion Asshownintheexperiments,thecurrentstate-of- the-artmodelsstillunderperformhumanbeingsby alargemarginonourdataset.Thereisconsider- ableroomforimprovementonseveraldirections. First,therearesomequestionsinourdataset thathavenotbeenextensivelystudiedbefore, suchasyes-noquestionsandopinionquestions requiringmulti-documentMRC.Newmethods areneededforopinionrecognition,cross-sentence reasoning,andmulti-documentsummarization. Hopefully,DuReader'srichannotationswouldbe usefulforstudyofthesepotentialdirections. Second,ourbaselinesystemsemployasimple paragraphselectionstrategy,whichresultsingreat degradationofthesystemperformanceascom- paredtogoldparagraph'sperformance.Itisnec- essarytodesignamoresophisticatedparagraph rankingmodelforthereal-worldMRCproblem. Third,thestate-of-the-artmodelsformulate readingcomprehensionasaspanselectiontask. However,asshowninprevioussection,humanbe- ingsactuallysummarizeanswerswiththeirown comprehensioninDuReader.Howtosummarize orgeneratetheanswersdeservesmoreresearch. Forth,asthereleaseofthedataset,itisfar fromperfectionanditleavesmuchroomforim- provement.Forexample,weannotateonlyopin- iontagsforyes-noquestions,wewillalsoanno- tateopiniontagsfordescriptionandentityques- tions.Wewouldliketogatherfeedbackfromthe communitytoimproveDuReadercontinually. Overall,itisnecessarytoproposenewalgo- rithmsandmodelstotacklewithreal-worldread- ingcomprehensionproblems.Wehopethatthe DuReaderwouldbeagoodstartforfacilitatingthe MRCresearch. 5ASharedTask Toencouragetheexplorationofmoremodelsfrom theresearchcommunity,weorganizeanonline competition 6 .Eachparticipantcansubmitthere- sultandevaluatethesystemperformanceatthe onlinewebsite.Sincethereleaseofthetask,there areimprovementsoverthebaselines, Forexample,ateamobtained51.2ROUGE-Lon ourdataset(whenthepaperwassubmitted).The gapbetweenourBiDAFbaselinemodel(with39.0 ROUGE-L)andhumanperformance(with57.4 ROUGE-L)hasbeenreduced.Itis expectedthattheremaininggapthesystemper- formancesandhumanperformancewillbeharder toclose,butsucheffortswillleadtoadvancesin machinereadingcomprehension. 6Conclusion ThispaperannouncedthereleaseofDuReader,a newdatasetforresearchersinterestedinmachine readingcomprehension(MRC).DuReaderhas threeadvantagesoverpreviousMRCdatasets:(1) datasources(basedonsearchlogsandtheques- tionansweringcommunity),(2)questiontypes (fact/opinion&entity/description/yes-no)and (3)scale(largestChineseMRCdatasetsofar). Wehavemadeourdatasetfreelyavailableand organizeasharedcompetitiontoencouragetheex- plorationofmoremodels.Sincethereleaseof thetask,wehavealreadyseenimprove- mentsfrommoresophisticatedmodels. 6 https://ai.baidu.com/broad/ leaderboard?dataset=dureader Acknowledgements WewouldliketothankDr.KennethWardChurch forhisvaluablesuggestionsandrevisionsonthis paper,Prof.SujianLiforhersupportsonthispa- per,andtheanonymousreviewersfortheirhelpful commentsonthiswork. References YimingCui,ZhipengChen,SiWei,ShijinWang, TingLiu,andGuopingHu.2017.Attention-over- attentionneuralnetworksforreadingcomprehen- sion.In Proceedingsof55thAnnualMeetingofthe AssociationforComputationalLinguistics ,pages 593Œ602. YimingCui,TingLiu,ZhipengChen,ShijinWang,and GuopingHu.2016.Consensusattention-basedneu- ralnetworksforchinesereadingcomprehension. MatthewDunn,LeventSagun,MikeHiggins,Ugur Guney,VolkanCirik,andKyunghyunCho.2017. Searchqa:Anewq&adatasetaugmentedwith contextfromasearchengine. arXivpreprint arXiv:1704.05179 . KarlMoritzHermann,TomasKocisky,Edward Grefenstette,LasseEspeholt,WillKay,MustafaSu- leyman,andPhilBlunsom.2015.Teachingma- chinestoreadandcomprehend.In AdvancesinNeu- ralInformationProcessingSystems ,pages1693Œ 1701. FelixHill,AntoineBordes,SumitChopra,andJason Weston.2015.Thegoldilocksprinciple:Reading children'sbookswithexplicitmemoryrepresenta- tions. arXivpreprintarXiv:1511.02301 . MandarJoshi,EunsolChoi,DanielS.Weld,andLuke Zettlemoyer.2017.Triviaqa:Alargescaledistantly supervisedchallengedatasetforreadingcomprehen- sion. CoRR . DiederikP.KingmaandJimmyBa.2014.Adam:A methodforstochasticoptimization. CoRR . Tom ´ a  sKo  cisk ˚ y,JonathanSchwarz,PhilBlunsom, ChrisDyer,KarlMoritzHermann,G ´ aborMelis, andEdwardGrefenstette.2017.Thenarrativeqa readingcomprehensionchallenge. arXivpreprint arXiv:1712.07040 . GuokunLai,QizheXie,HanxiaoLiu,YimingYang, andEduardHovy.2017.Race:Large-scalereading comprehensiondatasetfromexaminations. arXiv preprintarXiv:1704.04683 . Chin-YewLin.2004.Rouge:Apackageforautomatic evaluationofsummaries.In TextSummarization BranchesOut:ProceedingsoftheACL-04Work- shop ,pages74Œ81. TriNguyen,MirRosenberg,XiaSong,JianfengGao, SaurabhTiwary,RanganMajumder,andLiDeng. 2016.Msmarco:Ahumangeneratedmachine readingcomprehensiondataset. arXivpreprint arXiv:1611.09268 . KishorePapineni,SalimRoukos,ToddWard,andWei- JingZhu.2002.Bleu:amethodforautomaticeval- uationofmachinetranslation.In Proceedingsof 40thAnnualMeetingoftheAssociationforCompu- tationalLinguistics ,pages311Œ318. PranavRajpurkar,JianZhang,KonstantinLopyrev,and PercyLiang.2016.Squad:100,000+questions formachinecomprehensionoftext. arXivpreprint arXiv:1606.05250 . MatthewRichardson,ChristopherJ.C.Burges,and ErinRenshaw.2013.Mctest:Achallengedataset fortheopen-domainmachinecomprehensionof text.In Proceedingsofthe2013ConferenceonEm- piricalMethodsinNaturalLanguageProcessing , pages193Œ203. MinJoonSeo,AniruddhaKembhavi,AliFarhadi,and HannanehHajishirzi.2016.Bidirectionalattention wformachinecomprehension. CoRR . AdamTrischler,TongWang,XingdiYuan,JustinHar- ris,AlessandroSordoni,PhilipBachman,andKa- heerSuleman.2017.Newsqa:Amachinecompre- hensiondataset.In Proceedingsofthe2ndWork- shoponRepresentationLearningforNLP ,pages 191Œ200. ShuohangWangandJingJiang.2017.Machinecom- prehensionusingmatch-lstmandanswerpointer.In ICLR ,pages1Œ15. WenhuiWang,NanYang,FuruWei,BaobaoChang, andMingZhou.2017.GatedSelf-MatchingNet- worksforReadingComprehensionandQuestion Answering.In Proceedingsofthe55thAnnual MeetingoftheAssociationforComputationalLin- guistics(Volume1:LongPapers) ,pages189Œ198. CaimingXiong,VictorZhong,andRichardSocher. 2017.Dynamiccoattentionnetworksforquestion answering.In ProceedingsofInternationalConfer- enceonLearningRepresentations .  
InternationalJointConferenceonNaturalLanguageProcessing ,pages1209Œ1215, Nagoya,Japan,14-18October2013. 1209 1210 1211 1212 1213 1214 1215  
Proceedingsofthe2015ConferenceonEmpiricalMethodsinNaturalLanguageProcessing ,pages817Œ822, Lisbon,Portugal,17-21September2015. c  2015AssociationforComputationalLinguistics. 817 818 819 820 821 822  
Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) ,pages1665Œ1675, October25-29,2014,Doha,Qatar. c  2014AssociationforComputationalLinguistics 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675  
ﬁShallIBeYourChatCompanion?ﬂ TowardsanOnlineHuman-ComputerConversationSystem RuiYan 1;31InstituteofComputerScience andTechnology(ICST) PekingUniversity Beijing100871,China yanrui02@baidu.com YipingSong 22DepartmentofComputer ScienceandTechnology PekingUniversity Beijing100871,China songyiping@pku.edu.cnXiangyangZhou 33BaiduInc. No.10XibeiwangEastRoad, Beijing100193,China zhouxiangyang@baidu.com HuaWu 33BaiduInc. No.10XibeiwangEastRoad, Beijing100193,China wu_hua@baidu.comABSTRACT Toestablishanautomaticconversationsystembetweenhumanand computerisregardedasoneofthemosthardcoreproblemsincom-  puterscience.Itrequiresinterdisciplinarytechniquesininforma- tionretrieval,naturallanguageprocessing,anddatamanagement, etc.Thechallengeslieinhowtorespondlikeahuman,andto  maintainarelevant,meaningful,andcontinuousconversation.The  arrivalofbigdataerarevealsthefeasibilitytocreatesuchasystem  empoweredbydata-drivenapproaches.Wecannoworganizethe  conversationaldataasachatcompanion.Inthispaper,weintro-  duceachatcompanionsystem,whichisapracticalconversation  systembetweenhumanandcomputerasarealapplication.Giv-  enthehumanutterancesasqueries,ourproposedsystemwillre-  spondwithcorrespondingrepliesretrievedandhighlyrankedfrom  amassiveconversationaldatarepository.Notethatﬁpracticalﬂhere  indicateseffectiveness andef :bothissuesareimportantfor areal-timesystembasedonamassivedatarepository.Wehave twoscenariosofsingle-turnandmulti-turnconversations.Inour system,wehaveabaserankingwithoutconversationalcontextin-  formation(forsingle-turn)andacontext-awareranking(formulti-  turn).Bothrankingscanbeconductedeitherbyashallowlearning  ordeeplearningparadigm.Wecombinethesetworankingsto-  getherinoptimization.Intheexperimentalsetups,weinvestigate  theperformancebetweeneffectivenessandefyforthepro-  posedmethods,andwealsocompareagainstaseriesofbaselines  todemonstratetheadvantageoftheproposedframeworkinterms  ofp@1,MAP,andnDCG.Wepresentanewangletolaunchaprac-  ticalonlineconversationsystembetweenhumanandcomputer. Keywords Human-computerconversation;bigdata;rankoptimization Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed  fororcommercialadvantageandthatcopiesbearthisnoticeandthefullcita-  tiononthepage.Copyrightsforcomponentsofthisworkownedbyothersthan  ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orre-  publish,topostonserversortoredistributetolists,requirespriorpermission and/orafee.Requestpermissionsfrompermissions@acm.org. CIKM'16,October24-28,2016,Indianapolis,IN,USA c2016ACM.ISBN978-1-4503-4073-1/16/10...$15.00 DOI:http://dx.doi.org/10.1145/2983323.2983360 1.INTRODUCTION Tocreateavirtualassistantand/orchatcompanionsystemwith adequateintelligencehasalwaysbeenalongcherished goalforresearchersandpractitioners.Itisbelievedtobechal-  lengingforcomputerstomaintainarelevant,meaningfulandcon-  tinuousconversationwithhumans.Howtorespondlikeahuman  generallyinvolvesinterdisciplinarytechniquessuchasinformation  retrieval,naturallanguageprocessing,aswellasbigdatamanage-  ment.Aamountofeffortshavebeendevotedtothe researchfordecades,andpromisingachievementshavebeengrad- uallyachievedsothatwecanexpectrealapplicationsinreallife,  ratherthaninSci-Fimoviesorresearchlabsonly. Thedemandforvirtualassistant/chatcompanionsystemlead- stocutting-edgetechnologyinthespotlightfrombothacademia  andindustry.Thearrivalofbigdataeraalsoacceleratesthede-  velopmentofhuman-computerconversationstudies.Owingtothe  publicresourcesforconversationsontheweb,wearelikelytolearn  whattoreplygiven(almost)anyinputsbyretrievingfromthecon-  versationalrepository.Itisprobablyagreattimingtobuildsuch  data-drivenconversationsystemsbetweenhumansandcomputer-  s.Wearemotivatedtoestablishanonlinechatcompanionsystem  forreal-timeservices.Sincethereisaclearindustry-drivenback- groundforourstudy,weoughttomakethesystempractical,where ﬁpracticalﬂmeansboth effectiveness andef .Thesetwois- suesarefundamentalforanonlinesystem. Human-computerconversationsystemshavebeenevolvingfor years.Researchershaveinvestigatedintotask-orientedcon-  versationsystems[34,40,46].Tomaintainconversationswith-  inadomain,itwouldbemorefeasibletocreateprior  knowledgetohandletaskssuchasbookingorbus  routeenquiry[23,46].Oneofthemostobviousdrawbacksforthe  serviceisthattheconversationcannotgobeyond  thedomainofthesystem,andthatthewaytofunctionisnearly  impossibletobegeneralizedtoopendomain.Itisonlyrecently  thatnon-task-orienteddialogue,a.k.a.open-domainconversation, hasbeenattractingtheattentionforitsfunctional,social,andenter- tainmentroles[3,2,26,14,39,20,32,9,19,48].Inthispaper,we  setourtargetatcreatingahuman-computerchatcompanionsystem  (i.e.,aChatBot)intheopendomain. Buildinganopen-domainChatBotsystemtointeractwithhu- mansisinterestingbutextremelychallenging.Firstly,sincepeople (a).Query-replyw/ocontexts. (b).Query-replyw/contexts. Figure1:Takeashortmulti-turn(2turns)conversationfor example.Itwouldbequiteconfusingandlessmeaningfulto  selectareplytothequeryalonewithoutusingtheconversation  context.Itisnaturaltoselectareplyabouttraveltothequery  whenconsideringthecontextabouttravelsinNationalParks. arefreetosayanythingtothesystem,itisinfeasibletoprepare theknowledgeforinteractionbeforehand.Secondly,thepossi-  blecombinationsofconversationstatusisvirtuallysothat  conventionalhand-craftedruleswouldfailtoworkforunexpected  humanutterances[35].Asmentioned,theemergingincreaseofbig  webdatacangreatlyadvancethedevelopmentofconversationsys-  temsinopen-domainconversations.Owingtothediversityofthe  web,aretrieval-basedsystemcanretrieveatleastsomerepliesfor  almostanyoftheuserinput,andthenreturnsareply,whichisa  greatadvantage.Tothisend,weestablishtheconversationsystem basedonretrieval-and-rankingbasedmethod. Ingeneral,therearetwotypicalscenariosforcomputerstoun- derstandconversations:1)single-turnand2)multi-turnconversa- tion.Asingle-turnconversationmayrefertothebeginningofa  (multi-turn)conversation,oradialogueshiftfromthecurrentcon-  versation.Amulti-turnconversationusuallydenoteanon-going  conversationwhichlastsforseveralturns.Formulti-turnconversa-  tion,theseriesofconversationhistorycanbeutilizedasadditional  information,namelyﬁcontextsﬂ.Single-turnconversationsareeasy  tounderstand.Nocontextinformationisavailable,nornecessary.  Weillustrateasimplemulti-turnscenarioinFigure1,wherecon-  textinformationisneededfortheconversation.Basedontheobser-  vations,wedesigntworankingmechanismsinparticulartohandle single-turnandmulti-turnconversationinthispaper,namely baseranking andcontext-awareranking .Wecombinebothrankingsin optimizationinordertosuitbothscenarios. Inthispaper,weintroduceadata-driven,real-timeconversa- tionsystemwhichprovideschatservicebasedonthetremendously largecorpusfromtheWeb.Duringeachconversationsession,users  inputasequenceofmessagestoﬁchatﬂwiththecomputer,andthe  ChatBotwillbeabletoreturnacorrespondingseriesofreplies.We  organizefunctionalcomponentstogetherintopipelinesandapply  timelyefoptimizationtomakethesystemframeworkpracti-  cal.Tosummarize,ourcontributionsareasfollows:  The1 stcontributionisthatweintroduceastraightforwardyet effective rankoptimizationframeworkforthehuman-computercon- versationsystembasedonretrieval.Theframeworkisgeneral,and  itadaptswellforbothsingle-turnandmulti-turnconversationsce-  nariosinopendomain. The2 ndcontributionisthatweexaminethe issuefor theproposedsystemtobepractical.Weexaminethedifferenceof  shallowlearning anddeeplearning basedcandidaterankingsfor bothbaserankingandcontext-awareranking.Weinvestigatethe  trade-offbetweeneffectivenessandefy. Wealsorunexperimentstoexaminethe effectiveness andef ciencyofourproposedframeworkincomparisonwithseveralal- ternativemethods.Weareabletoachievethelevelforef-  fectivenessusingthedeeplearningbasedranking,comparedwith  state-of-the-artapproaches.Usingtheshallowlearningbasedrank-  ing,wecanchoosetooutperformalmostallrivalsintermsof  runningtime,whileremainarelativelyhigheffectivenessperfor-  mance.Weprovideausefulperspectiveofviewtolaunchapracti-  calsystemforonlinechatservice. InSection2westartbyreviewingpreviouswork.Thenweintro- ducetheconversationsystemframework,includingpipelinecom-  ponents,rankingparadigmwithoptimizedcombinationdesigns.  WedescribeexperimentsandevaluationsinSection4,including  experimentalsetups,performancecomparisonsandresultdiscus- sions.FinallywedrawconclusionsinSection5. 2.RELATEDWORK Earlyworkonconversationsystemsisingeneralbasedonrules ortemplatesandisdesignedfordomains[34,40].These rule-basedapproachesrequirenodataorlittledatafortraining,  whileinsteadrequiremuchmanualefforttobuildthemodel,orto  handcrafttherules,whichisusuallyverycostly.Theconversation  structureandstatustrackinginverticaldomainsaremorefeasible  tolearnandinfer[46].However,thecoverageofthesystemsisal-  sofarfromsatisfaction.Later,peoplebegintopaymoreattention  toautomaticconversationsystemsinopendomains[32,9]. Fromdomainstoopendomains,theneedforabigamoun- tofdataisincreasingsubstantiallytobuildaconversationsys-  tem.Asinformationretrievaltechniquesaredevelopingfast,re-  searchersobtainpromisingachievementsin(deep)questionand  answeringsystem.Inthisway,analternativeapproachistobuild  aconversationsystemwithaknowledgebaseconsistingofanum-  berofquestion-answerpairs.Leuski etal. buildsystemstos- electthemostsuitableresponsetothecurrentmessagefromthe  question-answerpairsusingastatisticallanguagemodelincross-  lingualinformationretrieval[16],buthaveamajorbottleneckof  thecreationoftheknowledgebase(i.e.,question-answerpairs)  [17].Researchersproposeaugmentingtheknowledgebasewith  question-answerpairsderivedfromplaintexts[24,5].Thenum-  berofresourcepairsinthiswaycanbetosomeextentexpanded butstillrelativelysmallwhiletheperformanceisnotquitestable either.Knowledgebasesareimportantsourcesforbetterhuman  language/textunderstanding[36,37,38].Itwillbeinterestingto  incorporatesuchknowledgeintohuman-computerconversation. Nowadays,withtheprosperityofsocialmediaandotherWeb 2.0resources,suchascommunityquestionandanswering(cQA)  ormicrobloggingservices,averylargeamountofconversationda-  tabecomesavailable[39].Aseriesofinformationretrievalbased  methodsareappliedonsingle-roundshorttextconversationbased  onmicroblogdata[20].Higashinaka etal. alsocombinetemplate generationwiththesearchbasedmethods[9].Ritter etal. have investigatedthefeasibilityofconductingshorttextconversationby  usingstatisticalmachinetranslation(SMT)techniques,aswellas millionsofnaturallyoccurringconversationdatainTwitter[26].In theapproach,aresponseisgeneratedfromamodel,notretrieved  fromarepository,andthusitcannotbeguaranteedtobealegiti-  matenaturallanguagetext. Figure2:TheillustrationoftheChatBotsystemframework, includingbothoff-lineandonlineprocess.Thecandidatesare rankedwithandwithoutcontexts(ifany)andcombinedboth  rankingsinoptimization. Recently,withthefastdevelopmentofdeeplearningtechniques, effortsaredevotedintheneuralnetwork-basedconversationsys-  tems.Aneuralconversationmodelisproposedusingasequence-  to-sequencemanner[33].NeuralRespondingMachine(NRM)is  developedasagenerativewaytorespondtohumanmessages,us-  ingrecurrentneuralnetworks(RNN)[29].Thesingle-turncon-  versationgenerationisthenextendedtomulti-turnconversation  withconversationalcontextsareencodedforRNN[31].Atten-  tionschemaisalsoincorporatedintotheconversationmodel[45].  Ahierarchicalneuralnetworkmodelisproposedtomodelhuman  conversations[28].Thesegeneration-basedneuralnetworkscan-  notguaranteenaturalgenerationsaswell.Whatismore,neu- ralconversationmodelstendtogeneratetrivialorsafe,common- placeresponses(e.g.,Idon'tknow)regardlessoftheinput[18].  Retrieval-basedmethodsaremorelikelytointerestingcandi-  datereplieswhichareoriginallywrittenbyhumans.Ji etal. intro-duceaninformationretrievalapproachforshort-textconversation  usingdeepmatchmetrics[14].Thearchitectureisfurtherdevel-  opedasalearning-to-matchschemaforshorttexts[20].These  methodsareforsingle-turnconversationsonly,withnocontextin-  formationincorporated.Inourpreviouswork,wehaveproposed  adeeplearning-to-respondschemawhichincorporatescontextual  informationformulti-turnconversations[43]. Wecanseethesemethodscharacterizenewinsightsandformu- lategoodmodels.Yetnopreviousstudiesexaminetheconversation systemfromapracticalenterpriseperspective,investigatingtheis- suesof effectiveness andef .WeherebylaunchtheChatBot platform,andsharethehands-onexperiencefromanindustryan-  gel.Weprovidethereal-timeservice,accessibleforeveryone. 3.SYSTEMFRAMEWORK Tocreatetheconversationsystem,weneedtoestablishapipeline ofcomponentsincludingdatapreparation,searchandretrieval,and rankingswithoptimization.Roughly,wehaveanoff-lineprocess  andanonlineprocess.Wewillgothroughtheseproceduresinthis  section.Table1:Anexampleoftheoriginalmicrologpostandits  replies.Weanonymizeuserinformationforprivacy. POSTING :...!(Itisunbelievabletohavemyopiaatanﬁoldﬂage...Wishapairof glassesasmygift!) REPLY 1: !(Iwillofferoneforyou!) REPLY 2: (Itcanberecovered.Relax.) Table2:Tocreatetheconversationresource,weseparatethe postandrepliesas hposting-replyipairs.Inthisway,weprovide differentwaystorespondtoagivenqueryandmakediv  conversations. POSTING :˘À4à,º'l?ü ...!(Itisunbelievabletohavemyopia atanﬁoldﬂage...Wishapairof glassesasmygift!) REPLY :!(Iwillofferoneforyou!) POSTING :˘À4à,º'l?ü ...!(Itisunbelievabletohavemyopia atanﬁoldﬂage...Wishapairof glassesasmygift!) REPLY :(Itcanberecovered.Relax.) 3.1Off-lineProcess DataPreparation. WiththeprosperityofWeb2.0,peopleinter- activelyhaveconversationswitheachotherontheWeb.InBaidu Inc.,therearetrillionsofwebpagesindexedinPBlevelofstorage  forBaidusearchengine 1,whichprovidesahugethesaurusforcon- versationdata.Thereforeweareabletocollectsufhuman conversationdatatakenfromsocialmediasuchasmicroblogging websitesorforums,whereuserscanpublisha postingmessagevis- ibletothepublic,andthenreceiveabunchof replies inresponse totheirposting.Suchconversationsdonotoccurinstrictreal-time  stylebuttheyareliterallyrealhumanconversations.Wecollectthe  conversationaldatastoredas hposting-reply ipairs.Weshowsome oftheexamplesinTable1-2. Inthesampleshownabove,themessageofaconversation istypicallyunique,notdirectedatanyparticularuserbutinstead  broadcasttoallaudience.Therearemanyxibleformstoreply  toagivenmessage,whichisexactlythenatureofrealconversa-  tions:variousresponsebutallpossiblyappropriate,withdifferent  aspectsofinformationtoaconversation.Wethenseparate theposting-repliesintoagroupof posting-reply pairs,eachwith twoutterancesfromtheconversation.Thedatasetisdemonstrated tobeaperfectlyrichresourcetoexplorecandidatestohelpcom-  pletingahuman-computerconversationinnaturallanguages. Whenthedataiscollected,wethenpre-processtheobtainedre- sourcesfordatacleaning.Wethedatabyremovingextreme-  lyshortrepliesandthoseoflowlinguisticqualitysuchaspoint-  lessbabblingsaccordingtotheevaluationframeworkputforward  in[42,44,41],soastomaintainmeaningful,high-qualitycon-  versationrecords.Wealsoremoveinappropriateconversationsor  incompletesessions.Wedonotintendtoincludeconversational  resourceswithimpolite,rude,orevendirtywords.Besides,we  alsoremovetheconversationsessionswithmeaninglesssymbols, out-of-vocabularywordsorpurenumbers,whichwebelievethey 1http://www.baidu.comarelesslikelytoconstituteagoodconversation.Besides,wealso conductstopwordremoval. Afterdatacleaning,westoretheresourcedataonBaidusearch platformanddeployastandardinvertedindexingusingtermsusing Baidusystemsandretrievalinfrastructure,whichsupportsregu-  larupdates(weeklyormonthly)andincorporatesnewconversation resources.Thedataisincrementallybiggerandbiggerastimegoes by.Theproceduresmighttaketimebutitisacceptableforanoff-  lineprocess.Thentheindexeddocumentsinthe hposting-reply iformatwillnowbereadyforonlineretrieval. 3.2OnlineProcess SearchandRetrieval. Aftertakingintheuserissuedquery, whichcouldbeoneormoreterms,weapplyastandardretrieval processviakeywordsearchontheconversationdataresourceusing  thelight-weightsearchplatforminBaiduInc.Unlikethetraditional  searchtaskdeployedongeneralBaiduSearchEnginewhichper-  formsveryheavycomputingwiththousandsoffeatures,thecon-  versationbasedsearchismuchlighter.Wetreateachpairofpost-  ingandreplyasashortﬁvirtualﬂdocument.Henceweincorporate  minoradaptionthatnoweach virtualdocument toretrieveactually consistsofapairof postingandreply ,eventhoughweonlyneed toreturnthereplyastheoutput.Thewholecorpusisformattedin  aninvertedindexpreparedoff-line.Thisretrievalprocessreturnsa  listofallpotentiallyrelevantrepliestothequeryfromthecorpus. Withthesecandidatereplies,weproceedtothenextstep. RankingswithOptimization. Wenextconducttherelevancerank- ingprocedure,todistinguishthehighlyrelevantcandidatereplies fromlessrelevantrepliesretrievedfromtheconversationarchive.  Thisstepaimsatmatchingpostingsand/orrepliesandthen-  ingdownthecandidatepool.Tooptimizetheranking,theranking  paradigmwillneedtobegeneralenoughsoastotackleeverysce-  narioofhuman-computerconversationnomatterwhen1)theuser  initiatesanewconversation,2)theusercontinuesanon-goingcon-  versationand3)theusercontinuestheconversationwithnewcon-  tentshiftsinthemeanwhile.Thesesituationscanbemappedinto  twoscenarios,eithersingle-turnconversation,ormulti-turncon-  versationwithcontexts.Correspondingly,wedesigntworankings namelybaseranking forsingle-turnsand context-awareranking formulti-turns.Wealsointroducesanintermediatecomponentto segmenttheconversationsessions,soastodecidewhattouseas  thecontextsandhowtousecontexts:weoughttouseasucces-  siondialogueasthecontexts.Withallthesemetrics,weareableto  optimizethecandidatereplyselectionwithbothrankingschemes.  Moreimportantly,sinceweprovidereal-timeconversationservice,  weneedtobepracticalaboutcontextutilization.Itisnaturalthat  ascontextgrows,thecosttousecontextswillgrowaswell.We  willmanagetomakethecostremain(almost)thesame,ratherthan  letthecostgrowlinearlyorexponentiallyascontextlengthgrows. Sincetherankoptimizationisacorecomponentintheretrieval basedconversationsystemandmixedwithstrategies,wenextpro- ceedtoelaboratetherankingwithoptimization. 4.RANKINGWITHOPTIMIZATION 4.1ProblemFormulation Theresearchproblemofautomatichuman-computerconversa- tionisasoneormoreturnsofchattingsbetweenmanand computer.Withineachturnoftheconversation,giventhemessage  issuedfromhuman,thecomputerwouldprovideareplyinresponse  tothecomingmessage.Giventheusermessagesasqueries,our  systemretrievesrelatedrepliesfromavastrepositoryofconversa-  tiondataandreturnsthemostappropriateresponse.Wepropose Table3:Differentformulationsforsingle-turnconversation  andmulti-turnconversationwithcontextinformation. Single-TurnMulti-TurnInputs.qnqn,COutputs.rnrnObj.Func.r?n=argmaxrnf(rnjqn)r?n=argmaxrnf(rnjqn;C)tosuitbothscenariosofsingle-turnandmulti-turnconversationus- ingabaserankingaswellasacontext-awareapproach,andexpect  theconversationsystemcanrespondlikeahuman.Wehavethe  followinginTable3. Wehaveaconversationresourcein hposting-reply ipairs.Fora single-turnconversation,thesituationissimpleandclear.Given  aparticularquery q,weoughttoreturntheselectedreply r.Ina multi-turnconversationscenario,forthereply rninthe n-thturn, asingle-turnsystemutilizesonlythe n-thquery qnwhileamulti- turnsystemalsoincludesasuccessivechattinghistoryascontexts  C=fq1;r 1;:::;q n1;r n1g.f(:)isacertainscoringmetricto indicatethetendencytochoose rasoutputforthegiveninputs. Themetricwillbediscussedindetailsinthefollowingsections.  Weoptimizethebestreply r?ngiventhecontext C.4.2Rankings Weconductarankingtodistinguishhighlyrelevantcandidate repliesfromlessrelevantrepliesretrievedfromtheconversation  archive.Thisstepaimsatmatchingpostingsand/orrepliesand thendownthecandidatepool.Wehavebothbaseranking (i.e.,single-turnwithoutcontexts)andcontext-awarerankingdur-  ingthisstep.Weareabletoobtaintworankingscoresforeachof  thecandidatereplies.Sinceweproposetoutilizethecontextin-  formation,weoughttoatselectcontextssinceweonlyusea  successionconversationassociatedwith qasthecontexts C.ContextSelection. Firstly,wewilladdressthecontextse- lectionissue.Theintuitionisthatgiventhewholeconversation  session,notallofthetextsaresuccessivewiththequery,neither  shouldtheadditionalinformationbeincorporated.Weherebyuse  anintermediatesteptodecidewhenandwhatsessiontextstobe  utilizedasﬁcontextsﬂ.WedeployaTextTiling-like[30,7,8]seg-  mentationmethodwithdialogueacttosegmentdialogueﬁsession- sﬂintheconversation,andweuseasuccessiondialogueassociated withqasthecontexts.Thesessionsegmentationensuresthatwe useappropriatecontextsasadditionalinformation. BaseRanking. Weofferabaserankingfortheretrieved replies,whichistodistinguishthehighlyrelevantcandidatereplies  fromtheirrelevantresponsesretrievedfromtheconversationarchive.  Thisstepaimsatmatchingpostingsand/orrepliesandthen  downthecandidatepoolthroughsemanticrelevancemeasurement  andaseriesofotherevaluationmetrics.Givenaquery q,weobtain arankinglist ˝ofthetop- krankedreplies Rq=fr1;r 2;:::;r kgaccordingtotherelevancescoregivenbytheranker.Apermuta-  tion(orranking) ˝isgeneratedinthedescendingorder.Each riisrepresentedasavector xiwhereeach xi2Rjxjhasjxjdimen-sions.Arankingfunction Rjxj!Ristrainedandappliedtorank thereplies,and ˝(ri)<˝ (rj)meansriismorepreferredover rjintherankinglist(a.k.a., f(rijq)>f (rjjq)sinceitisbasedons- ingleturns).Wehavetwoinstinctivelydifferentrankingparadigms  basedonshallowlearninganddeeplearning.Theshallow/deep  rankerswillbedescribedinSection4.3.Throughthisprocess,we  areabletoobtainarankingscoreforeachofthecandidatereplies  andthenweranktherepliesaccordingly.Thisrankinglistisre- gardedasabaserankingusing qonly.Weomitthesubscriptof qanduse Rtorepresent Rqwhenthereisnoambiguity. Tillnow,weonlyconsidertheinformationwithoutcontextual informationtomatchthecandidaterepliestothequery,andhence createthebaseranking ˝b.However,thecontext-insensitiveinfor- mationmightnotbesufenough,especiallyunderthescenari- oofmulti-turnconversationwhenwehaveaconversationhistory toevaluatewhichreplywouldbebettergivenaparticularcontext.  Wenextincorporatethecontext-awareranking. Context-AwareRanking. Nowweaimatexaminingwhich candidatereplyismoreappropriategiventhecontextunderamulti-  turnconversationscenario.Wealsohaveavector-basedcontext  representationstomatchwiththetop-rankedcandidatereplies,and  hencetherankings.Likewise,weuseashallowrankerora  deepranker. Afterthebaserankingandthecontext-awareranking,weoptto combinebothdimensionsofrankingstogetherinordertooptimize  therankingordersforallcandidatereplies.Then,theconver-  sationsystemselectsthereplywiththehighestrankastheoutput torespond.Next,wewillfurthergothroughtheshallowanddeep rankers,andthenthedetailsofrankcombinationandoptimization. 4.3ShallowRankerv.s.DeepRanker Inthissection,weintroducetworankingmethodsforcandidate replies,usingshallowlearninganddeeplearningcorrespondingly. 4.3.1ShallowLearning Fortheshallowranker,wedeployaMaximumEntropy [13]toofferthebaserankingandthecontext-awarerankingforthe retrievedreplies.Theutilizesaseriesoffeatures.At weintroducesomeshallowrepresentationsofthedocuments,i.e., queries,replies,andpostings. Term-basedRepresentation. Mostintuitively,werepresentthe textsbythevectorspacemodel[21].Inthisway,thecontentof  thecontextcanberepresentedas c(d)=hˇ(w1;d ),ˇ(w2;d ),::: ,ˇ(wjdj;d )iwhereˇ(wi;d )denotestheweightforterm wiwhichoccursinthetext d.Fortheconversationdata,weevaluatetheter- mweightingusingthestandard tf-idfmeasurement,which termimportance[21].Wecalculatethecosinesimilaritybetween  twotermvectors. Topic-basedRepresentation. ﬁTopicsﬂhavelongbeeninvesti- gatedastheabstractivesemanticrepresentation[10].Weapplythe  unsupervisedLatentDirichletAllocation[4]todiscovertopics.We obtaintheprobabilitydistributionovertopicsforthetextpieces. Theinferredtopicrepresentationistheprobabilitiesforthepiece  oftextbelongingtoacertaintopic.Weempiricallytraina1000-  topicmodelandrepresenttextsastopicvectors.Wealsocalculate  thecosinesimilaritybetweentwovectors. Entity-basedRepresentation. Namedentitiesareaspecialform ofterms.Inthisstudy,wedistinguish persons ,locationsandor- ganizationsfromplaintextswiththehelpofnamedentityrecogni- tiontechniquesandmaintainavectorofrecognizedentitiesforthe  conversation.Basedanestablishedknowledgegraphminedfrom  Baidusearchlogs,wecancalculatethesimilarity(measuredby  entitydistanceintheknowledgegraph)betweentwoentity-based  vectorrepresentations. Weformulatethefollowingpairwisefeatures.Forpointwisefea- tures,webasicallycalculateamatchingscore,eitherbysimilarity  usingthestandardcosinemetric,orbydependencymeasurement  usingthemutualinformation[21]metric. Query-ReplyMatchingScore. Wecalculatethematchingscore betweenthequeryandthecandidatereplyby1)cosinesimilarity  and2)mutualinformationbasedonthementionedrepresentations. Context-ReplyMatchingScore. Similarly,wecalculatethematch- ingscorebetweenthecontextandthecandidatereply,calculatedby  thematchingmetricsbasedonthethreeshallowrepresentations. Query-PostingMatchingScore. Wealsoconsiderthematch- ingscorebetweenqueryandtheoriginalpostingassociatedwith  thecandidatereplyinthe hposting-reply ipairs,calculatedbythe matchingmetrics. Context-PostingMatchingScore. Again,wecalculatethematch- ingscorebetweenthecontextandtheoriginalpostingassociat-  edwiththecandidatereply,calculatedbythementionedmatching  metrics.StatisticalMachineTranslation. Statisticalmachinetranslation (SMT)isamachinetranslationparadigmwhichtranslatesonesen-  tenceinalanguagetoasentenceinanotherlanguage.Ifwetreat  queries/contextsandrepliesasdifferentlanguages,wecantraina  translationmodeltoﬁtranslateﬂqueries/contextsintorepliesbased  onthetrainingcorpora.NotethatfortheSMTscore,weonlycal-  culatebasedonterm-basedrepresentation. Wealsoincludepointwisefeaturesasfollows: LanguageModel. Generativescorefromhighqualitylanguage modelforthecandidatereply.Thelanguagemodelistrainedbased onlargenewsdata.Wetrainthelanguagemodelinunigrambased  onmillionsofnewsarticlesarchivedfromvariousmainstreamnews  websites.AverageTermWeighting. Wecalculatetheaverage tf-idfscoreforthereplyastheweight.Acandidatereplywithhigherweights  ismorelikelytobeimportance. Length.Thisfeaturedenotesthelengthofreplies.Tooshort repliesarenotpreferred.Weconductanormalizationtomapthe  valueto[0,1]. Fluency. Fluencyistoexaminewhethertwoneighboringterms havealargeco-occurrencelikelihood.Wecalculatetheco-occurrence probabilityforthebi-gramsofthecandidatereplyandthentakethe averagevalueasthey. Wefeedallthecalculatedfeaturesofacandidatereplyintothe scoringfunctionandrankthereplyaccordingly.Thefeaturesare  empiricallyhand-craftedandthelearningisshallow.Incontrastto  theshallowlearningranker,wenextproposeadeeplearningranker. 4.3.2DeepLearning Thedeeprankerbasedondeeplearningtechniquesdonotrely onhand-craftedfeaturesfromempiricalexpertise.Inrecentyears,  deepneuralnetworks(DNNs,alsoknownas deeplearning )have madeimprovement.Withbigdataavailable,DNNsare  highlyautomatedlearningmachines;theycanextractunderlying  abstractfeaturesofdataautomaticallybyexploringmultiplelayers  ofnon-lineartransformation[1]. Asmentioned,wehavetwoparts,i.e., hposting-reply i,tocom- parewiththequeryand/orthecontext.Thescoringfunctionout-  putsascalarin R(appropriatenessorinappropriateness)infora particularcandidatereply,giveneitherthequeryitself(i.e.,the  baserankingpart)orthecontext(i.e.,thecontext-awareranking  part).Boththebaserankingscoreandthecontext-awareranking  scorearecomputedbythesamedeepneuralnetworkarchitecture,  buttheirparametersaredifferentsothatthescoringfunctionscan  depictdifferentmeanings.Inparticular,thedeepstructureforsen- tencepairmodelingincludesthefollowingcomponents. WordEmbeddings. Traditionalmodelsusuallytreatawordas adiscretetoken;thus,theinternalrelationbetweensimilarwords wouldbelost.Wordembeddings[22]areastandardapparatusin  neuralnetwork-basedtextprocessing.Awordismappedtoalow  dimensional,real-valuedvector.Thisprocess,knownasvector-  ization,capturessomeunderlyingmeanings.Givenenoughdata, usage,andcontext,wordembeddingscanmakehighlyaccurate guessesaboutthemeaningofaparticularword.Embeddingscan equivalentlybeviewedthatawordisrepresentedasaone-hot  vectorandmultipliedbyalook-uptable[22]. Inourmodel,wevectorizeallwordsusingtheirembed- dings,whichserveasthefoundationofourdeepneuralnetworks. Wordembeddingsareinitializedrandomly,andthentunedduring trainingaspartofmodelparameters. Bi-DirectionalLSTM. Weuseabi-directionallongshortterm memory(Bi-LSTM)recurrentnetworktopropagateinformationa-  longthewordsequence.Arecurrentneuralnetwork(RNN)keeps  ahiddenstatevector,whichchangesaccordingtotheinputineach  timestep.AsRNNscaniterativelyaggregateinformationalonga  sequence,theyarenaturallysuitableforsentencemodeling. LSTMisanadvancedtypeofRNNbyfurtherusingmemory cellsandgatestolearnlongtermdependencieswithinasequence  [33,25].LSTMmodelsareasfollows:givenasequence  ofinputs,anLSTMassociateseachpositionwith input,forget ,and outputgates ,denotedas it,ft,and otrespectively.Thevector ltisusedtoadditivelymodifythememorycontents.Givenaninput  sentenceS=fx0;x 1;:::;x Tg,where xtisthewordembedding atposition tinthesentence.LSTMoutputsarepresentation htforpositiont,givenby 2664itftotlt3775=2664˙˙ ˙tanh3775Wht1et~ht=ft~ht1+itlthst=ot~ht(1)where~hisanauxiliaryvariableandcanbeviewedastheinfor- mationstoredinmemorycell. ˙()= 11+eisaknownasa sigmoid/logisticfunction. AsingledirectionalLSTMtypicallypropagatesinformationfrom thewordtothelast;hencethehiddenstateatacertainstepis dependentonitspreviouswordsonlyandblindoffutureword-  s.ThevariantBi-LSTM[6]isproposedtoutilizebothprevious  andfuturewordsbytwoseparateRNNs,propagatingforwardand  backward,andgeneratingtwoindependenthiddenstatevectors ! htand  ht,respectively.Thetwostatevectorsareconcatenat- edtorepresentthemeaningofthe t-thwordinthesentence,i.e., ht=h! ht;  hti.Convolution. Wefurtherapplyaconvolutionalneuralnetwork (CNN)toextractlocalneighboringfeaturesofsuccessivewordsŠ i.e.,discriminativewordsequencescanbedetectedŠyieldinga  morecompositerepresentationofthesentences.Thestructureof  CNNinthisworkissimilarto[15].UnlikeRNNs,CNNsonly  imposelocalinteractionsbetweensuccessivewordswithina  (sizem).Concretely,webuildaCNNupontheoutputofBi-LSTM.For everywindowwiththesizeof minBi-LSTMoutputvectors,i.e., (Ht)m=[ht;h t+1; ;h t+m1],where tisacertainposition, theconvolutional F=[F(0);:::;F (m1)]willgeneratea vectorsequenceusingtheconvolutionoperationﬁ ﬂbetweenthe twovectors.Moreformally,theconvolutionresultsinavector  whereeachcomponentisasfollows: oF=tanhm1Xi=0h(t+i)F(i)(2)Inpractice,wealsoaddascalarbias btotheresultofconvolu- tion.Inthisway,weobtainthevector oFisavector,eachdimen- sioncorrespondingtoeachwordinthesentence. Noticethattheaboveequationdescribesasingleﬁsliceﬂofcon- volution.Infact,wemayhavemultiplefeatureandthus  multiplefeaturemaps.Differentdonotshareparameters( Fandb),sothattheycancapturedifferentmeanings. Pooling,Concatenation,andMatching. Onthebasisofsen- tencerepresentationsusingBi-LSTMwithCNN,wecanmodelthe  interactionsbetweentwosentences.Weapplypoolingtoaggregate  informationalongthewordsequence.Inparticular,amaxpooling  layerchoosesthemaximumvalueineachdimensioninthefeature  mapsaftertheconvolution,indicatinghowmuchthefeatureismost  alongthesequence. Wehavetwomatchingmatricebetweenposting-query(orcon- text)andreply-query(orcontext)usingstandardcosinesimilarity.  Hencewehavetwoscalarscoresforthematchingmatrice.We  concatenatetheseindividualsentences'vectorrepresentations(i.e.,  query/context,posting,andreply)withthescoresfromthematch- ingmatrice.Thentheconcatenatedvectorsarefedtoanensuing networkforfurtherinformationmixing.Vectorconcatenationfor  sentencematchingisalsoappliedinotherstudieslike[47],which  iseffectiveyetoflowcomplexityorder,comparedwithotherword-  by-wordmatching[11],orattentionmethods[27]. Thejointvectoristhenpassedthrougha3-layer,fully-connected, feed-forwardneuralnetwork,alsoknownas multi-layerperception (MLP)[1],whichallowsrichinteractionsbetweenasentencepair  fromoneofthethreecomponents.Thenetworkenablestoextract  featuresautomatically,startingfromlower-levelrepresentationsto  higher-levelones.Finally,asingleneuronoutputsthescorebe-  tweenaquery(orthecontext)andareply.Thescoringneuron  isessentiallyalinearregression. 4.4RankwithOptimization Giventhetworankinglists(i.e.,baserankingandcontext-aware ranking),ourproposedsystemappliesanoptimizedcombination oftherankedlists.Weoptimizethecombinationsothatthe  rankshouldnotdeviatetoomuchfromeitherofthetworankings.  Then,thesystemselectsthereplywiththehighestrankingasthe  outputtorespond. Wedonotdirectlyaddthebaserankingscoresandcontext-aware rankingscorestogetherasthemeasurementtocalculatethe  rankings.Let ˝bbethebaserankingand ˝cbethecontext-aware ranking,weneedtoestimatetherankingas ˝,andalsoesti- matetherank ˝(ri)foreachcandidatereply ri.Weaimatopti- mizingthefollowingobjectivecostfunctionO( ˝),O(˝)= j˝jXi=1Bijj˝(ri)ijjjj ˝b(ri)Bijj2+j˝jXi=1Cijj˝(ri)ijjjj ˝c(ri)Cijj2(3)whereBiisthebaserankingscorewhile Ciisthecontext-aware rankingscore. iisexpectedtobethemergedrankingscore, namelytheﬁappropriatenessﬂ,whichwillbelater.Among  thetwocomponentsintheobjectivefunction,thecomponent  meansthattherankshouldbeclosetothebaserank.We use(jj˝(ri)ijjjj ˝b(ri)Bijj)2insteadof(˝(ri)˝b(ri))2inorder todistinguishthedetaileddifferencesassociatedwithscoresand ranks.Thesecondcomponentissimilarbymakingrankcloseto  context-awarerank. Ourgoalisto ˝(ri)=˝(r?i)tominimizethecostfunction, i.e.,˝=argmin O(˝).˝istherankmergedbyouralgorithm. Tominimize O(˝),wecomputeitspartialderivatives. @O (˝)@˝(ri)=2i(Bii˝(ri)˝b(ri))+2i(Cii˝(ri)˝c(ri))(4)Let@O(˝)@˝(ri)=0 ,weget ˝(r?i)= i˝b(ri)+ i˝c(ri)Bi+Ci(5)Twospecialcasesarethatif(1) =0, 6=0:indicatingweonly usethebaseranking,i.e.,single-turnwithoutcontexts.(2) 6=0,=0,indicatingweonlyusecontext-awarerankingwithoutbase ranking.We iastheweightedcombinationofmergedranking scoresfrombothrankings: i=Bi+Ci+(6)Inthisway, iisdependentonbaserankingscoreandthe additionalcontext-awarerankingscore.WeplugEquation(6)into Equation(5),andobtainamoreconciseformatwithnoranking  scoreof :˝(r?)isaweightedcombinationofbaserankingand context-awarerankingby: ˝(r?i)= +˝b(ri)++˝c(ri)(7)Therankingequationindicatesanefcalculationap- proachwithananalyticalsolution.Evenifthecontextsarecon-  sistedbyseveralutterances,wejustneedtoprocessallthecontexts  togetherasabatch,ratherthanasentence-by-sentencestyle,which makestheserviceefpractical. 5.EXPERIMENTSANDEVALUATION Inthissection,weevaluatetheproposedsystemforconversation taskagainstaseriesofbaselinesbasedonthehugeconversation resource.Theobjectivesofourexperimentsareto1)evaluatethe effectivenessofourproposedframework,2)evaluatetheefy  issueofdifferentrankingapproaches,and3)investigatethetrade-  offbetweeneffectivenessandefy. 5.1ExperimentalSetups 5.1.1Dataset Asmentioned,wecollectedmassiveconversationresourcesfrom variousforums,microblogwebsites,andcQAplatformsincluding BaiduZhidao 2,Doubanforum 3,BaiduTieba 4,SinaWeibo 5,etc. Thedatasetisthesamedatasetthatweusedinourpreviouswork  [43].Intotal,thedatabasecontains ˘10million hposting;reply ipairs.SomestatisticsaresummarizedinTable4. Weconstructedthedatasetof1,606,583samplestotrainthedeep neuralnetworks,357,018forvalidation,and11,097fortesting.It  isimportantthatthedatasetforlearningdoesnotoverlapwiththe  databaseforretrieval,sothatwestrictlycomplywiththemachine learningregime.Foreachtrainingandvalidationsample,weran- domlychoseareplyasanegativesample.Validationwasbased  ontheaccuracyofpositive/negativeForthetestset, 2http://www.zhidao.baidu.com3http://www.douban.com 4http://www.tieba.baidu.com 5http://www.weibo.com Table4:Datastatistics.Postingsandrepliesareallunique. Source#Posting#Reply#Vocabulary Zhidao8,915,6943,705,3021,499,691Douban10,618,9812,963,226483,846Tieba4,189,1603,730,2481,046,130Weibo186,963393,654119,163Misc.3,0561,5484,297Total9,023,8547,293,9782,857,378wehiredworkersonacrowdsourcingplatformtojudgetheappro- priatenessof30candidaterepliesretrievedforeachquery.Each  samplewasjudgedby7annotatorsviamajorityvotingbasedon theappropriateness fortheresponsegiventhequeryandcontexts (ifany):ﬁ1ﬂdenotesanappropriateresponseandﬁ0ﬂindicatesan inappropriateone. 5.1.2Hyperparameters Inourproposedmodel,weused128-dimensionalwordembed- dings,andtheywereinitializedrandomlyandlearnedduringtrain-  ing.AsourdatasetisinChinese,weperformedstandardChinese  wordsegmentation.Wemaintainedavocabularyof177,044phras-  esbychoosingthosewithmorethan2occurrences. Thebi-directionalLSTMhas128hiddenunitsforeachdimen- sion;CNNis256dimensionalwithawindowsizeof3.Weused  stochasticgradientdescent(withamini-batchsizeof100)foropti- mization,gradientcomputedbystandardback-propagation.Initial learningratewassetto0.8,andamultiplicativelearningratedecay  wasapplied.Theaboveparameterswerechosenempirically.We  usedthevalidationsetforearlystopping. 5.1.3EvaluationMetrics Giventherankinglists(annotatedbycrowdsourcedworkers)for testqueries,weevaluatedtheperformanceintermsofthefollow-  ingmetrics:precision@1(p@1),meanaverageprecision(MAP)  [32],andnormalizeddiscountedcumulativegain(nDCG)[12].S-  incethesystemoutputsthebestselectedreply,p@1istheprecision  atthe1stposition,andshouldbethemostnaturalwaytoindicate  thefractionofsuitableresponsesamongthetop- 1replyretrieved. Besides,wealsoprovidedthetop- krankinglistforthetestqueries usingnDCGandMAP,whichtestthepotentialforasystemtopro-  videmorethanoneappropriateresponsesascandidates.Weaimed  atselectingasmanyappropriateresponsesaspossibleintothetop-  klistandrewardingmethodsthatreturnsuitablerepliesonthetop. Formally,themetricsarecomputedasfollows. nDCG@i=1jTjXq2T1ZkXi=12ri1log(1+i)whereTindicatesthetestingqueryset, kdenotesthetop- kposi-tionintherankinglist,and Zisanormalizationfactorobtained fromaperfectranking. riistherelevancescoreforthe i-thcandi- datereplyintherankinglist(i.e.,1:appropriate,0:inappropriate). MAPiscomputedby MAP=1jTjXq2T1NqkXi=1PiriHereNqisthenumberofappropriateresponsesselected,and Piistheprecisionat i-thpositionforthequery. Table5:Overallperformancecomparison.For MicrosoftXiaoIce andgenerativemethods,wedonothaveMAPornDCGscores sincewegenerateonecandidatereplyforeachquery.Notethatfortheoftrainingandtestingqueries,wereportthetime costperquery .EffectivenessMetrics Efy(inmilliseconds) Systemp@1MAPnDCG@10nDCG@20TrainingTestingSMT(Ritteretal.,[26]) 0.363ŠŠŠ37.80.4NRM(Shangetal.,[29]) 0.465ŠŠŠ185.34.2LSTM-RNN(Sutskeveretal.,[33]) 0.501ŠŠŠ253.95.1MicrosoftXiaoIce0.533ŠŠŠŠˇ1.1RandomMatch0.2660.2460.2890.353Šˇ0.0OkapiBM250.2720.2530.3020.368Šˇ0.0DeepMatch(LuandLi,[20]) 0.4570.3170.4540.508193.83.9LSTM-RNN(Palangietal.,[25]) 0.3380.2830.3710.431221.64.8ARC-CNN(Huetal.,[11]) 0.3940.2940.4210.477165.33.5DL2R(Yanetal.,[43]) 0.7310.4160.6820.717396.56.8ROCFinDeepMatch 0.6680.4060.6280.691193.83.9ROCFinLSTM-RNN 0.6310.3940.6160.672221.64.8ROCFinARC-CNN 0.6470.4010.6080.661165.33.5ROCF-ShallowRanker 0.5930.3870.6020.6582.70.1ROCF-DeepRanker 0.7110.4120.6660.702250.85.05.2AlgorithmsforComparison Toillustratetheperformanceofourapproach,weincludeseveral alternativealgorithmsasbaselinesforcomparison.Thebaselines  canbedividedintotwocategories,i.e.,1)generation-basedmeth-  odsand2)retrieval-basedmethodsforconversationsystemsfrom  veryrecentstudies.Sinceourproposedapproachistechnicallya  retrieval-basedmethod,wemainlyfocusonthesecondcategory.  Forfairnessweconductedthesamepre-processingproceduresand  datacleaningforallalgorithms. Generation-basedConversation. Forthisgroupofalgorithms, theconversationsystemwillgeneratearesponsefromagivenin- put,i.e.,aqueryfromtheuserundertheconversationalscenario. StatisticalMachineTranslation(SMT) :SMTisamachinetrans- lationparadigmwhichtranslatesonesentenceinthesourcelan-  guagetoasentenceinthetargetlanguage.Ifwetreatqueriesand  repliesasseparatelanguages,wecantrainatranslationmodelto  ﬁtranslateﬂqueriesintoreplies.Weimplementedthephrase-based  translationideaforconversationproposedin[26]. LSTM-RNN:LSTM-RNNisbasicallyaRecurrentNeuralNet- work(RNN)usingtheLongShortTermMemory(LSTM)archi- tecture.TheRNNwithLSTMunitsconsistsofmemorycellsin ordertostoreinformationforextendedperiodsoftime.Weuse  LSTM-RNNforbothgenerationandretrievalbaselines.Forgen-  eration,weuseanLSTM-RNNtoencodetheinputsequence  (query)toavectorspace,andthenuseanotherLSTM-RNNtode-  codethevectorintotheoutputsequence(reply)[33];forretrievals,  weadopttheLSTM-RNNtoconstructsentencerepresentationsand  usecosinesimilaritytooutputthematchingscore[25]. NeuralRespondingMachine. Weimplementtheneuralre- spondingmachine(NRM)proposedin[29],whichisanRNN-  basedgenerationapproachwithaglobal-localattentionschema. Retrieval-basedConversation. Theapproacheswithinthisgroup ofbaselinesarebasedonretrievalsystems,whichreturnthebest matchedcandidatereplyoutoftheconversationalrepositorygiven  aparticularquery.Sinceourapproachisretrieval-based,weselect  strongretrieval-basedmethodstomakeathoroughcomparison. RandomMatch. Themethodrandomlyselectsrepliesfrom theretrievedlistforeachquery.Beawareitisnot truerandombecauseitonlyrandomizestheorderoftheretrievedresults.The  truerandommatchistooweaktobeincludedasadecentbaseline. OkapiBM25. Weincludethestandardretrievaltechniqueto rankcandidatereplies.Foreachquery,weretrievethemostrele-  vantreplyusingBM25model[21]fromthecorpus. DeepMatch. TheDeepMatchmethodconsidersmultiplegran- ularityfromtheperspectiveoftopics,obtainedviaLDA[20]. ARC-CNN.TheARCapproachisaCNNbasedmethodwith convolutionarylayerswhichconstructsentencerepresentationsand  producethematchingscoresviaaMLPlayer[11]. DeepLearning-to-Respond(DL2R) .WeproposetheDL2R systembasedonaqueryreformulationandresultlistmergingframe-  workin[43].DL2Risalsoageneralapproachforbothsingle-turn  andmulti-turnconversationscenarios. IndustryApplication(MicrosoftXiaoIce). Thereareseveral industrialcompaniesresearchingintoautomatichuman-computer  conversationaswell,suchasMicrosoftCortana,GoogleNow,Ap-  pleSiriandHuaweiNoah.Sincewedeveloptheentiresystem  basedonChinesecorpus,itisintuitivethatwecomparewiththe  MicrosoftChineseChatBotnamedXiaoIce 6( ),whichcould tosomeextentrepresentthestate-of-the-practiceconversationsoft-  warefromindustry.DuetolimitedopenAPIaccess,wecanonly  obtainthetop-1replyfromXiaoIceforeachquery. RankOptimizedConversationFramework(ROCF) .Wepro- poseROCFusingcontextinformation,whichaimsatretrieving  moreappropriaterepliesbasedonconversationalcontextsfrompre-  viousturns,whichisanoptimizedcombinationofbaserankingand  context-awareranking.SincetheproposedROCFisanadaptive  framework,wehavetwosubstitutivecomponentsofbothshallow  ranker,deeprankerandotherrankersfortherankingpart. 5.3Results OverallperformanceresultsareshowninTable5.Fromthere- sults,wehavesomeinterestingobservations. Forthe effectiveness concern,theperformanceofthegenerative methodsisquitemoderate,whichconcursthejudgmentfrom[18, 29].Theautomaticconversationgeneratorstendtoproduceuniver-  sal,trivialandambiguousreplieswhicharelikelytoanswerawide 6http://www.msxiaoice.com/rangeofqueries,butnotenoughtoconductameaningful conversation.Besides,despiteoftherelativelyhighp@1scorefor thesemethods,theydonotnaturallyprovidemorethanonecan-  didatereplies.Ingeneral,weobservethatgenerativeapproaches  usingdeeplearning(i.e.,NRMandLSTM-RNN)performbetter  thanthatwithoutdeeplearningtechniques. Forretrievalmethods,itiseasytoobtainarankedlistofcandi- datereplies,whichshowgreatpotentialtoconductconversations  withdiversity.Astotheretrieval-basedmethods,RandomMatchis  alowerboundforallbaselines.Aswementioned,itrandomizes  theorderoftheretrievedresults.Hencetheresultiscomparable  tothatofBM25,slightlyworse.OkapiBM25representsthes-  tandard(andsimple)retrievalsystem.TheperformanceforBM25  isnotasgoodastheotherdeeplearning-basedretrievalsystems,  whichisnotsurprising.Deeplearningsystemsareprovedtohave  strongcapabilitiestolearntheabstractiverepresentation[20,29,  31].ThebestperformanceisachievedbyDL2Rwhichisproposed  inourpreviouswork[43].Inthemeanwhile,DL2Risthemost  timeconsumingmethod.Theperformanceofdeeplearningbased algorithmsingeneraloverwhelmsthatofshallowlearningbased algorithms.TheDL2Rmethodhastheadvantageof  contextualmodeling. Notethat,ourproposedROCF-DeepRankeroutperformsalloth- erbaselines,andachievescomparable(slightlyweaker)resultwith  DL2R.TheROCF-ShallowRankeroutperformsthegeneralmatch-  ingalgorithmswithdeeplearningtechniques,duetotheconversa-  tionalscenarioformulation.Still,ROCF-ShallowRankerisnotas  effectiveasotherdeeplearningmetricsinROCFframework.Itis  naturalfortheshallowrepresentationoffeatureengineering.The  observationissomehowmeaningful:theproposedROCFframe-  workisextensiveandcompatiblefordifferentrankers. Anotherminorissueisthatweincludetheindustrialapplication ofMicrosoftXiaoIceduetotheindustrybackgroundofourstudy.  SincewecannotgetaccesstotheinternalAPIofXiaoIce,there  couldbesomenetworkdelayandhencethetestingtimeisinaccu-  rate.StillweoutperformsXiaoIceintermsofp@1metric,which  couldberegardedasoneoftheup-to-dateindustrialsystems. Sincewetargetatlaunchingapracticalonlinesystemtopro- videreal-timeservice,time isalsoamajorconcern.In general,deeplearningmethodsrequirebigdataformodeltraining,  theadvantageofourproposedapproachbecomesratherprominent.  Thetimefortrainingthedeeplearningmodels(eitherthegenera-  tivemethod,ortheretrievalmethodDeepMatch)wasseveralmag-  nitudeshighercomparedtothestraightforwardshallowlearning  modelsaccordingtothetrainingtimeandtestingtime.Itisintuitive thatmeasurementsontermsandhigh-levelfeatureswillbemuch fasterthanaseriesofheavily-computedcalculations:embedding,  convolution,andpooling,etc.Althoughthefastestmethodisthe  randommatch,theproposedROCF-ShallowRankerseemstoyield  thebesttrade-offbetweeneffectivenessandefyforapracti-  calsystem.Givenenormousnumberofconversationshappenedev-  eryday,thetimedifferencebetweenshallowinglearninganddeep  learningisgreatlyHowever,westillhavegreatpotential  toincreasetheeffectivenessperformancewhenweswitchtothe  deeplearningwayonline.TheproposedROCFisratheradaptive  andxible. 6.CONCLUSION Inthispaper,weproposeapractical,whichmeans effective andef ,frameworkforhuman-computerconversationinopendo- main,whichtargetsatprovidingreal-timeservices.Giventheuser issuedmessageasqueries,ourproposedsystemwillbeabletore-  turnacorrespondingreplyretrievedfromamassivedatareposito- ry( ˘10million hposting-reply ipairs)torespondthehumanutter- ance,fundamentallybasedonaninformationretrievalframework  givenavastconversationresource.Thesystemsuitsbothsingle-  turnandmulti-turnconversation.Westartfromabaseranking  withoutcontextinformation,andthenincorporateacontext-aware  ranking(ifany)inanoptimizedcombination.Performancecom- parisonsbetweenourproposedmethodagainstbaselinesdemon- stratetheadvantageofourROCFbasedontherankoptimization  andcombinationapproach.Thedeeplearningmethodsarecom-  petitivewithbettereffectivenessscoresbuthavedraw-  backsinefy.TheShallowRankerachievesslightlyweaker  effectivenessbutmuchfasterinatimelymanner.Weprovidean-  otherperspectiveofviewtoestablishapracticalconversationsys-  tem.Yet,westillhaveplentyroomtomakeimprovementusing  ROCF-DeepRanker. 7.ACKNOWLEDGMENTS ThisworkissupportedbytheNationalHighTechnologyR&D ProgramofChina(GrantNo.2015AA015403)andNationalBa- sicResearchProgramofChina(GrantNo.2014CB340505).We  thankallthereviewersfortheirvaluablecomments,andthankthe  supportfromtheDeepLearningPlatforminBaiduNaturalLan-  guageProcessingDepartment. 8.REFERENCES [1]Y.Bengio.LearningdeeparchitecturesforAI. Foundations andTrendsinMachineLearning ,2(1):1Œ127,2009. [2]F.Bessho,T.Harada,andY.Kuniyoshi.Dialogsystemusing real-timecrowdsourcingandtwitterlarge-scalecorpus.In SIGDIAL'12 ,pages227Œ231,2012. [3]T.W.BickmoreandR.W.Picard.Establishingand maintaininglong-termhuman-computerrelationships. ACM Trans.Comput.-Hum.Interact. ,12(2):293Œ327,June2005. [4]D.M.Blei,A.Y.Ng,andM.I.Jordan.Latentdirichlet allocation.J.Mach.Learn.Res. ,3:993Œ1022,Mar.2003. [5]G.Cong,L.Wang,C.-Y.Lin,Y.-I.Song,andY.Sun.Finding question-answerpairsfromonlineforums.In SIGIR'08 ,pages467Œ474,2008. [6]A.Graves,A.-r.Mohamed,andG.Hinton.Speech recognitionwithdeeprecurrentneuralnetworks.In Proc. Acoustics,SpeechandSignalProcessing ,pages6645Œ6649,  2013.[7]M.A.Hearst.Multi-paragraphsegmentationofexpository text.In ACL'94 ,pages9Œ16,1994. [8]M.A.Hearst.Texttiling:Segmentingtextinto multi-paragraphsubtopicpassages. Comput.Linguist. ,23(1):33Œ64,Mar.1997. [9]R.Higashinaka,K.Imamura,T.Meguro,C.Miyazaki, N.Kobayashi,H.Sugiyama,T.Hirano,T.Makino,and  Y.Matsuo.Towardsanopendomainconversationalsystem  fullybasedonnaturallanguageprocessing.In COLING'14,2014.[10]T.Hofmann.Unsupervisedlearningbyprobabilisticlatent semanticanalysis. MachineLearning ,42(1-2):177Œ196, 2001.[11]B.Hu,Z.Lu,H.Li,andQ.Chen.Convolutionalneural networkarchitecturesformatchingnaturallanguage  sentences.In NIPS,pages2042Œ2050,2014. [12]K.JärvelinandJ.Kekäläinen.Cumulatedgain-based evaluationofirtechniques. ACMTrans.Inf.Syst. ,20(4):422Œ446,Oct.2002. [13]E. T.Jaynes.Informationtheoryandstatisticalmechanics. Physicalreview ,106(4):620,1957. [14]Z.Ji,Z.Lu,andH.Li.Aninformationretrievalapproachto shorttextconversation. CoRR,abs/1408.6988,2014. [15]N.Kalchbrenner,E.Grefenstette,andP.Blunsom.A convolutionalneuralnetworkformodellingsentences. arXivpreprintarXiv:1404.2188 ,2014. [16]A.Leuski,R.Patel,D.Traum,andB.Kennedy.Building effectivequestionansweringcharacters.In SIGDIAL'09 ,pages18Œ27,2009. [17]A.LeuskiandD.Traum.Npceditor:Creatingvirtualhuman dialogueusinginformationretrievaltechniques. AIMagazine ,32(2):42Œ56. [18]J.Li,M.Galley,C.Brockett,J.Gao,andB.Dolan.A diversity-promotingobjectivefunctionforneural conversationmodels.2016. [19]X.Li,L.Mou,R.Yan,andM.Zhang.Stalematebreaker:A proactivecontent-introducingapproachtoautomatic  human-computerconversation.In Proceedingsofthe25th InternationalJointConferenceonIntelligence , IJCAI'16,pages2845Œ2851,2016. [20]Z.LuandH.Li.Adeeparchitectureformatchingshort texts.In NIPS'13,pages1367Œ1375,2013. [21]C.D.Manning,P.Raghavan,andH.Schütze. Introductionto informationretrieval ,volume1.2008. [22]T.Mikolov,K.Chen,G.Corrado,andJ.Dean.Ef estimationofwordrepresentationsinvectorspace.  arXiv:1301.3781,2013. [23]M.Nakano,N.Miyazaki,N.Yasuda,A.Sugiyama,J.-i. Hirasawa,K.Dohsaka,andK.Aikawa.Wit:Atoolkitfor  buildingrobustandreal-timespokendialoguesystems.In  SIGDIAL'00 ,pages150Œ159. [24]E.Nouri,R.Artstein,A.Leuski,andD.R.Traum. Augmentingconversationalcharacterswithgenerated  question-answerpairs.In AAAIFallSymposium:Question Generation ,2011. [25]H.Palangi,L.Deng,Y.Shen,J.Gao,X.He,J.Chen, X.Song,andR.Ward.Deepsentenceembeddingusingthe longshorttermmemorynetwork:Analysisandapplication toinformationretrieval. arXivpreprintarXiv:1502.06922 ,2015.[26]A.Ritter,C.Cherry,andW.B.Dolan.Data-drivenresponse generationinsocialmedia.In EMNLP'11 ,pages583Œ593, 2011.[27]T.Rocktäschel,E.Grefenstette,K.M.Hermann,T.Ko cisk˚y,andP.Blunsom.Reasoningaboutentailmentwithneural  attention.arXivpreprintarXiv:1509.06664 ,2015. [28]I.V.Serban,A.Sordoni,Y.Bengio,A.Courville,and J.Pineau.Buildingend-to-enddialoguesystemsusing  generativehierarchicalneuralnetworkmodels.2016. [29]L.Shang,Z.Lu,andH.Li.Neuralrespondingmachinefor short-textconversation.In ACL-IJCNLP ,pages1577Œ1586, 2015.[30]Y.Song,L.Mou,R.Yan,L.Yi,Z.Zhu,X.Hu,and M.Zhang.Dialoguesessionsegmentationby  embedding-enhancedtexttiling.In INTERSPEECH'16,2016. [31]A.Sordoni,M.Galley,M.Auli,C.Brockett,Y.Ji, M.Mitchell,J.-Y.Nie,J.Gao,andB.Dolan.Aneural  networkapproachtocontext-sensitivegenerationof conversationalresponses.In NAACL'15 ,pages196Œ205, 2015.[32]H.Sugiyama,T.Meguro,R.Higashinaka,andY.Minami. Open-domainutterancegenerationforconversational  dialoguesystemsusingweb-scaledependencystructures.In  Proc.SIGDIAL ,pages334Œ338,2013. [33]I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequence learningwithneuralnetworks.In NIPS,pages3104Œ3112, 2014.[34]M.A.Walker,R.Passonneau,andJ.E.Boland.Quantitative andqualitativeevaluationofdarpacommunicatorspoken  dialoguesystems.In ACL'01 ,pages515Œ522. [35]R.S.Wallace. TheanatomyofALICE .Springer,2009. [36]C.Wang,Y.Song,A.El-Kishky,D.Roth,M.Zhang,and J.Han.Incorporatingworldknowledgetodocument  clusteringviaheterogeneousinformationnetworks.In KDD,pages1215Œ1224,2015. [37]C.Wang,Y.Song,H.Li,M.Zhang,andJ.Han.Knowsim:A documentsimilaritymeasureonstructuredheterogeneous  informationnetworks.In ICDM,pages1015Œ1020,2015. [38]C.Wang,Y.Song,H.Li,M.Zhang,andJ.Han.Text withheterogeneousinformationnetwork  kernels.In AAAI,pages2130Œ2136,2016. [39]H.Wang,Z.Lu,H.Li,andE.Chen.Adatasetforresearch onshort-textconversations.In EMNLP'13,pages935Œ945, 2013.[40]J.Williams,A.Raux,D.Ramachandran,andA.Black.The dialogstatetrackingchallenge.In SIGDIAL'13 ,pages 404Œ413,2013. [41]R.Yan,L.Kong,C.Huang,X.Wan,X.Li,andY.Zhang. Timelinegenerationthroughevolutionarytrans-temporal  summarization.In ProceedingsoftheConferenceon EmpiricalMethodsinNaturalLanguageProcessing , EMNLP'11,pages433Œ443,2011. [42]R.Yan,M.Lapata,andX.Li.Tweetrecommendationwith graphco-ranking.In ACL'12 ,pages516Œ525. [43]R.Yan,Y.Song,andH.Wu.Learningtorespondwithdeep neuralnetworksforretrieval-basedhuman-computer  conversationsystem.In Proceedingsofthe39th InternationalACMSIGIRConferenceonResearchand  DevelopmentinInformationRetrieval ,SIGIR'16,pages  55Œ64,2016. [44]R.Yan,X.Wan,J.Otterbacher,L.Kong,X.Li,and Y.Zhang.Evolutionarytimelinesummarization:Abalanced  optimizationframeworkviaiterativesubstitution.In  Proceedingsofthe34thInternationalACMSIGIR  ConferenceonResearchandDevelopmentinInformation  Retrieval ,SIGIR'11,pages745Œ754,2011. [45]K.Yao,G.Zweig,andB.Peng.Attentionwithintentionfor aneuralnetworkconversationmodel. arXivpreprint arXiv:1510.08565,2015. [46]K.ZhaiandD.J.Williams.Discoveringlatentstructurein task-orienteddialogues.In ACL'14 ,pages36Œ46,2014. [47]B.Zhang,J.Su,D.Xiong,Y.Lu,H.Duan,andJ.Yao. Shallowconvolutionalneuralnetworkforimplicitdiscourse  relationrecognition.In EMNLP,pages2230Œ2235,2015. [48]X.Zhou,D.Dong,H.Wu,S.Zhao,R.Yan,D.Yu,X.Liu, andH.Tian.Multi-viewresponseselectionfor  human-computerconversation.In EMNLP'16,2016.  
Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) ,pages57Œ67, October25-29,2014,Doha,Qatar. c  2014AssociationforComputationalLinguistics 57 58 59 60 61 62 63 64 65 66 67  
PersistentRNNs:StashingRecurrentWeightsOn-Chip GregoryDiamos GREGDIAMOS @ BAIDU . COM ShubhoSengupta SSENGUPTA @ BAIDU . COM BryanCatanzaro BCATANZARO @ BAIDU . COM MikeChrzanowski MIKECHRZANOWSKI @ BAIDU . COM AdamCoates ADAMCOATES @ BAIDU . COM ErichElsen ERICHELSEN @ BAIDU . COM JesseEngel JENGEL @ BAIDU . COM AwniHannun AWNIHANNUN @ BAIDU . COM SanjeevSatheesh SANJEEVSATHEESTH @ BAIDU . COM BaiduSiliconValleyAILab,1195BordeauxDrive,Sunnyvale,CA94089,UNITEDSTATES Abstract Thispaperintroducesanewtechniqueformap- pingDeepRecurrentNeuralNetworks(RNN)ef- ontoGPUs.Weshowhowitispossi- bletoachievesubstantiallyhighercomputational throughputatlowmini-batchsizesthandirect implementationsofRNNsbasedonmatrixmul- tiplications.Thekeytoourapproachistheuseof persistentcomputationalkernelsthatexploitthe GPU'sinvertedmemoryhierarchytoreusenet- workweightsovermultipletimesteps.Ourinitial implementationsustains2.8TFLOP/satamini- batchsizeof4onanNVIDIATitanXGPU.This providesa16xreductioninactivationmemory footprint,enablesmodeltrainingwith12xmore parametersonthesamehardware,allowsusto stronglyscaleRNNtrainingto128GPUs,andal- lowsustoefexploreend-to-endspeech recognitionmodelswithover100layers. 1.Introduction RecurrentNeuralNetworks(RNNs)havebeenshownto bepowerfultoolsforsolvinggeneralsequencetosequence mappingproblemsindomainsrangingfromspeechrecog- nition(Sainathetal.,2015)tonaturallanguageprocess- ing(Gaoetal.,2015)(Sutskeveretal.,2014). Inthispaper,weexploretechniquesformappingRNNsto throughputoptimizedprocessorssuchasGPUs.Weusethe Multi-Bulk-Synchronous-Parallel(Valiant,2008)(MBSP) Proceedingsofthe 33 rd InternationalConferenceonMachine Learning ,NewYork,NY,USA,2016.JMLR:W&CPvolume 48.Copyright2016bytheauthor(s). modeltoanalyzethecomputation,communication,and synchronizationoperationsperformedbyaRNN.Wefo- cusonmappingstrategiesthatcarefullymanagedatamove- mentthroughtheprocessor'smemoryhierarchytobalance thesecosts.ThesechangesenableRNNimplementations onGPUsthatareveryefatsmallmini-batchsizes, evenonmini-batchsizesofjust4examples.Weexploit thisreductioninbatchsizetodecreasethememoryfoot- printofournetworksby16x,allowingustoexploredeeper networkswithoutexceedingGPUmemory. Weexploitthelargestsourceofon-chipmemoryonthe GPUŠthecollectiveregisterof6144hardwarethread contextsonaTitanXGPUŠtocachetheRNNparameters andreusethemovermultipletimestepsduringtraining.We attackthecostofinter-processorsynchronizationwithan optimizedassemblylevelbarrierimplementation,demon- stratingthatsuchbarriersimplementedinsoftwarecanre- ducelatencybyapproximately10xcomparedtorelyingon repeatedkernellaunches. Tomakeourresultsrelevantfordeployment,weonlycon- sidermodelswithahardconstraintof800msoffuturecon- text.Wethataccuracyimproveswithdeepermodels usingbatchnormalizationandskipconnections(Heetal., 2015;Srivastavaetal.,2015),reinforcingthetrendtowards deepermodelsinvisionapplications.Wepresentevidence thataccuracycontinuestoimprovewithincreaseddepth. 2.RelatedWork Thisworkismotivatedbyrecentadvancesinspeechrecog- nitionandnaturallanguageprocessingusingdeepRNNs. Itdrawsinsightfromrelatedworkonperformanceopti- mizationofDNNanddenselinearalgebralibraries,dis- tributedtrainingofDNNs,workongeneralpurposeGPU PersistentRNNs:StashingRecurrentWeightsOn-Chip performanceoptimization,andhighperformanceprocessor technologytrends. Incomputervisionrecognitiontasks,DeepNeuralNets (DNN)havedemonstratedsuperiorperformanceinobject detection(Krizhevskyetal.,2012;Simonyan&Zisser- man,2014;Szegedyetal.,2014;Heetal.,2015),local- ization(Heetal.,2015),andposeestimation(Toshev& Szegedy,2013).Innaturallanguageprocessing,DNNs haveenabledadvancementsinlanguagemod- eling(Bengioetal.,2003;Mikolovetal.,2010),senti- mentanalysis(Socheretal.,2013;Iyyeretal.,2015;Le &Zuidema,2015),syntacticparsing(Collobert&Weston, 2008;Socheretal.,2011;Chen&Manning,2014)and machinetranslation(Bahdanauetal.,2014;Devlinetal., 2014;Sutskeveretal.,2014). Inspeechrecognition,DNNshavebecomeainthe ASRpipeline(Mohamedetal.,2011;Hintonetal.,2012; Dahletal.,2011b;a;N.Jaitly&Vanhoucke,2012;Seide etal.,2011).CNNshavealsobeenfoundfor acousticmodels(Abdel-Hamidetal.,2012;Sainathetal., 2013).RNNs,typicallyLSTMs,arecommonlyusedin state-of-theartrecognizers(Gravesetal.,2013;H.Sak etal.,2014;Saketal.,2014)andworkwelltogetherwith convolutionallayersforfeatureextraction(Sainathetal., 2015).End-to-endspeechrecognitionwithacombina- tionofCNNsandRNNshasalsobeendeveloped(Amodei etal.,2015). AsDNNshavecontinuedtoincreaseapplication-levelper- formance,moreefforthasbeenappliedtohardwareand softwareoptimizationstargetingDNNs.Highperformance librariesfollowingasimilardesignphilosophyasBLAS haveemerged(Chetluretal.).Theselibrarieshavebegun toincludeoptimizedRNNroutines,althoughtheyhavenot yetusedpersistentGPUkernels.Additionalworkhasfo- cusedonimprovingalgorithmicefyofthefunda- mentaloperationsusedbyDNNs(Vasilacheetal.,2014; Lavin&Gray,2015).Finally,DNNsoftenrelyondense linearalgebraoperations,andfrompriorworkthat hasresultedinhighlytunedimplementationsformodern processors(Dongarraetal.,2014;Gray,2014). Anopen-sourceimplementationofthePersistentRNN GPUkernelshasbeenreleased(Diamosetal.). 3.RNNtoHardwareMappingStrategy Letasingleinputsequence x andcorrespondingout- putsequence y besampledfromatrainingset X = f ( x (1) ;y (1) ) ; ( x (2) ;y (2) ) ;::: g .Eachinputsequence, x ( i ) , isatime-seriesoflength T ( i ) whereeverytime-sliceisa vectoroffeatures(e.g.audiosamples), x ( i ) t ;t =0 ;:::;T ( i )  1 . Theforwardintime h l recurrentlayeractivationsare h l t = f ( h l  1 t ;h l t  1 ) (1) Thefunction f canbethestandardrecurrentoperation h l t = ˙ ( W l h l  1 t + U l h l t  1 + b l ) (2) where W l istheinput-hiddenweightmatrix, U l isthere- currentweightmatrixand b l isabiasterm. Implementationsofrecurrentneuralnetworkstypically separatethecomputationintotwostages. Inthestage( W l h l  1 t ),thecontributiontotheoutputof eachneuronforeachtimestepiscomputedusingtheneuron inputsforthattimestep.Likeafeedforwardnetwork,the stagerepresentstheinputweightsofalltheneuronsin thelayerasadensetwo-dimensionalmatrixandtheinputs tothelayerforeachtimestepasaone-dimensionaldense vector.Acommonoptimizationistounrollthetimedi- mensionandpackmultipleone-dimensionalinputvectors togetherintoasingletwo-dimensionalmatrix.Thisispos- siblebecausetheweightmatrixissharedoveralltimesteps. Inthesecondstage( U l h l t  1 ),theconnectionsbetweenthe outputsofthelayerneuronsonagiventimesteptothein- putsofthelayerneuronsonthenexttimesteparerepre- sentedbyatwo-dimensionalmatrix,referredtoasthere- currentweightmatrix.Inthiscase,eachtimestepmust beprocessedsequentiallybecausetheoutputsofthenext timestepdependontheoutputsofthecurrenttimestep, requiringthisoperationtobeperformedusingamatrix- vectorproduct,followedbyanapplicationoftheactiva- tionfunction.Thisisthemostcomputationallyexpensive step,sincethesequentialdependencebetweentimesteps requiresexplicitsynchronizationbetweenthemandthere- currentweightmatrixhastobereloadedfrommemoryon eachtimestep.Sowefocusourattentiononoptimizing thisstageusingtheMulti-Bulk-Synchrononus-Parallelma- chinemodel. 3.1.Multi-Bulk-Synchronous-ParallelMachineModel TheMulti-Bulk-Synchronous-Parallel(MBSP)abstract machinemodel(Valiant,2008)isaprocessorperformance modelthattakesintoaccountthephysicalrealitiesofmul- tipleprocessorcores,eachwithmemoryandcompu- tationalresources,aswellascommunicationandsynchro- nizationcosts.Thesecostsareafunctionofthephysicaldi- mensionsofanddistancesbetweenprocessorcores.They typicallyincreasewiththenumberofcores. TheMBSPmodelisahierarchicalmodel,withanarbitrary numberoflevels.Ateachlevel,itdescribesacollectionof processorcoresandtheassociatedon-chipmemory/cache capacitieswithfourparameters(computationalbandwidth, PersistentRNNs:StashingRecurrentWeightsOn-Chip Figure1:AdepictionoftheMBSPmachinemodelhierarchyforanNVIDIATitanXGPU.Eachlevelofthehierarchy containsmemorycapacity(leftsquare)withassociatedlatencyandbandwidth(arrows),aswellasasetofsublevelsor computeunits(rightsquare).Ateachlevelofthehierarchy,thePersistentRNNkernelsstoreallneuronparametersin memoryandmatchlatenciesbetweencommunication,synchronization,andcomputation. memorycapacity,memorybandwidth,andmemoryla- tency).Amemoryhierarchyisneededduetothephysical limitsontheamountofmemorythatcanbeaccessedina edamountoftimefromaprocessorcore. WegeneratetheMBSPmodelforthetargetprocessor orfamilyofprocessorsthatweplantoexecuteourRNN. Ourmodeldescribeseachlevelofaprocessor'smemory hierarchywithatuple ( p;b;c;m ) ,where p representsthe numberofsubmodulesorprocessorcores, b representsthe communicationbandwidth, c representsthesynchroniza- tioncostamongprocessorsatthislevel,and m represents thecacheormemorycapacityatthislevel.Anexampleof theseparametersfortheTitanXGPUisshowninFigure1. Startingwiththelowestlevelofhierarchyandworking uptothehighest,wedivideindividualneuronsintologi- calmodulesandarrangetheconnectionsbetweenmodules suchthatthefollowingconstraintsaremet: Ł theparametersrepresentingtheneuronsselectedfora givenprocessorcompletelyintothecacheormem- orycapacityforthislevelofthememoryhierarchy. Ł thecommunicationcostimpliedbyintra-modulecon- nectionsandinter-moduleconnectionsisapproxi- matelyequaltothecomputationalcostofevaluating themodule'sneurons. Ł thesynchronizationcostimpliedbyinter-modulecon- nectionsisapproximatelyequaltothecomputational costofevaluatingthemodule'sneurons. Thesechangesbalancethecomputational,communication, synchronization,andmemorycapacityrequirementsofthe RNNsuchthatnooneresourcebecomesabot- tleneck.ItdoessobyexploitingthereuseofRNNweights overmultipletimestepstoavoidrepeatedlyloadingweights fromDRAM,andtakingintoaccountthe highercostofsynchronizationandoff-chipmemoryac- cessesascomparedtomathoperations. 4.ImplementationonaTitanXGPU ThepeakpointthroughputofaTitanXis6.144 TFLOP/s.AstraightforwardimplementationofaRNNus- ingGEMMoperationsachieves0.099TFLOP/satalayer sizeof1152usingNervanaSystemsGEMMkernelsata mini-batchsizeof4.OurinitialPersistentRNNimple- mentationwiththesamelayerandmini-batchsizeachieves over2.8TFLOP/sresultingina30xspeedup. 4.1.Approach Incontrasttoapproachesbasedonmatrixmultiplication, wedividetherecurrentweightmatrixintoblocksofcon- tiguousrows,eachofwhichisprocessedbyasingleSM, asshowninFigure2.Eventhoughthisapproachrequires moreglobalmemorybandwidththanthemoretraditional approachofdividingintotiles,wechoseitbecauseitavoids theneedtoperformaninter-SMreductiontocomputethe activationsforagivenblockofrows. Ourimplementationloadstheweightmatrixintoreg- isters.TheneachSMloadsalloftheinputactivationsfrom theprevioustimestepfromglobalmemorytosharedmem- ory,computesthedotproductforeachrow,performsthe nonlinearity,writestheresultforthecurrenttimestep,and performsaglobalbarrierwithallotherSMs.Thelatency requiredtoperformtheloadoperationsisapproximately fourtimeshigherthanthetimerequiredtoperformthe mathoperationsforasingletimestep.Sowebreakthe PersistentRNNs:StashingRecurrentWeightsOn-Chip Figure2:Adepictionoftherecurrentweightmatrixtiling strategy.Eachofthe24SMsontheGPUprocessesa 48x1152blockrow,reading1152activationsforthecurrent timestep,andwriting48activationsforthenexttimestep. WithinanSM,eachof8warpsprocessesa6x1152block row,andallwarpsshareaccesstotheactivationsforthe currenttimestepinCUDAsharedmemory.Groupsof16 threadsarrangedinaninterleavedpatterncooperatetopro- cessa3x1152tile. computationintofourindependentstagesandusesoftware pipeliningtooverlaptheloadoperationwithmath,reduce andbarrieroperations.Weuseamini-batchsizeoffouror greatertokeepthepipelinefull. 4.2.StashingtheWeightsOn-Chip EachthreadintheTitanXGPUhasaccesstoapproximately 1KBofmemorythatcanbereadathighenoughband- widthtosaturatethepointdatapath.Outofthis, wededicate896bytestostorerecurrentweightsasshown inFigure2,andtherestforintermediatecomputations. Theseweightsareloadedonceatthestartofthekernel, andreusedovereachtimestep. 4.3.AFastGlobalBarrier SynchronizationbetweenGPUprocessorscoresistypi- callyachievedimplicitlybetweendependentkernelcalls inbothCUDAandOpenCLdevelopmentframeworks. However,thismechanismforsynchronizationbetween timestepsrequireslaunchinganewkernelthatforcesthe weightstobereloadedfromoff-chipmemory.Thiscauses thesynchronizationlatencyofdependentkernelstobeap- proximately6-10xlargerthanthetimespentperforming themathoperationsforasingletimestep,andthiscannot beoverlappedwithcomputation.Weaddressthisproblem withanoptimizedimplementationofaglobalbarrierthat canbecompletelyoverlappedwiththemathoperationsfor asingletimestep. 4.4.SavingMemorytoEnableDeeperNetworks Whentrainingourspeechrecognitionmodel,weencounter verylongutterancesthatareuptothirtysecondslong,cor- respondingto3,000timesteps.ForaRNNlayerwith1760 hiddenunits,andamini-batchsizeof64,thiscorresponds to1.3GBofstorageperlayer.Thisismuchmorethan the12.3MBrequiredtostorethelayerweights.Inprac- tice,withGPUswith12GBofDRAM,wethatthis limitsustonetworkswithabout9layers.Acommonso- lutiontothisproblemistousetruncatedback-propagation throughtime(Sutskever,2013)(BPTT).However,wehave observeda 20% relativeperformancedegradationofthe convergedmodelusingthisapproach,makingothertech- niquesthatreducememoryfootprint,suchasreducingthe mini-batchsize,moreattractive. 4.5.GRUsandLSTMs ThepersistentRNNapproachcanalsobeappliedtoGRU andLSTMrecurrentnetworkswithsimple Inbothcases,theupdateruleforasingletimestepcanbe factoredintoacomponentthatdependsonthelayerinput h l  1 t ,andacomponentthatdependsontheoutputfrom theprevioustimestep h l t  1 .Asbefore,thecompo- nentcanbecomputedinparalleloveralltimestepsusing matrixmultiplications,butthesecondcomponentrequires synchronizationbetweentimesteps.Similarlythesecond componentforbothGRUsandLSTMscanbefactoredinto asinglematrixmultiplicationbypackingtheindividualre- currentweightmatricestogetherintoasinglematrixthatis appliedtotheconcatenationofeachoftheactivationsig- nalsŠreset,update,activation,etc.Thismatrixcannow bedistributedthroughouton-chipmemory. 5.Experiments Thissectionfocusedmainlyonthecomputationalthrough- putofourPersistentRNNimplementation.Wealsoinclude experimentalresultsofverydeepforward-onlyrecurrent networksonalarge-scalespeechrecognitiontask,similar totheDeepSpeech1(Hannunetal.,2014)(DS1)andDeep Speech2(Amodeietal.,2015)(DS2)systems.Thesevery deepnetworkswouldnotbefeasibletorunefon oursystemswithoutPersistentRNNsduetomemorylimi- tations. 5.1.ComputationalThroughput Inthissectionwecomparethecomputationalefy ofPersistentRNN,againstanoptimizedRNNimplemen- tationbasedonmatrixmultiplicationroutinesfromthe NVIDIAandNervanaSystemsBLASlibraries.We thatourimplementationissubstantiallymoreefat smallmini-batchsizesthaneitherofthoseimplementa- tions. PersistentRNNs:StashingRecurrentWeightsOn-Chip Figure3:Throughputscalingwithmini-batchsize. Figure4:Throughputscalingwithtimesteps. 5.1.1.S ENSITIVITYTO M INI -B ATCH -S IZE Figure3comparespointthroughputforPersistent RNNagainsttwootherRNNimplementationsforsmall mini-batchsizes.Notethatafteramini-batchsizeof four,PersistentRNNconsistentlydeliverapproximately 2.8TFLOP/s,butmatrix-multiplybasedimplementations startoutmuchslower,andneedrelativelylargeminibatch sizestobecomecompetitive.Eventhen,wethatlayer sizesaround1152unitsaresomewhattoosmallformatrix multiplicationlibrariestobeefonlyachievingabout 1.5TFLOP/satamini-batchsizeof64.Performanceis generallymuchbetteratlayersizesof2560units,suggest- ingtheadvantagesofpersistentRNNimplementationswill growasmodelsbecomedeeperandthinner. 5.1.2.S ENSITIVITYTO T IMESTEPS Figure4showsthesensitivityofPersistentRNNtostartup overheadsassociatedwithlaunchingthekernelsandload- ingtherecurrentweightmatrixontheiteration.We Figure5:Throughputscalingofthe48RNN,61totallayer RNNwithaedalgorithmicmini-batchof512. thatmostoftheperformanceisachievedafterapprox- imately30timesteps,whichissubstantiallysmallerthan theaverageutterancelengthinourtrainingsetofabout350 timesteps.However,itdoessuggestthatrealtimeimple- mentationsthatrelyonprocessingasmallnumberofut- terancesatatimeshouldstillbufferupabout30timesteps (600msofaudioataframesizeof10ms). 5.1.3.S TRONG S CALING Figure5showsscalabilityofthe48RNNlayernetwork from1to128GPUs.Ourclusteriscomposedofnodes with8GPUsand2CPUs.GPUsareconnectedlocallyvia PCIev3usingtwo4-widefullbisectionbandwidthPCIe switches,whichareinterconnectedusingtheQPIbusbe- tweenCPUs.Nodesareinterconnectedby12x QDRlinkstoafullbisectionbandwidthrouter.Weuse MPIasthecommunicationlayer.Weusesynchronous SGDasthetrainingalgorithm,withdataparallelismtosup- portmultipleGPUs.Thereisnoneedtouseatechnique thatreducesinterconnectbandwidthsuchasasynchronous SGDbecauseoursystemisfastenoughtocompletelyover- laptheall-reduceoperationinSGDwiththebackpropaga- tionevaluation. Thealgorithmicmini-batchsizeisedat512forallex- periments(i.e.themini-batchperGPUis64whenrun on16GPUsand4whenrunon128GPUs).TheGEMM basedRNNimplementationscaleswellupto16GPUs,but doessubstantiallyworseasmoreGPUsareadded.The PersistentRNNimplementationscalesnearlylinearly,and achieves250TFLOP/son128GPUs(about30%ofpeak). Notethatthisnumberisthesustainedthroughputofthe entiresystem,notjusttheRNNkernels. PersistentRNNs:StashingRecurrentWeightsOn-Chip Figure6:ThetheoreticalpointthroughputforvariousRNNlayersizes,withdifferentplotsforvariationsofthe TitanXGPUmicroarchitecture.Eachpointrepresentsadifferenttilingstrategy.Thesolidlineindicatesthemaximum throughputofthatGPU,anypointabovethatlineiscomputelimited,andanypointbelowitislimitedbylatency,memory bandwidth,orloadimbalance. 5.1.4.P ROCESSOR D ESIGN S PACE Ourexperimentusesananalyticalperformancemodel topredicttheperformanceofpersistentRNNimplemen- tationsofvariouslayersizesonversionsofthe TitanXGPUmicroarchitecture,offeringaviewintothe performancelandscapeoffutureGPUsfordifferentlayer sizes.WeexploredoublingtheregistersizeperSM, doublingthenumberofSMs,performingallofthecompu- tationsin16-bitpoint,doublingtheDRAMband- width,andhalvingthememorylatency. Generally,wethatfutureGPUswillprobablyen- ablebiggerrecurrentlayersizes,andrunthemathigher throughput.Oftheoptionsthatarestraightforwardtoim- plementthroughprocessscalingandminorarchitecture changes,movingto16-bitpointoperationspro- videsthebiggestimprovement,followedbymorememory perSM,andincreasingthenumberofSMs.Reduc- ingsynchronizationlatencyisexpectedtobemuchmore difbutitisinterestingtonotethatsubstantiallyreduc- ingthememoryandsynchronizationlatencywouldenable theefimplementationofalargerrangeoflayersizes (smallerlayersizesinparticular).Itisalsointerestingto notethatsubstantiallyincreasingDRAMbandwidthisun- likelytoimpacttheefyofpersistentRNNkernels, althoughothercomponentsofdeepnetworksareknownto bememoryboundandwouldprobably 5.2.SpeechRecognitionTask Thissectionexploresthedesignspacearoundverydeep stacksofforward-onlyRNNmodelsusingalargevocabu- laryend-to-endEnglishspeechrecognitiontask. Figure7showsthearchitectureoftheDS2whichweuse asthebaselinefortheseexperiments:arecurrentneu- ralnetwork(RNN)trainedtoingestspeechspectrograms andgeneratetexttranscriptionsusingtheCTClossfunc- tion(Gravesetal.,2006).Weevaluatevariousarchitec- turesbyvaryingthenumberofrecurrentlayersandthe numberandspanofskipconnectionsbetweenthem.We useadatasetof500hoursofaudiointheseexperimentsto quicklyperformmodeldesignspaceexploration.Were- portWordErrorRate(WER)onanEnglishspeakerheld outdevelopmentsetwhichisaninternaldatasetcontain- ing2048utterancesofprimarilyreadspeech.Weintegrate alanguagemodelinabeamsearchdecodingstepasde- scribedin(Amodeietal.,2015). Althoughthenetworkarchitecturesexploredhereareal- mostidenticaltotheDS2network,thereisone difference.WeuseunidirectionalRNNlayersandrowcon- PersistentRNNs:StashingRecurrentWeightsOn-Chip ArchitectureDev(WER) 48RNN,61total,noskip100.0 48RNN,61total,skip138.77 48RNN,61total,skip233.28 48RNN,61total,skip330.32 48RNN,61total,skip429.40 48RNN,61total,skip529.82 48RNN,61total,skip630.04 48RNN,61total,skip729.87 48RNN,61total,skip827.44 Table1:WERformodelswithskipconnectionsaddedbe- tweeneveryNRNNlayers,eachwith1152units. volutions(Amodeietal.,2015)withaedcontextsizeof 800msratherthanbidirectionalRNNlayerstomakesure thatthenetworkscanbereadilydeployedinonlinespeech recognitiontasks. 5.3.Methodology Allmodelsaretrainedfor20epochsontheEnglishdataset. WeusestochasticgradientdescentwithNesterovmomen- tum(Sutskeveretal.,2013)alongwithaminibatchfrom therangeof [64 ; 512] utterances.Ifthenormofthegradi- entexceedsthethresholdof400,itisrescaledto400(Pas- canuetal.,2012).Themodelthatperformsthebeston aheld-outdevelopmentsetduringtrainingischosenfor evaluation.Thelearningrateischosenfromtherange [1  10  5 ; 6  10  4 ] toyieldthefastestconvergenceand annealedbyaconstantfactorof1.2aftereachepoch.We useamomentumof0.99forallmodels. 5.3.1.S ENSITIVITYTO R ESIDUAL C ONNECTIONS Table1showstheimpactofresidualskipconnectionson verydeepRNNarchitectureswithoverlayers.We thatskipconnectionsareessentialfortrainingthese models,evenwhenbatchnormalizationisenabled.Mod- elswithoutskipconnectionsfailtoconverge.Forthese networks,wethatskippingthreeorfourRNNlayersis substantiallybetterthanskippingasinglelayer,andmod- eratelybetterthananyother(exceptforthe outlierof8,whichwecannotexplain).Thissuggeststhat residualskipconnectionsenableeffectiveoptimizationof verydeepstacksofRNNlayers. 5.3.2.S ENSITIVITYTO D EPTH Table2showstheimpactofdepthonresidualRNNmodels withaconstantnumberofparameters.Wethatdepth helpsuptoapoint,about50layers,afterwhichperfor- mancedegrades.Wehypothesizethatthismaybedueto thethinningofindividuallayersinverydeepnetworks. Figure7:Architectureofthespeechrecognitionsystem usedinthispaper.Allnetworksuse2layersof2Din- variantconvolutions,andonefullyconnectedlayer.The basicmoduleisastackoffoursimplerecurrentlayersfol- lowedbyasinglerowconvolutionlayer,allofwhichmay bebypassedbyasingleresidualskipconnection.Deeper networksareconstructedbyaddingmultipleofthesebasic modules.Allnetworksuse10layersofrowconvolutions. Alllayersexceptrowconvolutionsusebatchnormaliza- tion. 5.3.3.S ENSITIVITYTO P ARAMETERS Table3showshowtrainingisaffectedbyincreasingthe numberofparametersinthenetwork.Wethatincreas- ingthenumberofparametersbystackingadditionallayers generallyimprovesperformanceofthenetworkwithcon- tinuedgainsouttothelargestnetworkswithapproximately 100layersand200millionparameters. 5.3.4.S ENSITIVITYTO M INI -B ATCH S IZE Table8showshowtrainingisaffectedbyincreasingthe mini-batchsize.Fortheseexperiments,weperforma searchoverthelearningrate,momentum,andannealing rateforeachbatchsizetoagoodvalue.Wethat thereisverylittledifferenceintheamountofcomputa- tionalwork(inthiscase,thenumberofepochs)neededto convergeforbatchsizesbelowathresholdof512to1024, butconvergenceismuchslowerbeyondit.Thissuggests thatevenifGPUmemorycapacitieswereincreased,run- ningefwithasmallerbatchsizeperGPUwould enabledata-parallelapproachestoscaletomoreGPUs. 5.4.Discussion Ingeneral,theseresultsreinforcethetrendofdeepermod- elsbeingmorediftotrain,butdeliveringbetterper- formanceiftheycanbetrainedsuccessfully.Weboth batchnormalizationandresidualskipconnectionstobeef- PersistentRNNs:StashingRecurrentWeightsOn-Chip Arch#ActsDev(WER) 8RNN163233.61 24RNN92831.99 40RNN70431.64 56RNN60830.67 72RNN54433.35 88RNN48042.67 Table2:WERonatraininganddevelopmentsetforvari- ousdepthsofRNN.Thenumberofparametersiskeptap- proximatelyconstantasthedepthincreases,thusthenum- berofactivationsperlayerdecreases.Forthearchitecture ﬁMRNNﬂimpliesMuni-directionalRNNlayers. Architecture#ParamsDev(WER) 8RNN,21total22M35.69 24RNN,37total65M31.32 40RNN,53total107M28.90 56RNN,69total149M28.12 72RNN,85total192M27.84 88RNN,101total234M27.23 Table3:WERonatraininganddevelopmentsetforvari- ousdepthsofRNN.Thenumberofparametersperlayeris keptconstantasthedepthincreasesinthisexperiment,thus thenumberofparametersincreasesasthedepthincreases. ForthearchitectureﬁMRNN,NtotalﬂimpliesMconsec- utiveuni-directionalRNNlayerswithNtotallayersinthe network. fectivetechniquesthatallowtrainingdeeperRNNmod- elsforspeechrecognition,reinforcingtheimportanceof thesetechniquesthathasbeenpreviouslydemonstratedfor CNNsappliedtovisionapplications. Fromacomputationalperspective,itseemsclearthatthe developmentoffasterprocessorswithmorememoryca- pacitywilllikelyenableevenlargermodelstobetrainedon biggerdatasets,unlockingadditionalaccuracy.Thiswork hasshownthatsomemodelarchitecturesareconstrained notonlybyhardwareperformance,butalsobythestrat- egyusedtomapthemtohardware.MappingRNNsto GPUsusingmatrixmultiplicationisefforshallow networkswithlargelayers,butpersistentRNNkernelsare muchmoreefforverydeepnetworkswithrelatively narrowlayers. Inadditiontoworkonmodelarchitectureexploration,and workonimprovingtheperformanceofgeneralpurpose processors,itmayalsobefruitfultoconsiderstrategiesfor mappingcurrentlyinefmodelarchitecturesontoex- istinghardwareplatforms.WehavefoundtheMBSPab- stractmachinemodeltogetherwiththeguidelinesinSec- Figure8:Developmentsetcostafter10epochsforvarious mini-batchsizes.Thearchitectureisa61-layermodelwith 2layersof2D-invariantconvolution,48RNNlayers(with 1152activationsandskipconnectionsevery4layers),10 rowconvolutionlayers,andonefullyconnectedfeedfor- wardlayer.Notethatthenumberofepochsneededtoreach agivencostisapproximatelyconstantuntilthemini-batch becomeslargerthan512-1024,atwhichpointitgrowscon- siderably. tion3.1tobeausefultoolforquicklydecidingwhether ornotahypotheticalmodelarchitecturecanbeef mappedtohardware. 6.Conclusion Wedemonstrateatechniqueforachievinghighperfor- manceforRNNevaluationatverylowbatchsizesonan NVIDIATitanXGPU,achieving2.8TFLOP/satamini- batchsizeof4.Thisprovidesa16xreductioninactivation memoryfootprint,andallowsustotrainmodelswithover 100layersonthesamehardwarewhichisaboutanorder ofmagnitudedeeperthanwithoutthistechnique.Wefocus ourevaluationonunidirectionalRNNswithatmost800ms offuturecontext,anddemonstratethataccuracycontinues toscalewithincreaseddepth.Weexpectthesegainsto directlyenablethetrainingofdeeperRNNnetworkson muchlargerdatasetsthanwouldbepossiblewithoutthis technique. References Abdel-Hamid,Ossama,Mohamed,Abdel-rahman,Jang, Hui,andPenn,Gerald.Applyingconvolutionalneural networksconceptstohybridnn-hmmmodelforspeech recognition.In ICASSP ,2012. Amodei,Dario,Anubhai,Rishita,Battenberg,Eric,Case, PersistentRNNs:StashingRecurrentWeightsOn-Chip Carl,Casper,Jared,Catanzaro,Bryan,Chen,Jingdong, Chrzanowski,Mike,Coates,Adam,Diamos,Greg,etal. Deepspeech2:End-to-endspeechrecognitioninenglish andmandarin. arXivpreprintarXiv:1512.02595 ,2015. Bahdanau,Dzmitry,Cho,Kyunghyun,andBengio, Yoshua.Neuralmachinetranslationbyjointlylearning toalignandtranslate. CoRR ,abs/1409.0473,2014.URL http://arxiv.org/abs/1409.0473 . Bengio,Yoshua,Ducharme,RÃl'jean,Vincent,Pascal, andJauvin,Christian.Aneuralprobabilisticlanguage model. JOURNALOFMACHINELEARNINGRE- SEARCH ,3:1137Œ1155,2003. Chen,DanqiandManning,ChristopherD.Afastand accuratedependencyparserusingneuralnetworks.In Proceedingsofthe2014ConferenceonEmpiricalMeth- odsinNaturalLanguageProcessing,EMNLP2014,Oc- tober25-29,2014,Doha,Qatar,AmeetingofSIG- DAT,aSpecialInterestGroupoftheACL ,pp.740Œ750, 2014.URL http://aclweb.org/anthology/D/D14/ D14-1082.pdf . Chetlur,Sharan,Woolley,Cliff,Vandermersch,Philippe, Cohen,Jonathan,Tran,John,Catanzaro,Bryan,and Shelhamer,Evan.cuDNN:Efprimitivesfordeep learning.URL http://arxiv.org/abs/1410.0759 . Collobert,R.andWeston,J.Aarchitecturefor naturallanguageprocessing:Deepneuralnetworkswith multitasklearning.In InternationalConferenceonMa- chineLearning,ICML ,2008. Dahl,G.E.,Yu,D.,andDeng,L.Largevocabularycontin- uousspeechrecognitionwithcontext-dependentDBN- HMMs.In Proc.ICASSP ,2011a. Dahl,G.E.,Yu,D.,Deng,L.,andAcero,A.Context- dependentpre-traineddeepneuralnetworksforlargevo- cabularyspeechrecognition. IEEETransactionsonAu- dio,Speech,andLanguageProcessing ,2011b. Devlin,Jacob,Zbib,Rabih,Huang,Zhongqiang,Lamar, Thomas,Schwartz,Richard,andMakhoul,John.Fast androbustneuralnetworkjointmodelsforstatisticalma- chinetranslation.In Proceedingsofthe52ndAnnual MeetingoftheAssociationforComputationalLinguis- tics ,volume1,pp.1370Œ1380,2014. Diamos,Gregory,Sengupta,Shubho,Catanzaro,Bryan, Chrzanowski,Mike,Coates,Adam,Elsen,Erich,Engel, Jesse,Hannun,Awni,andSatheesh,Sanjeev.Persis- tentRNNs. https://github.com/baidu-research/ persistent-rnn .Accessed:2016-05-23. Dongarra,Jack,Gates,Mark,Haidar,Azzam,Kurzak, Jakub,Luszczek,Piotr,Tomov,Stanimire,andYa- mazaki,Ichitaro.Acceleratingnumericaldenselinear algebracalculationswithgpus. NumericalComputations withGPUs ,pp.1Œ26,2014. Gao,Haoyuan,Mao,Junhua,Zhou,Jie,Huang,Zhiheng, Wang,Lei,andXu,Wei.Areyoutalkingtoama- chine?datasetandmethodsformultilingualimageques- tionanswering. CoRR ,abs/1505.05612,2015.URL http://arxiv.org/abs/1505.05612 . Graves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J. ConnectionisttemporalLabellingunseg- mentedsequencedatawithrecurrentneuralnetworks.In ICML ,pp.369Œ376.ACM,2006. Graves,Alex,Mohamed,Abdel-rahman,andHinton,Ge- offrey.Speechrecognitionwithdeeprecurrentneural networks.In ICASSP ,2013. Gray,Scott.Assemblerfornvidiamaxwellarchitecture, 2014.URL https://github.com/NervanaSystems/ maxas . H.Sak,Hasim,Senior,Andrew,andBeaufays,Francoise. Longshort-termmemoryrecurrentneuralnetworkar- chitecturesforlargescaleacousticmodeling.In Inter- speech ,2014. Hannun,Awni,Case,Carl,Casper,Jared,Catanzaro, Bryan,Diamos,Greg,Elsen,Erich,Prenger,Ryan, Satheesh,Sanjeev,Sengupta,Shubho,Coates,Adam, andNg,AndrewY.Deepspeech:Scalingup end-to-endspeechrecognition.1412.5567,2014. http://arxiv.org/abs/1412.5567. He,K.,Zhang,X.,Ren,S.,andSun,J.DeepResidual LearningforImageRecognition. ArXive-prints ,Decem- ber2015. Hinton,G.E.,Deng,L.,Yu,D.,Dahl,G.E.,Mohamed, A.,Jaitly,N.,Senior,A.,Vanhoucke,V.,Nguyen,P., Sainath,T.,andKingsbury,B.Deepneuralnetworksfor acousticmodelinginspeechrecognition. IEEESignal ProcessingMagazine ,29(November):82Œ97,2012. Iyyer,Mohit,Manjunatha,Varun,Boyd-Graber,Jordan, andIII,HalDaume.Deepunorderedcompositionri- valssyntacticmethodsfortextIn As- sociationforComputationalLinguistics ,2015.URL docs/2015_acl_dan.pdf . Krizhevsky,Alex,Sutskever,Ilya,andHinton,Geoff.Im- agenetwithdeepconvolutionalneuralnet- works.In AdvancesinNeuralInformationProcessing Systems25 ,pp.1106Œ1114,2012. PersistentRNNs:StashingRecurrentWeightsOn-Chip Lavin,AndrewandGray,Scott.Fastalgorithmsforconvo- lutionalneuralnetworks. CoRR ,abs/1509.09308,2015. URL http://arxiv.org/abs/1509.09308 . Le,PhongandZuidema,Willem.Compositionaldistribu- tionalsemanticswithlongshorttermmemory. arXiv preprintarXiv:1503.02510 ,2015. Mikolov,Tomas,Martin,Burget,Lukas,Cer- nock ˚ y,Jan,andKhudanpur,Sanjeev.Recurrentneu- ralnetworkbasedlanguagemodel.In INTERSPEECH 2010,11thAnnualConferenceoftheInternational SpeechCommunicationAssociation,Makuhari,Chiba, Japan,September26-30,2010 ,pp.1045Œ1048,2010. Mohamed,A.,Dahl,G.E.,andHinton,G.E.Acousticmod- elingusingdeepbeliefnetworks. IEEETransactionson Audio,Speech,andLanguageProcessing ,(99),2011. URL http://ieeexplore.ieee.org/xpls/abs_all. jsp?arnumber=5704567 . N.Jaitly,P.Nguyen,A.SeniorandVanhoucke,V.Applica- tionofpretraineddeepneuralnetworkstolargevocabu- laryspeechrecognition.In Interspeech ,2012. Pascanu,Razvan,Mikolov,Tomas,andBengio,Yoshua. Onthedifoftrainingrecurrentneuralnetworks. abs/1211.5063,2012.http://arxiv.org/abs/1211.5063. Sainath,Tara,Vinyals,Oriol,Senior,Andrew,andSak, Hasim.Convolutional,longshort-termmemory,fully connecteddeepneuralnetworks.In ICASSP ,2015. Sainath,TaraN.,rahmanMohamed,Abdel,Kingsbury, Brian,andRamabhadran,Bhuvana.Deepconvolutional neuralnetworksforLVCSR.In ICASSP ,2013. Sak,Hasim,Vinyals,Oriol,Heigold,Georg,Senior,An- drew,McDermott,Erik,Monga,Rajat,andMao,Mark. Sequencediscriminativedistributedtrainingoflong shorttermmemoryrecurrentneuralnetworks.In Inter- speech ,2014. Seide,Frank,Li,Gang,andYu,Dong.Conversational speechtranscriptionusingcontext-dependentdeepneu- ralnetworks.In Interspeech ,pp.437Œ440,2011. Simonyan,KarenandZisserman,Andrew.Verydeepcon- volutionalnetworksforlarge-scaleimagerecognition. CoRR ,abs/1409.1556,2014.URL http://arxiv.org/ abs/1409.1556 . Socher,Richard,Lin,CliffC.,Ng,AndrewY.,andMan- ning,ChristopherD.ParsingNaturalScenesandNatural LanguagewithRecursiveNeuralNetworks.In Proceed- ingsofthe26thInternationalConferenceonMachine Learning(ICML) ,2011. Socher,Richard,Perelygin,Alex,Wu,JeanY,Chuang,Ja- son,Manning,ChristopherD,Ng,AndrewY,andPotts, ChristopherPotts.Recursivedeepmodelsforsemantic compositionalityoverasentimenttreebank.In EMNLP , 2013. Srivastava,RupeshKumar,Greff,Klaus,andSchmidhu- ber,Jürgen.Highwaynetworks. CoRR ,abs/1505.00387, 2015.URL http://arxiv.org/abs/1505.00387 . Sutskever,I.,Martens,J.,Dahl,G.,andHinton,G.On theimportanceofmomentumandinitializationindeep learning.In 30thInternationalConferenceonMachine Learning ,2013. Sutskever,Ilya.Trainingrecurrentneuralnetworks,2013. Sutskever,Ilya,Vinyals,Oriol,andLe,QuocV.Sequence tosequencelearningwithneuralnetworks.In Proc. NIPS ,Montreal,CA,2014.URL http://arxiv.org/ abs/1409.3215 . Szegedy,Christian,Liu,Wei,Jia,Yangqing,Sermanet, Pierre,Reed,Scott,Anguelov,Dragomir,Erhan,Du- mitru,Vanhoucke,Vincent,andRabinovich,Andrew. Goingdeeperwithconvolutions. CoRR ,abs/1409.4842, 2014.URL http://arxiv.org/abs/1409.4842 . Toshev,AlexanderandSzegedy,Christian.Deeppose:Hu- manposeestimationviadeepneuralnetworks. CoRR , abs/1312.4659,2013.URL http://arxiv.org/abs/ 1312.4659 . Valiant,LeslieG.Abridgingmodelformulti-corecomput- ing.In Proceedingsofthe16thAnnualEuropeanSympo- siumonAlgorithms ,ESA'08,pp.13Œ28,Berlin,Heidel- berg,2008.Springer-Verlag.ISBN978-3-540-87743- 1.doi:10.1007/978-3-540-87744-8_2.URL http: //dx.doi.org/10.1007/978-3-540-87744-8_2 . Vasilache,Nicolas,Johnson,Jeff,Mathieu,Michaël,Chin- tala,Soumith,Piantino,Serkan,andLeCun,Yann.Fast convolutionalnetswithfbfft:AGPUperformanceeval- uation. CoRR ,abs/1412.7580,2014.URL http:// arxiv.org/abs/1412.7580 .  
DetailedSurfaceGeometryandAlbedoRecoveryfromRGB-DVideo UnderNaturalIllumination XinxinZuo 1 ; 2 ; *SenWang 2 ; 1 ;  JiangbinZheng 2 RuigangYang 1 ; 3 1 UniversityofKentucky 2 NorthwesternPolytechnicalUniversity 3 BaiduLexington,KY,USXi'an,P.R.ChinaBeijing,P.R.China f xinxin.zuo,sen.wangg @uky.eduzhengjb@nwpu.edu.cnryang@cs.uky.edu AbstractInthispaperwepresentanovelapproachfordepthmap enhancementfromanRGB-Dvideosequence.Thebasic  ideaistoexploitthephotometricinformationinthecolor  sequence.Insteadofmakinganyassumptionaboutsurface  albedoorcontrolledobjectmotionandlighting,weusethe  lightingvariationsintroducedbycasualobjectmovement.  Weareeffectivelycalculatingphotometricstereofroma  movingobjectundernaturalilluminations.Thekeytech-  nicalchallengeistoestablishcorrespondencesovertheen-  tireimageset.Wethereforedevelopalightinginsensitive  robustpixelmatchingtechniquethatout-performsoptical  methodinpresenceoflightingvariations.Inaddition  wepresentanexpectation-maximizationframeworktore-  coverthesurfacenormalandalbedosimultaneously,with-  outanyregularizationterm.Wehavevalidatedourmethod  onbothsyntheticandrealdatasetstoshowitssuperiorper-  formanceonbothsurfacedetailsrecoveryandintrinsicde-  composition. 1.Introduction Theavailabilityofaffordabledepthsensorshassparkeda revolutioninmanyareasofcomputervision,suchashuman  computerinteractions,robotics,andvideoanalysis.Among  these3Dmodelinghasprobablyreceivedthemost  fromthisadvancementofsensors.Nevertheless,thecurrent  generationofdepthsensorsstillsuffersfromlimitedreso-  lutionandaccuracy.Asaresult,structuraldetails  ofanobjectcannotberecovered. Manyresearchershaveexploitedhighqualitycolorim- ageasguidancetoenhancethedepthmap,suchasde-  noiseandup-sampling[ 28,11,40,6,25].Amongthese approaches,thefusionofdepthmapswithshadinginfor-  mationcontainedincolorimageshasshownitseffective-  nessinrecoveringsurfacegeometrydetails.Mostofthese *Thetwoauthorscontributedequallytothiswork methodsimplementshadingonsingleRGB-  DframeonthebasisofShape-from-Shading(SfS)tech-  niques[ 41,27,37].However,theinherentambiguityfor SfSstillexistsandsmoothregularizationtermisusually  incorporated.Anotherproblemisthatanalbedoimageis  neededtopredicttheappearance,whichisalsounknown.  Typicallyaconstantalbedoassumptionismade.Inorderto  handlevaryingalbedos,previousshading-based  methodsoftenapproachedthischicken-and-eggproblem  byassumingpriorassumptionsorbyenforcingparticular  albedoregularizers.However,theseregularizersareheuris-  ticandmaynotworkallthetime.Inthispaperwepropose  totakeadvantageofanRGB-Dsequencetouniquelyre-  solvethenormalmapsandalbedoimages. Weonlymaketwobasicassumptionsabouttheobject, itisprimarilydiffuseandseconditisacquiredby  casualmovementoftheobject(notthecamera).Inthis  way,theobject'smovementinducesilluminationchanges  intheimagesequence,whichiscriticaltoresolvethesur-  facenormalandalbedowithoutanyambiguity.Itresem-  blesthephotometricstereo.Butinsteadofcontrollingthe  lightwhenimagingthestaticobject,wemovetheobject  undergeneralnaturallighting.Thiskindofcuehasbeen  exploitedinmulti-viewphotometricstereo[ 10]andshape fromvideo[ 22,32,42].However,theyhavetheenviron- mentallightingconstrainedtobecalibrateddirectionallight  andtheobjectisexperiencingturntablemotionorthemo-  tionisalreadycalibrated.Onthecontraryourapproach  worksundernaturallightingwiththeobjectunderarbitrary  motion,whichmakesourmethodmorewidelyusedinev-  erydayenvironment. GiventhecapturedRGB-Dsequence,wetrytoalign theRGB-Dsequenceandthecorrespondencesamong  theimagesusinganovelrobustmatchingtechnique.Then  theenvironmentallightingisestimatedusingtheintensity  ratiosofthealignedsequence,whicheffectivelyfactorsout  theimpactofvaryingalbedo.Finally,weformulatean  Expectation-Maximizationframeworkinwhichthesurface  normalanditsalbedocanbecalculatedrobustly,inthepres- 13133 enceofsomenon-Lambertianorcastshadow.A  detailedsurfacemeshisobtainedafterintegrationoftheini-  tialdepthmapwithestimatednormalmap. Themaincontributionisthatweutilizethedynamicpho- tometricinformationalongthesequencetorecoverthesur-  facedetailsbeyondtheresolutionofcurrentdepthsensors.  Comparedtopreviousdepthenhancementschemesthatuse  thecolorinformation,ourmethod,tothebestofourknowl-  edge,istheleastrestrictive.Itallowsarbitrarysurface  albedo,doesnotrequirecontrolledorcalibratedlighting  orturntablecapture.Toachievethese,wemaketwotech-  nicalcontributions.Theisanovelimageregistration  schemethatisrobusttolightingvariationsandthesecond  isanEMoptimizationschemetoproduceper-pixelnormal  andalbedomapundergenerallighting.  2.RelatedWork Inthissection,wewillreviewthepreviousworksintwo relatedtopics:surfacegeometryordepthenhancementwith  shadinginformation,andintrinsicimagedecomposition.  2.1.Shapefromshadingandphotometricstereofor surfaceordepthenhancement TheShape-from-shading(SfS)problemhaslongbeen studiedsincethepioneeringworkbyHorn[ 16].Itaimsto estimatesurfacenormal(andthenindirectlysurfaceshape)  fromasingleimage.Therearevariousregularizationterms  orpriorassumptions[ 1,2]thathavebeenenforcedtodeal withtheinherentlyill-posedproblem. RecentmethodshaveshownthatSfScanbeusedto thenoisydepthmapcapturedfromRGB-Dcam-  eras[ 14,41].TheinherentambiguityofSfSisnotre- solvedexactly,butwiththeinitialdepthclosetothereal  surface,Wu[ 37]andRoy[ 27]haveachievedgoodperfor- manceinrecoveringsurfacedetails.Varyingalbedoposes  anotherchallengeasitneedstobefactoredoutbeforelight-  ingestimationandshadingSome[ 14]assumes uniformorpiecewiseconstantalbedo.Yu[ 41]dealswith thisbyclusteringaedsetofdiscretealbedosbeforeop-  timizinggeometry.Abetter,yetmorecomplexstrategy,is  tosimultaneouslyoptimizeforunknownalbedosand  geometry[ 18].Therearealsopreviousworksthatadopt theshadingconstraintstoimprovethecoarse3Dshapere-  constructedusingmulti-viewstereo[ 36].Themajordif- ferencebetweenourmethodandthesepriorworksisthat  wefullyexploitlightingeffectscontainedinavideose-  quencetouniquelyandsimultaneouslydeterminethesur-  facealbedoandnormalinapixel-wisemanner.Therefore  weareabletodealwitharbitraryalbedo. Photometricstereomethodshavebeendevelopedto computethesurfacenormalusingmultipleimagesofa  scenetakenunderdifferentorcontrolledillumination[ 38,39].UnlikeSfS,photometricstereoisa problem,whichisthenincorporatedtoenhancetheraw  depthmap[ 15,43]capturedfromthecurrentdepthsensor. Wu[ 35]andZhou[ 46]havealsoshownitseffectivenessun- dermulti-viewsetuporforimagesacquiredfromtheinter-  net[ 31].Besides,somehaveusedtheIRimagesinsteadof colorimagesfornormalestimation[ 34].RecentlyChatter- jee[ 8]exploitedtheIRimageandproposedafactorization methodtohandleobjectswithvaryingalbedo.Intheseap-  proaches,theobjectsarekepttobestaticorcapturedalmost  underthesameviewpointwithdifferentlightingconditions.  Differentfromthemweallowtheobjecttorotatearbitrarily  underuncontrolledenvironment.  2.2.Intrinsicdecomposition Therearesomeworksaboutintrinsicdecompositionthat focusonseparatingthealbedomapfromshadingimage,to  whichourworkisalsorelated.Theproblemofintrinsicim-  agedecomposition,introducedbyBarrow[ 4],istosep- arateandilluminationfromasingleimage.Itis  againanill-posedproblemsincetherearetwounknownsfor  everyobservation.Additionalconstraintsmustbeadopted  tomaketheproblemsolvable.TheRetinextheory[ 23]is widelyusedforthispurpose.Itassumesthatshadingvaria-  tionsaremostlylowfrequencywhilealbedochangesare  mostlyhigh-frequency.Basedonthisassumption,many  approacheshavebeenproposed.Forexample,Tappenet  al.[ 33]trainafromandshadingdata sets.Globalpriors,suchasthesparsity,arede-  veloped[ 13,5,44]andtheyperformclusteringorenforce non-localsmoothnessonimageThesepriors  orregularizationtermsarenotguaranteedtoworkwellin  allcases,especiallywhenthealbedovariationis  (e.g.,atexturedsurface). Moreinformationhasbeenusedtoreducetheambigu- ity.Forexample,researchersproposetouseRGB-Dimage  asinputanddepthmapsornormalsaretakenasadditional  cues[ 9,17,3].Leeetal.[ 24]employedtemporalalbedo consistencyconstraintsforRGB-Dvideo.Anothersolution  istoleveragemultipleimagesofthescenetakenundervary-  inglightingconditions[ 19,21,20].However,withoutany informationaboutthesurfacegeometryandenvironmental  lighting,theproblemisstillill-posedwithregularization  termsorpairwiseconstraintsneeded. Themaindifferencebetweenourmethodandtheprior worksisthatwecansolvenormalandalbedoforeachpixel  independently, withouttheneedforadditionalregulariza- tionterms .3.PreliminaryTheory Whileenvironmentallightingcanbearbitrarilycomplex, theappearanceofadiffuseobjectcanbedescribedbyalow  dimensionalmodel[ 29].Underthisassumption,theshad- ingfunction s forLambertiancanbemodeled 3134 asaquadraticfunctionofthesurfacenormalwith A,b ,c representedasthelightingparameters. s ( n )= n T An + b T n + c (1)Thenthecapturedimageisgeneratedbymultiplyingthe shadingfunctionwithsurfacealbedo ˆ( p ) I ( p )= ˆ( p )  s ( n ( p )) (2)Givenasingleimageasobservation,itmaynotbefeasi- bletorecoverthesurfacenormalandalbedosinceforeach  pixelwehavethreeequationswithveunknownstobeesti-  mated.Photometricstereo,withmorelightingvariations,is  atypicalsolutiontoresolvetheambiguity.Mathematically,  thesurfacenormalanditsalbedocanbecomputedbymin-  imizingthefollowingobjectivefunctionthatisformulated  foreachpixelindependentlyundernaturallightingcondi-  tion.Nosmoothnessoralbedoregularizationisneeded. E ( n ;ˆ )= X k jj ˆ ( n T Ak n + b T  k n + c k )  I k jj 2 (3)Theunderlyingprincipleofourenhancementmethodis basedonphotometricstereo,butwedonotneedtochange  thelightingconditionwiththeobjectbeingstatic;instead  wecapturedtheRGB-Dsequenceoftheobjectunderarbi-  trarymotioninuncalibratednaturalillumination.Suppose  wekeeptheframeasthereferenceframe,andwecan  thecorrespondencesforpixelsalongthesequence.For  example,forpixel p inthereferenceframe,itscorrespon- denceinframe k isW ( p ) .Theappearanceofthepixel W ( p ) isgeneratedas, I k ( W ( p ))= ˆ( p )   ( R k n ) T A( R k n )+ b T ( R k n )+ c  = ˆ( p )   n T ( R T k AR k ) n +( b T R k ) n + c  (4)whereˆisthealbedoforpixel p whichequalstopixel W ( p ) andn issurfacenormalunderreferenceframecoor- dinate.R k istherotationfromthereferenceframetoframe k .Thereforethechangesoflightinginducedbytheobject motionprovidevaluablecuestorecoverthesurfacenormal  anditsalbedoresemblingphotometricstereo.  4.Pipeline Anoverviewofourdepthenhancementandalbedore- coveryframeworkisshowninFigure 1.First,wefuseevery M =20 RGB-DframesviaKinect- Fusion[ 26]togenerate N keyframedepthmaps.They aresmootherandmoreaccuratethantherawdepthmaps.  Theextrinsicparametersbetweenthesekeyframesarecom-  putedandthenafterbundleadjustment.Arobust Color  sequenceDepth  sequenceKey frame  fusion Rigid  alignment Local match  refinement Lighting  estimation Pixel-wise  normal and  albedo recovery Surface  integration Detailed  surface lighting correspondence  Figure1:SystemPipeline. pixelmatchingstrategyisproposedtodealwithmisalign-  ment.Forlightingestimation,theentiresequenceisused  tomaketheestimationmorerobust.Finally,giventhecom-  putedlightingandcorrespondencesalongthesequence,we  recoverthesurfacenormalandalbedoimageunderourro-  bustEMframework.Therecoverednormalcanbeinte-  gratedwiththereferencekeyframedepthmaptogenerate  surfacemodelwithmuchmorestructuraldetails.  5.Approach Therearethreemajorpartsinourapproach,including searchingcorrespondencesamongtheimages,lightinges-  timation,andnormalandalbedorecovery.Theywillbe  describedindetailsinthefollowingsections.  5.1.RobustPixelMatching Thekeyframedepthmaps D 0 ˘ D N areobtainedvia depthfusionandthecorrespondingcolorimagesarede-  notedas I 0 ˘ I N .Wesettheframeasthereference frame( D ref= D 0 ,I ref= I 0 ).Weneedtoestablishcorre- spondencesbetweenpixelsinreferenceframeandthosein  otherframes.  5.1.1Rigidalignment  First,theglobalrigidtransformationfromthisreference  frametootherframesarecalculatedbydetectingSIFTor  ORBfeaturesfollowedbyfeaturematching.Theseextrin-  sicparametersarefurtherwithbundleadjustment  andwegettherotation R 0 ˘ R N andtranslation matrixT 0 ˘ T N fromthereferenceframetootherframes. 5.1.2Lightinginsensitiverobustmatch  Thesekeyframescanbewarpedintothereferenceframe  giventhecurrenttransformation.Howevermisalignments  stillexistafterbundleadjustmentasshowninFig. 2(c),whichiscausedbytheimprecisedepthmapsandtheimper-  fectsynchronizationbetweenthecapturedcoloranddepth  sequence.Opticalwisoftenusedasasolutionto  correspondencesbetweentwoimages.Consideringthatthe 3135 (a) Reference frame (b) Sampled frame(d) Optical flow overlay (c) Initial overlay (e) Our overlay Figure2:Demonstrationofmatching.(a)isthereferenceframeand(b)isonesampledkeyframe.Image(b)iswarpedto  thereferenceframewithcurrenttransformation,and(c)displaysthewarpedimageoverlaidwithimage(a).(d)showsthe  overlaidresultusingthewmapcomputedfromwarpedimageandthereferenceimage.(e)istheoverlaidimageafter  applyingourproposedlightinginsensitiverobustmatching.  misalignmentmaybesevere,wehavetriedtouselargedis-  placementopticalw[ 7]tothecorrespondencesbe- tweenthewarpedimageandthereferenceimage.However,  sincetheconsistencyassumptionisnotmaintainedinour  case,thealignmenthasgotevenworseinsomepartwith  greatilluminationchangesasdisplayedinFig. 2(d).There- fore,wehavedevelopedarobustmethodtodealwiththese  issues.Supposewehavethereferencedepthmapandcolorim- agedenotedas D refandI refrespectively.Foreachpixel p =( u;v ) inI ref,itscurrentcorrespondingpixel( q )after bundleadjustmentinimage I k iscomputedas,   q 1 = K   R k  K  1 2  4 u v D ref( u;v ) 3  5  + T k ! (5)Thecorrespondingpixel p inI refandq inI k maynotbe thecorrectcorrespondencebecauseofthemisalignment.  Therefore,weimplementalocalsearchstrategytoits  bestmatchingpixelin I k .Foreachpixel p inI ref,wesetasearchingregionaround itanditsbestmatchin I k viaNCC.However,theinten- sityconsistencyisnotpreservedastheobjectissubjectto  arbitrarymovements.ThismakestheoriginalNCCnotsuit-  ableformatchinginthiscase.Todealwiththisproblem,  weapplychromaticitynormalizationinthecolorimageto  eliminatetheeffectoflightingvariations[ 12]andusethe normalizedimagesformatching.Foreachpixel p ,itsap- pearanceisgeneratedas, I ch( p )= ˆch( p ) s ( p ) ch 2f R;G;B g; (6)inwhich s ( p ) istheshadingfunctionthataccountsforthe lightingornormalvariation. Sothechromaticitynormalizationisimplementedas, I cn ch ( p )= I ch ( p ) I R ( p )+ I G ( p )+ I B ( p ) ch 2f R;G;B g; (7)Afterthisnormalization,NCCcanthenbeappliedfor matchinginsensitivetothephotometricinconsistencyin-  ducedbylightingfactor. Besides,thecolorimage I refiswarpedtothecolorframe I k undertheguidanceof D refandwegetthewarpedcolor imageI refk.TheNCCpatchmatchingisimplementedin I refkwithI k insteadofusing I refdirectly.Since I k andI refkareinthesameviewpoint,thefatteningeffectofNCCis  successfullyavoided. Althoughforeachpixelin I ref(orI refk)wecanthe correspondingpixelin I k thathasthelargestmatching score,wecannotguaranteetheyarealwaysthecorrectcor-  respondence.Totacklethisproblem,weonlykeepthepix-  elsthatarereliableandusethesepixelsascontrolvertices  todeformalltheotherpixelstotheircorrectcorrespon-  dences.Ourcriteriaofreliablematchesisthat,1)thelargest matchingscoreshouldbelargerthanthres S ;2)thediffer- encebetweenthelargestscoreandsecondlargestscoreof  localpeaksshouldbelargerthanthres  .Iftheseprinciples aremaintained,thepixelinthesearchingregionthathas  thelargestscoreischosenasthecorrespondence.thres S issettobe0.75andthres  is0.05inthispaperforallthe experiments. Nextweusethesereliablematchesascontrolverticesto deformtheimage I refksothatithasanoptimalmatchwith I k .Asforeachcontrolvertices o l inI refk,thedeformation 3136 functionisas f ( o l )= o l + l ; (8)where l isthemotionvectorbetweentheoptimalcorre- spondenceanditsinitialcorrespondencein I k .Forotherpixelsthedeformationisformulatedviabilin- earinterpolationwithcontrolvertices[ 45],f ( u )= u + X l (  u l  l ) (9)Theinterpolationcoef  u l issetaccordingtothe distancetocontrolverticesandonlyneighboringvertices  willaffectthedeformation. Finally,ourobjectivefunctionistomaintainthe photoconsistencyofthetwonormalizedimages.  E ()= X p  I cn refk( p )  I cn k ( f  p; )   +  X l jj  l  ^ l jj 2 (10)Forthecontrolvertices ^ istheinitialdeformationvec- torbetweenthecurrentoptimalcorrespondenceobtained  frommatchinganditsinitialcorrespondence.  isthecon- trolweightsettobe10inthispaper.Sincewehavegood  initials^ ,theoptimizationwillconvergequitefast. ThematchingresultcanbeseeninFig. 2(e).5.2.Lightingestimation Theunknownalbedoposeschallengesforlightinges- timation.Inthispaper,weemploythealignedcolorse-  quenceanddepthmapsforrobustlightingestimation,elim-  inatingtheneedtomakeassumptionsaboutalbedo.With  thealignedcolorimageswecancomputetheratioimages  tothereferenceimage,forwhichthealbedoiseliminated.  Foreachpixel p inI ref,supposeitscorrespondingpixelin I k isdenotedas q ,thentheratiovalueiscomputedas, I k ( q ) I ref ( p ) = ˆ( q )( n q T R T AR n q + b T R n q + c ) ˆ( p )( n p T R T AR n p + b T R n p + c ) = nq T R T AR nq + b T R nq + c np T R T AR np + b T R np + c (11)Therefore,theenvironmentallightingcanbeachieved fromthefollowingminimization, argmin A;b;c X k X p 2 I ref p jj n q T R T AR n q + b T R n q + c n p T R T AR n p + b T R n p + c  I k ( q ) I ref( p ) jj 2 (12)Thenormal n areapproximatedusingnormalscomputed withthekeyframedepthmaps.Theweightingterm  p issettopreventtheeffectsofdarkpixelswhicharenoisyand  mightbecausedbyshadow.  5.3.Normalandalbedorecovery Withthekeyframecolorimagesallalignedintotheref- erenceframe( I W 0 ˘ I W N ),estimatedenvironmentallight- ing( A;b;c ) ,andobjectrotationmatrix R 0 ˘ R N foreach frame,wearereadytorecoverthesurfacenormalandits  albedo.Foreachpixel p intheframeifwedenoteobserva- tionsas O = fI W k ( p ) g,thenourgoalistotheoptimal albedoˆandnormal n confrontingthepixelobservations. Wedroptheindexofpixellocationsforsimplicityinthefol-  lowingdescription.Theobjectivefunctioncanbe  as:E ( n ;ˆ j O )= X k jj ˆ s k ( n )  I W k jj 2 ; (13)s k ( n )= n T R T k AR k n + b T R k n + c (14)Thesurfacenormalandalbedocanbeestimatedafter minimizationoftheabovefunction.However,theoutliers  havenotbeentakenintoconsideration.Theywillaffectthe  resultiftheobservationsviolatetheLambertianassump-  tionorareincastshadow.Todealwiththeseoutliers,we  introduceasetofhiddenstates H k = f0; 1gindicatingwhethertheobservationisactuallygeneratedbytheLam-  bertianmodel.Anexpectation-maximization(EM)algo-  rithmisdevelopedtosolvetheproblem.Whileourfor-  mulationisinspiredby[ 38],weextenditfromitsorigi- naldirectionallightassumptiontogenerallighting.More  ,wedenotetheparameterstobeestimatedas  = fn ;ˆ;˙; gandtheobservationprobabilitycondi- tionedonparameters  isgivenas, P( O k j )=   1p 2ˇ˙ exp (  jj ˆ s k ( n )  I W k jj 2 2˙ 2 ) +(1   )  1C (15)P( H k =1)=  isthepriorprobabilityof H k indicatingtheproportionofobservationsgeneratedbytheLambertian  model.1 C istheprobabilityasbeinganoutlier. Theposteriorprobabilityofthehiddenvariable H k given parameters 0 incurrentiterationandtheobservation O k iscomputedineveryE-step, ! k = P( H k =1 j O k ;  0 ) =   exp (  jj ˆ  s k( n )  I W kjj 2 2 ˙2 )   exp (  jj ˆ  s k( n )  I W kjj 2 2 ˙2 )+ 1   C (16)Next,inthefollowingM-step,wemaximizethe complete-datalog-likelihoodgiventhemarginaldistribu-  tionH k obtainedfromtheE-step. P()= X k logP ( O k ;H k =1 j ) ! k + X k logP ( O k ;H k =0 j )(1  ! k ) = X k log (  p 2ˇ˙ exp (  jj ˆ s k ( n )  I W k jj 2 2˙ 2 )) ! k + X k log ( 1  C )(1  ! k ) (17)3137 Tomaximizetheabovefunction,wesetthederiva- tiveof Pwithrespectto  ,˙ andˆequaltozero.Inthis way,theupdatingrulesfortheseparametersareobtained,  = 1N X k ! k ˙ = sP k jj ˆ s k ( n )  I W k jj 2 ! k P k ! k ˆ= 1P k s k ( n ) 2 ! k X k s k ( n )  ! k  I W k (18)Sincethefunction Pisnonlineartosurfacenormal n ,theupdatednormalisachievedbyotherparameters  andsolvingthefollowingenergyminimization. argmin n X k jj ˆ ( n T R T k AR k n + b T R k n + c )  I W k jj 2 ! k (19)TheaboveEMiterativeoptimizationprocessisper- formeduntilnofurtherimprovementontherecoverednor-  malandalbedo.Theinitialparameterof  and˙ issetto be0.75and0.05respectivelyforallthedatasetsusedinthis  paper. Finally,thenormalisintegratedwiththereferencedepth maptogetenhancedsurfacegeometrywithstructuralde-  tails[ 43].5.4.Implementationdetails Asapreprocessingstep,theobjectissegmented fromtheimagebyintegratingbothcoloranddepthinfor-  mationintoGrabCut[ 30]framework.Wemanuallymasked theframewiththerestofframessegmentedautomati-  cally. WeimplementmostpartsofourframeworkinMatlab andittakesusapproximately800stoprocessadatasetwith  500˘ 600frames.Consideringthatthenormalandalbedo iscomputedinpixel-wisemanner,therunningtimecould  bereducedfurtherwithparallelcomputation.  6.Experimentalresults Intheexperiments,wevalidateourmethodonsynthetic andrealdatasetswithquantitativeandqualitativecompari-  son. 6.1.Syntheticdata InthissectionweuseStanfordBunnyasoursynthetic modelandpreformaquantitativecomparisonbetweenour  methodandashadingmethod[ 27].Firstgiven the3Dmodel,twentyimagestogetherwiththedepthmaps  arerenderedundernaturalillumination.Therendered  groundtruthdepthmapsareoversmoothedtooutthe structuraldetails.Thosesmootheddepthmapsandren-  deredcolorimagesaretakenasinputforourmethod. Fig.3showsthecomparisonresultsonrecoverednormal mapandsurface.Thecolumnisthereferencecolor  mapandoversmoothedmesh(displayedasnormalmap).  Thesearetheinputforshadingmethod.Theout-  putofshadingmethodisdisplayedinthesecond  column.Thetexturecopyartifactsarecausedbyimperfect  separationofalbedoandshadinglayers.Incomparison,the  surfacenormalcanberecoveredsuccessfullywithpixel-  wiserecoverymethodwithquitesmallerrorshowninthe  thirdcolumn.Thealbedomapcomputedfromourmethod  togetherwithitserrormapisdemonstratedinthelastcol-  umn.(a) Color (b) Shading Normal  (e) Smoothed Normal (c) Our Normal (d) Our Albedo (h) Albedo error (g) Our Normal error (f) Shading Normal error Figure3:ResultsonsyntheticBunnymodel.(a)istheren-  deredcolorimageofthereferenceframe;(e)showsthenor-  malmapofthegroundtruthmeshafteroversmoothing;(b)  isthenormalmapcomputedafterapplyingshading  mentonthereferenceframewithitserrormapdisplayedin  (f);(c)and(g)arethenormalmapanditscorresponding  errormapachievedbyourmethod.Ourrecoveredalbedo  mapanditserrormapisalsodemonstratedin(d)and(h)  respectively. (a) W/O EM(b) W/O EM error(c) With EM(d) With EM error Figure4:Resultswhenaddingsaltandpeppernoise.(a)  showsthecomputednormalmapwithoutourEMframe-  workand(b)isitserrormap;Thenormalanderrormap  afterapplyingourEMoptimizationareshownin(c)and(d)  respectively. Fig.4isshowntodemonstratetheeffectivenessofour EMframeworkforrobustnormalrecoveryinthepresence  ofoutliers.Wehavepickedfouroutofthosetwentyimages  randomlyandaddedthesaltandpeppernoisewith0.50  density.Itmeanstheabruptnoisewillaffectapproximately  percentoftheimagepixels.Aswecanseefromthe  twocolumns,therecoverednormalmapwithoutEM 3138 Figure5:ComparisonresultsonBackpackandTurtlemodel.(a1)and(b1)arethereferencecolorimagesofBackpackand  Turtlerespectively.TheoutputfromKinectFusionisshownin(a2)and(b2).Theresultscomputedbyshading  methodaredisplayedin(a3)and(b3).(a4)and(b4)arethemeshesacquiredusingourmethodbutwithoutapplyingour  locallyrobustmatchingprocedure.Finally,(a5)and(b5)arethemeshesachievedbyourapproach.Thenormalmapisgiven  in(a6)and(b6).  optimizationisnoisy(themeanerroris8.37degree),while  wecanachievemuchbetterperformanceafterapplyingour  EMmethodandthemeanerroris1.49degree,whichis  showninthelasttwocolumns.  6.2.Realdataset Wehavecapturedtherealdatasetusingthedepthsen- sorofKinectV2andacolorcamerawithresolutionof  1920 1080.Thereareseveralmodelscaptured,namely theBackpack,Turtle,Book,etc. Fig.5showsthecomparisonresultsofacolorfulBack- packandTurtlemodelrespectively.Fig. 5(a2)and(b2) displaysthemeshmodelacquiredfromKinectFusion.As  wecanseethesurfacedetailsarenotrevealedasrestricted  bytheresolutionandaccuracyoftheKinectdepthsensor.  Afterapplyingtheshading[ 27]onthefused referenceframe,somesurfacedetailsarerecovered,while  sometexturesarehallucinatedasgeometrydetailsaswell 3139 (Fig.5(a3)and(b3)).Fig. 5(a4)and(b4)showsourresults withoutapplyingourlocalmatchstep.Theun-  evensurfaceinsomepartasmarkedinrediscausedby  misalignment.Fig. 5(a5,b5)andFig. 5(a6,b6)displaysour resultsofrecoveredmeshesandsurfacenormals.For  theBackpack,itactuallyexperiencesnon-rigiddeformation  duringthemovementandthereforeweonlyconsiderthe  frontpartofthebackpackwhichismostlyrigid.Wehave  markedsomecolorfulpatternsontheTurtlesurfacetomake  thetexturemorecomplextoshowthesuperiorperformance  ofourpixel-wiserecoverymethod. Tofurthervalidateourrobustnessagainsttexturecopy problem,theresultsofacolorfulBookcoveraredemon-  stratedinFig. 6.AsdisplayedinFig. 6(b)thetextureshave beensuccessfullyfactoredoutfromtheimageandthere-  coveredmodelkeepsasaplanarsurfaceaftertheenhance-  ment.Incomparison,theresultfromshading  method(Fig.6(c))isaffectedbythetexturecopyeffectwith fakegeometricdetailsappeared. Figure6:ResultsonBookModel.(a)isthereferencecolor  image.Therecoveredmeshsurfacefromourmethodisdis-  playedin(b).(c)showsthemeshwithshadingre-  method.  IntrinsicImageDecomposition Inordertoshowtheper- formanceofourmethodinalbedorecovery,wehavealso  madesomecomparisonswithtwostate-of-the-artintrinsic  imagedecompositionapproachesasdisplayedinFigure 7.Forthesetwocomparedmethods,theytaketheRGB-Dim-  agesofthereferenceframeasinput,asdisplayedinthe  column.Thesecondcolumnshowstheresultfrom  Chen[ 9].Theshadingimageisoversmoothedandthege- ometrydetailsaredecomposedintoalbedomapincorrectly.  ThemethodfromJeon[ 17]hasbetterresultsonrecovered shadingimagesfortheTurtlemodelasdisplayedinthethird  column.However,sometexturesstillstayattheshading  imageespeciallyfortheBackpack.Incomparison,with  ourpixel-wisealbedocomputationmethod,weareableto  recoveramuchsharperalbedomapandtheﬂtexturecopyﬂ  effectinthegeometryisbarelynoticeable.  7.Conclusion Inthispaper,wepresentanovelapproachtorecover surfacedetailsanditsalbedomapfromanRGB-Dvideo Figure7:Comparisonresultsonalbedorecoveryorintrin-  sicdecompositionoftheTurtleandBackpackmodel.The  columnistheinputcolorimagewithitsdepthmap.  ThesecondcolumnshowstheresultofChen[ 9].Thethird columnisthedecomposedalbedoandshadingimagesfrom  methodin[ 17].Finally,thelastcolumndemonstratesthe resultachievedbyourmethod.  sequence.Theobjectisexperiencingcasualmotionfrom  whichtheinducedilluminationvariationprovidesusthe  cuetorecoverthesurfacenormalanditsalbedoaswell.  Arobustlightinginsensitivelocalmatchstrategyispro-  posedtoestablishcorrectcorrespondencesalongthese-  quence.Then,theenvironmentallightingisestimatedby  exploitingthewholesequencetogetridofeffectofalbe-  dos.Finally,thesurfacenormalanditsalbedoiscalculated  robustlywithourEMframework.Wehavevalidatedour  methodonbothsyntheticandrealdatasetandcompared  withsomestate-of-the-artsurfaceandintrinsic  decompositionmethods.Asdemonstratedintheexperi-  ments,wehaveachievedgoodperformanceonbothsurface  detailsrecoveryandintrinsicdecomposition. Thereconstructedobjectislimitedtoobjectswithpri- marilydiffusesurface.Asafuturework,wecouldex-  tendourideaintoobjectsthathaveagreatportionofnon-  LambertianRightnowwearemainlyfocusing  ondepthenhancement,whilewewouldliketoimplement  alltheseprocedureinfull3Dspace.  Acknowledgments ThisworkispartiallysupportedbytheUSNFS (IIS-1231545,IIP-1543172),USArmyResearchgrant  W911NF-14-1-0437,NSFC(No.61332017,51475373,  61603302,51375390),KeyIndustrialInnovationChain  ofShaanxiProvinceIndustrialArea(2015KTZDGY04-01,  2016KTZDGY06-01),thefundamentalResearchFundsfor  theCentralUniversities(No.3102016ZY013).Jiangbin  ZhengandRuigangYangaretheco-correspondingauthors  forthispaper. 3140 References [1]J.T.BarronandJ.Malik.High-frequencyshapeandalbedo fromshadingusingnaturalimagestatistics.In CVPR,pages 2521Œ2528,2011. 2[2]J.T.BarronandJ.Malik.Shape,illumination,andre- fromshading. IEEETransactionsonPatternAnal- ysisandMachineIntelligence ,37(8):1670Œ1687,2015. 2[3]J.T.BarronandJ.Malik.Intrinsicscenepropertiesfroma singlergb-dimage. IEEETransactionsonPatternAnalysis andMachineIntelligence ,38(4):690Œ703,2016. 2[4]H.G.BarrowandJ.M.Tenenbaum.Recoveringintrinsic scenecharacteristicsfromimages. Comput.Vis.Syst. ,1978. 2[5]S.Bi,X.Han,andY.Yu.Anl1imagetransformforedge- preservingsmoothingandscene-levelintrinsicdecomposi-  tion.ACMTrans.Graph. ,34(4):78,2015. 2[6]M.B ¨ohme,M.Haker,T.Martinetz,andE.Barth.Shad- ingconstraintimprovesaccuracyofmea-  surements.Computervisionandimageunderstanding ,114(12):1329Œ1335,2010. 1[7]T.BroxandJ.Malik.Largedisplacementopticalw:de- scriptormatchinginvariationalmotionestimation. IEEEtransactionsonpatternanalysisandmachineintelligence ,33(3):500Œ513,2011. 4[8]A.ChatterjeeandV.M.Govindu.Photometricof depthmapsformulti-albedoobjects.In CVPR,pages933Œ 941,2015. 2[9]Q.ChenandV.Koltun.Asimplemodelforintrinsicimage decompositionwithdepthcues.In ICCV,pages241Œ248, 2013.2,8[10]C.H.Esteban,G.Vogiatzis,andR.Cipolla.Multiviewpho- tometricstereo. IEEETransactionsonPatternAnalysisand MachineIntelligence ,30(3):548Œ554,2008. 1[11]D.Ferstl,C.Reinbacher,R.Ranftl,M.Rther,and H.Bischof.Imageguideddepthupsamplingusing  anisotropictotalgeneralizedvariation.In ICCV,pages993Œ 1000,2013. 1[12]G..FinlaysonandR.Xu.Illuminantandgammacompre- hensivenormalisationinlogrgbspace. PatternRecognition Letters ,24(11):1679Œ1690,2003. 4[13]E.Garces,A.Munoz,J.Lopez-Moreno,andD.G.Diego.In- trinsicimagesbyclustering.In ComputerGraphicsForum ,volume31,pages1415Œ1424,2012. 2[14]Y.Han,J.Lee,andI.Kweon.Highqualityshapefroma singlergb-dimageunderuncalibratednaturalillumination.  InICCV,pages1617Œ1624,2013. 2[15]S.M.Haque,A.Chatterjee,andV.M.Govindu.Highquality photometricreconstructionusingadepthcamera.In CVPR,pages2283Œ2290,2014. 2[16]B.K.P.Horn. ShapefromShading:AMethodforObtaining theShapeofaSmoothOpaqueObjectfromOneView .PhD thesis,MIT,1970. 2[17]J.Jeon,S.Cho,X.Tong,andS.Lee.Intrinsicimagede- compositionusingstructure-textureseparationandsurface  normals.In ECCV,pages218Œ233,2014. 2,8[18]K.Kim,A.Torii,andM.Okutomi.Jointestimationofdepth, andilluminationfordepthIn ICCVWorkshops ,2015. 2[19]N.Kong,P.Gehler,andM.Black.Intrinsicvideo.In ECCV,pages360Œ375,2014. 2[20]P.LaffontandJ.Bazin.Intrinsicdecompositionofimage sequencesfromlocaltemporalvariations.In ICCV,pages 433Œ441,2015. 2[21]P.Laffont,A.Bousseau,S.Paris,F.Durand,andG.Dret- takis.Coherentintrinsicimagesfromphotocollections.  ACMTrans.Graph. ,31(6),2012. 2[22]A.LakdawallaandA.Hertzmann.Shapefromvideo:Dense shape,texture,motionandlightingfrommonocularimage  streams.In ProceedingsoftheFirstInternationalWorkshop onPhotometricAnalysisForComputerVision .INRIA,2007. 1[23]E.H.LandandJ.J.McCann.Lightnessandretinextheory. JOSA,1971. 2[24]K.J.Lee,Q.Zhao,X.Tong,M.Gong,S.Izadi,S.U.Lee, P.Tan,andS.Lin.Estimationofintrinsicimagesequences  fromimage+depthvideo.In ECCV,pages327Œ340,2012. 2[25]D.Min,J.Lu,andM.N.Do.Depthvideoenhancement basedonweightedmode IEEETransactionson ImageProcessing ,21(3):1176Œ1190,2012. 1[26]R.Newcombe,S.Izadi,O.Hilliges,D.Molyneaux,D.Kim, A.Davison,P.Kohi,J.Shotton,S.Hodges,andA.Fitzgib-  bon.Kinectfusion:Real-timedensesurfacemappingand  tracking.In ISMAR,pages127Œ136,2011. 3[27]R.Or-El,G.Rosman,A.Wetzler,R.Kimmel,andA.M. Bruckstein.Rgbd-fusion:Real-timehighprecisiondepthre-  covery.In CVPR,pages5407Œ5416,2015. 1,2,6,7[28]J.Park,H.Kim,Y.Tai,M.S.Brown,andI.Kweon. High-qualitydepthmapupsamplingandcompletionfor  rgb-dcameras. IEEETransactionsonImageProcessing ,23(12):5559Œ5572,2014. 1[29]R.RamamoorthiandP.Hanrahan.Anefrepresenta- tionforirradianceenvironmentmaps.In SIGGRAPH,pages 497Œ500.ACM,2001. 2[30]C.Rother,V.Kolmogorov,andA.Blake.Grabcut:Interac- tiveforegroundextractionusingiteratedgraphcuts. ACM transactionsongraphics ,23(3):309Œ314,2004. 6[31]B.Shi,K.Inose,Y.Matsushita,P.Tan,S.Yeung,and K.Ikeuchi.Photometricstereousinginternetimages.In  3DV ,pages361Œ368,2014. 2[32]D.Simakov,D.Frolova,andR.Basri.Denseshaperecon- structionofamovingobjectunderarbitrary,unknownlight-  ing.In ICCV,volume3,pages1202Œ1209,2003. 1[33]M.F.Tappen,W.T.Freeman,andE.H.Adelson.Recov- eringintrinsicimagesfromasingleimage. IEEETPAMI ,27(9):1459Œ1472,2005. 2[34]C.Ti,R.Yang,J.Davis,andZ.Pan.Simultaneoustime-of- sensingandphotometricstereowithasingletofsensor.  InCVPR,pages4334Œ4342,2015. 2[35]C.Wu,Y.Liu,Q.Dai,andB.Wilburn.Fusingmultiview andphotometricstereofor3dreconstructionunderuncali-  bratedillumination. IEEEtransactionsonvisualizationand computergraphics ,17(8):1082Œ1095,2011. 23141 [36]C.Wu,K.Varanasi,Y.Liu,H.Seidel,andC.Theobalt. Shading-baseddynamicshapefrommulti-view  videoundergeneralillumination.In ICCV,pages1108Œ 1115,2011. 2[37]C.Wu,M.Zollh ¨ofer,M.Niener,M.Stamminger,S.Izadi, andC.Theobalt.Real-timeshading-basedfor  consumerdepthcameras. ACMTransactionsonGraphics (TOG) ,33(3),2014. 1,2[38]T.WuandC.Tang.Photometricstereoviaexpectationmax- imization.IEEEtransactionsonpatternanalysisandma- chineintelligence ,32(3):546Œ560,2010. 2,5[39]T.Wu,K.Tang,C.Tang,andT.Wong.Densephotometric stereo:Amarkovrandomapproach. IEEEtransactions onpatternanalysisandmachineintelligence ,28(11):1830Œ 1846,2006. 2[40]J.Yang,X.Ye,K.Li,C.Hou,andY.Wang.Color-guided depthrecoveryfromrgb-ddatausinganadaptiveautore-  gressivemodel. IEEETransactionsonImageProcessing ,23(8):3443Œ3458,2014. 1[41]L.Yu,S.Yeung,Y.Tai,andS.Lin.Shading-basedshape ofrgb-dimages.In CVPR,pages1415Œ1422, 2013.1,2[42]L.Zhang,B.Curless,A.Hertzmann,andS.M.Seitz.Shape andmotionundervaryingillumination:Unifyingstructure  frommotion,photometricstereo,andmultiviewstereo.In  ICCV,pages618Œ625,2003. 1[43]Q.Zhang,M.Ye,R.Yang,Y.Matsushita,B.Wilburn,and H.Yu.Edge-preservingphotometricstereoviadepthfusion.  InCVPR,pages2472Œ2479,2012. 2,6[44]Q.Zhao,P.Tan,Q.Dai,L.Shen,E.Wu,andS.Lin.A closed-formsolutiontoretinexwithnonlocaltexturecon-  straints.IEEETransactionsonPatternAnalysisandMachine Intelligence ,34(7):1437Œ1444,2012. 2[45]Q.ZhouandV.Koltun.Colormapoptimizationfor3drecon- structionwithconsumerdepthcameras. ACMTransactions onGraphics(TOG) ,33(4):155,2014. 5[46]Z.Zhou,Z.Wu,andP.Tan.Multi-viewphotometricstereo withspatiallyvaryingisotropicmaterials.In ICCV,pages 1482Œ1489,2013. 23142  
Semi-SupervisedLearningforNeuralMachineTranslation YongCheng # ,WeiXu # ,ZhongjunHe + ,WeiHe + ,HuaWu + ,MaosongSun y andYangLiu y # InstituteforInterdisciplinaryInformationSciences,TsinghuaUniversity,Beijing,China y StateKeyLaboratoryofIntelligentTechnologyandSystems TsinghuaNationalLaboratoryforInformationScienceandTechnology DepartmentofComputerScienceandTechnology,TsinghuaUniversity,Beijing,China + BaiduInc.,Beijing,China chengyong3001@gmail.comweixu@tsinghua.edu.cn f hezhongjun,hewei06,wu hua g @baidu.com f sms,liuyang2011 g @tsinghua.edu.cn Abstract Whileend-to-endneuralmachinetransla- tion(NMT)hasmaderemarkableprogress recently,NMTsystemsonlyrelyonpar- allelcorporaforparameterestimation. Sinceparallelcorporaareusuallylimited inquantity,quality,andcoverage,espe- ciallyforlow-resourcelanguages,itis appealingtoexploitmonolingualcorpora toimproveNMT.Weproposeasemi- supervisedapproachfortrainingNMT modelsontheconcatenationoflabeled (parallelcorpora)andunlabeled(mono- lingualcorpora)data.Thecentralideais toreconstructthemonolingualcorporaus- inganautoencoder,inwhichthesource- to-targetandtarget-to-sourcetranslation modelsserveastheencoderanddecoder, respectively.Ourapproachcannotonly exploitthemonolingualcorporaofthe targetlanguage,butalsoofthesource language.ExperimentsontheChinese- Englishdatasetshowthatourapproach achievesimprovementsover state-of-the-artSMTandNMTsystems. 1Introduction End-to-endneuralmachinetranslation(NMT), whichleveragesasingle,largeneuralnetworkto directlytransformasource-languagesentenceinto atarget-languagesentence,hasattractedincreas- ingattentioninrecentseveralyears(Kalchbren- nerandBlunsom,2013;Sutskeveretal.,2014; Bahdanauetal.,2015).Freeoflatentstructure designandfeatureengineeringthatarecriticalin conventionalstatisticalmachinetranslation(SMT) (Brownetal.,1993;Koehnetal.,2003;Chi- ang,2005),NMThasproventoexcelinmodel-  YangLiuisthecorrespondingauthor. inglong-distancedependenciesbyenhancingre- currentneuralnetworks(RNNs)withthegating (HochreiterandSchmidhuber,1993;Choetal., 2014;Sutskeveretal.,2014)andattentionmecha- nisms(Bahdanauetal.,2015). However,mostexistingNMTapproachessuf- ferfromamajordrawback:theyheavilyrely onparallelcorporafortrainingtranslationmod- els.ThisisbecauseNMTdirectlymodelsthe probabilityofatarget-languagesentencegivena source-languagesentenceanddoesnothaveasep- aratelanguagemodellikeSMT(Kalchbrennerand Blunsom,2013;Sutskeveretal.,2014;Bahdanau etal.,2015).Unfortunately,parallelcorporaare usuallyonlyavailableforahandfulofresource- richlanguagesandrestrictedtolimiteddomains suchasgovernmentdocumentsandnewsreports. Incontrast,SMTiscapableofexploitingabundant target-sidemonolingualcorporatoboosty oftranslations.Therefore,theunavailabilityof large-scale,high-quality,andwide-coveragepar- allelcorporahinderstheapplicabilityofNMT. Asaresult,severalauthorshavetriedtouse abundantmonolingualcorporatoimproveNMT. Gulccehreetal.(2015)proposetwomethods, whicharereferredtoasshallowfusionanddeep fusion,tointegratealanguagemodelintoNMT. Thebasicideaistousethelanguagemodelto scorethecandidatewordsproposedbythetransla- tionmodelateachtimesteporconcatenatingthe hiddenstatesofthelanguagemodelandthede- coder.Althoughtheirapproachleadsto cantimprovements,onepossibledownsideisthat thenetworkarchitecturehastobetoin- tegratethelanguagemodel. Alternatively,Sennrichetal.(2015)propose twoapproachestoexploitingmonolingualcorpora thatistransparenttonetworkarchitectures.The approachpairsmonolingualsentenceswith dummyinput.Then,theparametersofencoder arXiv:1606.04596v3  [cs.CL]  10 Dec 2016Figure1:Examplesof(a)sourceautoencoderand(b)targetautoencoderonmonolingualcorpora.Our ideaistoleverageautoencoderstoexploitmonolingualcorporaforNMT.Inasourceautoencoder,the source-to-targetmodel P ( y j x ; !  ) servesasanencodertotransformtheobservedsourcesentence x intoalatenttargetsentence y (highlightedingrey),fromwhichthetarget-to-sourcemodel P ( x 0 j y ;    ) reconstructsacopyoftheobservedsourcesentence x 0 fromthelatenttargetsentence.Asaresult, monolingualcorporacanbecombinedwithparallelcorporatotrainbidirectionalNMTmodelsina semi-supervisedsetting. andattentionmodelareedwhentrainingon thesepseudoparallelsentencepairs.Inthesec- ondapproach,theytrainanerualtranslation modelontheparallelcorpusandthenusethe learnedmodeltotranslateamonolingualcorpus. Themonolingualcorpusanditstranslationscon- stituteanadditionalpseudoparallelcorpus.Simi- larideashavealsobeensuggestedinconventional SMT(Uefetal.,2007;BertoldiandFederico, 2009).Sennrichetal.(2015)reportthattheirap- proachsiimprovestranslationquality acrossavarietyoflanguagepairs. Inthispaper,weproposesemi-supervised learningforneuralmachinetranslation.Givenla- beled(i.e.,parallelcorpora)andunlabeled(i.e., monolingualcorpora)data,ourapproachjointly trainssource-to-targetandtarget-to-sourcetrans- lationmodels.Thekeyideaistoappendare- constructiontermtothetrainingobjective,which aimstoreconstructtheobservedmonolingualcor- porausinganautoencoder.Intheautoencoder,the source-to-targetandtarget-to-sourcemodelsserve astheencoderanddecoder,respectively.Asthe inferenceisintractable,weproposetosamplethe fullsearchspacetoimprovetheefy.Specif- ically,ourapproachhasthefollowingadvantages: 1. Transparenttonetworkarchitectures :ourap- proachdoesnotdependonarchitec- turesandcanbeeasilyappliedtoarbitrary end-to-endNMTsystems. 2. Boththesourceandtargetmonolingualcor- poracanbeused :ourapproachcanbene- NMTnotonlyusingtargetmonolingual corporainaconventionalway,butalsothe monolingualcorporaofthesourcelanguage. ExperimentsonChinese-EnglishNISTdatasets showthatourapproachresultsinim- provementsinbothdirectionsoverstate-of-the-art SMTandNMTsystems. 2Semi-SupervisedLearningforNeural MachineTranslation 2.1SupervisedLearning Givenaparallelcorpus D = fh x ( n ) ; y ( n ) ig N n =1 , thestandardtrainingobjectiveinNMTistomax- imizethelikelihoodofthetrainingdata: L (  )= N X n =1 log P ( y ( n ) j x ( n ) ;  ) ; (1) where P ( y j x ;  ) isaneuraltranslationmodeland  isasetofmodelparameters. D canbeseen as labeled dataforthetaskofpredictingatarget sentence y givenasourcesentence x . As P ( y j x ;  ) ismodeledbyasingle,largeneu- ralnetwork,theredoesnotexistaseparatetarget languagemodel P ( y ;  ) inNMT.Therefore,par- allelcorporahavebeentheonlyresourceforpa- rameterestimationinmostexistingNMTsystems. Unfortunately,evenforahandfulofresource-rich languages,theavailabledomainsareunbalanced andrestrictedtogovernmentdocumentsandnews reports.Therefore,theavailabilityoflarge-scale, high-quality,andwide-coverageparallelcorpora becomesamajorobstacleforNMT. 2.2AutoencodersonMonolingualCorpora Itisappealingtoexplorethemorereadilyavail- able,abundantmonolingualcorporatoimprove NMT.Letusconsideran unsupervised set- ting:howtotrainNMTmodelsonamonolingual corpus T = f y ( t ) g T t =1 ? Ourideaistoleverage autoencoders (Vincentet al.,2010;Socheretal.,2011):(1) encoding anob- servedtargetsentenceintoalatentsourcesentence usingatarget-to-sourcetranslationmodeland(2) decoding thesourcesentencetoreconstructthe observedtargetsentenceusingasource-to-target model.Forexample,asshowninFigure1(b), givenanobservedEnglishsentenceﬁBushheld atalkwithSharonﬂ,atarget-to-sourcetranslation model(i.e.,encoder)transformsitintoaChinese translationﬁbushiyushalongjuxinglehuitanﬂthat isunobservedonthetrainingdata(highlightedin grey).Then,asource-to-targettranslationmodel (i.e.,decoder)reconstructstheobservedEnglish sentencefromtheChinesetranslation. Moreformally,let P ( y j x ; !  ) and P ( x j y ;    ) be source-to-target and target-to-source transla- tionmodelsrespectively,where !  and    arecor- respondingmodelparameters.Anautoencoder aimstoreconstructtheobservedtargetsentence viaalatentsourcesentence: P ( y 0 j y ; !  ;    ) = X x P ( y 0 ; x j y ; !  ;    ) = X x P ( x j y ;    ) | {z } encoder P ( y 0 j x ; !  ) | {z } decoder ; (2) where y isanobservedtargetsentence, y 0 isa copyof y tobereconstructed,and x isalatent sourcesentence. WerefertoEq.(2)asa targetautoencoder . 1 Likewise,givenamonolingualcorpusofsource language S = f x ( s ) g S s =1 ,itisnaturaltointroduce a sourceautoencoder thataimsatreconstructing 1 OurofauotoencodersisinspiredbyAmmaret al.(2014).Notethatourautoencodersinheritthesamespirit fromconventionalautoencoders(Vincentetal.,2010;Socher etal.,2011)exceptthatthehiddenlayerisdenotedbyalatent sentenceinsteadofreal-valuedvectors. theobservedsourcesentenceviaalatenttarget sentence: P ( x 0 j x ; !  ;    ) = X y P ( x 0 ; y j x ;    ) = X y P ( y j x ; !  ) | {z } encoder P ( x 0 j y ;    ) | {z } decoder : (3) PleaseseeFigure1(a)forillustration. 2.3Semi-SupervisedLearning Astheautoencodersinvolvebothsource-to-target andtarget-to-sourcemodels,itisnaturaltocom- bineparallelcorporaandmonolingualcorporato learnbirectionalNMTtranslationmodelsina semi-supervisedsetting. Formally,givenaparallelcorpus D = fh x ( n ) ; y ( n ) ig N n =1 ,amonolingualcorpusoftarget language T = f y ( t ) g T t =1 ,andamonolingualcor- pusofsourcelanguage S = f x ( s ) g S s =1 ,weintro- duceournewsemi-supervisedtrainingobjective asfollows: J ( !  ;    ) = N X n =1 log P ( y ( n ) j x ( n ) ; !  ) | {z } source-to-targetlikelihood + N X n =1 log P ( x ( n ) j y ( n ) ;    ) | {z } target-to-sourcelikelihood +  1 T X t =1 log P ( y 0 j y ( t ) ; !  ;    ) | {z } targetautoencoder +  2 S X s =1 log P ( x 0 j x ( s ) ; !  ;    ) | {z } sourceautoencoder ; (4) where  1 and  2 arehyper-parametersforbalanc- ingthepreferencebetweenlikelihoodandautoen- coders. Notethattheobjectiveconsistsoffourparts: source-to-targetlikelihood,target-to-sourcelikeli- hood,targetautoencoder,andsourceautoencoder. Inthisway,ourapproachiscapableofexploiting abundantmonolingualcorporaofbothsourceand targetlanguages. Theoptimalmodelparametersaregivenby !   =argmax ( N X n =1 log P ( y ( n ) j x ( n ) ; !  )+  1 T X t =1 log P ( y 0 j y ( t ) ; !  ;    )+  2 S X s =1 log P ( x 0 j x ( s ) ; !  ;    ) ) (5)     =argmax ( N X n =1 log P ( x ( n ) j y ( n ) ;    )+  1 T X t =1 log P ( y 0 j y ( t ) ; !  ;    )+  2 S X s =1 log P ( x 0 j x ( s ) ; !  ;    ) ) (6) Itisclearthatthesource-to-targetandtarget-to- sourcemodelsareconnectedviatheautoencoder andcanhopefullyeachotherinjointtrain- ing. 2.4Training Weusemini-batchstochasticgradientdescentto trainourjointmodel.Foreachiteration,be- sidesthemini-batchfromtheparallelcorpus,we alsoconstructtwoadditionalmini-batchesbyran- domlyselectingsentencesfromthesourceandtar- getmonolingualcorpora.Then,gradientsarecol- lectedfromthesemini-batchestoupdatemodel parameters. Thepartialderivativeof J ( !  ;    ) withrespect tothesource-to-targetmodel !  isgivenby @J ( !  ;    ) @ !  = N X n =1 @ log P ( y ( n ) j x ( n ) ; !  ) @ !  +  1 T X t =1 @ log P ( y 0 j y ( t ) ; !  ;    ) @ !  +  2 S X s =1 @ log P ( x 0 j x ( s ) ; !  ;    ) @ !  : (7) Thepartialderivativewithrespectto    canbecal- culatedsimilarly. Unfortunately,thesecondandthirdtermsinEq. (7)areintractabletocalculateduetotheexponen- tialsearchspace.Forexample,thederivativein Chinese English #Sent. 2.56M Parallel #Word 67.54M 74.82M Vocab. 0.21M 0.16M #Sent. 18.75M 22.32M Monolingual #Word 451.94M 399.83M Vocab. 0.97M 1.34M Table1:Characteristicsofparallelandmonolin- gualcorpora. thethirdterminEq.(7)isgivenby P x 2X ( y ) P ( x j y ;    ) P ( y 0 j x ; !  ) @ log P ( y 0 j x ; !  ) @ !  P x 2X ( y ) P ( x j y ;    ) P ( y 0 j x ; !  ) : (8) Itisprohibitivelyexpensivetocomputethesums duetotheexponentialsearchspaceof X ( y ) . Alternatively,weproposetouseasubsetofthe fullspace ~ X ( y ) ˆX ( y ) toapproximateEq.(8): P x 2 ~ X ( y ) P ( x j y ;    ) P ( y 0 j x ; !  ) @ log P ( y 0 j x ; !  ) @ !  P x 2 ~ X ( y ) P ( x j y ;    ) P ( y 0 j x ; !  ) : (9) Inpractice,weusethetop- k listofcandidate translationsof y as ~ X ( y ) .As j ~ X ( y ) j˝Xj ( y ) j , itispossibletocalculateEq.(9)efby enumeratingallcandidatesin ~ X ( y ) .Inpractice, wethisapproximationresultsin improvementsand k =10 seemstosufto keepthebalancebetweenefyandtransla- tionquality. 3Experiments 3.1Setup WeevaluatedourapproachontheChinese- Englishdataset. AsshowninTable1,weusebothaparallel corpusandtwomonolingualcorporaasthetrain- ingset.TheparallelcorpusfromLDCconsistsof 2.56Msentencepairswith67.53MChinesewords and74.81MEnglishwords.Thevocabularysizes ofChineseandEnglishare0.21Mand0.16M,re- spectively.WeusetheChineseandEnglishparts oftheXinhuaportionoftheGIGAWORDcor- pusasthemonolingualcorpora.TheChinese monolingualcorpuscontains18.75Msentences with451.94Mwords.TheEnglishcorpuscontains 22.32Msentenceswith399.83Mwords.Thevo- cabularysizesofChineseandEnglishare0.97M and1.34M,respectively. Figure2:Effectofsamplesize k ontheChinese- to-Englishvalidationset. Figure3:Effectofsamplesize k ontheEnglish- to-Chinesevalidationset. ForChinese-to-Englishtranslation,weusethe NIST2006Chinese-Englishdatasetasthevali- dationsetforhyper-parameteroptimizationand modelselection.TheNIST2002,2003,2004, and2005datasetsserveastestsets.EachChi- nesesentencehasfourreferencetranslations.For English-to-Chinesetranslation,weusetheNIST datasetsinareversedirection:treatingthe Englishsentenceinthefourreferencetransla- tionsasasourcesentenceandtheoriginalinput Chinesesentenceasthesinglereferencetrans- lation.Theevaluationmetriciscase-insensitive BLEU(Papinenietal.,2002)ascalculatedbythe multi-bleu.perl script. Wecomparedourapproachwithtwostate-of- the-artSMTandNMTsystems: 1. M OSES (Koehnetal.,2007):aphrase-based SMTsystem; Figure4:EffectofOOVratioontheChinese-to- Englishvalidationset. Figure5:EffectofOOVratioontheEnglish-to- Chinesevalidationset. 2. RNN SEARCH (Bahdanauetal.,2015):an attention-basedNMTsystem. ForM OSES ,weusethedefaultsettingtotrain thephrase-basedtranslationontheparallelcorpus andoptimizetheparametersoflog-linearmodels usingtheminimumerrorratetrainingalgorithm (Och,2003).WeusetheSRILMtoolkit(Stolcke, 2002)totrain4-gramlanguagemodels. ForRNNS EARCH ,weusetheparallelcorpusto traintheattention-basedneuraltranslationmodels. Wesetthevocabularysizeofwordembeddings to30KforbothChineseandEnglish.Wefollow Luongetal.(2015)toaddressrarewords. OntopofRNNS EARCH ,ourapproachiscapa- bleoftrainingbidirectionalattention-basedneural translationmodelsontheconcatenationofparallel andmonolingualcorpora.Thesamplesize k isset to10.Wesetthehyper-parameter  1 =0 : 1 and  2 =0 whenweaddthetargetmonolingualcor- pus,and  1 =0 and  2 =0 : 1 forsourcemonolin- gualcorpusincorporation.Thethresholdofgra- dientclippingissetto 0 : 05 .Theparametersof ourmodelareinitializedbythemodeltrainedon parallelcorpus. 3.2EffectofSampleSize k Astheinferenceofourapproachisintractable,we proposetoapproximatethefullsearchspacewith thetop- k listofcandidatetranslationstoimprove efy(seeEq.(9)). Figure2showstheBLEUscoresofvariousset- tingsof k overtime.OnlytheEnglishmono- lingualcorpusisappendedtothetrainingdata. Weobservethatincreasingthesizeoftheapprox- imatesearchspacegenerallyleadstoimproved BLEUscores.Therearegapsbetween k =1 and k =5 .However,keepingincreas- ing k doesnotresultinimprovements anddecreasesthetrainingefy.Wethat k =10 achievesabalancebetweentrainingef ciencyandtranslationquality.AsshowninFig- ure3,similararealsoobservedonthe English-to-Chinesevalidationset.Therefore,we set k =10 inthefollowingexperiments. 3.3EffectofOOVRatio Givenaparallelcorpus,whatkindofmonolingual corpusismostforimprovingtransla- tionquality?Toanswerthisquestion,weinvesti- gatetheeffectof OOVratio ontranslationquality, whichisas ratio = P y 2 y J y= 2V D t K j y j ; (10) where y isatarget-languagesentenceinthemono- lingualcorpus T , y isatarget-languagewordin y , V D t isthevocabularyofthetargetsideofthepar- allelcorpus D . Intuitively,theOOVratioindicateshowasen- tenceinthemonolingualresemblestheparallel corpus.Iftheratiois0,allwordsinthemono- lingualsentencealsooccurintheparallelcorpus. Figure4showstheeffectofOOVratioon theChinese-to-Englishvalidationset.OnlyEn- glishmonolingualcorpusisappendedtothepar- allelcorpusduringtraining.Weconstructedfour monolingualcorporaofthesamesizeintermsof sentencepairs.ﬁ0%OOVﬂmeanstheOOVra- tiois0%forallsentencesinthemonolingualcor- pus.ﬁ10%OOVﬂsuggeststhattheOOVratiois nogreater10%foreachsentenceinthemono- lingualcorpus.Wethatusingamonolingual corpuswithalowerOOVratiogenerallyleadsto higherBLEUscores.Onepossiblereasonisthat low-OOVmonolingualcorpusisrelativelyeasier toreconstructthanitshigh-OOVcounterpartand resultsinbetterestimationofmodelparameters. Figure5showstheeffectofOOVratioonthe English-to-Chinesevalidationset.OnlyEnglish monolingualcorpusisappendedtotheparallel corpusduringtraining.Wethatﬁ0%OOVﬂ stillachievesthehighestBLEUscores. 3.4ComparisonwithSMT Table2showsthecomparisonbetweenM OSES andourwork.M OSES usedthemonolingual corporaasshowninTable1:18.75MChinese sentencesand22.32MEnglishsentences.We thatexploitingmonolingualcorporadramat- icallyimprovestranslationperformanceinboth Chinese-to-EnglishandEnglish-to-Chinesedirec- tions. Relyingonlyonparallelcorpus,RNN SEARCH outperformsM OSES trainedalsoonlyonpar- allelcorpus.Butthecapabilityofmaking useofabundantmonolingualcorporaenables M OSES toachievemuchhigherBLEUscoresthan RNN SEARCH onlyusingparallelcorpus. Insteadofusingallsentencesinthemonolin- gualcorpora,weconstructedsmallermonolingual corporawithzeroOOVratio:2.56MChinesesen- tenceswith47.51Mwordsand2.56MEnglish Englishsentenceswith37.47Mwords.Inother words,themonolingualcorporaweusedinthe experimentsaremuchsmallerthanthoseusedby M OSES . ByaddingEnglishmonolingualcorpus,our approachachievessubstantialimprovementsover RNN SEARCH usingonlyparallelcorpus(upto +4.7BLEUpoints).Inaddition,im- provementsarealsoobtainedoverM OSES using bothparallelandmonolingualcorpora(upto+3.5 BLEUpoints). AninterestingisthataddingEnglish monolingualcorporahelpstoimproveEnglish-to- ChinesetranslationoverRNN SEARCH usingonly parallelcorpus(upto+3.2BLEUpoints),sug- gestingthatourapproachiscapableofimproving NMTusingsource-sidemonolingualcorpora. IntheEnglish-to-Chinesedirection,weob- tainsimilarInparticular,addingChi- System TrainingData Direction NIST06 NIST02 NIST03 NIST04 NIST05 CE C E M OSES p   C ! E 32.48 32.69 32.39 33.62 30.23 E ! C 14.27 18.28 15.36 13.96 14.11 p  p C ! E 34.59 35.21 35.71 35.56 33.74 p p  E ! C 20.69 25.85 19.76 18.77 19.74 RNN SEARCH p   C ! E 30.74 35.16 33.75 34.63 31.74 E ! C 15.71 20.76 16.56 16.85 15.14 p  p C ! E 35.61  ++ 38.78  ++ 38.32  ++ 38.49  ++ 36.45  ++ E ! C 17.59 ++ 23.99 ++ 18.95 ++ 18.85 ++ 17.91 ++ p p  C ! E 35.01 ++ 38.20  ++ 37.99  ++ 38.16  ++ 36.07  ++ E ! C 21.12  ++ 29.52  ++ 20.49  ++ 21.59  ++ 19.97 ++ Table2:ComparisonwithM OSES andRNN SEARCH .M OSES isaphrase-basedstatisticalmachine translationsystem(Koehnetal.,2007).RNN SEARCH isanattention-basedneuralmachinetranslation system(Bahdanauetal.,2015).ﬁCEﬂdonatesChinese-Englishparallelcorpus,ﬁCﬂdonatesChinese monolingualcorpus,andﬁEﬂdonatesEnglishmonolingualcorpus.ﬁ p ﬂmeansthecorpusisincludedin thetrainingdataand  meansnotincluded.ﬁNIST06ﬂisthevalidationsetandﬁNIST02-05ﬂaretest sets.TheBLEUscoresarecase-insensitive.ﬁ*ﬂ:betterthanMOSES( p< 0 : 05 );ﬁ**ﬂ: betterthanMOSES( p< 0 : 01 );ﬁ+ﬂ:betterthanRNN SEARCH ( p< 0 : 05 ); ﬁ++ﬂ:betterthanRNN SEARCH ( p< 0 : 01 ). Method TrainingData Direction NIST06 NIST02 NIST03 NIST04 NIST05 CE C E Sennrichetal.(2015) p  p C ! E 34.10 36.95 36.80 37.99 35.33 p p  E ! C 19.85 28.83 20.61 20.54 19.17 thiswork p  p C ! E 35.61  38.78  38.32  38.49  36.45  E ! C 17.59 23.99 18.95 18.85 17.91 p p  C ! E 35.01  38.20  37.99  38.16 36.07  E ! C 21.12  29.52  20.49 21.59  19.97  Table3:ComparisonwithSennrichetal.(2015).BothSennrichetal.(2015)andourapproachbuild ontopofRNN SEARCH toexploitmonolingualcorpora.TheBLEUscoresarecase-insensitive.ﬁ*ﬂ: betterthanSennrichetal.(2015)( p< 0 : 05 );ﬁ**ﬂ:cantlybetterthanSennrichetal. (2015)( p< 0 : 01 ). nesemonolingualcorpusleadstomore toEnglish-to-ChinesetranslationthanaddingEn- glishmonolingualcorpus.Wealsotriedtouse bothChineseandEnglishmonolingualcorpora throughsimplysettingallthe  to 0 : 1 butfailed toobtainfurtherimprovements. Therefore,ourngscanbesummarizedas follows: 1. Addingtargetmonolingualcorpusimproves overusingonlyparallelcorpusforsource-to- targettranslation; 2. Addingsourcemonolingualcorpusalsoim- provesoverusingonlyparallelcorpusfor source-to-targettranslation,buttheimprove- mentsaresmallerthanaddingtargetmono- lingualcorpus; 3. Addingbothsourceandtargetmonolingual corporadoesnotleadtofurther improvements. 3.5ComparisonwithPreviousWork Were-implementedSennrichetal.(2015)'s methodontopofRNN SEARCH asfollows: 1. Trainthetarget-to-sourceneuraltranslation model P ( x j y ;    ) ontheparallelcorpus D = fh x ( n ) ; y ( n ) ig N n =1 . 2. Thetrainedtarget-to-sourcemodel     is usedtotranslateatargetmonolingualcorpus T = f y ( t ) g T t =1 intoasourcemonolingual corpus ~ S = f ~ x ( t ) g T t =1 . 3. Thetargetmonolingualcorpusispairedwith itstranslationstoformapseudoparallelcor- pus,whichisthenappendedtotheoriginal parallelcorpustoobtainalargerparallelcor- pus: ~ D = D[h ~ S ; Ti . 4. Re-trainthethesource-to-targetneuraltrans- lationmodelon ~ D toobtainthemodel parameters !   . Monolingual hongsenshuo,ruguoyounajiafamugongsidanganyishenshifa,name tamenjiangzihuiqiancheng. Reference hongsensaid,ifany loggingcompanies daretodefythelaw,thentheywill destroytheirownfuture . Translation hunsensaid,ifanyof thosecompanies daredefythelaw,thentheywill havetheirownfate .[ iteration0 ] hunsensaidifany treefellingcompany daredtobreakthelaw,thenthey would killthemselves .[ iteration40K ] hunsensaidifany loggingcompanies daretodefythelaw,theywould destroythefuturethemselves .[ iteration240K ] Monolingual danyidanpanjuejieguozuizhongqueding,zebixuzai30tianneizhixing. Reference Butonce theverdictis ,itmustbeexecutedwithin30days . Translation however, intheanalysis ,itmustbecarriedoutwithin30days. [ iteration0 ] however, intheanalysis ,thedecisionwillbecarriedoutwithin 30days.[ iteration40K ] however,once theverdictis ,itmustbecarriedoutwithin 30days.[ iteration240K ] Table4:Exampletranslationsofsentencesinthemonolingualcorpusduringsemi-supervisedlearning. Weourapproachiscapableofgeneratingbettertranslationsofthemonolingualcorpusovertime. Table3showsthecomparisonresults.Boththe twoapproachesusethesameparallelandmono- lingualcorpora.Ourapproachachieves cantimprovementsoverSennrichetal.(2015)in bothChinese-to-EnglishandEnglish-to-Chinese directions(upto+1.8and+1.0BLEUpoints). OnepossiblereasonisthatSennrichetal.(2015) onlyusethepesudoparallelcorpusforparame- terestimationforonce(seeStep4above)while ourapproachenablessource-to-targetandtarget- to-sourcemodelstointeractwitheachotheritera- tivelyonbothparallelandmonolingualcorpora. Tosomeextent,ourapproachcanbeseenasan iterativeextensionofSennrichetal.(2015)'sap- proach:afterestimatingmodelparametersonthe pseudoparallelcorpus,thelearnedmodelparam- etersareusedtoproduceabetterpseudoparallel corpus.Table4showsexampleViterbitransla- tionsontheChinesemonolingualcorpusoverit- erations: x  =argmax x n P ( y 0 j x ; !  ) P ( x j y ;    ) o : (11) WeobservethatthequalityofViterbitransla- tionsgenerallyimprovesovertime. 4RelatedWork Ourworkisinspiredbytwolinesofresearch:(1) exploitingmonolingualcorporaformachinetrans- lationand(2)autoencodersinunsupervisedand semi-supervisedlearning. 4.1ExploitingMonolingualCorporafor MachineTranslation Exploitingmonolingualcorporaforconventional SMThasattractedintensiveattentioninrecent years.Severalauthorshaveintroducedtransduc- tivelearningtomakefulluseofmonolingual corpora(Uefetal.,2007;BertoldiandFed- erico,2009).Theyuseanexistingtranslation modeltotranslateunseensourcetext,whichcan bepairedwithitstranslationstoformapseudo parallelcorpus.Thisprocessiteratesuntilcon- vergence.WhileKlementievetal.(2012)pro- poseanapproachtoestimatingphrasetranslation probabilitiesfrommonolingualcorpora,Zhang andZong(2013)directlyextractparallelphrases frommonolingualcorporausingretrievaltech- niques.Anotherimportantlineofresearchisto treattranslationonmonolingualcorporaasade- ciphermentproblem(RaviandKnight,2011;Dou etal.,2014). CloselyrelatedtoGulccehreetal.(2015)and Sennrichetal.(2015),ourapproachfocuseson learningbirectionalNMTmodelsviaautoen- codersonmonolingualcorpora.Themajorad- vantagesofourapproacharethetransparencyto networkarchitecturesandthecapabilitytoexploit bothsourceandtargetmonolingualcorpora. 4.2AutoencodersinUnsupervisedand Semi-SupervisedLearning Autoencodersandtheirvariantshavebeenwidely usedinunsuperviseddeeplearning((Vincentet al.,2010;Socheretal.,2011;Ammaretal.,2014), justtonameafew).Amongthem,Socheretal. (2011)'sapproachbearscloseresemblancetoour approachastheyintroducesemi-supervisedrecur- siveautoencodersforsentimentanalysis.Thedif- ferenceisthatweareinterestedinmakingabet- teruseofparallelandmonolingualcorporawhile theyconcentrateoninjectingpartialsupervision toconventionalunsupervisedautoencoders.Dai andLe(2015)introduceasequenceautoencoder toreconstructanobservedsequenceviaRNNs. Ourapproachdiffersfromsequenceautoencoders inthatweusebidirectionaltranslationmodelsas encodersanddecoderstoenablethemtointeract withintheautoencoders. 5Conclusion Wehavepresentedasemi-supervisedapproachto trainingbidirectionalneuralmachinetranslation models.Thecentralideaistointroduceautoen- codersonthemonolingualcorporawithsource-to- targetandtarget-to-sourcetranslationmodelsas encodersanddecoders.ExperimentsonChinese- EnglishNISTdatasetsshowthatourapproach leadstoimprovements. AsourmethodissensitivetotheOOVspresent inmonolingualcorpora,weplantointegrateJean etal.(2015)'stechniqueonusingverylargevo- cabularyintoourapproach.Itisalsonecessaryto furthervalidatetheeffectivenessofourapproach onmorelanguagepairsandNMTarchitectures. Anotherinterestingdirectionistoenhancethe connectionbetweensource-to-targetandtarget-to- sourcemodels(e.g.,lettingthetwomodelsshare thesamewordembeddings)tohelpthem morefrominteractingwitheachother. Acknowledgements ThisworkwasdonewhileYongChengwasvis- itingBaidu.Thisresearchissupportedbythe 973Program(2014CB340501,2014CB340505), theNationalNaturalScienceFoundationofChina (No.61522204,61331013,61361136003),1000 TalentPlangrant,TsinghuaInitiativeResearch Programgrants20151080475andaGoogleFac- ultyResearchAward.Wesincerelythankthe viewersfortheirvaluablesuggestions. References WaleedAmmar,ChrisDyer,andNoahSmith.2014. Conditionalrandomautoencodersforunsuper- visedstructredprediction.In ProceedingsofNIPS 2014 . DzmitryBahdanau,KyungHyunCho,andYoshua Bengio.2015.Neuralmachinetranslationby jointlylearningtoalignandtranslate.In Proceed- ingsofICLR . NicolaBertoldiandMarcelloFederico.2009.Domain adaptationforstatisticalmachinetranslationwith monolingualresources.In ProceedingsofWMT . PeterF.Brown,StephenA.DellaPietra,VincentJ. DellaPietra,andRobertL.Mercer.1993.The mathematicsofstatisticalmachinetranslation:Pa- rameterestimation. ComputationalLinguisitics . DavidChiang.2005.Ahierarchicalphrase-based modelforstatisticalmachinetranslation.In Pro- ceedingsofACL . KyunhyunCho,BartvanMerri ¨ enboer,DzmitryBah- danau,andYoshuaBengio.2014.Ontheproperties ofneuralmachinetranslation:Encoder-decoderap- proaches.In ProceedingsofSSST-8 . AndrewM.DaiandQuocV.Le.2015.Semi- supervisedsequencelearning.In Proceedingsof NIPS . QingDou,AshishVaswani,andKevinKnight.2014. Beyondparalleldata:Jointwordalignmentandde- ciphermentimprovesmachinetranslation.In Pro- ceedingsofEMNLP . CaglarGulccehre,OrhanFirat,KelvinXu,Kyunghyun Cho,Lo ¨ Barrault,Huei-ChiLin,FethiBougares, HolgerSchwenk,andYoshuaBengio.2015.On usingmonolingualcorporainneuralmachinetrans- lation.arXiv:1503.03535[cs.CL]. SeppHochreiterandJ ¨ urgenSchmidhuber.1993.The mathematicsofstatisticalmachinetranslation:Pa- rameterestimation. ComputationalLinguisitics . SebastienJean,KyunghyunCho,RolandMemisevic, andYoshuaBengio.2015.Onusingverylargetar- getvocabularyforneuralmachinetranslation.In ProceedingsofACL . NalKalchbrennerandPhilBlunsom.2013.Recur- rentcontinuoustranslationmodels.In Proceedings ofEMNLP . AlexandreKlementiev,AnnIrvine,ChrisCallison- Burch,andDavidYarowsky.2012.Towardstatisti- calmachinetranslationwithoutparalelcorpora.In ProceedingsofEACL . PhilippKoehn,FranzJ.Och,andDanielMarcu.2003. Statisticalphrase-basedtranslation.In Proceedings ofNAACL . PhilippKoehn,HieuHoang,AlexandraBirch,Chris Callison-Burch,MarcelloFederico,NicolaBertoldi, BrookeCowan,WadeShen,ChristineMoran, RichardZens,ChrisDyer,OndrejBojar,Alexandra Constantin,andEvanHerbst.2007.Moses:Open sourcetoolkitforstatisticalmachinetranslation.In ProceedingsofACL(demosession) . Minh-ThangLuong,IlyaSutskever,QuocV.Le,Oriol Vinyals,andWojciechZaremba.2015.Addressing therarewordprobleminneuralmachinetranslation. In ProceedingsofACL . FranzOch.2003.Minimumerrorratetraininginsta- tisticalmachinetranslation.In ProceedingsofACL . KishorePapineni,SalimRoukos,ToddWard,andWei- JingZhu.2002.Bleu:amethofforautomaticeval- uationofmachinetranslation.In Proceedingsof ACL . SujithRaviandKevinKnight.2011.Decipheringfor- eignlanguage.In ProceedingsofACL . RicoSennrich,BarryHaddow,andAlexandraBirch. 2015.Improvingnerualmachinetranslationmodels withmonolingualdata.arXiv:1511.06709[cs.CL]. RichardSocher,JeffreyPennington,EricHuang,An- drewNg,andChristopherManning.2011.Semi- supervisedrecursiveautoencodersforpredicting sentimentdistributions.In ProceedingsofEMNLP . AndreasStolcke.2002.Srilm-amextensiblelan- guagemodelingtoolkit.In ProceedingsofICSLP . IlyaSutskever,OriolVinyals,andQuocV.Le.2014. Sequencetosequencelearningwithneuralnet- works.In ProceedingsofNIPS . NicolaUefGholamrezaHaffari,andAnoop Sarkar.2007.Trasductivelearningforstatistical machinetranslation.In ProceedingsofACL . PascalVincent,HugoLarochelle,IsabelleLajoie, YoshuaBengio,andPierre-AutoineManzagol. 2010.Stackeddenoisingautoencoders:Learning usefulrepresentationsinadeepnetworkwithalocal denoisingcriterion. JournalofMachineLearning Research . JiajunZhangandChengqingZong.2013.Learning aphrase-basedtranslationmodelfrommonolingual datawithapplicationtodomainadaptation.In Pro- ceedingsofACL .  
SelectiveTransferLearningforCrossDomainRecommendation ZhongqiLu  ErhengZhong  LiliZhao  EvanWeiXiang  WeikePan  QiangYang  Abstract C ollaborative f iltering(CF)aimstopredictusers'ratingsonitems accordingtohistoricaluser-itempreferencedata.Inmanyreal- worldapplications,preferencedataareusuallysparse,whichwould makemodelsovandfailtogiveaccuratepredictions.Recently, severalresearchworksshowthatbytransferringknowledgefrom somemanuallyselectedsourcedomains,thedatasparsenessprob- lemcouldbemitigated.Howeverformostcases,partsofsource domaindataare notconsistent withtheobservationsinthetar- getdomain,whichmaymisguidethetargetdomainmodelbuild- ing.Inthispaper,weproposeanovelcriterionbasedonempirical predictionerroranditsvariancetobettercapturetheconsistency acrossdomainsinCFsettings.Consequently,weembedthiscri- terionintoaboostingframeworktoperform selective knowledge transfer.Comparingtoseveralstate-of-the-artmethods,weshow thatourproposedselectivetransferlearningframeworkcansignif- icantlyimprovetheaccuracyofratingpredictiononseveralreal- worldrecommendationtasks. Keywords :TransferLearning;CollaborativeFiltering;CrossDo- mainRecommendation; 1Introduction Recommendationsystemsattempttorecommenditems(e.g., movies,TV,books,news,images,webpages,etc.)thatare likelytobeofinteresttousers.Asastate-of-the-arttech- niqueforrecommendationsystems,collaborative aimsatpredictingusers'ratingsonasetofitemsbasedon acollectionofhistoricaluser-itempreferencerecords.In thereal-worldrecommendationsystems,althoughtheitem spaceisoftenverylarge,usersusuallyrateonlyasmallnum- berofitems.Thus,theavailableratingdatacanbeextremely sparseforeachuser,whichisespeciallytruefornewonline services.SuchdatasparsityproblemmaymakeCFmod- elsovthelimitedobservationsandresultinlow-quality predictions. Inrecentyears,differenttransferlearningtechniques havebeendevelopedtoimprovetheperformanceoflearning amodelviareusingsomeinformationfromotherrelevant systemsforcollaborative[11,26].Andwiththe increasingunderstandingsofauxiliarydatasources,some  HongKongUniversityofScience&Technology. f zluab,ezhong, skyezhao,wxiang,weikep,qyang g @cse.ust.hk works(like[5,21])starttoexploredatafrommultiple sourcedomainstoachievemorecomprehensiveknowledge transfer.However,thesepreviousmethodsover-trustthe sourcedataandassumethatthesourcedomainsfollowthe verysimilardistributionswiththetargetdomain,whichis usuallynottrueinthereal-worldapplications,especially underthecrossdomainCFsettings.Forexample,inalocal musicratingwebsite,nativesmaygivetrustfulratingsfor thetraditionalmusic;whileinaninternationalmusicrating website,theratingsonthosetraditionalmusiccouldbe diverseduetotheculturedifferences:thoseuserswithgood culturebackgroundwouldconstantlygivetrustfulratings, otherscouldbeinaccurate.Ifthetargetdomaintaskisthe musicrecommendationofastartuplocalwebsite,obviously wedonotwantalltheInternationalwebsite'sdataassource domainwithoutselection.Tobettertacklethecrossdomain CFproblems,wefacethechallengetotellhowconsistentthe dataoftargetandsourcedomainsareandadoptonlythose consistentsourcedomaindatawhiletransferringknowledge. Severalresearchworks(like[2])havebeenproposedto performinstanceselectionacrossdomainsfor tasksbasedonempiricalerror.Buttheycannotbeadopted tosolveCFproblemsdirectly.Especiallywhenthetarget domainissparse,becauseofthelimitedobservationsof user'sratingsontheitemsinthetargetdomain,gettinga lowempiricalerroroccasionallyinthetargetdomaindoes notmeanthesourcedomainsaretrulyhelpfulinbuilding agoodmodel.Inotherwords,theinconsistentknowledge fromsourcedomainsmaydominatethetargetdomainmodel buildingandhappentothefewobservationsinthetarget domain,whichgiveshighaccuracyunacceptably. Wetakecarefulanalysisonthisproblemandinour observationonthemusicratingexample,someusers,such asdomainexperts,followstandardcriteriatorateandhence shareaconsistentdistributionoverthemutualitemsetacross domains.Andfurther,wethisconsistencycanbebetter describedbyaddingthevarianceofempiricalerrorproduced bythemodel.Thesmallerthevarianceofempiricalerroron predictionsforauser,themorelikelythisuserisconsistent withthosefromotherdomains.Andwewouldliketoadopt thosewhoaremorelikelytoshareconsistentpreferencesto performknowledgetransferacrossdomains.Basedonthis observation,weproposeanewcriterionusingbothempirical erroranditsvariancetocapturetheconsistencybetween sourceandtargetdomains.Asanimplementation,we Figure1: Selectivetransferlearningwithmultiplesources.Therow illustratesthecasewhereitemsthatareinthetargetdomainalsoappearin thesourcedomains.Areal-worldexampleistheratingpredictionforthe moviesthatappearinseveralwebsitesinvariousforms;thesecondrow illustratesthecasewhereusersthatareinthetargetdomainalsoappearin thesourcedomains.Areal-worldexampleistheDoubanrecommendation system,whichprovidesmusic,bookandmovierecommendationforusers. embedthiscriterionintoaboostingframeworkandpropose anovelselectivetransferlearningapproachforcollaborative (STLCF).STLCFworksinaniterativewaytoadjust theimportanceofsourceinstances,wherethosesourcedata withlowempiricalerroraswellaslowvariancewillbe selectedtohelpbuildtargetmodels. Ourmaincontributionsaresummarizedasfollows:  First,wethatselectingconsistentauxiliarydata forthetargetdomainisimportantforthecross-domain collaborativewhiletheconsistencybetween sourceandtargetdomainisbymultiple factors.Todescribethesefactors,weproposeanovel criterion,basedonbothempiricalerroranditsvariance.  Second,weproposea selective transferlearningframe- workforcollaborative-anextensionofthe boostingbasedtransferlearningalgorithmthattakethe abovefactorsintoconsideration,sothatthesparseness issueintheCFproblemscanbebettertackled.  Third,theproposedframeworkisgeneral,wheredif- ferentbasemodelscanbeembedded.Weproposean implementationbasedonGaussianprobabilitylatent semanticanalysis,whichdemonstratestheproposed frameworkcansolvethesparsenessproblemonvarious real-worldapplications. 2Preliminaries 2.1ProblemSettings Supposethatwehaveatargettask D wherewewishtosolvetheratingpredictionproblem. Takingtheregularrecommendationsystemforillustration, D isassociatedwith m d usersand n d itemsdenotedby U d and V d respectively.Inthistask,weobserveasparsematrix X ( d ) 2 R m d  n d withentries x d ui .Let R ( d ) = f ( u;i;r ): r = x d ui ; where x d ui 6 =0 g denotethesetofobservedlinks inthesystem.Fortheratingrecommendationsystem, r can eithertakenumericalvalues,forexample [1 ; 5] ,orbinary values f 0 ; 1 g .Weaimtotransferknowledgefromother N sourcedomains S = f S t g N t =1 witheachsourcedomain S t contains m t s usersand n t s itemsdenotedby U s t and V s t .Similartothetargetdomain,eachsourcedomain S t containsparsematrices X ( s t ) 2 R m s t  n s t andobserved links R ( s t ) = f ( u;i;r ): r = x s t ui ; where x s t ui 6 =0 g . ThesettingsofSTLCFareillustratedinFigure1.We adoptasettingcommonlyusedintransferlearningfor collaborativeeithertheitemsortheusersthatarein thetargetdomainalsoappearinthesourcedomains.Inthe followingderivationanddescriptionofourSTLCFmodel, fortheconvenienceofinterpretation,wefocusonthecase thattheusersetissharedbybothtargetdomainandthe sourcedomains.Thecasethattheitemsetissharedcan beeasilytackledinasimilarmanner. Undertheassumptionthattheobservation R ( f d;s t g ) is obtainedwith u and i beingindependent,weformally aco-occurrencemodelinboththetargetandthesource domainstosolvethecollaborativeproblem: Pr ( x f d;s t g ui = r;u;i )= Pr ( u ) Pr ( i j u ) Pr ( x f d;s t g ui = r j u;i ) = Pr ( u ) Pr ( i ) Pr ( x f d;s t g ui = r j u;i ) / Pr ( x f d;s t g ui = r j u;i ) Inthefollowing,basedonGaussianprobabilisticla- tentsemanticanalysis(GPLSA),wepresenta transferlearningmodelforcollaborative-trans- ferredGaussianprobabilisticlatentsemanticanalysis(TG- PLSA)asanexample,whichisdesignedtointegrateinto ourlaterproposedframeworkasabasemodel.Afterthat, wepresentourselectivetransferlearningforcollaborative (STLCF)toperformknowledgetransferbyanalyz- ingtheinconsistencybetweentheobserveddataintargetdo- mainandthesourcedomains.Carefulreadersshallnotice thatotherthantheTGPLSAexample,STLCFiscompatible tousevariousgenerativemodelsasthebasemodel. 2.2CollaborativeFilteringviaGaussianProbabilistic LatentSemanticAnalysis(GPLSA) Following[7],for everyuser-itempair,weintroducehiddenvariables Z with latenttopics z ,sothatuser u anditem i arerendered conditionallyindependent.Withobservationsofitemset V , userset U andratingset R inthesourcedomain,we Pr ( x ui = r j u;i )= X z Pr ( x ui = r j i;z ) Pr ( z j u ) WefurtherinvestigatetheuseofaGaussianmodelfor estimating p ( x ui = r j u;i ) byintroducing  iz 2R forthe meanand ˙ 2 iz forthevarianceoftheratings.Withthese,we aGaussianmixturemodelforasingledomainas: Pr ( x ui = r j u;i )= X z Pr ( z j u ) Pr ( r ;  iz ;˙ iz ) where Pr ( z j u ) isthetopicdistributionoverusers,and Pr ( r ;  iz ;˙ iz ) followsaGaussiandistribution. Maximumlikelihoodestimationamountstominimize: L =  X r 2 R X x ui 2 X log[ Pr ( x ui = r j u;i ;  )] (2.1) where  referstoaparticularmodel. 2.3TransferLearningforCollaborativeFiltering (TLCF) Whenthetargetdata X ( d ) issparse,GPLSAmay ovthelimitedobserveddata.Followingthesimilaridea in[24],weextendGPLSAtotheTransferredGaussianProb- abilisticLatentSemanticAnalysis(TGPLSA)model.Again weuse s todenoteindexofthesourcedomainwherethe knowledgecomefrom,and d todenotetheindexofthetar- getdomainwheretheknowledgeisreceived.Forsimplicity, wepresenttheworkwithonesourcedomain,andthismodel canbeeasilyextendedtomultiplesourcedomains.More- over,weassumealltheusersappearinboththesourcedo- mainsandthetargetdomain.Suchscenariosarecommonin thereal-worldsystems,likeDouban 1 . TGPLSAjointlylearnthetwomodelsforboththe sourcedomainandthetargetdomainusingarelativeweight parameter 2  .Sincetheitemsets V s and V d aredifferentor evendisjointwitheachother,therecouldbeinconsistency acrossdomainsaswediscussedinSection1.Clearly,the moreconsistentsourceandtargetdomainsare,themorehelp targettaskcouldgetfromsourcedomain(s).Weproposea weightedTLCFmodel(wTGPLSA)tofurtheranalyzethis inconsistencyinourworkbylearningitemweightvectors w s = f w s i g n s i =1 and w d = f w d i g n d i =1 oftheinstancesin sourceandtargetdomainrespectively.Then,theobjective functioninEq.(2.1)canbeextendedas: (2.2) L =  X r 2 R (  X x s ui 2 X s log( w s i  Pr ( x s ui = r j u s ;i s ;  s )) +(1   ) X x d ui 2 X d log( w d i  Pr ( x d ui = r j u d ;i d ;  d ))) Noticethatinoursetting,either u s = u d or i s = i d .We adopttheexpectation-maximization(EM)algorithm,astan- dardmethodforstatisticalinference,tothemaximum log-likelihoodestimatesofEq.(2.2).Detailsofderivations canbefoundintheappendix. 1 http://www.douban.com-awidelyusedonlineserviceinChina,which providesmusic,bookandmovierecommendations. 2  2 (0 ; 1) ,whichisintroducedtorepresentthetradeoffofsourceand targetinformation. Algorithm1 SelectiveTLCF. Input: X d , X s , T X d 2 R m  n d :thetargettrainingdata X s 2 R m  n s :theauxiliarysourcedata G :theweightedTLCFmodelwTGPLSA T :numberofboostingiterations Initialize: Initialize w s : w s i   1 n s , w d : w d i   1 n d for iter =1to T do Step1: Apply G togenerateaweaklearner G ( X d ; X s ; w d ; w s ) thatminimizeEq.(2.2) Step2: Getweakhypothesisforboththe d and s domains h iter : X d ; X s ! ^ X d ; ^ X s Step3: Calculateempiricalerror E d and E s using Eq.(3.6) Step4: Calculateweight  s k foreachsource domain s k usingEq.(3.13) Step5: Choosemodelweight  iter viaEq.(3.9) Step6: Updatesourceitemweight w s viaEq.(3.12) Step7: Updatetargetitemweight w d viaEq.(3.11) endfor Output: Hypothesis Z = H ( X ( d ) )= T t =1  t h t ( X ( d ) ) 3SelectiveTLCF Aswehavediscussedbefore,usingthesourcedomaindata withoutselectionmayharmthetargetdomainlearning.By proposingtheselectiveknowledgetransferwiththenovel factors(empiricalerrorandvarianceofempiricalerror), wecomeupwiththedetailsofSelectiveTransferLearning frameworkforCFinthissection.Asillustratedinthesecond exampleinFigure1wherethedomainshavemutualuserset, wewouldliketotransferknowledgeofthoseitems'records thatconsistentlytheuser'spreferences.Becauseof ourthattheconsistentrecordshavesmallempirical errorandvariance,theselectionshallconsiderthesetwofac- tors.Weembedthesetwofactorsintoaboostingframework, wherethesourcedatawithsmallempiricalerrorandvari- ancereceivehigherweightssincetheyareconsistentwith thetargetdata.Thisboostingframeworkmodelsthecross- domainCFfromtwoaspects:ononehand,wetakemore careofthosemis-predictedtargetinstances;ontheother hand,weautomaticallyidentifytheconsistencyofthesource domainsduringthelearningprocessandselectiveusethose sourcedomainswithmoretrustfulinformation. AsshowninAlgorithm1,ineachiteration,weapply basemodelTGPLSAoverweightedinstancestobuilda weaklearner G (  ) andhypothesis h iter .Thentoupdate thesourceandtargetitemweights,domainlevel weight  s k ischosenforeachsourcedomain s k based ondomainlevelconsistency[4].And  iter forbase modelisalsoupdated,consideringempiricalerrorsand variances.Accordingly,theweightsofmis-predictedtarget itemsareincreasedandtheweightsofthoselesshelpful sourcedomainsaredecreasedineachiteration.The ensembleisgivenbyanadditivemodel,whichgiveslarger weightstothehypotheseswithlowererrors.Weprovidea detailedderivationofSTLCFintherestofthissection. Inpreviousworksincollaborativethemean absoluteerror(MAE)isusuallychosenasthelossfunction. InadditiontotheMAEloss,ifwetoleratesomeprediction error ˝ ,we (3.3) l 1 ( X  i ; ^ X  i )= 8 > > < > > :  1 ; X x ui 2 X  i j ^ x ui  x ui j ˝  nnz ( X  i ) +1 ; X x ui 2 X  i j ^ x ui  x ui j >˝  nnz ( X  i ) where nnz (  ) isthenumberofobservedratings. X  i and ^ X  i denotethetruevaluesandpredictionsrespectively.We mayalsotheitemlevelMAEerrorfortargetdomain withrespectto ˝ as:  d i = l 1 ( X d  i ; ^ X d  i ) (3.4) Tofacilitatetheoptimization,weconsiderthefollowing exponentiallossforempiricalriskminimization: l 2 ( i )= l 2 ( X d  i ; ^ X d  i )= e  d i (3.5) Asstatedinprevioussection,thelowervarianceofempirical errorscanprovidemoreconsistencyestimation, wecombinethesefactorsandreformulatethelossfunction: L = n d X i =1 l 2 ( i )+  v u u t n d X i>j ( l 2 ( i )  l 2 ( j )) 2 (3.6) Aboveall,themodelminimizetheabovequantityforsome scalar > 0 : Assumethatthefunctionofinterest H forpredictionis composedofthehypothesis h t fromeachweaklearner.The functiontobeoutputwouldconsistofthefollowingadditive modeloverthehypothesisfromtheweaklearners: ^ x d ui = f ( x d ui )= X t =1  t h t ( x d ui ) (3.7) where  t 2 R + . Sinceweareinterestedinbuildinganadditivemodel, weassumethatwealreadyhaveafunction h (  ) .Subse- quently,wederiveagreedyalgorithmtoobtainaweak learner G t (  ) andapositivescalar  t suchthat f (  )= h (  )+  t G t (  ) . Inthefollowingderivation,fortheconvinceofpresen- tation,weomitthemodelindex t ,anduse G torepresent G t ,  torepresent  t . By  1 =(1+( n  1)  ) ,  2 =(2  2  ) ,  , w d i = e l 1 ( h ( X d  i ) ; X d  i ) and G d i = l 1 ( G ( X d  i ) ; X d  i ) ,Eq.(3.6) canbeequivalentlyposedasoptimizingthefollowingloss withrespectto  : (3.8) L =  1 ( X i 2 I ( w d i ) 2 e 2  + X i 2 J ( w d i ) 2 e  2  )+  2 n d X i>j : i;j 2 I w d i w d j e 2  +  2 n d X i>j : i;j 2 J w d i w d j e  2  +  2 n d X i>j : i 2 I;j 2 Jori 2 J;j 2 I w d i w d j Forbrevity,wethefollowingsetsofindicesas I = f i : G d i =+1 g and J = f i : G d i =  1 g .Here J denotes thesetofitemswhosepredictionby G (  ) fallsintothefault tolerablerange,while I denotestherestset.Bymakingthe lasttransformationinEq.(3.8)equaltozero,weget: (3.9)  = 1 4 log   (1   )( P i 2 I w d i ) 2 + n d P i 2 I ( w d i ) 2 (1   )( P i 2 J w d i ) 2 + n d P i 2 J ( w d i ) 2 ! Ifweset  =0 ,thenitisreducedtotheformofAdaBoost: (3.10)  = 1 4 log   ( P i 2 I w d i ) 2 ( P i 2 J w d i ) 2 ! = 1 2 log   ( P i 2 I w d i ) ( P i 2 J w d i ) ! Finally,theupdatingrulefor w d i is w d i   w d i e (   d i ) (3.11) Andfortheinstanceweight w d i inthesourcedomain,wecan alsoadoptthesimilarupdatingruleinEq.(3.11). Otherthantheinstancelevelselectiondiscussedabove, wealsowanttoperformthedomainlevelselectionto penalizethosedomainsthatarelikelytobeirrelevant,so thatthedomainswithmorerelevantinstancesspeakloudly. Followingtheideaoftask-basedboosting[3],wefurther introduceare-weightingfactor  foreachsourcedomain tocontroltheknowledgetransfer.Soweformulatethe updatingrulefor w s i tobe: w s i   w s i e (   s i   ) (3.12) where  canbesetgreedilyinproportiontotheperformance gainofthesinglesourcedomaintransferlearning:  = P w d i ( " i  ~" i ) jj w d jj 1 (3.13) where " i isthetrainingerrorofthetransferlearningmodel, and ~" i isthetrainingerrorofthenon-transferlearningmodel, whichutilizesonlytheobservedtargetdomaindata. Table1: Datasetsinourexperiments. Notation DataSet DataType InstancesNo. D1 DoubanMusic Rating[1,5] 1 : 2  10 6 D2 DoubanBook Rating[1,5] 5 : 8  10 5 D3 DoubanMovie Rating[1,5] 1 : 4  10 6 D4  Rating[1,5] 1 : 8  10 4 D5 Wikipedia EditingLog 1 : 1  10 6 D6 IMDB Hyperlink 5 : 0  10 3 4Experiments 4.1DataSetsandExperimentalSettings Weevaluate theproposedmethodonfourdatasources: 3 ,Douban IMDB 4 ,andWikipedia 5 usereditingrecords.The ratingdatacontainsmorethan 100 millionratingswith valuesin f 1 ; 2 ; 3 ; 4 ; 5 g ,whicharegivenbymorethan 4 : 8  10 5 usersonaround 1 : 8  10 4 movies.Doubancontains movie,bookandmusicrecommendations,withratingvalues alsoin f 1 ; 2 ; 3 ; 4 ; 5 g .IMDBhyperlinkgraphisemployed asameasureofsimilaritybetweenmovies.Inthegraph, eachmoviebuildslinkstoits 10 mostsimilarmovies.The Wikipediausereditingrecordsprovidea f 0 ; 1 g indicatorof whetherauserconcernsornotaboutacertainmovie. Thedatasetsusedintheexperimentsaredescribedas follows.Fortoretaintheoriginalfeaturesofthe userswhilekeepingthesizeofthedatasetsuitablefor theexperiments,wesampledasubsetof 10 ; 000 users.In Doubandatasets,weobtained 1 : 2  10 6 ratingson 7 : 5  10 3 music, 5 : 8  10 5 ratingson 3 : 5  10 3 books,and 1 : 4  10 6 ratingson 8  10 3 movies,givenby 1 : 1  10 4 users.Forboth theIMDBdatasetandtheWikipediadataset,we thembymatchingthemovietitlesinboththeandthe DoubanMoviedatasets.Afterpre-processing,theIMDB hyperlinkdatasetcontains ˘ 5  10 3 movies.TheWikipedia usereditingrecordsdatasethas 1 : 1  10 6 editinglogsby 8 : 5  10 3 usersonthesame ˘ 5  10 3 moviesasIMDB dataset.Topresentourexperiments,weusetheshorthand notationslistedinTable1todenotethedatasets. Weevaluatetheproposedalgorithmonvecross- domainrecommendationtasks,asfollows:  Thetaskistosimulatethecross-domaincollab- orativeusingthedataset.Thesam- pleddataispartitionedintotwopartswithdisjointsets ofmoviesbutidenticalsetofusers.Onepartconsists ofratingsgivenby 8 ; 000 movieswith 1 : 6% density, whichservesasthesourcedomain.Theremaining 7 ; 000 moviesareusedasthetargetdomainwithdif- ferentlevelsofsparsitydensity.  Thesecondtaskisareal-worldcross-domainrecom- mendation,wherethesourcedomainisDoubanBook 3 http://www 4 http://www.imdb.com 5 http://en.wikipedia.org andthetargetdomainisDoubanMovie.Inthissetting, thesourceandthetargetdomainssharethesameuser setbuthavedifferentitemsets.  ThethirdtaskisonandDoubandata.Weextract theratingsonthe 6 ; 000 sharedmoviesfromand DoubanMovie.Thenweget 4 : 9  10 5 ratingsfrom Doubangivenby 1 : 2  10 4 userswithdensity 0 : 7% ,and 10 6 ratingsfromgivenby 10 4 userswithdensity 1 : 7% .Thegoalistotransferknowledgefromto DoubanMovie.Inthistask,itemsetisidenticalacross domainsbutusersetsaretotallydifferent.  Thefourthtaskistoevaluatetheeffectivenessofthe proposedalgorithmunderthecontextofmultiplesource domains.ItusesbothDoubanMusicandDouban Bookasthesourcedomainsandtransferknowledgeto DoubanMoviedomain.  Thetaskvariesthetypeofsourcedomains.It utilizestheWikipediausereditingrecordsandIMDB hyperlinkgraph,togetherwithDoubanMovieasthe sourcedomainstoperformratingpredictionsonthe moviedataset. Forevaluation,wecalculatetheRootMeanSquare Error(RMSE)ontheheldout ˘ 30% ofthetargetdata: RMSE = s X ( u;i;x ui ) 2 T E ( x ui  ^ x ui ) 2 = j T E j where x ui and ^ x ui arethetrueandpredictedratings,respec- tively,and j T E j isthenumberoftestratings. 4.2STLCFandBaselinesMethods Weimplementtwo variationsofourSTLCFmethod.STLCF(E)isanSTLCF methodthatonlytaketrainingerrorintoconsiderationwhen performingselectivetransferlearning.STLCF(EV)notonly considerstrainingerror,butalsoutilizestheempiricaler- rorvariance.TodemonstratetheofourSTLCF, weselectedthefollowingbaselines 6 : PMF [19]isare- centlyproposedmethodformissingvalueprediction.Previ- ousworkshowedthatthismethodworkedwellonthelarge, sparseandimbalanceddataset. GPLSA [7]isaclassical non-transferrecommendationalgorithm. CMF [23]ispro- posedforjointlyfactorizingtwomatrices.Beingadoptedas atransferlearningtechniqueinseveralrecentworks,CMF hasbeenproventobeaneffectivecross-domainrecommen- dationapproach. TGPLSA isanuniformlyweightedtrans- ferlearningmodel,whichutilizeallsourcedatatohelpbuild thetargetdomainmodel.Itisusedasoneofthebaselines becauseweadoptitasthebasemodelofourboosting-based selectivetransferlearningframework. 4.3ExperimentalResults 6 Parametersforthesebaselinemodelsareviacrossvalidation. Table2: PredictionperformanceofSTLCFandthebaselines. Datasets Source Target Non-TL Non-SelectiveTL SelectiveTL sparseness sparseness GPLSAPMF TGPLSACMF STLCF(E)STLCF(EV) D4(Simulated) 1.6% 0.1% 1.00120.9993 0.96520.9688 0.9596 0.9533 to 0.2% 0.98390.9814 0.95280.9532 0.9468 0.9347 D4(Simulated) 0.3% 0.97690.9728 0.94750.9464 0.9306 0.9213 1.5% 0.1% 0.89390.8856 0.80980.8329 0.7711 0.7568 D2toD3 0.2% 0.83700.8323 0.74620.7853 0.7353 0.7150 0.3% 0.73140.7267 0.70040.7179 0.6978 0.6859 1.7% 0.1% 0.89390.8856 0.81450.8297 0.7623 0.7549 D4toD3 0.2% 0.83700.8323 0.75190.7588 0.7307 0.7193 0.3% 0.73140.7267 0.71270.7259 0.6982 0.6870 4.3.1PerformanceComparisons Wetesttheperfor- manceofourSTLCFmethodsagainstthebaselines.The resultsofthecollaborativetasksunderthreediffer- enttargetdomainsparsenessareshowninTable2. First,weobservethatthenon-transfermethods,i.e. GPLSAandPMF,failtogiveaccuratepredictions,espe- ciallywhenthetargetdomainisseverelysparse.Withthe helpofsourcedomains,the(non-selective)transferlearn- ingmethodswithequallyweightsonthesourcedomains, likeTGPLSAandCMF,canincreasetheaccuracyoftherat- ingpredictions.Andourselectivetransferlearningmethods (i.e.,STLCF(E)andSTLCF(EV))candoevenbetter.The factthatourSTLCFoutperformsothersisexpectedbecause byperformingthe selective knowledgetransfer,weusethe trulyhelpfulsourcedomain(s),whichisdesignedtohandle thesparsenessissueinCFproblems. Second,comparingthetwonon-selectiveTLCFmeth- odswiththeothertwoselectiveTLCFmethods,weobserve thatonthelasttworealworldtasks(D2toD3andD4toD3) whenthetargetdomainisextremelysparse(say0.1%),the improvementofaccuracyachievedbyourSTLCFmethods againstthenon-selectivetransferlearningmethodsismore thanitdoesonthesimulationdatasetbasedon (D4toD4).Noticethattheinconsistencyofthetar- getdomainandthesourcedomainsonthesimulationdata setsismuchsmallerthanthatonthereal-worldcases.The experimentresultsshowthatourSTLCFalgorithmiseffec- tiveinhandlingtheinconsistencybetweenthesparsetarget domainandthesourcedomains. Third,wenoticethatsomefactors,likeempiricalerror variance,mayaffecttheprediction.InTable2,wecompare ourtwoSTLCFmethods,i.e.,STLCF(E)andSTLCF(EV) whenthetargetdomainsparsityis 0 : 1% .Wecan thatonthetaskﬁD2toD3ﬂ,i.e.,DoubanBooktoMovie, STLCF(EV)ismuchbetterthanSTLCF(E).Butonthe taskﬁD4(Simulated)toD4(Simulated)ﬂ,theimprovementof STLCF(EV)isnotsoagainstSTLCF(E).These observationsmaybeduetothedomainconsistency.For thetasksﬁD4(Simulated)toD4(Simulated)ﬂ,boththesource andtargetentitiesaremovieratingsfromdataset, Table3: PredictionperformanceofSTLCFforLong-TailUsersontheD2 toD3task.STLCF(E)doesnotpunishthelargevarianceofempiricalerror, whileSTLCF(EV)does. Ratings Non-TL Non-SelectiveTL SelectiveTL per i.e.STLCF user GPLSA TGPLSACMF (E)(EV) 1-5 1.1942 0.92940.9312 0.8307 0.8216 6-10 0.9300 0.78590.7929 0.7454 0.7428 11-15 0.8296 0.73310.7390 0.7143 0.7150 16-20 0.7841 0.70790.7113 0.7042 0.7050 21-25 0.7618 0.69410.6947 0.6942 0.6910 26-30 0.7494 0.69180.6884 0.6917 0.6852 31-35 0.7370 0.69090.6911 0.6915 0.6818 36-40 0.7281 0.68960.6856 0.6907 0.6776 41-45 0.7219 0.68780.6821 0.6890 0.6740 46-50 0.7187 0.68810.6878 0.6800 0.6734 whilethetaskﬁD2toD3ﬂtriestotransfertheknowledge fromabookrecommendationsystemtothemovierecom- mendationsystem,whichmaycontainsomedomainspe- items.Whenthetargetdomainisverysparse,i.e.the user'sratingsontheitemsarerare,therearechancestoget highpredictionaccuracyoccasionallyontheobserveddata withabadmodelonthesourcedomainsthatareinconsis- tentwithtargetdomain.Inthiscase,itisimportanttocon- siderthevarianceofempiricalerroraswell.Comparingto STLCF(E),STLCF(EV),whichpunishesthelargevariance, canbetterhandlethedomaininconsistencyintransferlearn- ing,especiallywhenthetargetdomainissparse. 4.3.2ResultsonLong-TailUsers Tobetterunderstand theimpactofSTLCFwiththehelpofthesourcedomain,we conductaanalysisontheperformanceimprove- mentonDoubandatasets,withDoubanBookassourcedo- mainandDoubanMovieastargetdomain.Theresultson differentusergroupsinthetargetdomainareshowninTable 3.First,weobservethattheSTLCFmodels,i.e.,STLCF(E) andSTLCF(EV)canachievebetterresultsonthoselong- tailuserswhohaveveryfewratingsinhistoricallogs.Such factimpliesthatourSTLCFmethodscouldhandlethelong- tailusersthatreallyneedaanalysiswhenper- formingknowledgetransferfromsourcedomains.Current Table4: PredictionperformanceofSTLCFwithmultiplesourcedomainscontainingmuchirrelevantinformation. SourceDomain: None D3 D3&D5 D3&D6 D5&D6 D3&D5&D6 Target 0.1% 0.9983 0 : 9789 0 : 9747 0 : 9712 0 : 9923 0.9663 (D4) 0.2% 0.9812 0 : 9625 0 : 9583 0 : 9572 0 : 9695 0.9505 sparseness 0.3% 0.9703 0 : 9511 0 : 9409 0 : 9464 0 : 9599 0.9383 Table5: PredictionperformanceofSTLCFwithmultiplesourcedomains (Douban). SourceDomain: None D1 D2 D1&D2 Target 0.1% 0.8856 0 : 7521 0 : 7568 0.7304 (D3) 0.2% 0.8323 0 : 7163 0 : 7150 0.6904 sparseness 0.3% 0.7267 0 : 6870 0 : 6859 0.6739 CFmodelswithoutanyanalysisonthe usersusuallyfailtocapturethepreferencesofthelong-tail users,whileourSTLCFmethodsworkwellbecausetheycan selectivelyaugmenttheweightofthecorrespondingsource domaininstanceswithrespecttothoselong-tailcasesatboth instancelevelanddomainlevel.Second,STLCF(EV)works betterthanSTLCF(E)onthosenon-long-tailusers,i.e.,with morethan25ratingsperuserinthehistoricallog.Thisis expectedbecauseuserswithmoreratingscanbenemore fromtheerrorvarianceanalysistoavoidnegativeknowledge transfer. 4.3.3STLCFwithMultipleSourceDomains Weapply STLCF(EV)ontheextremelysparsetargetmoviedomain, withtwosetsofsourcedomains:oneiscomposedof DoubanMusicandDoubanBook,theotheriscomposed ofDoubanMovie,IMDBhyperlinkgraphandWikipedia usereditingrecords.TheresultsareinTable5andTable4 respectively.WedemonstrateourSTLCFmethodcanutilize multiplesourcedomainsofvarioustypesbyhandlingthe inconsistencybetweenthetargetandthesourcedomains. First,fortheDoubanexperimentsshowninTable5,we observethatcomparingtoonlyusingeitherDoubanBook orDoubanMusicassourcedomain,thereare improvementswhenbothofthemareused.Theresultis expectedbecauseeachofthesourcedomainshasitsown partsofeffectiveinformationforthetargetdomain.For example,auserwhoshowmuchinterestsinthemovieﬁThe LordoftheRingsﬂmayhaveconsistentpreferencesinits novel.Inthiscase,withthehelpofmoreauxiliarysources, betterresultsareexpected. Second,weexplorethegeneralizationofthechoicesof sourcedomainsbyintroducingdomainslikeWikipediauser editingrecordsandIMDBhyperlinkgraph,whicharenot directlyrelatedtothetargetdomainbutstillcontainsome usefulinformationinhelpingthetargettaskrating prediction).TheresultsareshowninTable4.Comparing theresultsoftheexperimentthatusesnosourcedomain Figure2: ChangeoftheRMSEswithdifferent ˝ s. (non-transfer)tothosethatusesourcedomainsD5&D6, weobservethatalthoughtheWikipediausereditingrecords orIMDBhyperlinkgraphisnotcloselyrelatedtothetarget domainandcanhardlybeadoptedassourcedomainsby previoustransferlearningtechniques,ourSTLCFmethod canstilltransferusefulknowledgesuccessfully.Inaddition, comparingtheresultsoftheexperimentthatusessingle sourcedomainD3tothosethatusesourcedomainsD3 &D5,D3&D6,orD3&D5&D6,wethatthe WikipediausereditingrecordsorIMDBhyperlinkgraph couldprovidesomeusefulknowledgethatisnotcoveredby therelatedmoviesourcedomains.Despiteofthenoiseand heterogeneoussetting,ourSTLCFmethodcanstillutilize thesesourcedomainstohelpthetargetdomaintasks.Aswe havediscussedinSection3,ourSTLCFperformsselective transferlearningatbothdomainlevelandinstancelevel.On onehand,thedomainlevelselectivetransfercanblockthe noisyinformationglobally.Aswecansee,D5&D6are noisyandthereforecontainmuchdatathatareinconsistent withtheobserveddatainthetargetdomain,thereforethe overalltransferofD5&D6ispenalized.Ontheother hand,theinstancelevelselectivetransferlearningcanfurther eliminatetheaffectionsofthoseirrelevantsourceinstances. Aboveall,ourSTLCFishighlyadaptivetoutilize sourcedomainsthatarerelativelyinconsistentwiththetarget domain,evenwhenthetargetdomainisrathersparse. 4.3.4ParametersAnalysisofSTLCF Therearetwopa- rametersinourSTLCF,i.e.,thepredictionerrorthreshold ˝ andtheempiricalerrorvarianceweight  .Since ˝ and  are independent,weoneandadjustanother. Wetheempiricalerrorvarianceweighttobe  =0 : 5 andadjusttheparameter ˝ .Basedonourresultsshownin Figure2,themodelhasgoodperformancewhen ˝ isoforder 10  2 .Wealsotunedtheparameter  ,whichbalancesthe empiricalerroranditsvariance.Wethepredictionerror thresholdtobe ˝ =0 : 03 intuning  .AsshowninFigure3, whenwevarytheparameter  from 0 to 1 ,thebestchoices of  arefoundtobearound 0 : 4  0 : 5 . Figure3: ChangeoftheRMSEswithdifferent  s. Figure4: ChangeoftheRMSEsand  swhenmoreandmoreweak learnersjoininthecommittee. 4.3.5ConvergenceandOvTest Figure4shows theRMSEsofSTLCF(EV)asthenumberofweaklearners changesontheDoubanBooktoMovietask.Fromthe ontheleft,weobservethatSTLCF(EV)convergeswellafter 40iterations.Wecanalsothatthecorresponding  also convergetoaround0.68after40iterationsaswell. ThenumberoflatenttopicsofthebaselearnerTGPLSA themodel'sabilitytotrainingdata.Whenwe keepincreasingthenumberoflatenttopics,themodeltends tobetterthetrainingdata.Butifthenumberoflatent topicsistoolarge,themodelmaysufferfromovWe investigatetheovissuebyplottingthetrainingand testingRMSEsofthenon-transferlearningmodelGPLSA, thenon-selectivetransferlearningmodelTGPLSAandour selectivetransferlearningmodelSTLCF(EV)overdifferent numbersoflatenttopicsinFigure5.Thedatasparsityfor thetargetdomainisaround0.3%. WecanobservethatcomparingtoourSTLCF,the trainingRMSEsofGPLSAandTGPLSAdecreasefaster, whiletheirtestingRMSEsgodownslower.When k isabout 50 ,thetestingRMSEsofGPLSAstarttogoup.Andfor TGPLSA,itstestingRMSEsalsogoupslightlywhen k is largerthan 75 .ButthetestingRMSEsofourSTLCFkeep decreasinguntil k =125 andevenwhen k islargerthan 125 ,theraiseofourSTLCF'stestingRMSEsisnotobvious. Clearlywhenthetargetdomainisverysparse,ourSTLCF methodismorerobustagainsttheovbyinheriting theadvantagefromboostingtechniquesandthe selectiononknowledgetransfer. 5RelatedWorks Theproposed S elective T ransfer L earningfor C ollaborative F iltering(STLCF)algorithmismostrelatedtotheworks incollaborativeInTable6,wesummarizethe relatedworksunderthecollaborativecontext.To thebestofourknowledge,nopreviousworkfortransfer learningoncollaborativehaseverfocusedonthe Figure5: ChangeoftheRMSEswithdifferentnumbersoflatenttopics. Table6: OverviewofSTLCFinabigpictureofcollaborative Selective Non-Selective Transfer STLCF RMGM[11],CMF[23], Learning TIF[15],etc. Non-Transfer Œ MMMF[ ? ],GPLSA[7], Learning PMF[19],etc. analysisofconsistencybetweensourcedomains andthetargetdomain,i.e.,theselectivetransferlearning. CollaborativeFiltering asanintelligentcomponent inrecommendersystemshasgainedextensiveinterestin bothacademiaandindustry.Variousmodelshavebeen proposed,includingfactorizationmodels[10,15,16,18], probabilisticmixturemodels[8,9],Bayesiannetworks[17] andrestrictedBoltzmanmachines[20].However,mostof thepreviousworkwouldsufferfromovtothesmall setofobserveddata.Inthispaper,weintroducetheconcept ofselectivetransferlearningtobettertackletheov anddatasparsenessissue. TransferLearning PanandYang[14]surveyedthe oftransferlearning.Someworksontransferlearn- ingareinthecontextofcollaborativeMehtaand Hofmann[13]considerthescenarioinvolvingtwosystems withsharedusersandusemanifoldalignmentmethodsto jointlybuildneighborhoodmodelsforthetwosystems.They focusonmakinguseofanauxiliaryrecommendersystem whenonlypartoftheusersarealigned,whichdoesnot distinguishtheconsistencyofusers'preferencesamongthe alignedusers.Li etal. [12]designedaregularizationframe- worktotransferknowledgeofcluster-levelratingpatterns, whichdoesnotmakeuseofthecorrespondencebetween sourceandtargetdomains. Recently,researchersproposetheMultiSourceTrAd- aBoost[25]toallowautomaticallyselectingtheappropri- atedataforknowledgetransferfrommultiplesources.The newestworkTransferBoost[3]wasproposedtoiteratively constructanensembleofviare-weightingsource andtargetinstanceviabothindividualandtask-basedboost- ing.Moreover,EBBoost[22]suggestsweighttheinstance basedontheempiricalerroraswellasitsvariance.However sofar,theworkslimittothetasks.Ourworkis thetosystematicallystudy selective knowledgetransfer inthesettingsofcollaborativeBesides,wepropose thenovelfactor-varianceempiricalerrorthatisshowntobe ofmuchhelpinsolvingtherealworldCFproblems. 6Conclusions Inthispaper,weproposedtoperform selective knowledge transferforCFproblemsandcameupwithasystematical studyonhowthefactorssuchasvarianceofempiricalerror couldleveragetheselection.Wefoundalthoughempirical erroriseffectivetomodeltheconsistencyacrossdomains, itwouldsufferfromthesparsenessprobleminCFsettings. Byintroducinganovelfactor-varianceofempiricalerror tomeasurehowtrustfulthisconsistencyis,theproposed criterioncanbetteridentifytheusefulsourcedomainsand thehelpfulproportionsofeachsourcedomain.Weembed- dedthiscriterionintoaboostingframeworktotransferthe mostusefulinformationfromthesourcedomainstothetar- getdomain.Theexperimentalresultsonreal-worlddatasets showedthatourselectivetransferlearningsolutionperforms betterthanseveralstate-of-the-artmethodsatvariousspar- sitylevels.Furthermore,comparingtoexistingmethods,our solutionworkswellonlong-tailusersandismorerobustto ov 7Acknowledgments WethankthesupportofHongKongCERGProjects621010, 621211andHongKongITFProjectGHX/007/11. References [1]W.BurgardandD.Roth,editors. ProceedingsoftheTwenty- FifthAAAIConferenceonIntelligence,AAAI2011, SanFrancisco,California,USA,August7-11,2011 .AAAI Press,2011. [2]W.Dai,Q.Yang,G.-R.Xue,andY.Yu.Boostingfortransfer learning.InGhahramani[6],pages193Œ200. [3]E.EatonandM.desJardins.Selectivetransferbetween learningtasksusingtask-basedboosting.InBurgardand Roth[1]. [4]E.Eaton,M.desJardins,andT.Lane.Modelingtransfer relationshipsbetweenlearningtasksforimprovedinductive transfer.In TheEuropeanConferenceonMachineLearning , pages317Œ332,2008. [5]H.EldardiryandJ.Neville.Across-modelcollectiveensem- bleInBurgardandRoth[1]. [6]Z.Ghahramani,editor. MachineLearning,Proceedingsofthe Twenty-FourthInternationalConference(ICML2007),Ore- gon,USA,June20-24,2007 ,volume227of ACMInterna- tionalConferenceProceedingSeries .ACM,2007. [7]T.Hofmann.Collaborativeviagaussianprobabilistic latentsemanticanalysis.In SIGIR ,pages259Œ266,2003. [8]T.Hofmann.Latentsemanticmodelsforcollaborative- ing. ACMTrans.Inf.Syst. ,22(1):89Œ115,2004. [9]R.Jin,L.Si,C.Zhai,andJ.Callan.Collaborative withdecoupledmodelsforpreferencesandratings.In Pro- ceedingsofCIKM2003 ,2003. [10]Y.Koren,R.Bell,andC.Volinsky.Matrixfactorization techniquesforrecommendersystems. Computer ,42(8):30Œ 37,2009. [11]B.Li,Q.Yang,andX.Xue.Canmoviesandbookscollabo- rate?:cross-domaincollaborativeforsparsityreduc- tion.In Proceedingsofthe21stinternationaljontconference onintelligence ,pages2052Œ2057,2009. [12]B.Li,Q.Yang,andX.Xue.Transferlearningforcollabora- tiveviaarating-matrixgenerativemodel.In Interna- tionalConferenceonMachineLearning ,volume26,2009. [13]B.MehtaandT.Hofmann.Crosssystempersonalizationand collaborativebylearningmanifoldalignments.In KI2006:AdvancesinIntelligence ,pages244Œ259. 2007. [14]S.J.PanandQ.Yang.Asurveyontransferlearning. IEEETransactionsonKnowledgeandDataEngineering , 22(10):1345Œ1359,October2010. [15]W.Pan,E.W.Xiang,andQ.Yang.Transferlearningin collaborativewithuncertainratings.In AAAI ,2012. [16]A.Paterek.Improvingregularizedsingularvaluedecomposi- tionforcollaborative ProceedingsofKDDCupand Workshop ,2007. [17]D.M.Pennock,E.Horvitz,S.Lawrence,andC.L.Giles. Collaborativebypersonalitydiagnosis:Ahybrid memoryandmodel-basedapproach.In Proc.ofUAI ,pages 473Œ480,2000. [18]S.Rendle.Factorizationmachineswithlibfm. ACMTrans- actionsonIntelligentSystemsandTechnology(ACMTIST) , 3:19:1Œ19:22,May2012. [19]R.SalakhutdinovandA.Mnih.Probabilisticmatrixfactor- ization.In NIPS ,2007. [20]R.Salakhutdinov,A.Mnih,andG.E.Hinton.Restricted boltzmannmachinesforcollaborativeInGhahra- mani[6],pages791Œ798. [21]X.Shi,J.-F.Paiement,D.Grangier,andP.S.Yu.Learning fromheterogeneoussourcesviagradientboostingconsensus. In SDM ,pages224Œ235.SIAM/Omnipress,2012. [22]P.K.ShivaswamyandT.Jebara.Empiricalbernsteinboost- ing. JournalofMachineLearningResearch-Proceedings Track ,9:733Œ740,2010. [23]A.P.SinghandG.J.Gordon.Relationallearningvia collectivematrixfactorization.In KDD ,pages650Œ658, 2008. [24]G.-R.Xue,W.Dai,Q.Yang,andY.Yu.Topic-bridgedplsa forcross-domaintextInS.-H.Myaeng,D.W. Oard,F.Sebastiani,T.-S.Chua,andM.-K.Leong,editors, SIGIR ,pages627Œ634.ACM,2008. [25]Y.YaoandG.Doretto.Boostingfortransferlearningwith multiplesources.In CVPR ,pages1855Œ1862,2010. [26]Y.Zhang,B.Cao,andD.-Y.Yeung.Multi-domaincollabora- tiveIn UAI ,pages725Œ732,2010.  
Proceedingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguistics andthe7thInternationalJointConferenceonNaturalLanguageProcessing ,pages1723Œ1732, Beijing,China,July26-31,2015. c  2015AssociationforComputationalLinguistics 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732  
TopicSegmentationofWebDocumentswithAutomatic CuePhraseandBLSTM-CNN LiangWang 1 ,SujianLi 1 ; 3 ,XinyanXiao 2 ,andYajuanLyu 2 1 KeyLaboratoryofComputationalLinguistics,PekingUniversity,MOE,China 2 BaiduInc.,Beijing,China 3 CollaborativeInnovationCenterforLanguageAbility,Xuzhou,Jiangsu,China f intfloat,lisujian g @pku.edu.cn , f xiaoxinyan,lvyajuan g @baidu.com Abstract. Topicsegmentationplaysanimportantrolefordiscourseanalysisand documentunderstanding.Previousworkmainlyfocusonunsupervisedmethod fortopicsegmentation.Inthispaper,weproposetousebidirectionallongshort- termmemory(BLSTM)model,alongwithconvolutionalneuralnetwork(CNN) forlearningparagraphrepresentation.Besides,wepresentanovelalgorithm basedonfrequentsubsequenceminingtoautomaticallydiscoverhigh-quality cuephrasesfromdocuments.Experimentsshowthatourproposedmodelisable toachievemuchbetterperformancethanstrongbaselines,andourminedcue phrasesarereasonableandeffective.Also,thisistheworkthatinvestigates thetaskoftopicsegmentationforwebdocuments. Keywords: topicsegmentation,neuralnetwork,webdocuments,sequencemin- ing 1Introduction Topicsegmentationisanaturallanguageprocessing(NLP)task,whichaimstosegment adocumentintotopicallysimilarparts,itisalsocalledtextsegmentationordiscourse segmentationinvariousscenarios.Thislevelofanalysisprovidesabetterunderstanding aboutdocumentstructureandtopicshift,whicharehelpfulinformationformanyNLP taskssuchasdiscourseparsing,dialoguegenerationetc. Therehavebeendecadesofresearchabouttopicsegmentation,previousworkmainly focusonunsupervisedapproach.TextTiling[12]isoneofthemostfamousandearliest algorithmsfortopicsegmentation.Itisbasedononesimpleintuition:lexicalcohe- sionwithineachtopicsegmentishigh,whilelexicalcohesionbetweendifferenttopic segmentsislow.Therefore,thecorepartofTextTilingalgorithmistocalculatelexi- calsimilarityofadjacentsegmentsandthenchooseanappropriatethresholdtodeter- minetopicboundaries.Thisalgorithmissimpleandcomputationallyefient,however, whenannotatedcorpusisavailable,itfailstoutilizetrainingdataandunabletolearn anaccuratemodel.OtherunsupervisedvariantsofTextTilingalgorithmsuchasC99[6] andTopicTiling[18]alsosufferfromthisissue. Onthecontrary,supervisedapproachisabletolearnmorecomplexandaccurate model.Aslongastrainingdataissufandfeaturesetisgoodenough,itsper- formanceismuchbetterthanunsupervisedapproaches.Inthispaper,weconductex- 2 perimentswithconditionalrandomandLSTM,bothofthems outperformTextTilingalgorithm. Forsequencemodelingtasksuchastopicsegmentation,capturinglongdistance informationisakeyissue.ThankstothegatingmechanisminLSTM,itpreserves usefulinformationforalongtimeperiod.BidirectionalLSTMisacombinationof forwardLSTMandbackwardLSTM,thereforeitisabletoexploitusefulfeaturesfrom bothsides.OurexperimentsshowthatBLSTMconsistentlybeatsCRFandLSTM,and achievesbestf1-score. Differentfrompreviousworkwhichfocusonnewsordocuments,inour work,weconducttextsegmentationforwebdocuments.Accordingtoourobservation, cuephraseisastrongindicatorfortopicboundary.Forexample,ﬁ ,  ﬂ,ﬁ , „ ﬂare frequentlyusedtostartanewtopic.[9]presentedabayesianframeworktoidentifycue phrasesautomatically.Differentfromtheirwork,wetreatcuephraseasa frequentsubsequenceminingproblem,andcomeupwithavariantof Apriori algorithm tominecuephrasesfromannotatedcorpus.Itturnsoutthatourproposedalgorithmis abletopreciselylocatethosecuephrasesandthereforeboostsystemperformance. Thispapermakesthreemajorcontributions:1.Tothebestofourknowledge,this istheworkthatsuccessfullyappliesneuralnetworkmodelforsupervisedtopic segmentationandachievespromisingresults;2.Wepresentanovelalgorithmbasedon frequentsubsequenceminingtoidentifycuephrases;3.Forthetime,thepossibility oftopicsegmentationforwebdocumentsisexamined. 2RelatedWork Duetothelackoflargescalehigh-qualitytopicsegmentationdatasets,unsupervised approachismostwidelyadopted.TwooftheearlyalgorithmsareTextTiling[12]and C99[6],bothofwhicharebasedontheintuitionthatworddistributionsdiffersignif- icantlyifthereisatransitionintopic.TextTilingismorecomputationallyef whileC99algorithmshowsbetterperformance.Vectorspacemodelisusedtocom- putecosinesimilarityofsentences,butitfailstocapturesemanticsimilarityofdif- ferentwords.LaterworkusedLatentSemanticAnalysis(LSA)[7]andLatentDirichlet Allocation(LDA)[18][16]tocomputesentencesimilaritymoreaccurately. Generativemodelswerealsopresentedtoimproveperformanceoftopicsegmen- taionsystem.SimilartoLDA,topicsareseenaslatentvariablesandwordsareseen asvisiblevariables.HiddenMarkovModel(HMM)[20]andseveralvariantsofLDA[8] [14][17]wereproposed.Carefullydesignedgenerativemodelsoutperformlexicalsim- ilaritybasedmodels,howevertheyareusuallymuchmorecomplicatedandrequire efinferencealgorithms. Supervisedapproachisalsoexaminedwhenlargeamountoftrainingdataisavail- able.Therein,topicsegmentationisformulatedasabinaryclatask.[10] traineddecisiontreeonarichsetoffeaturessuchascuephrasesandlexical cohesion.SupportVectorMachine(SVM)[11]wasalsotriedondatasetsfromdifferent domains.Experimentresultsshoweditssuperiorityoverunsupervisedmodels. Evaluationoftopicsegmentationsystemsisnon-trivial.met- ricssuchasprecision,recallandf1-scoreareusefulbutsometimestoostrict. P k metric 3 wasproposedby[2]toalleviatethisproblem.Tocalculate P k ,weneedaslidingwin- dowofedlength,andcheckwhetherthepredictedsegmentationisconsistentwith groundtruthwithinthiswindow.Now P k isthemostwidelyusedmetricwithinthe literatureoftopicsegmentation.However, P k metricalsohasitsownproblems,Win- dowDiff(WD)[15]andworderrorratebased[3]metricswereproposedinlaterresearch. LSTM[13]isavariantofvanillarecurrentneuralnetwork(RNN),whichaimsto solvegradientvanishing/explodingissueduringtrainingandallowusefulinformation towoveralongdistance.Ithasbeenwidelyusedforsequencemodelingtaskssuch aswordsegmentation[4],namedentityrecognition[5]andPart-of-Speechtagging[19] etc. 3Models 3.1BLSTM(BidirectionalLongShortTermMemory) LSTMisarecurrentnetworkwithgatingmechanism.TherearemanyvariantsofLSTM unit,hereweadoptonewidelyusedarchitecturewiththreetypesofgates:inputgate i t , forgetgate f t andoutputgate o t , t denotestimestep.Theformulaforcalculatingeach gateandmemorycellunitareasfollows: i t = tanh ( W i x t + R i y t  1 + b i ) (1) f t = tanh ( W f x t + R f y t  1 + b f ) (2) o t = tanh ( W o x t + R o y t  1 + b o ) (3) z t = tanh ( W z x t + R z y t  1 + b z ) (4) c t = i t  z t + f t  c t  1 (5) y t = o t  tanh ( c t ) (6) Here x t 2 R d , y t 2 R d are d dimensionalinputvectorsandoutputvectors. c t isthe memorycellvectorattimestep t . W z , W i , W f , W o areweightmatricesforinput. R z , R i , R f , R o areweightmatricesforoutput. b z , b i , b f , b o arecorrespondingbias. tanh isusedasnon-linearactivationfunction. ComparedtoLSTM,bidirectionalLSTMcapturesinformationfrombothdirections ofasequence.ItisacombinationoftwoindependentLSTMwithoppositedirections: forwardLSTMandbackwardLSTM.Theoutputvector y t isconcatenationof thesetwoLSTM. BelowshowsBLSTM'supdatingformulaformemorycell c t ,outputgate o t ,we omitothergatesforsimplicityastheyaresimilar. c t k = i t k  z t k + f t k  c t  1 k ;k 2f f;b g (7) o t k = tanh ( W k o x t + R k o y t  1 k + b k o ) ;k 2f f;b g (8) y t k = o t k  tanh ( c t k ) ;k 2f f;b g (9) y t =[ y t f ; y t b ] (10) 4 Fig.1:BLSTM-CNNfortopicsegmentation Exampletext:   ﬂ t û Ñ Ï „ (Ihaveveyearsofexperienceasatranslator.) Amongthoseequations, f denotesforwardpasslayer, b denotesbackwardpass layer, y t isaconcatenationof y t f ; y t b 2 R d andtherefore y t 2 R 2 d . Forourtask,thereisasoftmaxlayeroveroutputvectors: P t ( y j x )= softmax ( W h y t + b h ) (11) Y pred = argmax P t ( y j x ) (12) Thepredictionisthelabelwithhighestprobability argmax P t ( y j x ) . 3.2CNNforparagraphrepresentation Therearemanywaystorepresentparagraphtext,one-hotencodingrepresentation wouldresultextremelysparsefeaturevector.Inthispaper,weadoptapopularCNN architecturetorepresententireparagraphasalowdimensionaldensevector.Inother words,CNNservesasatextfeatureextractorforBLSTMmodel. TheinputtoCNNisawordsequence,eachword w ismappedtoanembedding vector x w bymatrix-vectorproduct: x w = W d  V j v w (13) W d  V j isa d dimensionalwordembeddingmatrixforentirevocabulary V , v w is one-hotvectorrepresentationofword w . 5 Theoutputofembeddinglayerisawordembeddingsequence x 1: n . x 1: n =[ x 1 ; x 2 ;:::; x n ] (14) Everyconvolutionoperationinvolvesapplyinga v 2 R hd toawindowof h wordstoproduceanewfeature.Forexample,afeature f i isgeneratedfromawindow ofwords x i : i + h  1 by f i = h ( v  x i : i + h  1 + b ) : (15) b 2 R isabiasterm, h isanon-lineartransformationfunctionsuchas tanh .This operationisappliedtoeverypossiblewindow f x 1: h ; x 2: h +1 ;:::; x n  h +1: n g toproduce afeaturemap: f =[ f 1 ;f 2 ;:::;f n  h +1 ] ; (16) with f 2 R n  h +1 .Max-poolingoperationappliestotheentirefeaturemapandchooses themaximumvalueasthefeature. ^ f =max f f g (17) TheoverallnetworkarchitectureisshowninFigure1.ParametersofCNNand BLSTMarejointlylearned.TheoutputofCNNisavectorrepresentationofgiven paragraph. 3.3ModelLearning Weformulatetopicsegmentationasabinarytask,andusecrossentropy lossfunction: J =  1 N N X i =1 ( y  log y +(1  y  )log(1  y )) (18) where N representsthesizeoftrainingset; y  isthegroundtruthlabel, y ismodel's probabilityoutput. Totrainournetwork,weusemini-batchstochasticgradientdescent(SGD)with adaptivelearningratecomputedby Adadelta [21],whichshowsbetterperformanceand convergenceproperty. 4Features 4.1Frequentsubsequenceminingbasedcuephrase Whenwritingarticles,peopleoftenusecuephrasetostartanewtopic,suchasﬁ Œ H ﬂ,ﬁ 6  ﬂ,ﬁ   ﬂ.Theyarestrongindicatorsfordetectingtopicboundary.Collect- ingthosecuephrasesbyhandwouldbetime-consuming.Moreover,cuephrasesare oftendependentoncorpusdomainandlanguage.Ifwetransplantoursystemtoanew domain,wehavetomanuallysummarizecuephrasesalloveragain. Inthispaper,weproposeanovelalgorithmtoautomaticallydiscovercuephrases baseonfrequentsubsequencemining.Itisavariantofthefamous Apriori algorithm[1] 6 Algorithm1 CuePhraseSequenceMiningAlgorithm Input :Corpus D = f d j disadocument g , minsup :minimumsupporttobecomeafrequentsubsequence, maxlen :maximumlengthofcuephrasesequence Output :alistofcuephrasesequence S = f p j pisacuephrasesequence g 1: function MINE( D , minsup , maxlen ) 2: C 1   counteachword w 2 D 3: P 1  f w j w.count  minsup g 4:S  fg 5: for i   2to maxlen do 6: C i   CANDIDATE-GEN( P i  1 , i -1) 7: for candidate in C i do 8: for eachdocument d 2 D do 9: if IS-SUBSEQUENCE( d , candidate ) then 10: candidate .count++ 11: P i  f candidate j candidate .count  minsup g 12:S  [f P 1 , P 2 ... P maxlen g 13: return S 1: function CANDIDATE-GEN( C i , len ) 2: candidates  fg 3: for t a in C i do 4: for t b in C i do 5: if t a [1:len]== t b [0:(len-1)]andCO-OCCURRENCE( t a , t b )  minsup then 6: candidates   candidates [ ( t a [1:len]+ t b [len-1]) 7: return candidates forfrequentitemsetmining,withonekeydifferencethatitemsetisunorderedwhile cuephrasesequenceisordered.Theintuitionbehindouralgorithmisthatcuephrase sequenceusuallyappearmoreoftenattopicboundarythanotherwords. Ourproposedalgorithmissummarizedabove. Weomittheimplementationdetailsoftwofunctions: IS-SUBSEQUENCE(d,candi- date) and CO-OCCURRENCE( t a , t b ) . IS-SUBSEQUENCE(d,candidate) checkswhether given candidate isasubsequenceofgivendocument d .Weadopttwosequencematch- ingstrategies:matchingandsufmatching.Inmatchingstrategy,acan- didatecuephrase w matchesaparagraph p when w isaof p ;Similarly,insuf matchingstrategy,acandidatecuephrase w matchesaparagraph p when w isasuf- of p . CO-OCCURRENCE( t a , t b ) calculatesthenumberofdocument d thatboth IS-SUBSEQUENCE(d, t a ) and IS-SUBSEQUENCE(d, t b ) evaluateto true . Thisisaniterativealgorithm,whichinvolvestwokeystepsateachiteration:can- didategenerationandcandidatevalidation.Incandidategenerationstep,foreachpair oflength len cuephrasesequence t a and t b ,ifthe ( len  1) -sufof t a isequalof ( len  1) of t b ,andtheirco-occurrencecountisnolessthan minsup ,then t a and t b canbecombinedintoalength len +1 candidate.Theco-occurrenceconstraintis notnecessary,butitcangreatlyreducethenumberofcandidates.Inourexperiments, 7 itreducesthenumberoflength-2candidatesfromover20,000downtolessthan300. Incandidatevalidationstep,foreach candidate ,thealgorithmcalculatesinhowmany documentsthis candidate sequenceappears,thencandidateswhosefrequencyisnoless than minsup getintotheresultset S . Theworsttimecomplexityofourproposedalgorithmisexponential,however,the numberofcuephrasesinrealdatasetisoftenlimited.Ourpythonimplementationwith- outanyfurtheroptimizationwithin10seconds. CuePhraseSequence 1.xxx 2.xxx 3.xxx   xxx „  xxx   xxx å w xxx ¹ Õ xxx è  ‰ y xxx xxx Ë Í xxx Ë Í xxx Ë Í EnglishExplanation  second.xxx third.xxx  second.xxx third.xxx toolsxxx methodxxx precautionsxxx xxxintroduction xxxintroduction xxxintroduction Table1:Examplesofcuephrasesequence,ﬂxxxﬂdenotessomeotherirrelevantwords SomeofthecuephrasesequencesarelistedinTable1.Itisclearthatouralgo- rithmisabletoouthighqualitycuephrases.Asexpected,peopleoftenusenumber sequencetostartanewtopic.Thelasttwoareduetothelargenumberoftutorialdoc- umentsonweb,andtheyaremuchlesscommoninotherdomains.Handcraftedcue phrasesetmayverylikelymissthem.Comparedtotime-consumingandpossiblyin- completemanualcuephraseselectionprocess,ouralgorithmismoreaccurate,efient andcanbeeasilyadaptedtootherdomains. 4.2Otherfeatures Besidescuephrase,hereisalistofotherfeaturesweusedinexperiments. 1. Lexicalfeature. ParagraphtextisencodedasadensefeaturevectorviaCNN modeldescribedabove. 2. Part-of-Speech(POS)feature. ThePOStagsofwordsincurrentparagraph.We performwordsegmentationandPOStaggingwithopensourcelibrary jieba 4 ,the sameistrueforotherfeaturesrelatedtowordsegmentation. 3. Lengthfeature. Thenumberofcharactersandwordsincurrentparagraph,pre- viousparagraphandnextparagraph.Italsoincludesthenumberofparagraphin currentdocument. 4. Positionfeature. Whethercurrentparagraphisthedocument'sparagraphor lastparagraph. 5. Hyperlinkfeature. Whethercurrentparagraphcontainstextwithembeddedhy- perlink. 4 https://github.com/fxsjy/jieba 8 6. Textfontfeature. Whethercurrentparagraphcontainstextwithboldoritalicfont. Forwebdocuments,theparagraphofanewtopicoftencontainssuchtext,they canbeusefulinformation. Manyothertypesoffeaturesarealsoexamined,includingLDAfeaturesandsyn- tacticfeatures,buttheyshownoperformancegain,thereforewechoosetonotlistthem here. 5Experiments 5.1DataandSetup OurdatasetisprovidedbyBaidu 5 andconsistsof2951webdocumentswithhuman annotatedgroundtruthlabels.Tothebestofourknowledge,thisisthelargesthuman annotatedtopicsegmentationdataset.Datasetusedinpreviousresearchareeithermuch smallerorconstructedautomaticallywiththehelpofsomeheuristics.Table2shows somestatisticsforourdatasets. NumberofDocuments AverageNumberofParagraphs AverageNumberofTopics 2951 15.75 2.33 Table2:Statisticsforourtopicsegmentationdataset. Datasetisrandomlysplitintotrainingset(70%),validationset(10%),testset(20%). Hyperparametersarechosenviagridsearchbymaximizingf1-scoreonvalidationset. Oncehyperparametersareed,wetrainonbothtrainingsetandvalidationset,then reportmodel'sperformanceontestset. OurimplementationofBLSTMisbasedonopensourcelibrary keras 6 .Weuse Adadelta [21]tocomputelearningrate.Thedimensionofmemorycellissetto50,mini- batchsizeis16.TocombatovweaddonedropoutlayeraboveBLSTMoutput, dropoutprobabilityissetto0.5.ForCNN,thenumberofissetto150,thewindow sizeissetto4for1Dconvolution,andthesizeofwordembedding d issetto32,word embeddingmatrix W d  V j isinitializedwithuniformrandomvaluesfrom [  0 : 5 ; 0 : 5] . Ascomparison,wealsoimplementedsomeotheralgorithmssuchasTextTiling, CRF 7 .SimilartoBLSTM,hyperparametersarechosenaccordingtovalidationset. Tohaveacomprehensivecomparisonofmodel'seffectiveness,weevaluateonmul- tiplemetrics:precision,recall,f1-scoreand P k . 5.2Results Table3showsmodel'sperformancewithallfeatures,exceptTextTilingalgorithmis unsupervisedanddoesn'tneedanyfeature.Noticethatfor P k metric,smallervalue meansbetterperformance. 5 Notpubliclyavailablefornow. 6 https://github.com/fchollet/keras 7 https://github.com/tpeng/python-crfsuite 9 ModelPrecisionRecallF1-score P k TextTiling0.7620.4480.5650.146 CRF 0.859 0.6240.7230.133 LSTM-CNN0.7860.7160.7500.092 BLSTM-CNN0.829 0.7300.7760.075 Table3:Performancecomparisonofdifferentmodels. Wecanclearlyseethatsupervisedmodelsoutperformunsupervised TextTilingalgorithmoneverymetric.CRFisawidelyusedmodelforsequencela- beling,itgetshighestprecision,whileperformsmuchworsethanourneuralnetwork modelsonothermetrics.LSTM-CNNisaforwardLSTMstackingwithCNN,allother parametersaresamewithBLSTM-CNN.Itsf1-scoreis2.6%lowerthanBLSTM-CNN, and P k metric0.017higherthanBLSTM-CNN.Thisperformancegapimpliesthat beingabletocaptureinformationfrombothleftandrightishelpfultodotopicseg- mentation.BLSTM-CNNachievesthebestoverallperformance,highestrecall,highest f1-scoreandlowest P k . FeatureSet PrecisionRecallF1-score P k 1+2+3+4 0.8310.6660.7390.095 1+2+3+4+5 0.850 0.6690.7490.085 1+2+3+4+5+6 0.8350.7080.7670.079 1+2+3+4+5+6+CuePhrase 0.829 0.7300.7760.075 Table4:ComparisonofBLSTM-CNNperformanceondifferentfeatureset. Toexaminetheeffectsofdifferentfeatures,weconductaseriesofexperiments withBLSTM-CNNmodel.TheresultsareshowninTable4.Forthemappingrelations betweenarabnumberandfeaturename,pleaserefertosection4.2. Feature1to4arecalledbasicfeatures,inthesensethattheyaresharedacrossdoc- umentsinalldomains,notjustwebdocuments.Experimentsshowthatbasicfeatures arealreadyenoughtodeliveracompetitiveresult.Hyperlink(feature5),boldanditalic text(feature6)areuniquecharacteristicsforwebdocuments.Incorporatingthesetwo featuresresultsinbetterperformance,f1-scoregoesupby2.8%and P k valuegoes downby0.016.Table4alsoshowsthatourfrequentsubsequenceminingbasedcue phrasealgorithmiscrucialtofurtherboostsystemperformance. 5.3ErrorAnalysis Byanalyzingbadcases,wetherearetwomajortypesofdocumentstructurethat ourmodelperformspoorly:thedocumentwithhierarchicaltopicstructureorimplicit topicstructure. Œ Hierarchicaltopicstructure. Whenweformulatetopicsegmentationasbinary task,weactuallymakeanimplicitassumptionthatdocumenttopics 10 havealinearstructure.However,somedocumentshavehierarchicaltopicstruc- ture.Suchdocumentcontainsseveralmajortopics,andeachmajortopiccontains manysubtopics.Forexample,onedocumentdescribeshowtoproperlya computer,itinvolveshardwaretopicandsoftwareinstallationtopic. Withinsoftwareinstallationtopic,itcontainsmanysubtopicsabouthowtoinstall differentsoftwares.Ourmodelhastroublewithdeterminingthegranularityoftop- ics. Œ Implicittopicstructure. Cuephraseisausefulfeaturetoidentifytopicboundary. However,sometimespeoplestartsanewtopicwithoutusinganycuephrase,and thelexicaldistributionbetweentopicshasnoobviousdifference.Forexample,one documentcontainstwotopics:onetopicisaboutpositiveeffectsofNATO 8 ,the otheroneisaboutnegativeeffectsofNATO.Thereisalexicaloverlap betweenthesetwotopics,andourmodelfailstorecognizethetransitionofunder- lyingtopic. Tohandledocumentwithhierarchicaltopicstructure,ourmodelneedtohavea betterunderstandingofdocument'sglobalstructure,ratherthanmerelyfocusonlocal structure;fordocumentwithimplicittopicstructure,moreaccuratesemanticanalysis algorithmsareneededtodetecttopicboundary. 6ConclusionandFutureWork Inthispaper,weproposetouseBLSTMstackingwithCNNtodotopicsegmenta- tionofwebdocuments.CNNenablesefandeffectiveparagraphrepresentation learning,whileBLSTMmanagestocaptureandpreserveusefulinformationfromboth directions.Basedonthecharacteristicsofwebdocuments,afrequentsubsequencemin- ingbasedcuephrasealgorithmispresentedtoidentifycuephrasesauto- matically.ExperimentsshowthatourBLSTM-CNNmodelcombinedwithcuephrase featureisabletoachievemuchbetterperformancethanstrongbaselinemodels. Forfuturework,wewouldliketoverifyourmodel'seffectivenessonotherdomains andotherlanguages.Also,othernetworkarchitectureswillbeexaminedtofurtherim- provetheperformanceofourtopicsegmentationsystem. Acknowledgements Wethankalltheanonymousreviewersfortheirinsightfulcommentsonthispaper.This workwaspartiallysupportedbyBaidu-PekingUniversityjointproject,andNational NaturalScienceFoundationofChina(61273278and61572049).Thecorrespondence authorofthispaperisSujianLi. References 1.Agrawal,R.,Srikant,R.,etal.:Fastalgorithmsforminingassociationrules.In:Proc.20th int.conf.verylargedatabases,VLDB.vol.1215,pp.487Œ499(1994) 8 NorthAtlanticTreatyOrganization 11 2.Beeferman,D.,Berger,A.,Lafferty,J.:Statisticalmodelsfortextsegmentation.Machine learning34(1-3),177Œ210(1999) 3.Carroll,L.:Evaluatinghierarchicaldiscoursesegmentation.In:HumanLanguageTechnolo- gies:The2010AnnualConferenceoftheNorthAmericanChapteroftheAssociationfor ComputationalLinguistics.pp.993Œ1001.AssociationforComputationalLinguistics(2010) 4.Chen,X.,Qiu,X.,Zhu,C.,Liu,P.,Huang,X.:Longshort-termmemoryneuralnetworks forchinesewordsegmentation.In:ProceedingsoftheConferenceonEmpiricalMethodsin NaturalLanguageProcessing(2015) 5.Chiu,J.P.,Nichols,E.:Namedentityrecognitionwithbidirectionallstm-cnns.arXivpreprint arXiv:1511.08308(2015) 6.Choi,F.Y.:Advancesindomainindependentlineartextsegmentation.In:Proceedingsofthe 1stNorthAmericanchapteroftheAssociationforComputationalLinguisticsconference. pp.26Œ33.AssociationforComputationalLinguistics(2000) 7.Choi,F.Y.,Wiemer-Hastings,P.,Moore,J.:Latentsemanticanalysisfortextsegmentation. In:InProceedingsofEMNLP.Citeseer(2001) 8.Du,L.,Buntine,W.L.,Johnson,M.:Topicsegmentationwithastructuredtopicmodel.In: HLT-NAACL.pp.190Œ200(2013) 9.Eisenstein,J.,Barzilay,R.:Bayesianunsupervisedtopicsegmentation.In:Proceedingsof theConferenceonEmpiricalMethodsinNaturalLanguageProcessing.pp.334Œ343.Asso- ciationforComputationalLinguistics(2008) 10.Galley,M.,McKeown,K.,Fosler-Lussier,E.,Jing,H.:Discoursesegmentationofmulti- partyconversation.In:Proceedingsofthe41stAnnualMeetingonAssociationforCom- putationalLinguistics-Volume1.pp.562Œ569.AssociationforComputationalLinguistics (2003) 11.Georgescul,M.,Clark,A.,Armstrong,S.:Worddistributionsforthematicsegmentationina supportvectormachineapproach.In:ProceedingsoftheTenthConferenceonComputational NaturalLanguageLearning.pp.101Œ108.AssociationforComputationalLinguistics(2006) 12.Hearst,M.A.:Texttiling:Segmentingtextintomulti-paragraphsubtopicpassages.Compu- tationallinguistics23(1),33Œ64(1997) 13.Hochreiter,S.,Schmidhuber,J.:Longshort-termmemory.Neuralcomputation9(8),1735Œ 1780(1997) 14.Jameel,S.,Lam,W.:Anunsupervisedtopicsegmentationmodelincorporatingwordorder. In:Proceedingsofthe36thinternationalACMSIGIRconferenceonResearchanddevelop- mentininformationretrieval.pp.203Œ212.ACM(2013) 15.Pevzner,L.,Hearst,M.A.:Acritiqueandimprovementofanevaluationmetricfortextseg- mentation.ComputationalLinguistics28(1),19Œ36(2002) 16.Riedl,M.,Biemann,C.:Howtextsegmentationalgorithmsgainfromtopicmodels.In:Pro- ceedingsofthe2012ConferenceoftheNorthAmericanChapteroftheAssociationfor ComputationalLinguistics:HumanLanguageTechnologies.pp.553Œ557.Associationfor ComputationalLinguistics(2012) 17.Riedl,M.,Biemann,C.:Textsegmentationwithtopicmodels.JournalforLanguageTech- nologyandComputationalLinguistics27(1),47Œ69(2012) 18.Riedl,M.,Biemann,C.:Topictiling:atextsegmentationalgorithmbasedonlda.In:Proceed- ingsofACL2012StudentResearchWorkshop.pp.37Œ42.AssociationforComputational Linguistics(2012) 19.Wang,P.,Qian,Y.,Soong,F.K.,He,L.,Zhao,H.:Part-of-speechtaggingwithbidirectional longshort-termmemoryrecurrentneuralnetwork.arXivpreprintarXiv:1510.06168(2015) 20.Yamron,J.P.,Carp,I.,Gillick,L.,Lowe,S.,vanMulbregt,P.:Ahiddenmarkovmodelap- proachtotextsegmentationandeventtracking.In:Acoustics,SpeechandSignalProcessing, 1998.Proceedingsofthe1998IEEEInternationalConferenceon.vol.1,pp.333Œ336.IEEE (1998) 12 21.Zeiler,M.D.:Adadelta:anadaptivelearningratemethod.arXivpreprintarXiv:1212.5701 (2012)  
Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) ,pages142Œ146, October25-29,2014,Doha,Qatar. c  2014AssociationforComputationalLinguistics 142 143 144 145 146  
PublishedasaconferencepaperatICLR2018 M IXED P RECISION T RAINING SharanNarang  ,GregoryDiamos,ErichElsen y BaiduResearch f sharan,gdiamos g @baidu.com PauliusMicikevicius  ,JonahAlben,DavidGarcia,BorisGinsburg,MichaelHouston, OleksiiKuchaiev,GaneshVenkatesh,HaoWu NVIDIA f pauliusm,alben,dagarcia,bginsburg,mhouston, okuchaiev,gavenkatesh,skyw g @nvidia.com A BSTRACT Increasingthesizeofaneuralnetworktypicallyimprovesaccuracybutalsoin- creasesthememoryandcomputerequirementsfortrainingthemodel.Weintro- ducemethodologyfortrainingdeepneuralnetworksusinghalf-precision ingpointnumbers,withoutlosingmodelaccuracyorhavingtomodifyhyper- parameters.Thisnearlyhalvesmemoryrequirementsand,onrecentGPUs, speedsuparithmetic.Weights,activations,andgradientsarestoredinIEEEhalf- precisionformat.Sincethisformathasanarrowerrangethansingle-precisionwe proposethreetechniquesforpreventingthelossofcriticalinformation.Firstly, werecommendmaintainingasingle-precisioncopyofweightsthataccumulates thegradientsaftereachoptimizerstep(thiscopyisroundedtohalf-precisionfor theforward-andback-propagation).Secondly,weproposeloss-scalingtopre- servegradientvalueswithsmallmagnitudes.Thirdly,weusehalf-precisionarith- meticthataccumulatesintosingle-precisionoutputs,whichareconvertedtohalf- precisionbeforestoringtomemory.Wedemonstratethattheproposedmethodol- ogyworksacrossawidevarietyoftasksandmodernlargescale(exceeding100 millionparameters)modelarchitectures,trainedonlargedatasets. 1I NTRODUCTION DeepLearninghasenabledprogressinmanydifferentapplications,rangingfromimagerecognition (Heetal.,2016a)tolanguagemodeling(Jozefowiczetal.,2016)tomachinetranslation(Wuetal., 2016)andspeechrecognition(Amodeietal.,2016).Twotrendshavebeencriticaltotheseresults -increasinglylargetrainingdatasetsandincreasinglycomplexmodels.Forexample,theneural networkusedinHannunetal.(2014)had11millionparameterswhichgrewtoapproximately67 millionforbidirectionalRNNsandfurtherto116millionforthelatestforwardonlyGatedRecurrent Unit(GRU)modelsinAmodeietal.(2016). Largermodelsusuallyrequiremorecomputeandmemoryresourcestotrain.Theserequirements canbeloweredbyusingreducedprecisionrepresentationandarithmetic.Performance(speed)of anyprogram,includingneuralnetworktrainingandinference,islimitedbyoneofthreefactors: arithmeticbandwidth,memorybandwidth,orlatency.Reducedprecisionaddressestwoofthese limiters.Memorybandwidthpressureisloweredbyusingfewerbitstotostorethesamenumberof values.Arithmetictimecanalsobeloweredonprocessorsthatofferhigherthroughputforreduced precisionmath.Forexample,half-precisionmaththroughputinrecentGPUsis2  to8  higher thanforsingle-precision.Inadditiontospeedimprovements,reducedprecisionformatsalsoreduce theamountofmemoryrequiredfortraining. Moderndeeplearningtrainingsystemsusesingle-precision(FP32)format.Inthispaper,weaddress thetrainingwithreducedprecisionwhilemaintainingmodelaccuracy.,wetrainvari-  Equalcontribution y NowatGoogleBraineriche@google.com 1 arXiv:1710.03740v3  [cs.AI]  15 Feb 2018PublishedasaconferencepaperatICLR2018 ousneuralnetworksusingIEEEhalf-precisionformat(FP16).SinceFP16formathasanarrower dynamicrangethanFP32,weintroducethreetechniquestopreventmodelaccuracyloss:maintain- ingamastercopyofweightsinFP32,loss-scalingthatminimizesgradientvaluesbecomingzeros, andFP16arithmeticwithaccumulationinFP32.Usingthesetechniqueswedemonstratethata widevarietyofnetworkarchitecturesandapplicationscanbetrainedtomatchtheaccuracyFP32 training.Experimentalresultsincludeconvolutionalandrecurrentnetworkarchitectures,trained forregression,andgenerativetasks.Applicationsincludeimageimage generation,objectdetection,languagemodeling,machinetranslation,andspeechrecognition.The proposedmethodologyrequiresnochangestomodelsortraininghyper-parameters. 2R ELATED W ORK TherehavebeenanumberofpublicationsontrainingConvolutionalNeuralNetworks(CNNs)with reducedprecision.Courbariauxetal.(2015)proposedtrainingwithbinaryweights,allotherten- sorsandarithmeticwereinfullprecision.Hubaraetal.(2016a)extendedthatworktoalsobinarize theactivations,butgradientswerestoredandcomputedinsingleprecision.Hubaraetal.(2016b) consideredquantizationofweightsandactivationsto2,4and6bits,gradientswererealnumbers. Rastegarietal.(2016)binarizealltensors,includingthegradients.However,alloftheseapproaches leadtonon-triviallossofaccuracywhenlargerCNNmodelsweretrainedforILSVRC tiontask(Russakovskyetal.,2015).Zhouetal.(2016)quantizeweights,activations,andgradients todifferentbitcountstofurtherimproveresultaccuracy.Thisstillincurssomeaccuracylossand requiresasearchoverbitwidthpernetwork,whichcanbeimpracticalforlarger models.Mishraetal.improveonthetop-1accuracyachievedbypriorweightandactivationquan- tizationsbydoublingortriplingthewidthoflayersinpopularCNNs.However,thegradientsare stillcomputedandstoredinsingleprecision,whilequantizedmodelaccuracyislowerthanthatof thewidenedbaseline.Guptaetal.(2015)demonstratethat16bitedpointrepresentationcanbe usedtotrainCNNsonMNISTandCIFAR-10datasetswithoutaccuracyloss.Itisnotclearhow thisapproachwouldworkonthelargerCNNstrainedonlargedatasetsorwhetheritwouldworkfor RecurrentNeuralNetworks(RNNs). TherehavealsobeenseveralproposalstoquantizeRNNtraining.Heetal.(2016c)trainquantized variantsoftheGRU(Choetal.,2014)andLongShortTermMemory(LSTM)(Hochreiterand Schmidhuber,1997)cellstousefewerbitsforweightsandactivations,albeitwithasmalllossin accuracy.Itisnotclearwhethertheirresultsholdforlargernetworksneededforlargerdatasets Hubaraetal.(2016b)proposeanotherapproachtoquantizeRNNswithoutalteringtheirstructure. AnotherapproachtoquantizeRNNsisproposedinOttetal.(2016).Theyevaluatebinary,ternary andexponentialquantizationforweightsinvariousdifferentRNNmodelstrainedforlanguage modellingandspeechrecognition.Alloftheseapproachesleavethegradientsdinsingle- precisionandthereforethecomputationcostduringbackpropagationisunchanged. Thetechniquesproposedinthispaperaredifferentfromtheaboveapproachesinthreeaspects. First,alltensorsandarithmeticforforwardandbackwardpassesusereducedprecision,FP16in ourcase.Second,nohyper-parameters(suchaslayerwidth)areadjusted.Lastly,modelstrained withthesetechniquesdonotincuraccuracylosswhencomparedtosingle-precisionbaselines.We demonstratethatthistechniqueworksacrossavarietyofapplicationsusingstate-of-the-artmodels trainedonlargescaledatasets. 3I MPLEMENTATION WeintroducethekeytechniquesfortrainingwithFP16whilestillmatchingthemodelaccuracyof FP32trainingsession:single-precisionmasterweightsandupdates,loss-scaling,andaccumulating FP16productsintoFP32.ResultsoftrainingwiththesetechniquesarepresentedinSection4. 3.1FP32 MASTERCOPYOFWEIGHTS Inmixedprecisiontraining,weights,activationsandgradientsarestoredasFP16.Inordertomatch theaccuracyoftheFP32networks,anFP32mastercopyofweightsismaintainedandupdatedwith theweightgradientduringtheoptimizerstep.IneachiterationanFP16copyofthemasterweightsis 2 PublishedasaconferencepaperatICLR2018 Figure1:Mixedprecisiontrainingiterationforalayer. usedintheforwardandbackwardpass,halvingthestorageandbandwidthneededbyFP32training. Figure1illustratesthismixedprecisiontrainingprocess. WhiletheneedforFP32masterweightsisnotuniversal,therearetwopossiblereasonswhya numberofnetworksrequireit.Oneexplanationisthatupdates(weightgradientsmultipliedbythe learningrate)becometoosmalltoberepresentedinFP16-anyvaluewhosemagnitudeissmaller than 2  24 becomeszeroinFP16.WecanseeinFigure2bthatapproximately5%ofweightgradient valueshaveexponentssmallerthan  24 .Thesesmallvaluedgradientswouldbecomezerointhe optimizerwhenmultipliedwiththelearningrateandadverselyaffectthemodelaccuracy.Usinga single-precisioncopyfortheupdatesallowsustoovercomethisproblemandrecovertheaccuracy. Anotherexplanationisthattheratiooftheweightvaluetotheweightupdateisverylarge.In thiscase,eventhoughtheweightupdateisrepresentableinFP16,itcouldstillbecomezerowhen additionoperationright-shiftsittoalignthebinarypointwiththeweight.Thiscanhappenwhen themagnitudeofanormalizedweightvalueisatleast2048timeslargerthatoftheweightupdate. SinceFP16has10bitsofmantissa,theimplicitbitmustberight-shiftedby11ormorepositionsto potentiallycreateazero(insomecasesroundingcanrecoverthevalue).Incaseswheretheratiois largerthan2048,theimplicitbitwouldberight-shiftedby12ormorepositions.Thiswillcausethe weightupdatetobecomeazerowhichcannotberecovered.Anevenlargerratiowillresultinthis effectforde-normalizednumbers.Again,thiseffectcanbecounteractedbycomputingtheupdate inFP32. ToillustratetheneedforanFP32mastercopyofweights,weusetheMandarinspeechmodel (describedinmoredetailinSection4.3)trainedonadatasetcomprisingofapproximately800hours ofspeechdatafor20epochs.Asshownin2a,wematchFP32trainingresultswhenupdatingan FP32mastercopyofweightsafterFP16forwardandbackwardpasses,whileupdatingFP16weights resultsin80%relativeaccuracyloss. Eventhoughmaintaininganadditionalcopyofweightsincreasesthememoryrequirementsforthe weightsby50%comparedwithsingleprecisiontraining,impactonoverallmemoryusageismuch smaller.Fortrainingmemoryconsumptionisdominatedbyactivations,duetolargerbatchsizes andactivationsofeachlayerbeingsavedforreuseintheback-propagationpass.Sinceactivations arealsostoredinhalf-precisionformat,theoverallmemoryconsumptionfortrainingdeepneural networksisroughlyhalved. 3.2L OSSSCALING FP16exponentbiascenterstherangeofnormalizedvalueexponentsto [  14 ; 15] whilegradient valuesinpracticetendtobedominatedbysmallmagnitudes(negativeexponents).Forexample, considerFigure3showingthehistogramofactivationgradientvalues,collectedacrossalllayers duringFP32trainingofMultiboxSSDdetectornetwork(Liuetal.,2015a).Notethatmuchof theFP16representablerangewasleftunused,whilemanyvalueswerebelowtheminimumrepre- sentablerangeandbecamezeros.Scalingupthegradientswillshiftthemtooccupymoreofthe representablerangeandpreservevaluesthatareotherwiselosttozeros.Thisparticularnetwork divergeswhengradientsarenotscaled,butscalingthembyafactorof8(increasingtheexponents by3)issuftomatchtheaccuracyachievedwithFP32training.Thissuggeststhatactivation 3 PublishedasaconferencepaperatICLR2018 (a)Trainingandvalidation(dev0)curvesforMandarin speechrecognitionmodel (b)GradienthistogramforMandarintrainingrun Figure2:Figure2ashowstheresultsofthreeexperiemnts;baseline(FP32),pseudoFP16with FP32mastercopy,pseudoFP16withoutFP32mastercopy.Figure2bshowsthehistogramforthe exponentsofweightgradientsforMandarinspeechrecognitiontrainingwithFP32weights.The gradientsaresampledevery4,000iterationsduringtrainingforallthelayersinthemodel. Figure3:HistogramofactivationgradientvaluesduringthetrainingofMultiboxSSDnetwork. Notethatthebinsonthex-axiscovervaryingrangesandthere'saseparatebinforzeros.For example,2%ofthevaluesareinthe [2  34 ; 2  32 ) range,2%ofvaluesareinthe [2  24 ; 2  23 ) range, and67%ofvaluesarezero. gradientvaluesbelow 2  27 inmagnitudewereirrelevanttothetrainingofthismodel,butvaluesin the [2  27 ; 2  24 ) rangewereimportanttopreserve. OneefwaytoshiftthegradientvaluesintoFP16-representablerangeistoscalethelossvalue computedintheforwardpass,priortostartingback-propagation.Bychainruleback-propagation ensuresthatallthegradientvaluesarescaledbythesameamount.Thisrequiresnoextraoperations duringback-propagationandkeepstherelevantgradientvaluesfrombecomingzeros.Weightgradi- entsmustbeunscaledbeforeweightupdatetomaintaintheupdatemagnitudesasinFP32training.It issimplesttoperformthisunscalingrightafterthebackwardpassbutbeforegradientclippingorany othergradient-relatedcomputations,ensuringthatnohyper-parameters(suchasgradientclipping threshold,weightdecay,etc.)havetobeadjusted. Thereareseveraloptionstochoosethelossscalingfactor.Thesimplestoneistopickacon- stantscalingfactor.Wetrainedavarietyofnetworkswithscalingfactorsrangingfrom8to32K (manynetworksdidnotrequireascalingfactor).Aconstantscalingfactorcanbechosenempir- 4 PublishedasaconferencepaperatICLR2018 icallyor,ifgradientstatisticsareavailable,directlybychoosingafactorsothatitsproductwith themaximumabsolutegradientvalueisbelow65,504(themaximumvaluerepresentableinFP16). Thereisnodownsidetochoosingalargescalingfactoraslongasitdoesnotcauseovwduring back-propagation-ovwswillresultinandNaNsintheweightgradientswhichwill irreversiblydamagetheweightsafteranupdate.Notethatovwscanbeefdetectedby inspectingthecomputedweightgradients,forexample,whenweightgradientvaluesareunscaled. Oneoptionistoskiptheweightupdatewhenanovwisdetectedandsimplymoveontothe nextiteration. 3.3A RITHMETICPRECISION Byandlargeneuralnetworkarithmeticfallsintothreecategories:vectordot-products,reductions, andpoint-wiseoperations.Thesecategoriesfromdifferenttreatmentwhenitcomestore- ducedprecisionarithmetic.Tomaintainmodelaccuracy,wefoundthatsomenetworksrequirethat FP16vectordot-productaccumulatesthepartialproductsintoanFP32value,whichisconverted toFP16beforewritingtomemory.WithoutthisaccumulationinFP32,someFP16modelsdidnot matchtheaccuracyofthebaselinemodels.WhereaspreviousGPUssupportedonlyFP16multiply- addoperation,NVIDIAVoltaGPUsintroduceTensorCoresthatmultiplyFP16inputmatricesand accumulateproductsintoeitherFP16orFP32outputs(NVIDIA,2017). Largereductions(sumsacrosselementsofavector)shouldbecarriedoutinFP32.Suchreductions mostlycomeupinbatch-normalizationlayerswhenaccumulatingstatisticsandsoftmaxlayers. BothofthelayertypesinourimplementationsstillreadandwriteFP16tensorsfrommemory, performingthearithmeticinFP32.Thisdidnotslowdownthetrainingprocesssincetheselayers arememory-bandwidthlimitedandnotsensitivetoarithmeticspeed. Point-wiseoperations,suchasnon-linearitiesandelement-wisematrixproducts,arememory- bandwidthlimited.Sincearithmeticprecisiondoesnotimpactthespeedoftheseoperations,either FP16orFP32mathcanbeused. 4R ESULTS Wehaverunexperimentsforavarietyofdeeplearningtaskscoveringawiderangeofdeeplearning models.Weconductedthefollowingexperimentsforeachapplication:  Baseline(FP32) :Single-precisionstorageisusedforactivations,weightsandgradients. AllarithmeticisalsoinFP32.  MixedPrecision(MP) :FP16isusedforstorageandarithmetic.Weights,activationsand gradientsarestoredusinginFP16,anFP32mastercopyofweightsisusedforupdates. Loss-scalingisusedforsomeapplications.ExperimentswithFP16arithmeticusedTensor CoreoperationswithaccumulationintoFP32forconvolutions,fully-connectedlayers,and matrixmultipliesinrecurrentlayers. TheBaselineexperimentswereconductedonNVIDIA'sMaxwellorPascalGPU.MixedPrecision experimentswereconductedonVoltaV100thataccumulatesFP16productsintoFP32.Themixed precisionspeechrecognitionexperiments(Section4.3)wereconductedusingMaxwellGPUsusing FP16storageonly.ThissetupallowsustoemulatetheTensorCoreoperationsonnon-Voltahard- ware.Anumberofnetworksweretrainedinthismodetothatresultingmodelaccuracies areequivalenttoMPtrainingrunonVoltaV100GPUs.ThisisintuitivesinceMParithmeticwas accumulatingFP16productsintoFP32beforeconvertingtheresulttoFP16onamemorywrite. 4.1CNN SFOR ILSVRCC LASSIFICATION WetrainedseveralCNNsforILSVRCtask(Russakovskyetal.,2015)usingmixed precision:Alexnet,VGG-D,GoogLeNet,Inceptionv2,Inceptionv3,andpre-activationResnet-50. Inallofthesecaseswewereabletomatchthetop-1accuracyofbaselineFP32trainingsession usingidenticalhyper-parameters.NetworksweretrainedusingCaffe(Jiaetal.,2014)framework touseVoltaTensorOps,exceptforResnet50whichusedPyTorch(Paszkeetal.,2017). 5 PublishedasaconferencepaperatICLR2018 Trainingscheduleswereusedfrompublicrepositories,whenavailable(trainingscheduleforVGG- Dhasnotbeenpublished).Top-1accuracyonILSVRCvalidationsetareshowninTable1.Baseline (FP32)accuracyinafewcasesisdifferentfrompublishedresultsduetosingle-croptestinganda simplerdataaugmentation.OurdataaugmentationinCaffeincludedrandomhorizontaland randomcroppingfrom256x256images,Resnet50traininginPyTorchusedthefullaugmentationin thetrainingscriptfromPyTorchvisionrepository. Table1:ILSVRC12top-1accuracy. Model Baseline MixedPrecision Reference AlexNet 56.77% 56.93% (Krizhevskyetal.,2012) VGG-D 65.40% 65.43% (SimonyanandZisserman,2014) GoogLeNet(Inceptionv1) 68.33% 68.43% (Szegedyetal.,2015) Inceptionv2 70.03% 70.02% (IoffeandSzegedy,2015) Inceptionv3 73.85% 74.13% (Szegedyetal.,2016) Resnet50 75.92% 76.04% (Heetal.,2016b) Loss-scalingtechniquewasnotrequiredforsuccessfulmixedprecisiontrainingofthesenetworks. WhilealltensorsintheforwardandbackwardpasseswereinFP16,amastercopyofweightswas updatedinFP32asoutlinedinSection3.1. 4.2D ETECTION CNN S Objectdetectionisaregressiontask,whereboundingboxcoordinatevaluesarepredictedbythe network(comparedtowherethepredictedvaluesarepassedthroughasoftmaxlayer toconvertthemtoprobabilities).Objectdetectorsalsohaveacomponent,whereprob- abilitiesforanobjecttypearepredictedforeachboundingbox.Wetrainedtwopopulardetection approaches:Faster-RCNN(Renetal.,2015)andMultibox-SSD(Liuetal.,2015a).Bothdetectors usedVGG-16networkasthebackbone.Modelsandtrainingscriptswerefrompublicrepositories (Girshick;Liu).Meanaverageprecision(mAP)wascomputedonPascalVOC2007testset.Faster- RCNNwastrainedonVOC2007trainingset,whereasSSDwastrainedonaunionofVOC2007 and2012data,whichisthereasonbehindbaselinemAPdifferenceinTable2. Table2:Detectionnetworkaveragemeanprecision. Model Baseline MPwithoutloss-scale MPwithloss-scale FasterR-CNN 69.1% 68.6% 69.7% MultiboxSSD 76.9% diverges 77.1% Ascanbeseenintable2,SSDdetectorfailedtotraininFP16withoutloss-scaling.Bylosing smallgradientvaluestozeros,asdescribedinSection3.2,poorweightsarelearnedandtraining diverges.AsdescribedinSection3.2,loss-scalingfactorof8recoverstherelevantgradientvalues andmixed-precisiontrainingmatchesFP32mAP. 4.3S PEECH R ECOGNITION WeexploremixedprecisiontrainingforspeechdatausingtheDeepSpeech2modelforbothEnglish andMandarindatasets.ThemodelusedfortrainingontheEnglishdatasetconsistsoftwo2Dcon- volutionlayers,threerecurrentlayerswithGRUcells,1rowconvolutionlayerandConnectionist temporal(CTC)costlayer(Gravesetal.,2006).Ithasapproximately115millionpa- rameters.Thismodelistrainedonourinternaldatasetconsistingof6000hoursofEnglishspeech. TheMandarinmodelhasasimilararchitecturewithatotalof215millionparameters.TheMan- darinmodelwastrainedon2600hoursofourinternaltrainingset.Forthesemodels,werunthe BaselineandPseudoFP16experiments.Allthemodelsweretrainedfor20epochsusingNesterov StochasticGradientDescent(SGD).Allhyper-parameterssuchaslearningrate,annealingschedule andmomentumwerethesameforbaselineandpseudoFP16experiments.Table3showstheresults oftheseexperimentsonindependenttestsets. 6 PublishedasaconferencepaperatICLR2018 Table3:CharacterErrorRate(CER)usingmixedprecisiontrainingforspeechrecognition.English resultsarereportedontheWSJ'92testset.Mandarinresultsarereportedonourinternaltestset. Model/Dataset Baseline MixedPrecision English 2.20 1.99 Mandarin 15.82 15.01 Similartoanddetectionnetworks,mixedprecisiontrainingworkswellforrecurrent neuralnetworkstrainedonlargescalespeechdatasets.Thesespeechmodelsarethelargestmodels trainedusingthistechnique.Also,thenumberoftime-stepsinvolvedintrainingaspeechmodelare unusuallylargecomparedtootherapplicationsusingrecurrentlayers.Asshownintable3,Pseudo FP16resultsareroughly5to10%betterthanthebaseline.Thissuggeststhatthehalf-precision storageformatmayactasaregularizerduringtraining. Figure4:EnglishtoFrenchtranslationnetworktrainingperplexity,3x1024LSTMmodelwith attention.Ref1,ref2andref3representthreedifferentFP32trainingruns. 4.4M ACHINE T RANSLATION ForlanguagetranslationwetrainedseveralvariantsofthemodelinTensorFlowtutorialforEn- glishtoFrenchtranslation(Google).Themodelusedword-vocabularies,100Kand40Kentriesfor EnglishandFrench,respectively.Thenetworkswetrainedhad3or5layersintheencoderand decoder,each.Inbothcasesalayerconsistedof1024LSTMcells.SGDoptimizerwasusedto trainonWMT15dataset.Therewasanoticeablevariationinaccuracyofdifferenttrainingsessions withthesamesettings.Forexample,seethethreeFP32curvesinFigure4,whichshowsthe3-layer model.Mixed-precisionwithloss-scalingmatchedtheFP32results,whilenoloss-scalingresulted inaslightdegradationintheresults.The5-layermodelexhibitedthesametrainingbehavior. 4.5L ANGUAGE M ODELING WetrainedEnglishlanguagemodel,designatedasbigLSTM(Jozefowiczetal.,2016),onthe1 billionworddataset.Themodelconsistsoftwolayersof8192LSTMcellswithprojectiontoa 1024-dimensionalembedding.Thismodelwastrainedfor50epochsusingtheAdagradoptimizer. Thethevocabularysizeis793Kwords.Duringtraining,weuseasampledsoftmaxlayerwith8K negativesamples.Batchsizeaggregatedover4GPUsis1024.TomatchFP32perplexitytraining thisnetworkwithFP16requiresloss-scaling,asshowninFigure5.Withoutlossscalingthetraining perplexitycurveforFP16trainingdiverges,comparedwiththeFP32training,after300Kiterations. Scalingfactorof128recoversalltherelevantgradientvaluesandtheaccuracyofFP16training matchesthebaselinerun. 4.6DCGAN RESULTS GenerativeAdversarialNetworks(GANs)combineregressionanddiscriminationtasksduringtrain- ing.Forimagetasks,thegeneratornetworkregressespixelcolors.Inourcase,thegeneratorpredicts threechannelsof8-bitcolorvalueseach.Thenetworkwastrainedtogenerate128x128pixelim- agesoffaces,usingDCGANmethodology(Radfordetal.,2015)andCelebFacesdataset(Liuetal., 7 PublishedasaconferencepaperatICLR2018 Figure5:bigLSTMtrainingperplexity Figure6:AnuncuratedsetoffaceimagesgeneratedbyDCGAN.FP32training(left)andmixed- precisiontraining(right). 2015b).Thegeneratorhad7layersoffractionally-stridedconvolutions,6withleakyReLUactiva- tions,1with tanh .Thediscriminatorhad6convolutions,and2fully-connectedlayers.Allused leakyReLUactivationsexceptforthelastlayer,whichusedsigmoid.Batchnormalizationwasap- pliedtoalllayersexceptthelastfully-connectedlayerofthediscriminator.Adamoptimizerwas usedtotrainfor100Kiterations.AnsetofoutputimagesinFigure6.Notethatweshowarandomly selectedsetofoutputimages,whereasGANpublicationstypicallyshowacuratedsetofoutputsby excludingpoorexamples.Unlikeothernetworkscoveredinthispaper,GANsdonothaveawidely- acceptedoftheirresultquality.QualitativelytheoutputsofFP32andmixed-precision trainingappearcomparable.Thisnetworkdidnotrequireloss-scalingtomatchFP32results. 5C ONCLUSIONSAND F UTURE W ORK Mixedprecisiontrainingisanimportanttechniquethatallowsustoreducethememoryconsump- tionaswellastimespentinmemoryandarithmeticoperationsofdeepneuralnetworks.Wehave demonstratedthatmanydifferentdeeplearningmodelscanbetrainedusingthistechniquewithno lossinaccuracywithoutanyhyper-parametertuning.Forcertainmodelswithalargenumberof smallgradientvalues,weintroducethegradientscalingmethodtohelpthemconvergetothesame accuracyasFP32baselinemodels. DNNoperationsbenchmarkedwithDeepBench 1 onVoltaGPUsee2-6xspeedupscomparedto FP32implementationsiftheyarelimitedbymemoryorarithmeticbandwidth.Speedupsarelower whenoperationsarelatency-limited.Fullnetworktrainingandinferencespeedupsdependonlibrary 1 https://github.com/baidu-research/DeepBench 8 PublishedasaconferencepaperatICLR2018 andframeworkoptimizationsformixedprecisionandareafocusoffuturework(experimentsinthis paperwerecarriedoutwithearlyversionsofbothlibrariesandframeworks). Wewouldalsoliketoextendthisworktoincludegenerativemodelsliketext-to-speechsystems anddeepreinforcementlearningapplications.Furthermore,automatingloss-scalingfactorselection wouldfurthersimplifytrainingwithmixedprecision.Loss-scalingfactorcouldbedynamically increasedordecreasedbyinspectingtheweightgradientsforovw,skippingweightupdates whenanovwisdetected. 9 PublishedasaconferencepaperatICLR2018 R EFERENCES D.Amodei,R.Anubhai,E.Battenberg,C.Case,J.Casper,B.Catanzaro,J.Chen,M.Chrzanowski, A.Coates,G.Diamos,etal.Deepspeech2:End-to-endspeechrecognitioninenglishand mandarin.In ProceedingsofThe33rdInternationalConferenceonMachineLearning ,pages 173Œ182,2016. K.Cho,B.VanMerri ¨ enboer,C.Gulcehre,D.Bahdanau,F.Bougares,H.Schwenk,andY.Bengio. Learningphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation. arXivpreprintarXiv:1406.1078 ,2014. M.Courbariaux,Y.Bengio,andJ.-P.David.Binaryconnect:Trainingdeepneuralnetworkswith binaryweightsduringpropagations.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama, andR.Garnett,editors, AdvancesinNeuralInformationProcessingSystems28 ,pages 3123Œ3131.CurranAssociates,Inc.,2015.URL http://papers.nips.cc/paper/ 5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations. pdf . R.Girshick.Fasterr-cnngithubrepository. https://github.com/rbgirshick/ py-faster-rcnn . Google.Twtutorial:Sequence-to-sequencemodels.URL https://www. tensorflow.org/tutorials/seq2seq . A.Graves,S.Fern ´ andez,F.Gomez,andJ.Schmidhuber.Connectionisttemporal labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In Proceedingsofthe23rd internationalconferenceonMachinelearning ,pages369Œ376.ACM,2006. S.Gupta,A.Agrawal,K.Gopalakrishnan,andP.Narayanan.Deeplearningwithlimitednumerical precision.In Proceedingsofthe32ndInternationalConferenceonMachineLearning(ICML-15) , pages1737Œ1746,2015. A.Hannun,C.Case,J.Casper,B.Catanzaro,G.Diamos,E.Elsen,R.Prenger,S.Satheesh,S.Sen- gupta,A.Coates,etal.Deepspeech:Scalingupend-to-endspeechrecognition. arXivpreprint arXiv:1412.5567 ,2014. K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimagerecognition.In Proceedings oftheIEEEconferenceoncomputervisionandpatternrecognition ,pages770Œ778,2016a. K.He,X.Zhang,S.Ren,andJ.Sun.Identitymappingsindeepresidualnetworks.In ECCV ,2016b. Q.He,H.Wen,S.Zhou,Y.Wu,C.Yao,X.Zhou,andY.Zou.Effectivequantizationmethodsfor recurrentneuralnetworks. arXivpreprintarXiv:1611.10176 ,2016c. S.HochreiterandJ.Schmidhuber.Longshort-termmemory. NeuralComput. ,9(8):1735Œ1780,Nov. 1997.ISSN0899-7667.doi:10.1162/neco.1997.9.8.1735.URL http://dx.doi.org/10. 1162/neco.1997.9.8.1735 . I.Hubara,M.Courbariaux,D.Soudry,R.El-Yaniv,andY.Bengio.Binarizedneuralnetworks.In AdvancesinNeuralInformationProcessingSystems ,pages4107Œ4115,2016a. I.Hubara,M.Courbariaux,D.Soudry,R.El-Yaniv,andY.Bengio.Quantizedneuralnet- works:Trainingneuralnetworkswithlowprecisionweightsandactivations. arXivpreprint arXiv:1609.07061 ,2016b. S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreduc- inginternalcovariateshift.InF.R.BachandD.M.Blei,editors, ICML ,volume37of JMLRWorkshopandConferenceProceedings ,pages448Œ456.JMLR.org,2015.URL http: //dblp.uni-trier.de/db/conf/icml/icml2015.html#IoffeS15 . Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,S.Guadarrama,andT.Darrell. Caffe:Convolutionalarchitectureforfastfeatureembedding. arXivpreprintarXiv:1408.5093 , 2014. 10 PublishedasaconferencepaperatICLR2018 R.Jozefowicz,O.Vinyals,M.Schuster,N.Shazeer,andY.Wu.Exploringthelimitsoflanguage modeling,2016.URL https://arxiv.org/pdf/1602.02410.pdf . A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenetwithdeepconvo- lutionalneuralnetworks.InF.Pereira,C.J.C.Burges,L.Bottou,andK.Q.Wein- berger,editors, AdvancesinNeuralInformationProcessingSystems25 ,pages1097Œ 1105.CurranAssociates,Inc.,2012.URL http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks. pdf . W.Liu.Ssdgithubrepository. https://github.com/weiliu89/caffe/tree/ssd . W.Liu,D.Anguelov,D.Erhan,C.Szegedy,andS.E.Reed.Ssd:Singleshotmultiboxdetec- tor. CoRR ,abs/1512.02325,2015a.URL http://dblp.uni-trier.de/db/journals/ corr/corr1512.html#LiuAESR15 . Z.Liu,P.Luo,X.Wang,andX.Tang.Deeplearningfaceattributesinthewild.In Proceedingsof InternationalConferenceonComputerVision(ICCV) ,2015b. A.Mishra,E.Nurvitadhi,J.Cook,andD.Marr.Wrpn:Widereduced-precisionnetworks. arXiv preprintarXiv:1709.01134,year=2017 . NVIDIA.Nvidiateslav100gpuarchitecture. https://images.nvidia.com/content/ volta-architecture/pdf/Volta-Architecture-Whitepaper-v1.0.pdf , 2017. J.Ott,Z.Lin,Y.Zhang,S.-C.Liu,andY.Bengio.Recurrentneuralnetworkswithlimitednumerical precision. arXivpreprintarXiv:1608.06902 ,2016. A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.DeVito,Z.Lin,A.Desmaison,L.Antiga, andA.Lerer.Automaticdifferentiationinpytorch.2017. A.Radford,L.Metz,andS.Chintala.Unsupervisedrepresentationlearningwithdeepconvolu- tionalgenerativeadversarialnetworks. CoRR ,abs/1511.06434,2015.URL http://dblp. uni-trier.de/db/journals/corr/corr1511.html#RadfordMC15 . M.Rastegari,V.Ordonez,J.Redmon,andA.Farhadi. XNOR-Net:ImageNetUsing BinaryConvolutionalNeuralNetworks ,pages525Œ542.SpringerInternationalPublishing,Cham, 2016.ISBN978-3-319-46493-0.doi:10.1007/978-3-319-46493-0 32.URL https://doi. org/10.1007/978-3-319-46493-0_32 . S.Ren,K.He,R.Girshick,andJ.Sun.FasterR-CNN:Towardsreal-timeobjectdetectionwith regionproposalnetworks.In NeuralInformationProcessingSystems(NIPS) ,2015. O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,S.Ma,Z.Huang,A.Karpathy,A.Khosla, M.Bernstein,A.C.Berg,andL.Fei-Fei.ImageNetLargeScaleVisualRecognitionChal- lenge. InternationalJournalofComputerVision(IJCV) ,115(3):211Œ252,2015.doi:10.1007/ s11263-015-0816-y. K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecogni- tion. arXivpreprintarXiv:1409.1556 ,2014. C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,andA.Ra- binovich.Goingdeeperwithconvolutions.In ComputerVisionandPatternRecognition(CVPR) , 2015.URL http://arxiv.org/abs/1409.4842 . C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna.Rethinkingtheinceptionarchitec- tureforcomputervision.In TheIEEEConferenceonComputerVisionandPatternRecognition (CVPR) ,June2016. Y.Wu,M.Schuster,Z.Chen,Q.V.Le,M.Norouzi,W.Macherey,M.Krikun,Y.Cao,Q.Gao, K.Macherey,etal.Google'sneuralmachinetranslationsystem:Bridgingthegapbetweenhuman andmachinetranslation. arXivpreprintarXiv:1609.08144 ,2016. 11 PublishedasaconferencepaperatICLR2018 S.Zhou,Z.Ni,X.Zhou,H.Wen,Y.Wu,andY.Zou.Dorefa-net:Traininglowbitwidthcon- volutionalneuralnetworkswithlowbitwidthgradients. CoRR ,abs/1606.06160,2016.URL http://arxiv.org/abs/1606.06160 . 12  
InternationalJointConferenceonNaturalLanguageProcessing ,pages849Œ853, Nagoya,Japan,14-18October2013. 849 850 851 852 853  
Multi-Transfer:TransferLearningwithMultipleViewsandMultipleSources BenTan  ErhengZhong  EvanWeiXiang  QiangYang  Abstract Transferlearning,whichaimstohelpthelearningtask inatargetdomainbyleveragingknowledgefromauxil- iarydomains,hasbeendemonstratedtobeein tapplications,e.g.,textmining,sentimentanal- ysis,etc.Inaddition,inmanyreal-worldapplications, auxiliarydataaredescribedfrommultipleperspectives andusuallycarriedbymultiplesources.Forexample, tohelpclassifyvideosonYoutube,whichincludethree views/perspectives:image,voiceandsubtitles,onemay borrowdatafromFlickr,Last.FMandGoogleNews. Althoughanysingleinstanceinthesedomainscanonly coverapartoftheviewsavailableonYoutube,actually thepieceofinformationcarriedbythemmaycompen- satewitheachother.Inthispaper,wethistrans- ferlearningproblemas TransferLearningwithMulti- pleViewsandMultipleSources .Astsources mayhavetprobabilitydistributionsand entviewsmaybecompensateorinconsistentwitheach other,mergingalldatainasimplisticmannerwillnot giveoptimalresult.Thus,weproposeanovelalgorithm toleverageknowledgefromtviewsandsources collaboratively,bylettingtviewsfrom tsourcescomplementeachotherthroughaco-training styleframework,whilerevisethedistribution intdomains.Weconductempiricalstudieson severalreal-worlddatasetstoshowthattheproposed approachcanimprovetheaccuracybyup to8%againsttstate-of-the-artbaselines. Keywords: TransferLearning,Multi-ViewLearn- ing,MultipleDataSources 1Introduction Inreal-worldapplications,thelackoflabeleddata makesmanysupervisedlearningalgorithmsfailtobuild accuratemodels.Tosolvethelimitedsupervisionprob- lem,transferlearningaimstoborrowknowledgefrom auxiliarydomainstoimprovethetarget-domainmod- elperformance.Manyapplicationshavebeenreport- ed,rangingfromtext[5],sentimentanaly- sis[4],eventrecognition[6],tomultimediaanalysis[16]. Althoughtraditionaltransferworksforsingle-source andsingle-viewscenario,infact,manyreal-worldappli-  HongKongUniversityofScienceandTechnology. f btan,ezhong,wxiang,qyang g @cse.ust.hk . cationsarecomplexwheretheauxiliaryexamplesareof- tendescribedfromtperspectivesandcomefrom avarietyofpotentialsources.Forexample,invideo analysis,avideocanbedescribedbytissues, suchasimages,voice,andsubtitles,wheredatafrom tviewscanbeborrowedfromtdomains, suchasnewsfromGoogleNews 1 ,voicefromLast.FM 2 andimagesfromFlickr 3 .Anotherexampleistextclas- onGoogleNews,where20Newsgroups 4 and Reuters 5 canconsideredassourcedomainsandcover tvocabulariesofgooglenews. Intherecentyears,severalapproacheshavebeen proposedtoplacetransferlearningunderthemulti- view(MVTL)setting[18]orthemulti-sourcesetting (MSTL)[10].ExistingalgorithmsinMVTLsolvethe transferlearningproblemwheresourceandtargetdo- mainssharethesameviewswhileexistingMSTLap- proachesconsiderthattherearemultiplesourcesbut withoneviewforsourceandtargetdata.Infact,in manyreal-worldapplications,multi-viewinformationis distributedonmultiplesourcedomainsandeachsource domaincancoveronlypartsofthetargetviews.Forex- ample,invideoanalysis,avideocanbedescribedfrom threetviews,includingimagesineachframe, voicesandtextsinsubtitles.Then,timage,text andvoicesourcescanbeexploitedwhilethesesources canonlycoverpartsofthetargetviews.Wethis problemas TransferLearningwithMultipleViewsand MultipleSources ,andTL-MVMSforshort.Anintuitive waytousetheserichdataistosimplymergeallsources orallviewstogether,anddirectlyemployMVTLorM- STLrespectively.Unfortunately,tsourceshave tfeaturespacesandmayfollowtdistri- butions,whiletviewsfromtsourcesmay beeveninconsistentwitheachother.Suchintuitiveso- lutionmaymakeusfailtomakefulluseofrichsource information.Forexample,songsonLast.FMandim- agesfromFlickrmayhavetprobabilitydensi- tiesandmaynotagreewitheachotheroncategorizing videos.Ononehand,ifweapplyMSTL,tviews willbeconsideredequallyandtheinconsistencecannot 1 http://news.google.com/ 2 http://www.last.fm 3 http://www.flickr.com/ 4 http://qwone.com/ ~ jason/20Newsgroups/ 5 http://www.daviddlewis.com/resources/testcollections/ beremoved;ontheotherhand,ifMVTLisapplied,dif- ferentsourcedistributionsmaymakealgorithmsfailto buildaconsistentmodel. Recently,co-training[2]hasbeendemonstratedto beetoutilizemulti-viewdata,wherea erbuiltfromoneviewwillprovidepseudolabeledda- tawithhightoenhancetheperformance ofanotherfromanotherview.Thus,eventhe knowledgeineachviewisincomplete,theycancompen- sateeachotherbyexchanginginformation.However, applyingco-trainingsimplymaycausetwoproblems:1. duetothedistributionshiftinboththemarginaldistri- butionandconditionalprobabilitybetweensourceand targetdomains,thedecisionboundariesofsourceand targetdomainscanbeverytandhencethecon- measureisnotanaccurateindicatoranymore; 2.onaccountofthejoint-distributionthe predictionsacrossdomainsarenolongerconsistent. Tocopewiththenatureofmultipleviewsandmul- tiplesourcesinTL-MVMS,weextendco-trainingac- cordinglyanddevelopanovelsolutioncalled multi- transfer .Multi-transferovercomestheabovechal- lengesfromtwoaspects.Itintroducesaharmonic- functionbasedcriteriontoselecttheappropriatetar- getinstances.Suchcriterionisinsensitivetothecon- ditionalprobabilityshift.Secondly,itappliesadensi- tyratioweightingschemetoaccountforthemarginal- distributionshiftandexploitsanon-parametricmethod tomeasurethejoint-distributionratiobetweendata fromtwodomains.Thisstrategyre-weightsthein- stancesinsourcedomains,inordertorevisethedis- tributionshiftandbuildaconsistentmodelforthetar- getdomain.Wewillshowthat,ononehand,theco- trainingstyleprocedurecanexploitknowledgefromd- tviewstohelpeachother;ontheotherhand, thedistributionrevisioncanguaranteetherobustness ofknowledgetransferacrosstsourcedomains. Weshowextensiveexperimentalstudiesthatourpro- posedmethodcanoutperformstate-of-the-arttransfer learningtechniquesonrealdatasets. 2ProblemFormulation Wetheproblem,TransferLearningwithMul- tipleViewsandMultipleSources(TL-MVMS)asfol- lows.ThenotationsaresummarizedinTable1.Let S = f S k g N k =1 denotethesourcedomains,where N isthenumberofsources.Foreach S k ,wehave S k = f X k s ;Y k s g = f ( x k i ;y k i ) g n k i =1 ,where n k denotesthe numberofinstancesinthe k -thsourcedomain.Let T = f X t g = f x g m j =1 denotethetargetdomain,where m isthenumberofinstances.Wetheviewset- sas V = f v ` g F ` =1 ,where F isthenumberofviews. Foreachsourcedomain S c ,itsviewsetisas Table1: ofNotations Notation NotationDescription S Sourcedomains, S = f S k g N k =1 S k The k -thsourcedomain, S k = f X k s ;Y k s g T Thetargetdomain, T = f X u g V k s Theviewsetofthe k -thsourcedomain V t Theviewsetofthetargetdomain n k Numberofinstancesin S k m Numberofinstancesin T N Numberofsourcedomains F Numberofviews p ( x ) Marginaldistributionof x p ( y j x ) Conditionaldistributionof( x ;y ) p ( y; x ) Jointdistributionof( x ;y ) Figure1: TL-MVMSandOtherLearningProblems V c s = f v c ` g f c s ` =1 2V andforthetargetdomain,itsview setis V t = f v t ` g f t ` =1 2V .Let p k s ( x ), p k s ( y j x )and p k s ( x ;y ) denotethemarginal,conditionalandjointdistribution- softhe k -thsourcedomainrespectively,and p t ( x ), p t ( y j x )and p t ( x ;y )betheparallelforthe targetdomain.ThegoalofTL-MVMSistobuildmod- elsfor T withthehelpof S .Weemphasizethatthis isageneralframework.ThebetweenTL- MVMSandthepreviouslearningproblems,i.e.,tra- ditionaltransferlearning(TTL),multi-viewlearning, multi-viewtransferlearning(MVTL)andmulti-source transferlearning(MSTL)areillustratedinFigure1. Theseapproachescanbeconsideredasspecialcasesof TL-MVMS.  Multi-viewlearning: N =0and f t > 1  TTL: N =1and f k s = f t =1  MVTL: N =1, f k s = f t > 1and V t = V k s  MSTL: N> 1and f k s = f t =1: Clearly,duetothedistributionshiftbetweensource andtarget,existingmulti-viewlearningalgorithmsmay failtobuildconsistentmodelsforthetargetdomain basedonsourcedata.Inaddition,TTLandMSTLdo notconsiderthemulti-viewsetting,andhencecannot takethefulladvantageofthesourcedata.Finally,on accountofthedistributionshiftamongsources,MVTL maynotbuildconsistentmodelsifsimplymergingall sourcedomainstogether. 3TheMulti-TransferAlgorithm Theintuitionofthemulti-transferalgorithmisto regulatethemodelbuiltfromsomeviewsinonesource domainfromtheknowledgeofotherviewsinanother sourcedomainwhileavoidinganynegativeimpacts ofdomainWeimplementthisideaby embeddingtransferlearninginaco-trainingframework. Forsimplicity,weassumethattherearetwosource domainsandonetargetdomain,wherethetarget domaincontainstwoviewsandeachsourcedomain coverseachofthemrespectively. 3.1Multi-transfer Basedontheco-trainingframe- work,ineachiteration,webuildtwomodels f 1 and f 2 fromtwotviewsets V 1 s and V 2 s ,fromtwod- tsourcedomains S 1 and S 2 respectively.Then, thesetwomodelsareusedtopredictthepseudolabel- softheremainingunlabeledtargetinstances.Analog toco-training,thosetargetinstanceswithhighpredic- tionandtheirpredictedlabelswillbeused tobuildtwonewmodelswiththeexistingsourcedo- maindataforthenextiteration.However,duetothe domainmulti-transferneedstoaddresst- wochallenges(1).howtosolvethedistributionshift acrossdomainsforselectingappropriateunlabeledtar- getdataand(2).howtorevisethedistributiongap betweensourceandtargetdomains,forbuildingconsis- tentmodels.Toselectthetargetinstances,wepropose aharmonicfunctionbasedinstance-selectioncriterion thatconsidersthejoindistributionshiftbetweentwo domains.Then,weintroduceanon-parametricmethod toestimatethejointdistributionratioofeachsource instance,whichcanbetreatedascluesforsettingtheir weightsandusedtoreducethenegativeofdistri- butionshift.Wediscussthesetwoprocessesasfollows. 3.2TargetInstanceSelection Ineachiteration, multi-transferselectsseveraltargetinstanceswithpre- dictedlabelstoenlargethetrainingsetofeachsource domainandexchangeknowledgeacrosstviews. Toavoidthenegativeimpactofdistributionshift,i.e, p t ( x ;y ) 6 = p s ( x ;y ),weproposeanunbiasedcriterion. Sincethemarginaldistributionsoftwodomainsaredif- ferent,i.e. p t ( x ) 6 = p s ( x ),themodelslearnedinsource domainsareinconsistenttothetargetdomain[12].For- mally,let f  t denotetheidealhypothesisofthetarget domainand f s denotethehypothesisconstructedfrom n sourceinstances.Then,if p t ( x ) 6 = p s ( x ),weobtain lim n !1 ( f s ) 6 = f  .Instead,ifthedensityratioofone instance x betweentwodomainsiscloseto1,itspredic- tionisconsistentandwouldbecorrectwithhighprob- ability.Thus,weamarginaldistance-measure j p c s ( x )  p c t ( x ) j foreachtargetinstance x 2 X t under view v c .Asweassumeeachsourcedomaincoversone targetview,theindex c alsoindicatesthe c -thsource domain,i.e., S c .However,itmaybehardtoestimate eachprobabilitydensity.Werewriteitas (3.1)   log p c s ( x )  log p c t ( x )   =   log p c s ( x ) p c t ( x )   Weestimatetheratio g c ( x )= p c s ( x ) p c t ( x ) viaGaussian Process(GP)[14],whichgeneratesafunctionfroma Gaussiandistribution.Sp,undertheview v c , weconsiderthatthelabelsofallsourceinstancesfrom S c tobepositiveandthelabelsofalltargetinstances tobenegative.Subsequently,afterbuildingtheGP model,wecanobtaintheestimation p c s ( x j GP ) p c t ( x j GP ) ˇ p c s ( x ) p c t ( x ) . Followingtheanalysisin[12],theselectionofunlabeled targetdataisunbiasedafterweightingwithEq.(3.1). Astheconditionaldistributionsoftwodomainsare t,i.e., p t ( y j x ) 6 = p s ( y j x ),thedecisionboundaries oftwodomainsarettoo.The measure j p ( y =1 j x ;f )  p ( y =  1 j x ;f ) j inco-training isnolongeranindicatortothepredictionrisk.Thus, weutilizethelocaldatastructureinthetargetdomain togenerateamorerobustcriterion.Weproposea harmonic-functionbasedmethod,whichissimilartothe onein[20].Let w c ij =exp   ˙ ( x c i  x c j ) 2  denotethe similaritybetweentwotargetinstancesunderview v c . Theharmonicmeasureof x c i is (3.2) X j 2 N ( i ) w c ij  f : c ( x i )  f : c ( x j )  2 where N ( i )isthenearestneighborsof x i and f : c is themodeltrainedfromanothersource.Itsphysical meaningisthatthepredictionsoftwoinstancesshould besimilariftheyareneighbors.Underthecluster- manifoldassumption[1],ofwhichmeaningisthatt- woinstancesareclosegeometrically,theytendtohave thesamelabel.Weobtainthat,iftheconditionaldis- tributionsoftwodomainsarenotveryt,e.g, p t ( y =1 j x ) ˇ p s ( y =  1 j x )doesnothold,thepredic- tionsof f : c andthetruelabelswouldbeidenticalon sometargetinstances.Consequently,iftheinstances inthetargetdomainfollowthemanifoldassumption, thevalueofEq.(3.2)canidentifytheseinstances.Com- biningEq.(3.1)andEq.(3.2)together,weobtaina selectioncriterionas (3.3) v c ( x i )=   log( p c s ( x i ) p c t ( x i ) +  )   X j 2 N ( i ) w c ij  f : c ( x i )  f : c ( x j )  2 where > 0istoavoidthezerovalue.Then,ineach iteration,weselect n a instanceswithsmallest v c values tohelpbuildnewmodelsforthenextiteration. 3.3SourceInstanceWeighting Duetothejoint- distributionshift,i.e., p s ( x ;y ) 6 = p t ( x ;y ),themodels builtonsourcedomainsmayproducebiasedpredictions onthetargetdomain.Inotherwords,foragiven instance,itsexistingprobabilitiesaretinthe sourceandtargetdomains.Wesolvethisproblemby weightingthesourceinstanceswiththeratiobetween thetargetandsourcejoint-distributions.Weshow that,afterweighting,themodelswouldbeunbiased.In modelbuilding,weaimtobuildamodel f c inthesource domain S c byminimizingtheobjective (3.4)min f c 1 n c X ( x ;y ) 2 S c p c t ( x ;y ) p c s ( x ;y )  f c ( x )  y  2 +  R ( f c ) Clearly,thelossesoftinstancesaret andhencethosesourceinstanceswhichareclosetothe targetinstancescanobtainhigherimpacts.Duetothe irrelevanceto x and y ,wecanignoretheregularization term  R ( f c )andobtaintheempiricallossof f c as followswhen n c !1 . " ( f c )= 1 n c X ( x ;y ) 2 S c p c t ( x ;y ) p c s ( x ;y )  f c ( x )  y  2 = E ( x ;y ) ˘ p c s ( x ;y ) Z x Z y p c t ( x ;y ) p c s ( x ;y )  f c ( x )  y  2 p c s ( x ;y ) d x dy = E ( x ;y ) ˘ p c s ( x ;y ) Z x Z y  f c ( x )  y  2 p c t ( x ;y ) d x dy = 1 n c X ( x ;y ) ˘ p c t ( x ;y )  f c ( x )  y  2 Toestimatetheratio,weextendthemethodin[8]. Let g ( x ;y ) c denotetheratiounderview c and^ g ( x ;y ) c denotetheestimatedone.Let T p c representthetarget dataundertheview v c withpseudolabelsfromanother viewineachiteration.Foreachlabeledinstance( x ;y ) insource S c ,wemodel^ g ( x ;y ) c asakernelfunction: (3.5)^ g ( x ;y ) c = j T p c j X i =1  i k x ( x ; x i ) k y ( y;y i )( x i ;y i ) 2 T p c where k x and k y denotethekernelfunctionsoninstances andlabels.We k x ( a;b )= k y ( a;b )=exp(  ˙ ( a  b ) 2 ),where ˙ isthekernelparameter.Theestimated jointdistributionofthetargetdomainisrepresentedas ^ p c t ( x ;y )=^ g ( x ;y ) c p c s ( x ;y ).Theobjectiveistolearnthe parameters  i sothattheKullback-Leiblerdivergence from^ p c t ( x ;y )to p c t ( x ;y )canbeminimized: KL [ p c t ( x ;y ) k ^ p c t ( x ;y )]= Z D p c t ( x ;y )log p c t ( x ;y ) ^ p c t ( x ;y ) dxdy = Z D p c t ( x ;y ) p c t ( x ;y ) p c s ( x ;y ) dxdy  Z D p c t ( x ;y )log^ g ( x ;y ) c dxdy Wecanignoretheterm,whichisindependentof theparameters.Inaddition,weaddanormalizedterm fortheparameters  = f  i g j T p c j i =1 ,since p c t ( x ;y )isa probabilitydensityfunction.Theobjectivebecomes max  h P j T p c j i =1 log  ^ g ( x i ;y i ) c  (3.6) s:t: P n c i =1 ^ g ( x i ;y i ) c = n c 8  j  0 Algorithm1 Multi-transfer 1: Input :sourcedomains: S = f S 1 ;S 2 g ,unlabeledtargetdata: X u , I , n a 2: Output :Builtmodel f 3: Setinstances'weightsof S 1 and S 2 as1 =n 1 and1 =n 2 4: Buildtwomodels f 0 1 and f 0 2 using S 1 and S 2 respectively 5: Let X 1 = X u , X 2 = X u , T p 1 = ; , T p 2 = ; 6: for i =1to I do 7: Performpredictionon X 1 using f i  1 2 : f X 1 ; ^ Y 1 g 8: Performpredictionon X 2 using f i  1 1 : f X 2 ; ^ Y 2 g 9: Selectinstancesfrom f X 1 ; ^ Y 1 g and f X 2 ; ^ Y 2 g as P 1 and P 2 usingEq.(3.3) 10: X 1 = X 1 n P 1 , X 2 = X 2 n P 2 , T p 1 = T p 1 [ P 1 , T p 2 = T p 2 [ P 2 11: Re-estimatetheweightsofinstancesofusingEq.(3.6) 12: Buildtwomodels f i 1 and f i 2 using S 1 [ T p 1 and S 2 [ T p 2 13: endfor 14: Return f =( f I 1 + f I 2 ) = 2 Figure2:MainFlowofMulti-transfer where n c denotesthenumberofinstancesinsource S c and x c isaninstanceunderview v c .Thisisaconvex optimizationproblemandthegloballyoptimalsolution canbeobtained.Afterlearningthe  ,wecanobtain theweightofeachsourceinstanceundertviews andthenutilizeweightedsourceinstancestobuilda unbiasedmodelforthetargetdomain. 3.4TheProposedFramework Theframeworkof Multi-TransferisillustratedinFigure2andAlgorith- m1.Initially,instancesintwosourcedomainsare weightedas1 =n 1 and1 =n 2 ,respectively.Ineachitera- tion,twomodelsarebuiltfromtwosourcesundertwo tviewsets.Afterthat,theyareutilizedtopre- dictthelabelsoftargetinstancesandobtaintwolabeled sets.Toavoidthenegativeimpactfromthewronglyla- beledtargetdata,weonlyselectthoseinstancesinthese twosetswithhighcorrectpredictionprobability,using theadaptivecriterioninEq.(3.3).Second,thesetwo setscanbeutilizedtoestimatetheweightsofeachin- stanceinsourcedomains.Thisprocessisrepeateduntil exceedsthemaximaliterations.Finally,thepredictions oftheinthelastiterationwillbeaveragedas thepredictions.Wenoticethat,ifwemergetwo sourcesintooneandexploitmulti-viewtransferlearn- ingapproaches,thetdistributionsandfeature spacesbetweentwosourcedomainswillmisguidethe modelbuilding.Inaddition,ifweignorethemulti- viewnatureandexploitmulti-sourcetransferlearning approaches,wecannotutilizethecross-viewcompensa- tionsandmayfromthenegativeimpactsofin- consistencesacrosstviews. Weanalyzethetimecomplexityasfollows.Suppose thenumberofiterationsis I andthetimecomplexity ofthebasemodelis O ( Q ).Ineachiteration,multi- transferneeds O ( n 2 t )tocomputetheselectioncriterion valuesforeverytargetinstancesand O ( n t log n t )to selecttheappropriatetargetinstances.Then,itneeds O ( n 1 n t + n 2 n t )tocomputetheweightofeachsource instanceand O ( n 1 + n 2 )toupdate.Insummary,the wholetimecomplexityis O ( I ( Q +( n t + n 1 + n 2 ) n t )). 4Experiment 4.1ASyntheticExample Webeginbyanalyzing multi-transferonasyntheticdataset,whichhastwo sourcedomainsandonetargetdomainwith tviews,asshowninFigure3.Clearly,sourcedomains havetheirowndomainspviews,i.e.,audiofeature, andalsohavesharedviews,imagefeatureforsourcedo- main1(Figure3(a))andtextfeatureforsourcedomain 2(Figure3(b)),withthetargetdomain(Figure3(c)) respectively.Inaddition,instancesintdomains followtdistributions.Sp,datafromdif- ferentsourcedomainsfollowtwoGaussiandistributions undertviews,whiletargetdataisconstructed alongparabolacurvesunderanotherfeaturespace. Firstofall,supposewehavealreadylearnedthe correctdecisionboundariesofsourcedomains.Asdif- ferentsourcedomainsonlycoveroneviewofthetarget domain,whenweapplytheconstructedboundarieson thetargetdata,theboundariesreducetoaverticalor horizontallineduetoabsenceofsourcespviewin targetdomain.Thatistosay,targetdata'saudiofea- tureisequaltozero.Theboundariesareshownasdash dotlinesinFigure3(c).Wenoticethat,usingsingle boundaryfromsourcedomainwillmisclassifymanyin- stances.However,ifwecombinethesetwoboundaries together,theboundary(thedashline)candiscriminate thetargetdatamuchbetter.Thus,weshouldexploit multipleviewstogethertoimprovetheperformance. However,duetothedistributionshift,wecannot directlyapplymulti-viewlearningalgorithms,e.g.,co- training,whoseselectioncriterionispurelybasedon Inappropriatetargetexamplescouldbes- electedtomisleadupdating.Forexample,in Figure3(c),2(i.e.,Boundary2)willassign negativelabelstopointsaround[  0 : 2 ;  0 : 3]withhigh Likewise,itwilltlyassignpositive labelstopointsaround[  1 : 3 ; 0 : 9],although,whichac- tuallyarebelongingtonegativeclass.Followedbypass- ingtheseselectedpointstosourcedomain1,weget Figure3(d),wheredatapointsalongline L ( L :audio feature=0)arenewlyadded.Thesedatawillpushthe boundarydownalongimagefeatureaxis,anddeterio- ratetheperformance.Similarincorrectresultsof sourcedomain2canbefoundinFigure3(e).Onthe contrary,byrevisingthedistributionshift,theselection processofmulti-transferisanunbiasedmodel.Itse- lectsdatapointswithhighaswellaslarge distributionsimilarity.Forexample,datainsourcedo- main2aremainlydistributedaround[  0 : 5 ;  0 : 5]and [0 : 5 ; 0 : 5].Takenthisdatadistributionand2's intoaccount,2willselectpoints around[  0 : 3 ;  0 : 35]and[0.4,0.5]intargetdomain forsourcedomain1(Figure3(f)).Likewise, er1willselectproperdataforsourcedomain2(Fig- ure3(g)).Theseselecteddatawillpushtheboundaries upalongimageandtextfeaturerespectivelyandim- provetheperformance.Afterre-weightingnewtrain- inginstancesinthisiteration,thedecisionboundaries constructedbyco-trainingandmulti-transferareshown inFigure3(h),fromwhichwecanseemulti-transfer's boundaryismovingtowardstheground-truthboundary (thesolidline)andcorrectlymostdatapoints, whileco-traininglotsoftargetinstances. 4.2ExperimentalSetting Weevaluatetheperfor- manceofmulti-transferalgorithmontworeal-world textdatacollections,20Newsgroupsandspamde- tection,andcomparetothreestate-of-the-artmeth- ods,i.e., LatentMap [15],co-adaptation[13]andco- training[2].Thesebaselinemethodsstandt learningparadigms.LatentMapisatraditionaltransfer learningapproachthatconsidersonlyonesourcedo- mainundersingle-view,co-adaptationisamulti-view transferlearningwhichdoesnotconsiderthedomain acrossmultiplesources,andco-trainingisa representativemulti-viewlearningalgorithm.Theper- formanceismeasuredwithaccuracyon unlabeledtargetdata.Forco-adaptation, LatentMap isintroducedasabaseForco-trainingand multi-transfer,SVMandC4.5areadopted. 4.3DataDescription Thedataprocessingproce- dureisasfollows.First,eachdocumentisconvertedto aterm-frequencyvector.Secondly,toreducethenum- beroffeatures,weremovethevocabularieswhosefre- quencycountsarelessthan1%ofthedocumentcoun- t.Finally,termfrequencyisusedasthefeatureval- ueintheexperiments. The20Newsgroupsdata set containsthetopcategories,suchas`comp',`sci', `rec'and`talk'.Eachcategoryhassomesub-categories, suchas`sci.crypt'and`sci.med'.Weuse4maincate- goriestogenerate5datasets,ineachofwhichtwotop categoriesarechosenforgeneratingbinary tiontasks.Withahierarchicalstructure,foreachcat- egory,allofthesubcategoriesarethenorganizedinto threeparts,whereeachpartisoftdistribution. (a) SourceDomain1(SD1) (b) SourceDomain2(SD2) (c) TargetDomain(TD) (d) SD1w/DataSelectedbyCT  (e) SD2w/DataSelectedbyCT (f) SD1w/DataSelectedbyMT  (g) SD2w/DataSelectedbyMT (h) TDw/Boundaries Figure3: ASyntheticExampletoIllustratetheProblemofTransferLearningwithMultipleViewsandMultipleSources. (  CT:forco-training;MT:multi-transfer;BoundaryCT:Boundaryconstructedbyco-training;BoundaryMT:boundary constructedbymulti-transfer.) Table2:DatasetDescription Dataset Sourcedomain-1 Sourcedomain-2 Targetdomain #S-1 #S-2 #T 20-Newsgroup rec-vs-comp autos:misc baseball:mac hockey:windows 1164 1169 1190 rec-vs-tech autos:guns motorcycles:mideast hockey:misc 1137 1160 1062 sci-vs-comp electronics:graphics med:misc space:windows 1172 1166 1185 sci-vs-tech crypt:guns electronics:mideast med:misc 1139 1155 970 comp-vs-tech graphics:guns misc:mideast windows:politics 1126 1136 1065 SpamDetection Filter1 User0 User1 User2 2500 2500 2500 Filter2 User0 User2 User1 2500 2500 2500 Filter3 User1 User2 User0 2500 2500 2500 Therefore,onepartcanbetreatedasthetargetdomain dataandtheothertwoareusedforthesourcedomain purpose.Togeneratethemulti-viewinmulti-source, wefurtherletthevocabulariesofthesetwosourcedo- mainsbeoverlappingtheonesinthetargetdomainbut notidentical.Tothisend,wecanseethateachdataset hastwosourcedomainsandonetargetdomain,eachof whichdiscussestsub-categorytopics.Wecan alsonoticethatthedatasethasfourviews:(1)each sourcedomainhasonespview(i.e.,domainspe- vocabularies)andonesharedviewwiththetarget domain(i.e.,vocabulariessharedwithtargetdomain) and(2)targetdomainhastwoviews,whicharerespec- tivelysharedwithtwosourcedomains.Besides,the distributionofsharedviewsinthetargetdomainisd- tfromthatinthesourcedomainsduetothedis- tinctsub-categorytopics. Thespamdetectiondata set isfromTaskAofECML/PKDDDiscoveryChal- lenge2006.Thetaskaimstoconstructspamfor 3users,eachofwhichhas2500emails.Theemailsofa userconsistof50%spamsand50%non-spams.Inad- dition,thedatadistributionbetweenusersist. Thatistosay,usershavetheirspvocabulariesas wellascommonones.Inourexperiment,weusetwo usersastwosourcedomainsandanotheruserasthe targetdomain.Thedetailsofdatasetsarereportedin Table2.Theirdimensionsrangefrom2405to5984.In alldatasets,thesourceinstancesarefullylabeled,while thetargetdomaincontainsonlyunlabeleddata. 4.4Performance Figure4presentstheperformance oneachdatasetgivenbyLatentMap,co-adaptation, co-trainingandmulti-transfer.Ineachthe methodwithhigherhistogramhasbetterperformance thanthatwithlowerone.Multi-transferalwaysachieves thebestaccuracy.LatentMapandco- adaptationseemsfailtotransferknowledgefromsource domaintotargetdomaininthismulti-viewsetting. Sp,LatentMapdoesnotconsiderthemulti- viewnatureintheproblemandco-adaptationdoes notconsiderthebetweensourcedomains. Besides,withouttakingdistributionshiftintoaccount, co-trainingobtainsloweraccuracythanmulti-transfer. Withinmulti-transfer,wecanseethatSVMasbase learnerismoreaccuratethanC4.5,sinceSVMisknown asachoicefortraditional problem.However,whenusingC4.5asbaselearner, multi-transferobtainsmuchhigheraccuracythanco- (a)rec-vs-comp (b)rec-vs-tech (c)sci-vs-comp (d)sci-vs-tech (e)comp-vs-tech (f)Filter1 (g)Filter2 (h)Filter3 Figure4: PerformanceontDatasets.(LM:LatentMap;CA:Co-Adaptation;CTS:Co-TrainingwithSVM;CTC: Co-TrainingwithC4.5;MTS:Multi-TransferwithSVM;MTC:Multi-TransferwithC4.5.) training.Especially,on20NGsci-vs-compdataset,the accuracyofmulti-transferisover10percenthigher. 4.5ofModelParameter Asmentionedbe- fore,ouralgorithmhasthreeparameters,whichdirectly impacttheperformance.Theoneisthenum- berofnearestneighborsinKNN.Theselectionprocess iseasilybynoisewhensmallKisused,while largeKleadstoaverysmoothdistribution onunlabeleddata.FromFigure5(a),wecanseethe trendwheretheperformanceimproveswhenKgrows anddecreaseswhenKgrowstoolarge.So K = f 5 ; 7 g aresuitablechoices.Fromtheresult,wecanalsosee thatourmethodalwaysoutperformsco-training. Secondly, n a ,thenumberofselectedunlabeled datawithmosttlabels,isalsoimportant forupdating.Itiseasytounderstandthat theisupdatedinasmoothwaywhensmall value n a isused.Onthecontrary,largevalue n a bringsunstableupdatingprocess.Thisphenomenon canbeseeninFigure5(b).However,addingalarge setaugmentslabeleddataquicklyandacceleratesthe algorithm.Consideringtheprosandcons,weadd10 percentunlabeleddataineachiteration.Therefore,the algorithmevolvesfastatthebeginningandgoesstable afterwards,asshowninFigure5(c). Thelastaspectwecareaboutistheconvergence. FromFigure5(c)wecanseethatthebecomes moreaccurateontargetdomainwhentargetdataare addedintothelearningprocesssuccessivelyandthen converges.Intheexperiments,wethatsetting maximumiterationas25workswellinmostcases. 4.6ModelAnalysis Besidestheparameters,base learnerselectionandtherelationshipbetweensource andtargetdomainsalsohaveimportantimpactsup- ontheaccuracy.Weconductextensive experimentstotesttheseimpacts.Firstly,thestrate- gyofbaselearnerselectioninoneiterationthe performanceinthenextiteration.Forexample,anal- ternativewayofusingasamebaselearnerisrunning tontwodomains.Inthismethod, canlearnfromeachotherineachiteration. Weusethisstrategyon20NG'srec-vs-compdataset andgetitsaccuracyof87.82%,whichlies betweenperformancesofmulti-transferwithSVMor C4.5asbaselearner.Also,fromtheFigure6(a),we canseethatthealgorithmconverges. Secondly,wetesthowtherelationshipbetween sourceandtargetdomainstheperformance ofmulti-transfer.Forinstance,inbothsourcedomains, weremovepartofcommonfeaturessharedbysource andtargetdomains,thentrainonthesein- completeviews.Bychangingtheremoval-ratio,which isthepercentageofremovedfeatures,weobtaintheper- formancecurvegivenbythesolidlineinFigure6(b).In thiswecanseethateven60%commonfeatures ineachviewareremoved,theperformanceofmulti- transferisstillbetterthanthatoftrainedon singlecompletedomain,whosefeaturesareneverdelet- ed.Wealsoremovethecommonfeaturesinoneview whilekeepingtheotherunchanged.Thisallowsusto studyhowsourceviewstheperformance. Byremovingwithincreasingproportions,weobtainthe accuracyresultsinFigure6(c).Clearly,in thissetting,theperformanceisbetterthanacomplete singlesource.Inaddition,theofeachsourceis quitet,sincethegapbetweencurvesislarge. Thatiswhyweexploitmulti-viewknowledgecollabo- rativelyratherthancombiningviews. Finally,weanalyzethesituationwhereonlycom- monfeaturesareusedasasingleviewinsourceandtar- getdomainsfollowedbyrandomlysplittingtheminto twoviews.Thissettingisusedtotesttheperformance ofmulti-transferandco-adaptationwiththecondition wherethedistributionoftwosourceviewsisthesame. (a)t K (b)t n a (c)Convergence Figure5:ParameterAnalysison20NG'srec-vs-compDataset (a)Heterogeneous (b)tSourceRatios (c)Accuracyvs.Coverage (d)SingleSource Figure6:ModelAnalysison20NG'srec-vs-compDataset Byrepeatingthesplittingetimes,weplotthemeans andvariancesofaccuraciesinFigure6(d), wheremulti-transferisstillbetterthanco-adaptation. Itisworthnotingthattheexperimentsareconduct- edontextdatasets,butthealgorithmcouldbedirectly appliedondatasetwithmultipleheterogeneousviews. Thereasonisthat,alltheviewsareusingcompletely twordsindatasets;anyviewcanbereplaced byothertypeoffeature,suchasimage. 5RelatedWorks Wesummarizetherelatedworksonmulti-viewlearning, transferlearningandtheirvariants.Generally,the studiedprobleminthispapercanbeconsideredasa generalframeworkwhichalltheselearningtasks. Multi-viewLearning Inmanyreal-worldapplica- tions,examplesarerepresentedbymultipleviews.Co- training[2]isarepresentingmulti-viewlearningmethod whichlearnsaseparateforeachviewus- inganylabeledexamplesandthenpickthemost dentpredictionsofeachontheunlabeleddata toiterativelyconstructadditionallabeledtrainingdata. In[9],theauthorsincorporatetheconsistencyLaplacian termintomulti-viewsemi-supervisedlearningproblems. However,mostexistingmulti-viewlearningmethodsare forthesingle-domainsettingsinsteadofcross-domain. TraditionalTransferLearning(TLL) Trans- ferlearning(TL)addressestheproblemof tlabeleddatainatargetdomainbyusingauxiliary datainrelatedbuttsourcedomains[11].T- worepresentativetechniquesfortransferlearningare instanceweighting[5],whichextendsAdaboostto terthoseuselesssourcedomaindata,andfeaturemap- ping[15,19]whichtransfersknowledgeacrossdomain- sthroughkernelbaseddimensionreduction.Howev- er,thesetraditionaltransferlearningapproachesfocus onaddressingthedistributionshiftacrossdomainsbut workwithasinglesourcedomainunderasingle-view. Multi-viewTransferLearning(MVTL) Sev- eralapproacheshavebeenproposedtohandlethesitu- ationwheredatafromsourceandtargetdomainsare composedbymultipleviews.Forexample,theco- adaptationalgorithmproposedin[13]usesthelabeled sourcedomainexamplestoconstructandthen appliestheco-trainingalgorithmtoconstructtheclas- forthetargetdomain.Co-traininghasbeenex- tendedtocross-domaincontextbyaddingafeatures- electionprocess[4].Recently,amaximalmarginbased method[18]isintroducedtointegratethemulti-view andtransferlearningnatureinaprincipledway.How- ever,theseworksassumethatthereisonlyonesource domainandsourceandtargetviewsareidentical. Multi-sourceTransferLearning(MSTL) Thereareafewresearchworksonmulti-sourcetrans- ferlearning.Forexample,theworkin[17]extends TrAdaboost[5]byaddingawrapperboostingframe- workonweightingeachsourcedomain.[3]presentsa linearcombinationovermultiplesourcestoreachacon- sensus.However,theseapproachesworkundersingle- viewsetting.Recently,acloserelatedworkisproposed in[7],whichaddressesamulti-tasklearningproblem undermultipleviews.Itproposesagraph-basedalgo- rithmtocapturetherelationsamongtviewsin ttasks.However,itrequiresthatalltaskscon- tainlabeleddataanddoesnotconsiderthedistribution shiftamongdomains. 6Conclusion Inthispaper,westudiedanovelandgeneraltrans- ferlearningproblem: TransferLearningwithMultiple ViewsandMultipleSources ,wherethesourceandtar- getdomainsareundermultipleviewsandtheknowledge oftargetviewsaredistributedontsourcedo- mains.Wehaveintroducedamulti-transferalgorithm, whichworksinaniterativemannertopredictthelabels oftheunlabeledtargetdata.Comparingwithprevious works,multi-transferconsidersthedomain andmulti-viewnaturetogethertoperformcross-domain knowledgetransfer.Followingtheco-trainingprocess, ineachiteration,thetargetdatawithpseudolabels fromonedomaincanbeexploitedtoenhancethemod- elbuildinginanotherdomain.Besides,weproposed twonovelheuristicsineachiterationtoovercomethe distributionshift.Wefoundthatbyapplyingdensity- weightedharmonicfunction,theproposedcriterionis unbiasedtoselecttargetdata.Inad- dition,itisbettertoestimatetheimportanceofeach sourceinstance,whichhelpsbuildaconsistentmodel forthetargetdomain.Weconductedempiricalstudies ontworealtextcollections,20-newsgroupandspamde- tection,wheretheproposedmethodcanboostseveral state-of-the-artalgorithmsashighas8%onaccuracy. Wecarriedoutexperimentsunderasettingthat containstwosourcedomainsandonetargetdomain whileeachsourcedomaincoversoneviewofthetarget. Inourfuturework,weplantoextendtheexperiments overmultiplesourcesandmultipleviews,andonamore generalsettingthatthesourceandtargetviewsmaybe inconsistentandthesourceviewsmaybeoverlapping. Inaddition,wewouldconsiderextendingthealgorithm underheterogeneouscontexts,wherethefeatureorlabel spacesaretinsourceandtargetdomains. Acknowledgement WethankthesupportofHong KongCERGProjects621010,621211andHongKong ITFProjectGHX/007/11. References [1] MikhailBelkin,ParthaNiyogi,andVikasSindhwani.Mani- foldregularization:Ageometricframeworkforlearningfrom labeledandunlabeledexamples. JournalofMachineLearn- ingResearch ,7:2399{2434,2006. [2] AvrimBlumandTomMitchell.Combininglabeledandun- labeleddatawithco-training.In Proceedingsoftheeleventh annualconferenceonComputationallearningtheory ,pages 92{100,1998. [3] RitaChattopadhyay,JiepingYe,SethuramanPan- chanathan,WeiFan,andIanDavidson.Multi-sourcedo- mainadaptationanditsapplicationtoearlydetectionof fatigue.In Proceedingsofthe17thACMSIGKDDinterna- tionalconferenceonKnowledgediscoveryanddatamining , pages717{725,2011. [4] M.Chen,K.Weinberger,andJ.Blitzer.Co-Trainingfor DomainAdaptation.In AdvancesinNeuralInformation ProcessingSystems24 ,2011. [5] WenyuanDai,QiangYang,Gui-RongXue,andYongYu. Boostingfortransferlearning.In Proceedingsofthe24th internationalconferenceonMachinelearning ,pages193{ 200,2007. [6] LixinDuan,DongXu,andShih-FuChang.Exploitingweb imagesforeventrecognitioninconsumervideos:Amultiple sourcedomainadaptationapproach.In IEEEInternational ConferenceonComputerVisionandPatternRecognition (CVPR) ,2012. [7] JingruiHeandRickLawrence.Agraphbasedframeworkfor multi-taskmulti-viewlearning.In Proceedingsofthe28th InternationalConferenceonMachineLearning ,pages25{ 32,2011. [8] T.Kanamori,T.Suzuki,andM.Sugiyama.Theoreticalanal- ysisofdensityratioestimation. IEICETransactionsonFun- damentalsofElectronics,CommunicationsandComputer Sciences ,E93-A(4):787{798,2010. [9] GuangxiaLi,StevenC.H.Hoi,andKuiyuChang.Two-view transductivesupportvectormachines.In Proceedingsofthe SIAMInternationalConferenceonDataMining ,pages235{ 244,2010. [10] PingLuo,FuzhenZhuang,HuiXiong,YuhongXiong,and QingHe.Transferlearningfrommultiplesourcedomain- sviaconsensusregularization.In Proceedingsofthe17th ACMconferenceonInformationandknowledgemanage- ment ,pages103{112,2008. [11] SinnoJialinPanandQiangYang.Asurveyontransferlearn- ing. IEEETransactionsonKnowledgeandDataEngineer- ing ,22(10):1345{1359,October2010. [12] H.Shimodaira.Improvingpredictiveinferenceundercovari- ateshiftbyweightingthelog-likelihoodfunction. Journalof StatisticalPlanningandInference ,90(2):227{244,2000. [13] GokhanTur.Co-adaptation:Adaptiveco-trainingforsemi- supervisedlearning.In Proceedingsofthe2009IEEE InternationalConferenceonAcoustics,SpeechandSignal Processing ,pages3721{3724,2009. [14] ChristopherK.I.WilliamsandDavidBarber.Bayesian withgaussianprocesses. IEEETransactionson PatternAnalysisandMachineIntelligence ,20:1342{1351, 1998. [15] SihongXie,WeiFan,JingPeng,OlivierVerscheure,and JiangtaoRen.Latentspacedomaintransferbetweenhigh dimensionaloverlappingdistributions.In Proceedingsofthe 18thinternationalconferenceonWorldwideweb ,pages91{ 100,2009. [16] JunYang,RongYan,andAlexanderG.Hauptmann.Cross- domainvideoconceptdetectionusingadaptivesvms.In Pro- ceedingsofthe15thinternationalconferenceonMultimedia , pages188{197,2007. [17] YiYaoandGianfrancoDoretto.Boostingfortransfer learningwithmultiplesources.In The23rdIEEEConference onComputerVisionandPatternRecognition ,pages1855{ 1862,2010. [18] DanZhang,JingruiHe,YanLiu,LuoSi,andRichard Lawrence.Multi-viewtransferlearningwithalargemar- ginapproach.In Proceedingsofthe17thACMSIGKDD internationalconferenceonKnowledgediscoveryanddata mining ,pages1208{1216,2011. [19] ErhengZhong,WeiFan,JingPeng,KunZhang,Jiangtao Ren,DeepakTuraga,andOlivierVerscheure.Crossdomain distributionadaptationviakernelmapping.In Proceedings ofthe15thACMSIGKDDinternationalconferenceon Knowledgediscoveryanddatamining ,pages1027{1036, 2009. [20] XiaojinZhu,ZoubinGhahramani,andJohnD.y. Semi-supervisedlearningusinggaussianandharmonic functions.In Proceedingsofthe20thInternationalConfer- enceonMachineLearning ,pages912{919,2003.  
NeuralArchitect:AMulti-objectiveNeuralArchitectureSearch withPerformancePrediction YanqiZhou BaiduSiliconValleyAILab Sunnyvale,California zhouyanqi@baidu.com GregoryDiamos BaiduSiliconValleyAILab Sunnyvale,California gregdiamos@baidu.com ABSTRACT DespiteofprioradvancesinautomaticNeuralArchitectureSearch (NAS),automaticNASstilllargelyreliesonvastcomputationalre- sources(e.g.hundredsofGPUs,thousandshoursoftrainingtime). AReinforcementLearning(RL)basedNAStakesprohibitivelylong timeasaRLagentrequiresmeasuringthevalidationerrorforeach generatedtargetarchitectureandthealgorithmhaslittleintrin- sicparallelism.Moreover,priorworkstargetonlyaccuracybut lackconsiderationforcomputationalresourcesore˝ciency.For instance,increasingthemodelsizecanimprovemodelaccuracy, butthenetworkmightbesuboptimalunderresourceconstraints (e.g.totalnumberofparameters).WeproposeNeuralArchitect,a resource-awaremulti-objectivereinforcementlearningbasedNAS withnetworkembeddingandperformanceprediction.Insteadof searchingatargetnetworkfromscratch,weusenetworkembed- dingtoencodeanexistingnetworktoatrainableembeddingvector. Basedontheembedding,acontrollerneuralnetworkgenerates nextvthattransformsthetargetnetwork.Weintroduce multi-objectiverewardfunctionthattakesnetworkaccuracy,com- putationalresource,andtrainingtimeintoconsideration.There- wardispredictedbymultipleperformancesimulationnetworks thatarepre-trainedortrainedjointlywiththecontrollernetwork. Withnetworkembeddingandperformanceprediction,NeuralAr- chitectcan˙ndagoodnetworkarchitecturefastande˝ciently. Withmulti-objectiveoptimization,wecan˙nde˝cientnetwork architecturewithresourceconstraints. 1INTRODUCTION Manytechniquesforautomaticneuralarchitecturesearchhave beenproposed[ 1 , 3  6 ]andyieldpromisingresultsofdesigning competitivemodelscomparedtohumandesignedmodels.However, manyresultsarebasedonvastcomputationalresourcessuchas hundredsofGPUsandthousandsofGPUhours,whichmakes NASresearchlessrealisticforindividualresearchersoruniversity students.Forexample,toexpeditetheprocess,Google[ 5 ]uses distributedandasynchronoustrainingwith800GPUs. Moreover,existingtechniquesdonottakeconsiderationofre- sourceconstraints(e.g.totalnumberofparameters,memoryre- quirement)fordi˛erenthardwareplatformsorneuralarchitecture e˝ciency(e.g.accuracypermillionparameters).Asaresult,those techniquesarelikelyto˙ndnetworkarchitecturesthataregigantic insizebutlesse˝cient.Forexample,severalexistingNASworks achievesimilaraccuracyasDenseNet[ 2 ],butthetotalnumber ofparametersforthesenetworksaresigni˙cantlyhigherthan DenseNet. Inthiswork,weproposeNeuralArchitect,asshowninFigure1,a resource-awaremulti-objectivereinforcementlearningbasedNAS Figure1:High-levelIdeasofNeuralArchitect.Apolicynet- workgeneratesnextvbasedonnetworkembedding. Aperformancepredictionnetworkpredictstargetnetwork accuracyandtrainingtimewithoutactuallyrunningthetar- getnetworktillconvergence.Policygradientisappliedto trainthecontrollernetwork. withnetworkembeddingandperformanceprediction.Thegoalis to˙ndresource-e˝cientneuralarchitectureswithamulti-objective reinforcementlearningalgorithm.Themaincontributionsinclude: (1) Areinforcementlearningbasedcontrollerneuralnetwork withtrainablenetworkembeddingthattakesacon˙guration ofanexistingnetworkandmakesadaptationstogetbetter neuralnetworks. (2) Amulti-objectiverewardfunctionthattakesintoconsidera- tionofnetworkaccuracy,totalnumberofparameters,and trainingtime.Thisenablesuserstocustomizethereward functionaccordingtodi˛erentneeds(e.g.modelaccuracy, hardwareresourcesandtrainingtime). (3) Aperformancepredictionmodelthatpredictsnetworkac- curacyandtrainingtimewithoutrunningthetargetmodel tillconvergence.Itcanbearegressionbasedmodelora trainableneuralnetwork. 2METHODOLOGY Figure1showshigh-levelideasofNeuralArchitect.NeuralArchi- tectconsistsoftwonetworks:apolicynetworkandaperformance simulationnetwork.Thepolicynetworktakesinnetworkembed- dingofcurrentnetworkandgeneratesnetworktransformation actionssuchas(insertalayer)or(scalealayer).The performancesimulationnetworktakesinnetworkembeddingof Figure2:NetworkEmbedding:anLSTMbasednetworkthat transformsanexistingneuralnetworkcon˙gurationintoa trainablerepresentation. thegeneratedtargetnetworkanddatadistributionstoapproximate therewardbypredictingbothnetworkaccuracyandtrainingtime. Boththeaccuracynetworkandtrainingtimenetworkaretrainable neuralnetworksthatarepre-trainedortrainedjointlywiththe policynetwork.The˙nalrewardenginesetsweightstonetwork accuracy,modelsize,andtrainingtimerespectivelyaccordingto userspeci˙cation.Thecon˙gurablerewardengineenables˙nd- ingneuralarchitectureswithvariousresourceconstraintssuchas memorysizeandGPUtime. 2.1NetworkEmbedding AncriticaldesignfeatureofNeuralArchitectisitsabilitytoadapt anexistingneuralarchitectureratherthanbuildingfromscratch. Toenablenetworkadaptation,wedesignaLSTMbasedembedding networktotransformanexistingneuralarchitecturecon˙gura- tionintoatrainablerepresentation.Figure2showstheembedding network,wherealayerembeddingnetworktakesalayerdescrip- tionandmapslayerfeaturesintomultiplelookuptables.Lookup tablestransformthediscretefeaturespaceintotrainablefeature vectors.AnLSTMnetworktakeslayerfeaturevectorsandgen- eratesalayerembedding.Aftermultiplelayerembeddinghave beenproduced,anetworkembeddingLSTMnetworkprocessesthe sequentialinformationintheselayerembeddingsandgeneratesa networkembedding.Thisnetworkembeddingwillbeusedasan inputtothepolicynetworkandvaluenetwork. 2.2PolicyNetwork Thepolicynetworkconsistsofseveraloperationnetworksthat transformstheexistingnetworkusingnetworkembedding.Opera- tionsincludingandcanbeperformedbythepolicy network,butthepolicynetworkcanbeextendedtosupportother operationsasnecessary.Atrainableactionselectorchoosesthe operationtoexecutethenexttrainingiteration.Aninsertnetwork generateslayertype,layersize,andrelatedhyperparametersfor thatlayer.Ascalenetworkchangesthesizeandhyperparametersof anexistinglayer,includingthechannelsize,˙ltersize,anddropout rate. Figure3showstwooftherepresentativeoperationsgen- eratedbythepolicynetwork."InsertConv"insertsaconvolution layerintoanexistingnetwork."InsertAdd"operationconcatenates Figure3:Tworepresentativeoperations:"Insert Conv"and"InsertAdd". intermediateresultsfromtwopreviouslayers,whichissimilarto residuallayer. 2.3PerformanceSimulationNetworkand Multi-objectiveReward Insteadofrunningthetargetnetworktillconvergence,weusea regressionmodeloraneuralnetworkbasedperformanceprediction toreducethetrainingtimeofthepolicynetwork.Theperformance simulationnetworktakesatargetnetworkembeddingandantrain- ingdatasetintermsofsize,distribution,andregularitytogenerate approximatedaccuracyandtrainingtime.Leveragingtheembed- dingnetwork,wecanunifylayerrepresentationandintegratethe informationfromindividuallayers.Givenasetofsamplenetworks, wecanobtainperformancecurvesforeachnetwork.Foreachnet- work x i ,wecanobtainavalidationaccuracy a i andtrainingtime t i . TheobjectiveistoreducetheL1lossofthepredictedaccuracyand targetevaluatedaccuracy,andtheL1lossofthepredictedtraining timeandtargettrainingtime.Oncetheperformanceprediction networkistrainedproperly,itcanbe˙xedandreusedforneuralar- chitecturesearchundervariousresourceconstraints.Thetraining timenetworkcouldbeusedtomodelarealsystem(e.g.Tensor˚ow runningonaV100),oritcoulduseamoreidealizedhardwaremodel (e.g.aroo˚inemodel).Forthelattercase,thetrainedpolicynetwork canbeusedtoguidefuturehardwareandsoftwareoptimizations. Iftrainedjointly,theperformancesimulationnetworkbecomes avaluenetwork V .Theparameters  ofthepolicynetworkare optimizedviagradientdescentfollowing: r  lo gˇ ¹ a t j s t ;  º A ¹ s t ; a t ;  v º (1) A ¹ s t ; a t º = r t +  V ¹ s t + 1 ;  v º V ¹ s t ;  v º (2) Theparameters  v ifthevaluenetworkisupdatedviagradient descentusing: r  v »¹ r t +  V ¹ s t + 1 ;  v º V ¹ s t ;  v ºº 2 ¼ Inthemulti-objectiverewardfunction,wepenalizelargemod- elsbyapplyingapiece-wiselinearnegativerewardfunctionover modelsizeandtrainingtime.Forinstance,wecanstartapplying negativerewardsoncethemodelsizeexceeds16MB. REFERENCES [1] ChrisanthaFernando,DylanBanarse,MalcolmReynolds,FredericBesse, MaxJaderbergDavidPfau,MarcLanctot,andDaanWierstra.2016.Convolution byEvolution:Di˛erentiablePatternProducingNetworks.https://arxiv.org/abs/ 1606.02580 2 [2] GaoHuang,ZhuangLiu,KilianQ.Weinberger,andLaurensvanderMaaten.2017. DenselyConnectedConvolutionalNetworks.https://arxiv.org/abs/1611.01578 [3] RistoMiikkulainen,JasonLiang,ElliotMeyerson,AdityaRawal,DanFink,Olivier Francon,BalaRaju,HormozShahrzad,ArshakNavruzyan,NigelDu˛y,andBabak Hodjat.2017.EvolvingDeepNeuralNetworks.https://arxiv.org/abs/1703.00548 [4] EstebanReal,SherryMoore,AndrewSelle,SaurabhSaxena,YutakaLeonSue- matsu,JieTan,QuocLe,andAlexKurakin.2017.Large-ScaleEvolutionofImage Classi˙ers.https://arxiv.org/abs/1703.01041 [5] BarretZophandQuocV.Le.2017.NeuralArchitectureSearchwithReinforcement Learning.https://arxiv.org/abs/1611.01578 [6] BarretZoph,VijayVasudevan,JonathonShlens,andQuocV.Le.2017.Learning TransferableArchitecturesforScalableImageRecognition.https://arxiv.org/abs/ 1707.07012 3  
Listen,InteractandTalk:LearningtoSpeakvia Interaction HaichaoZhang,HaonanYu,andWeiXu BaiduResearch-InstitueofDeepLearning Sunnyvale,CA94089 {zhanghaichao,haonanyu,xuwei06}@baidu.com Abstract Oneofthelong-termgoalsofintelligenceistobuildanagentthatcan communicateintelligentlywithhumaninnaturallanguage.Mostexistingwork onnaturallanguagelearningreliesheavilyontrainingoverapre-collecteddataset withannotatedlabels,leadingtoanagentthatessentiallycapturesthestatisticsof theedexternaltrainingdata.Asthetrainingdataisessentiallyastaticsnapshot representationoftheknowledgefromtheannotator,theagenttrainedthisway islimitedinadaptivenessandgeneralizationofitsbehavior.Moreover,thisis verydifferentfromthelanguagelearningprocessofhumans,wherelanguageis acquiredduringcommunicationbytakingspeakingactionandlearningfromthe consequencesofspeakingactioninaninteractivemanner.Thispaperpresents aninteractivesettingforgroundednaturallanguagelearning,whereanagent learnsnaturallanguagebyinteractingwithateacherandlearningfromfeedback, thuslearningandimprovinglanguageskillswhiletakingpartintheconversation. Toachievethisgoal,weproposeamodelwhichincorporatesbothimitationand reinforcementbyleveragingjointlysentenceandrewardfeedbacksfromtheteacher. Experimentsareconductedtovalidatetheeffectivenessoftheproposedapproach. 1Introduction Naturallanguageistheoneofthemostnaturalformofcommunicationforhuman,andtherefore itisofgreatvalueforanintelligentagenttobeabletoleveragenaturallanguageasthechannelto communicatewithhumanaswell.Recentprogressonnaturallanguagelearningmainlyrelieson supervisedtrainingwithlargescaletrainingdata,whichtypicallyrequiresahugeamountofhuman laborforannotating.Whilepromisingperformancehasbeenachievedinmanyapplications regardlessofthelabelingeffort,thisisverydifferentfromhowhumanslearn.Humansactupon theworldandlearnfromtheconsequencesoftheiractions[Skinner,1957].Formechanicalactions suchasmovement,theconsequencesmainlyfollowgeometricalandmechanicalprinciples,whilefor language,humansactbyspeakingandtheconsequenceistypicallyresponseintheformofverbal andotherbehavioralfeedbacks( e.g. ,nodding)fromconversationpartners.Thesefeedbackstypically containinformativesignalonhowtoimprovethelanguageskillsinsubsequentconversionsand playanimportantroleinhuman'slanguageacquisitionprocess[PetursdottirandMellor,2016,Kuhl, 2004,Weston,2016]. Thelanguageacquisitionprocessofababyisbothimpressiveasamanifestationofhumanintelligence andinspiringfordesigningnovelsettingsandalgorithmsforcomputationallanguagelearning.For example,babyinteractswithpeopleandlearnthroughmimickingandfeedbacks[Kuhl,2004,Skinner, 1957].Forlearningtospeak,babyinitiallyperformsverbalactionbymimickinghisconversational parter( e.g. parent)andmasterstheskillofgeneratingaword(sentence).Hecouldalsopossiblypick uptheassociationofawordwithavisualimagewhenhisparentssayingﬁ thisisapple ﬂwhilepointing toanappleoranimageofit.Later,onecanaskthebabyquestionlikeﬁ whatisthis ﬂwhilepointing arXiv:1705.09906v1  [cs.CL]  28 May 2017ŠŠŒquestion-answer-feedbackŠŠŒ Teacher: whatisonthenorth Learner: on.cabbageyeseast Teacher: onthenorthisavocado [  ] ŠŠŒstatement-repeat-feedbackŠŠŒ Teacher: onthewestisorange Learner: onthewestisapple Teacher: noorangeisonthewest [  ] ŠŠ-learner-statement-feedbackŠŠŒ Teacher:. Learner: cucumberisontheeast Teacher: cucumberisontheeast [ + ] (a)Training ŠŠcompositional-generalizationŠŠ Teacher: whatisontheeast Learner: avocadoisontheeast Teacher: yesontheeastisavocado Teacher: whereisavocado Learner: avocadoisontheeast Teacher: yesavocadoisontheeast ŠŠŠ-knowledge-transferringŠŠŠ Teacher: whatisonthesouth Learner: onthesouthisorange Teacher: yesorangeisonthesouth (b)Testing Figure1: Interactivelanguagelearningexample. (a)Duringtraining,teacherinteractsinnatural languagewithlearneraboutobjects.Theinteractionsareintheformof(1)question-answer-feedback, (2)statement-repeat-feedback,and(3)statementfromlearnerandthenfeedbackfromteacher.Certain formsofinteractionsmaybeexcludedforcertainsetofobject-directioncombinationsorobjects (referredtoas inactivecombinations/objects )duringtraining.Forexample,thecombinationof { avocado , east }doesnotappearinquestion-answersessions;theobject orange neverappearsin question-answersessionsbutonlyinstatement-repeatsessions.Teacherprovidesbothsentence feedbackaswellasrewardsignal(denotedas [ + ] and [  ] inthe(b)Duringtesting,teacher canaskquestionaboutobjectsaround,includingquestionsinvolving inactivecombinations/objects thathaveneverbeenaskedbefore, e.g. ,questionsaboutthecombinationof{ avocado , east }and questionsabout orange .Thistestingsetupinvolves compositionalgeneralization and knowledge transferring settingsandisusedforevaluatingtheproposedapproach( c.f.Section4 ). toanobject,andprovidesthecorrectanswerifthebabydoesn'trespondorrespondsincorrectly, whichistypicalintheinitialstage.Onecanalsoprovideatthesametimeaverbal ( e.g. ﬁ yes/no ﬂ)withanodding/smile/kiss/hugwhenheanswerscorrectlyasaformofencouragement feedback.Fromababy'sperspective,thewaytolearnthelanguageisbymakingverbalutterancesto parentandadjustinghisverbalbehavioraccordingtothe fromparent. Thisexampleillustratesthatthelanguagelearningprocessisinherently interactive ,apropertywhich ispotentiallydifculttobecapturedbyastaticdatasetasusedintheconventionalsupervisedlearning setting.Inspiredbybaby'slanguagelearningprocess,wepresentanovelinteractivesettingfor groundednaturallanguagelearning,wheretheteacherandthelearnercaninteractwitheachother innaturallanguagesasshowninFigure1.Inthissetting,thereisnodirectsupervisionstoguide thebehaviorofthelearnerasinthesupervisedlearningsetting.Instead,thelearnerhasto actin ordertolearn , i.e. ,engagingintheconversationwithcurrentlyacquiredspeakingskillstoobtain feedbacksfromthedialoguepartner,whichprovidelearningsignalsforfurtherimprovementonthe conversationskills. Toleveragethefeedbacksforlearning,itistemptingtomimictheteacherdirectly( e.g. ,usinga languagemodel).Whilethisisaviableapproachforlearninghowtospeak,theagenttrainedby pureimitationisnotnecessarilyabletoconverseadaptivelywithincontextduetothenegligence ofthereinforcementsignal.Anexampleisthatitishardtomakeasuccessfulconversationwitha well-trainedparrot,whichisonlygoodatmimicking.Thereasonisthatthelearnerismimicking fromathirdpersonperspective[Stadieetal.,2017],mimickingtheteacherwhoisconversingwithit, thuscertainwordsinthesentencesfromtheteachersuchasﬁ yes/no ﬂandﬁ you/I ﬂmightneedtobe removed/adaptedduetothechangeofperspectivefromteachertolearner.Thiscannotbeachieved withimitationonly.Ontheotherhand,itisalsochallengingtogenerateappropriateconversational actionsusingpurelythereinforcementsignalwithoutimitation.Thefundamentalreasonisthe inabilityofspeaking,thustheprobabilityofgeneratingasensiblesentencebyrandomlyutteringis low,letalonethatofaproperone.Thisisexbythefactthatbabiesdon'tfullydeveloptheir languagecapabilitieswithouttheabilitytohear,whichisoneofthemostimportantchannelsfor language-relatedimitation. Inthispaper,weproposea jointimitationandreinforcement approachforinteractivelanguage learning.Theproposedapproachleveragesbothverbalandencouragementfeedbacksfromthe teacherforjointlearning,thusovercomingthedifencounteredwitheitheronlyimitationor reinforcement.Thecontributionsofthispapercanbethereforesummarizedasthefollowing:  Wepresentanovelhuman-likeinteraction-basedgroundedlanguagelearningsettingwherelan- guageislearnedbyinteractingwiththeenvironment(teacher)innaturallanguage.  Wepresentagroundednaturallanguagelearningapproachundertheinteractivesettingbyleverag- ingfeedbacksfromtheteacherduringinteractionthroughjointimitationandreinforcement. 2 Tothebestofourknowledge,thisistheworkonusingimitationandreinforcementjointlyfor groundednaturallanguagelearninginaninteractivesetting. Theremainderofthepaperisstructuredasfollows.InSection2,wemakeabriefreviewofrelated workonnaturallanguagelearning.Section3introducestheformulationoftheinteraction-based naturallanguagelearningproblem,followedwithdetailedexplanationoftheproposedapproach. ExperimentsarecarriedoutinSection4toshowthelanguagelearningabilityoftheproposed approachintheinteractivesetting.Finally,weconcludethepaperinSection5. 2RelatedWork Deepnetworkbasedlanguagelearninghasreceivedgreatsuccessrecentlyandhasbeenapplied indifferentapplications,forexample,machinetranslation[Sutskeveretal.,2014],imagecaption- ing/visualquestionanswering[Maoetal.,2015,Vinyalsetal.,2015,Antoletal.,2015]anddialogue responsegeneration[VinyalsandLe,2015,Wenetal.,2015].Fortraining,alargeamountoftraining datacontainingsource-targetpairsisneeded,typicallyrequiringaamountofeffortsto collect.Thissettingessentiallycapturesthestatisticsofthetrainingdataanddoesnotrespectthe interactivenatureoflanguagelearningthusisverydifferentfromhowhumanslearn. Whileconventionallanguagemodelistrainedinasupervisedway,therearesomerecentworks usingreinforcementlearningfortraining.Theseworksmainlytargetattheproblemoftuningthe performanceofalanguagemodelpre-trainedinasupervisedwayaccordingtoareward functionwhichiseitherdirectlytheevaluationmetricsuchasstandardBLEUcore[Ranzatoetal., 2016,Bahdanauetal.,2017],manuallydesignedfunction[Lietal.,2016]ormetriclearnedinan adversarialsetting[Yuetal.,2017,Lietal.,2017b],whichisnon-differentiable,leadingtotheusage ofreinforcementlearning.Differentfromthem,ourmainfocusisonthepossibilityoflanguage learninginaninteractivesettingandrequiredmodeldesigns,ratherthanoptimizingaparticular modeloutputtowardsaevaluationmetric. Therearesomerecentworksonlearningtocommunicate[Foersteretal.,2016,Sukhbaataretal., 2016]andtheemergenceoflanguage[Lazaridouetal.,2017,MordatchandAbbeel,2017].The emergedlanguageneedtobeinterpretedviapost-processing[MordatchandAbbeel,2017].Dif- ferently,weaimtoachievenaturallanguagelearningfrombothperspectivesofunderstandingand generation( i.e. , speaking ),thusthespeakingactionoftheagentisreadilyunderstandablewithoutany post-processing.Therearealsoworksondialoguelearningusingaguesser/responsersettingwhere theguessertriestoachievethegoal( e.g. ,bycollectingadditional informationthroughaskingquestionstotheresponser[Strubetal.,2017,Dasetal.,2017].These workstrytooptimizethequestiontobeaskedinordertohelptheguessertoachievetheguessing goal.Thusthefocusisverydifferentfromourgoaloflanguagelearningthroughinteractionswith teacher. Ourworkisalsorelatedtoreinforcementlearningbasedcontrolwithnaturallanguageaction space[Heetal.,2016]inthesensethatourmodelalsooutputsactioninnaturallanguagespace. Wealsosharessimilarmotivationwith[Weston,2016,Lietal.,2017a],wherelanguagelearning throughtextualdialoguehasbeenexplored.However,intheseworks[Heetal.,2016,Weston,2016, Lietal.,2017a]asetofcandidatesequencesisprovidedandtheactionrequiredisselectingone fromthecandidateset,thusisessentiallya discretecontrol problem.Incontrast,ourmodelachieves sentencegenerationthroughcontrolina continuousspace ,withapotentiallysizedaction spaceconsistingofallpossiblesequences. 3Interaction-basedLanguageLearning Wewillintroducetheproposedinteraction-basednaturallanguagelearningapproachinthissection. Thegoalistodesignalearningagent 1 thatcanlearntoconversebyinteractingwiththeteacher, whichcanbeeitheravirtualteacherorahuman( c.f. Figure1 ˘ 2).Attimestep t ,accordingto avisualimage v ,teachergeneratesasentence w t whichcanbeaquestion( e.g. ,ﬁ whatisonthe east ﬂ,ﬁ whereisapple ﬂ),astatement( e.g. ,ﬁ bananaisonthenorth ﬂ),oranemptysentence(denoted asﬁ.ﬂ).Thelearnertakesteacher'ssentence w t andthevisualcontent v ,andproducesasentence response a t totheteacher.Theteacherwillthenprovidefeedbackstothelearneraccordingto itsresponseintheformofbothsentence w t +1 andreward r t +1 .Thesentence w t +1 represents verbalfeedbackfromteacher( e.g. ,ﬁ yesontheeastischerry ﬂ,ﬁ noappleisontheeast ﬂ)and r t +1 modelsthenon-verbalvefeedbacksuchasnodding/smile/kiss/hug,whichalsoappears 1 Weusetheterm agent interchangeablywith learner accordingtocontextinthepaper. 3 naturallyduringinteraction.Theproblemisthereforetodesignamodelthatcanlearngrounded naturallanguagefromteacher'ssentencesandrewardfeedbacks.Whileitmightlookspromisingto formulatetheproblemassupervisedtrainingbylearningfromthesubsetofsentencesfromteacher withonlypositiverewards,thisapproachwon'tworkbecauseofthedifcultiesduetothechangedof perspective[Stadieetal.,2017]asmentionedearlier.Ourformulationoftheproblemaswellasthe detailsoftheproposedapproacharepresentedinthesequel. 3.1ProblemFormulation Aresponsefromtheagentcanbemodeledasasamplefromaprobabilitydistributionoverthe possibleoutputsequences.,foroneepisode,giventhevisualinput v andtextualinput w 1: t fromteacheruptotimestep t ,theresponse a t fromtheagentcanbegeneratedbysampling fromapolicydistribution p R  (  ) ofthespeakingaction: a t ˘ p R  ( a j w 1: t ; v ) : (1) Theagent interacts withteacherbyoutputtingtheutterance a t andreceivesthe feedbacks from teacherattimestep t +1 as F = f w t +1 ;r t +1 g . w t +1 isintheformofasentencewhichrepresents averbalinaccordancewith w t and a t ,withes( yes/no )addedwitha probabilityofhalf( c.f. Figure1 ˘ 2).Reward r t +1 isascalar-valuedfeedbackwithpositivevalue asencouragementwhilenegativevalueasdiscouragementaccordingtothecorrectnessoftheagent utterance a t .Thetaskofinteraction-basedlanguagelearningcanbestatedas learningbyconversing withteacherandimprovingfromteacher'sfeedbacks F .Mathematically,weformulatetheproblem astheminimizationofacostfunctionasfollows: L  = L I  + L R  = E S h  P t log p I  ( w t +1 j w 1: t ; v ) i | {z } Imitation + E p R  h  P t [  ] t  r t +1 i | {z } Reinforce ; (2) where E S (  ) istheexpectationoverallthesentencesequences S generatedfromteacher, r t +1 isthe immediaterewardreceivedattimestep t +1 aftertakingspeakingactionfollowingpolicy p R  (  ) at timestep t and  istherewarddiscountfactor. [  ] t isusedtodenotetheexponentiationover  to differentiateitwithsuperscriptindexing.Asforbothcomponents,thetrainingsignalisobtained via interaction withtheteacher,wetermedthistaskas interaction -basedlanguagelearning.Forthe imitationpart,itessentiallylearnsfromteacher'sverbalresponse w t +1 ,whichcanonlybeobtained asaconsequenceofitsspeakingaction.Forthereinforcepart,itlearnsfromteacher'srewardsignal r t +1 ,whichisalsoobtainedaftertakingthespeakingactionandreceivedatthenexttimestep. Theproposedinteractivelanguagelearningformulationintegratestwocomponentswhichcanfully leveragethefeedbacksappearingnaturallyduringconversationalinteraction:  Imitation playstheroleoflearningagroundedlanguagemodelbyobservingteacher'sbehaviors duringconversionwiththelearneritself.Thisenablesthelearnertohavethebasicabilitytospeak withincontext.Thetrainingdatahereareonlythesentencesfromteacher,withoutanyexplicit labelingofground-truthandisamixtureofexpectedcorrectresponseandothers.Thewayof trainingisby predictingthefuture .More,themodelispredictingthenextfutureword atwordlevelandpredictingthenextsentenceatsentencelevel.Anotherimportantpointisthatit isineffect thirdpersonimitation [Stadieetal.,2017],asthelearnerisimitatingtheteacherwhois conversingwithit,ratherthananotherexpertstudentwhoisconversingwithteacher.  Reinforce 2 leveragesthevefeedbacksfromtheteacherforlearningtoconverseproperly byadjustingtheactionpolicydistribution.Itenablesthelearnertousetheacquiredspeaking abilityandadaptitaccordingtofeedbacks.Herewehavethelearningsignalintheformofreward. Thisisanalogoustobaby'slanguagelearningprocess,whousestheacquiredlanguageskillsby trialanderrorwithparentsandimprovesaccordingtotheencouragementfeedbacks. NotethatwhileimitationandreinforcearerepresentedastwoseparatecomponentsinEqn.(2),they aretiedviaparametersharinginordertofullyleveragebothformsoftrainingsignals.Thisformof jointlearningiscrucialforachievingsuccessfullanguagelearning,comparedwithapproacheswith onlyimitationorreinforcewhicharelesseffective,asvbyexperimentsinSection4. 2 Reinforcedenotesthemodulethatlearnsfromthereinforcement/encouragementsignalthroughoutthe paperandshouldbedifferentiatedwiththeREINFORCEalgorithmintheliterature[SuttonandBarto,1998]. 4 w t w t +1 a t v v v h t  1 last h t last t ! t +1 ! h t 0 h t +1 0 Encoding-RNN Action-RNN 6 ? shareparameter <bos> where is apple where is apple <eos> <bos> no west no west <eos> <bos> apple is on the south <eos> controller f k t visual encoder visual encoder visual encoder (a) spatialconvolution spatialsummation Hadamardproduct mixaggregation V att CNN att.  mask aggregated feature v h t 0 (b)visualencoder f h t last k t skipconnect. (c)controller Figure2: Networkstructure. (a)Illustrationofthenetworkstructurewithsampleinputs.(b)Visual encodernetwork V att (  ) .VisualimageisencodedbyaCNNandspatiallyaggregatedtoavector withanattentionmap.TheattentionmapisobtainedbyconvolvingthefeaturemapfromCNNwith aspatialgeneratedfromtheinitialstate h t 0 .Abinarymaskgeneratedfrom h t 0 isappliedtothe spatiallyaggregatedvectortoproducethevisualfeaturevector.Attimestep t ,theencoding- RNNtakesteacher'ssentence(ﬁ whereisapple ﬂ)andthevisualfeaturevectorfromthevisualencoder V att (  ) asinputs.Thelaststateoftheencoding-RNN h t last ispassedthroughacontroller f (  ) tothe action-RNNforresponsegeneration.Parametersaresharedbetweenencoding-RNNandaction-RNN. Duringtraining,theRNNistrainedbypredictingnextwordsandnextsentences.(c)Controller networkwitharesiduecontrolmodulefollowedbyaGaussianPolicymodule(c.f.Sec.3.2.2). 3.2Approach AhierarchicalRecurrentNeuralNetworkisusedforcapturingthesequentialstructurebothacross sentencesandwithinasentence[Yuetal.,2016,Serbanetal.,2016],asshowninFigure2(a). Attime-step t ,anencoding-RNNencodestheinputsentence w t fromteacheraswellashistory informationintoastatevector h t last ,whichispassedthroughanactioncontroller f (  ) toproducea controlvector k t asinputtotheaction-RNNforgeneratingtheresponse a t totheteacher'ssentence. Teacherwillgeneratefeedback F = f w t +1 ;r t +1 g accordingtoboth w t and a t .Inadditiontobeing usedasinputtoactioncontroller,thestatevectorisalsopassedtothenexttimestepandusedasthe initialstateoftheencoding-RNNinthenextstep( i.e. , h t +1 0 , h t last )forlearningfrom w t +1 ,thus forminganotherlevelofrecurrenceatthescaleoftimesteps. 3.2.1ImitationwithHierarchical-RNN-basedLanguageModeling Theteacher'swayofspeakingprovidesasourceforthelearnertomimic.Onewaytolearnfrom thissourceofinformationisbypredictiveimitation.,foraparticularepisode,wecan representtheprobabilityofthenextsentence w t +1 conditionedontheprevioussentences w 1: t and currentimage v as p I  ( w t +1 j w 1: t ; v )= p I  ( w t +1 j h t last ; v )= Q i p I  ( w t +1 i j w t +1 1: i  1 ; h t last ; v ) ; (3) where h t last isthelaststateofRNNattimestep t asthesummarizationof w 1: t ( c.f. Figure2),and i indexeswordswithinasentence.Itisnaturaltomodeltheprobabilityofthe i -thwordinthe t +1 -th sentencewithanRNNaswell,wherethesentencesupto t andwordsupto i withinthe t +1 -thsentence weconditioneduponiscapturedbyaed-lengthhiddenstatevectoras h t +1 i =RNN( h t +1 i  1 ;w t +1 i ) , thus p I  ( w t +1 i j w t +1 1: i  1 ; h t last ; v )=softmax( W h h t +1 i + W v V att ( v ; h t +1 0 )+ b ) ; (4) where W h , W v and b denotethetransformationweightandbiasparametersrespectively. V att (  ) denotesthevisualencodingnetworkwithspatialattentionincorporatedasshowninFigure2(b). V att (  ) takestheinitialRNNstate h t 0 andvisualimage v asinput.Thevisualimageisencoded byaCNNtoobtainthevisualfeaturemap(redcubeinFigure2(b)).Thevisualfeaturemapis appendedwithanothersetofmapswithlearnableparametersforencodingthedirectionalinformation (bluecubeinFigure2(b)).Thissetoffeaturemapsisspatiallyaggregatedtoavectorwithanattention 5 map,whichisobtainedbyconvolvingthefeaturemapwithaspatialgeneratedfromtheinitial RNNstate.Anattentionmaskforemphasizingvisualordirectionalfeaturesisgeneratedfrom h t 0 andisappliedtothespatiallyaggregatedvectortoproducethefeaturevector.Theinitialstateof theencoding-RNNisthelaststateofthepreviousRNN, i.e. , h t +1 0 = h t last and h 0 0 = 0 . Thelanguagemodeltrainedthiswaywillhavethebasicabilityofproducingasentenceconditioned ontheinput.Therefore,whenconnectinganencoding-RNNwithaction-RNNdirectly, i.e. ,inputing thelaststatevectorfromencoding-RNNintoaction-RNNastheinitialstate,thelearnerwillhave theabilitytogenerateasentencebymimickingthewayteacherspeaks,duetoparametersharing. However,thisbasicabilityofspeakingisnotenoughforthelearnertoconverseproperlywithteacher, whichrequirestheincorporationofreinforcementsignalsasdetailedinthefollowingsection. 3.2.2LearningviaReinforcementforSequenceActions Anagentgeneratesanactionaccordingto p R  ( a j w 1: t ; v ) .Assentences w 1: t canbesummarized asthelastRNNstate h t last ,theactionpolicydistributioncanberepresentedas p R  ( a j h t last ; v ) .To leveragethelanguageskillthatissimultaneouslylearnedfromimitation,wecangeneratethesentence usingalanguagemodelsharedwithimitation,butwithamodulatedconditionalsignalviaacontroller network f (  ) asfollows( c.f. Figure2(a,c)) p R  ( a t j h t last ; v )= p I  ( a t , w t +1 j f ( h t last ) ; v ) : (5) Thereasonforincorporatingacontroller f (  ) formodulationisthatthebasiclanguagemodelonly offersthelearnertheabilitytogenerateasentence,butnotnecessarilytheabilitytorespondcorrectly, ortoansweraquestionfromteacherproperly.Withoutanyadditionalmodule,theagent'sbehaviors wouldbethesameasthosefromteacherbecauseofparametersharing,thusagentcannotlearnto speakcorrectlyinanadaptivemannerbyleveragingthefeedbacksfromteacher. Controller f (  ) isacompositefunctionwithtwocomponents:(1)aresiduestructurednetworkfor transformingtheencodingvector h t last inordertomodifythebehavior;(2)aGaussianpolicymodule forgeneratingacontrolvectorfromaGaussiandistributionconditionedonthetransformedencoding vectorfromtheresiduecontrolnetworkasaformofexploration.Inpractice,wealsoincorporatea gradient-stoppinglayerbetweenthecontrolleranditsinput,toencapsulateallthemodulationability withinthecontroller. ResidueControl. Theactioncontrollershouldhavethepropertythatitcanpasstheinputvector tothenextmodulewhendesirablewhilecanmodifythecontentoftheinputvector otherwise.Therefore,aresiduestructurednetworkisonedesignsatisfyingthisrequirement,witha contentmodifyingvectoraddedtotheoriginalinputvector( i.e. ,skipconnection)asfollows c = ˝ ( h )+ h ; (6) where ˝ (  ) isacontenttransformationnetand c isthegeneratedcontrolvector.Thereasonfor includingaskipconnectionisthatitofferstheabilitytoleveragethelanguagemodelsimultaneously learnedviaimitationforgeneratingsensiblesentencesandthetransformationnet ˝ (  ) includes learnableparametersforadjustingthebehaviorsviainteractionswiththeenvironmentandfeedbacks fromteacher.Weimplement ˝ (  ) astwofully-connectedlayerswithReLUactivation. GaussianPolicy. GaussianpolicynetmodelstheoutputvectorasaGaussiandistributionconditioned ontheinputvector.Ittakesthegeneratedcontrolvector c asinputandproducesavector k whichis usedastheinitialstateoftheaction-RNN.TheGaussianpolicyismodeledasfollows: p R  ( k j c )= N ( c ;  T  ) ;  =diag[  ( c )] : (7) TheincorporationofGaussianpolicyintroducesstochasticunitintothenetwork,thusback- propagationcannotbeapplieddirectly.Wethereforeusepolicygradientalgorithmforoptimiza- tion[SuttonandBarto,1998].where  (  ) isasub-networkforestimatingthestandardderivation vectorandisimplementedusingafully-connectedlayerwithReLUactivation. 3 Thevector k gen- eratedfromthecontrolleristhenusedastheinitialstateofaction-RNNandthesentenceoutput isgeneratedusingbeamsearch( c.f. Figure2(a)).Forthereward r t +1 inEqn.(2),weintroduce abaselineforreducingvarianceas r t +1  V ˛ ( v ) ,where V ˛ (  ) representsthevaluenetworkwith parametervector ˛ andisestimatedbyaddingto L R anadditionalvaluenetworkcost L V asfollows L V = E p R   r t +1 +  ˛  ( v t +1 )  V ˛ ( v t )  2 ; (8) where ˛ denotesthesetofparametersinthevaluenetworkand V ˛  (  ) denotesthetargetversionof thevaluenetwork,whoseparametervector ˛  isperiodicallycopiedfromthetrainingversion[Mnih etal.,2013]. 3 Inpractice,weaddasmallvalue( 0 : 01 )to  ( c ) asaconstrainoftheminimumstandarddeviation. 6 3.3Training Traininginvolvesoptimizingthestochasticpolicybyusingtheteacher'sfeedback F asatraining signal,obtainingasetofoptimizedparametersbyconsideringjointlyimitationandreinforcementas showninEqn.(2).Stochasticgradientdescendisusedfortrainingthenetwork.For L I fromimitation module,wehaveitsgradientas: r  L I  =  E S [ r  P t log p I  ( w t +1 j w 1: t ; v )] : (9) Usingpolicygradienttheorem[SuttonandBarto,1998],wehavethefollowinggradientforthe reinforcemodule: r  L R  =  E p R   [ r  log p R  ( k t j c t )+ r ˛ V ˛ ( v )]    ; (10) where  isthetd-erroras  = r t +1 + V ˛  ( v )  V ˛ ( v ) .Thealgorithmisimplementedwith deeplearningplatform PaddlePaddle 4 .WetrainthenetworkwithAdagrad[Duchietal.,2011]with abatchsizeof 16 andalearningrateof 1  10  5 .Discountfactor  =0 : 99 .Experiencereplayis usedinpractice[Mnihetal.,2013]. 4ExperimentalResults Weevaluatetheperformanceoftheproposedapproachunderseveraldifferentsettingstodemon- strateitsabilityofinteractivelanguagelearning.Fortrainingefciency,weconstructasimulated environmentforlanguagelearningasshowninFigure1.Weconsiderthecasewithfourdifferent objectsaroundthelearnerineachdirection( S,N,E,W ),whicharerandomlysampledfromasetof objectsforeachsession.Withinthisenvironment,ateacherinteractswiththeagentaboutobjects aroundinthreedifferentforms:(1)askingaquestionasﬁ whatisonthesouth ﬂ,ﬁ whereisapple ﬂand theagentanswersthequestion;(2)describingobjectsaroundasﬁ appleisontheeast ﬂandagents repeatthestatement;(3)sayingnothing(ﬁ.ﬂ)thenagentdescribesobjectsaroundandgetsafeedback fromteacher.Theagentreceivesapositivereward( r =+1 )ifitbehavescorrectly(generatesacorrect answertoaquestionfromteacherorproducesacorrectstatementifteachersaysnothing)anda negativereward( r =  1 )otherwise.Rewardisusedtorepresentteacher'snon-verbalfeedbacksuch as nodding asaformofencouragement.Besidesrewardfeedback,teacheralsoprovidesaverbal feedbackincludingtheexpectedanswerintheformofﬁ Xisontheeast ﬂorﬁ ontheeastisX ﬂand with(ﬁ yes/no ﬂ)addedwithaprobabilityofhalf.Thespeakingactionfromtheagentiscorrect ifitoutputsasentencematchesexactlywiththeexpectedanswerinoneoftheaboveforms.Thereis apossibilityforthelearnertogenerateanewcorrectsentencethatbeyondteacher'sknowledge.This isnothandledinourcurrentworkduetotheusageofascriptedteacher. LanguageLearningEvaluation. Wevalidatethebasiclanguagelearningabilityoftheproposed approachundertheinteractivelanguagelearningsetting.Inthissetting,theteachergenerates asentenceforthelearner,thenthelearnerwillrespond,andtheteacherwillprovidefeedbackin termsofsentenceandreward.Wecomparetheproposedapproachwithtwobaselineapproaches: (1) Reinforce whichusesdirectlyreinforcementforlearningfromteacher'srewardfeedback[Sutton andBarto,1998];(2) Imitation whichlearnsbymimickingteacher'sbehavior[Sutskeveretal., 2014].ExperimentalresultsareshowninFigure3.Itisinterestingtonotethatlearningdirectly fromrewardfeedbackonly( Reinforce )doesnotleadtosuccessfullanguageacquisition.Thisis mainlybecauseofthelowpossibilityofgeneratingasensiblesentencebyrandomexploration,and theevenlowerpossibilityofgeneratingthecorrectsentence,thusthereceivedrewardcanstayat  1 .Ontheotherhand,the Imitation approachperformsbetterthan Reinforce ,duetothe speaking abilityitgainedthroughmimicking.Theproposedapproachachievesrewardhigherthanboth comparedapproaches,duetotheeffectivenessofthejointformulation,whichcanfullyleveragethe feedbacksignalsappearednaturallyduringconversionforlearning.Thisindicatestheeffectiveness oftheproposedapproachforlanguagelearningundertheinteractivesetting.Similarbehaviors havebeenobservedduringtesting.WefurthervisualizesomeexamplesasshowninFigure4along withthegeneratedattentionmaps.Ascanbeobservedfromtheresults,theproposedapproachcan successfullygeneratecorrectattentionmapsforboth what and where questions.Whenteachersays nothing(ﬁ.ﬂ),theagentcangenerateastatementdescribinganobjectaroundcorrectly. 4 https://github.com/PaddlePaddle/Paddle 7 (a) (b)QuantativeResutls Table1: TestingResultswithMixed Settings Reinforce Imitation Proposed Compositional-gen. 0.0% 83.7% 98.9% Knowledge-transfer 0.0% 81.6% 97.5% Table2: TestingResultswtihHeld-out Settings Reinforce Imitation Proposed Compositional-gen. 0.0% 75.1% 98.3% Knowledge-transfer 0.0% 70.4% 89.0% Figure3: Evaluationresults. (a)Evolutionofrewardduringtraining.(b)Comparisonofthepro- posedapproachwithReinforceandImitationapproachesacrossdifferentsettingsand Mixed denotestheinvolvinginteractionswithallobjects. Held-out denotes theinvolvinginteractionswithonlytheobjectsthatareinactiveduringtraining. T: whatisonthenorth L: onthenorthisapple T: yesappleisonthenorth [ + ] att.map (a) T: whatisontheeast L: ontheeastisavocado T: avocadoisontheeast [ + ] att.map (b) T: whereisstrawberry L: strawberryisonthewest T: yesstrawberryisonthewest [ + ] att.map (c) T:. L: ontheeastiscucumber T: yesontheeastiscucumber [ + ] att.map (d) Figure4: Exampleresults. (a-b) what questions.(c) where question.(d)teachersaysnothing(ﬁ.ﬂ) andtheagentisexpectedtoproduceastatement.Foreachexample,weshowthevisualimage,the conversiondialoguesbetweenteacherandlearner,aswellasthe attentionmap ( att.map )generated fromthelearnerwhenproducingtheresponsetoteacher(overlaidontop-right).Theattentionmapis renderedasaheatmap,withredcolorindicatinglargevaluewhileblueindicatingsmallvalue.Grid linesareoverlaidontopoftheattentionmaptoforvisualizationpurpose.Thepositionofthelearner ismarkedwithacrossintheattentionmap( T / L :teacher/learner, [ + /  ] :positive/negativerewards). Zero-shotDialogue. Anintelligentagentisexpectedtohaveanabilitytogeneralize.Whilethis isnotthemainfocusonthepaper,weuseitasawaytoassessthelanguagelearningabilityof anapproach.Experimentsaredoneinfollowingtwosettings.(1) Compositionalgeneralization : thelearnerinteractswiththeteacheraboutobjectsaroundduringtraining,butdoesnothaveany interactionwithcertainobjects(referredtoas inactive objects)atparticularlocations,whileintesting theteachercanaskquestionsaboutanobjectregardlessofitslocation.Itisexpectedthatagood learnershouldbeabletogeneralizetheconceptsitlearnedaboutboth objects and locations aswellas theacquiredconversationskillsandcaninteractsuccessfullyinnaturallanguagewithteacherabout novel{ object , location }combinationsthatitneverexperiencedbefore.(2) Knowledgetransferring : teacheraskslearnerquestionsabouttheobjectsaround.Forcertainobjects,theteacheronlyprovides descriptionswithoutaskingquestionsduringtraining,whileintesting,theteachercanaskquestions aboutanyobjectpresentinthescene.Thelearnerisexpectedtobeabletotransfertheknowledge learnedfromteacher'sdescriptiontogenerateananswertoteacher'squestionabouttheseobjects. Experimentsarecarriedoutunderthesetwosettingsfortwo( mixed and held-out ) andexperimentalresultsaresummarizedinTable1andTable2respectively. Mixedation denotesthecaseinwhichamixtureofinteractionswithallobjectsregardlessofwhethertheyare activeorinactiveduringtraining. Held-outation denotesthecaseinvolvinginteractionswith onlytheobjectsthatareinactiveduringtraining.Theresultsshowsthatthe Reinforce approach performspoorlyunderbothsettingsduetothelackofbasiclanguage-relatedabilitiesasmentioned intheprevioussection.The Imitation approachperformsbetterthan Reinforce mainlyduetoits languagespeakingabilitythroughmimicking.Notethattheheld-outisasubsetofthe mixinvolvingonlynovelobjects/combinations,thusismoredifcultthanthemixed case.Itisinterestingtonotethattheproposedapproachmaintainsaconsistentbehaviorunderthe moredifheld-outandoutperformstheothertwoapproachesunderbothsettings, demonstratingitseffectivenessininteractivelanguagelearning. 8 5Conclusion Wepresentaninteractivesettingforgroundednaturallanguagelearningandproposeanapproach thatachieveseffectiveinteractivenaturallanguagelearningbyfullyleveragingthefeedbacksthat arisenaturallyduringinteractionthroughjointimitationandreinforcement.Experimentalresults showthattheproposedapproachprovidesaneffectivewayfornaturallanguagelearninginthe interactivesettingandenjoysdesirablegeneralizationandtransferringabilitiesunderseveraldifferent scenarios.Asforfuturework,wewouldliketoexplorethedirectionofexplicitmodelingoflearned knowledge[Yang,2003]andfastlearningaboutnewconcepts[Andrychowiczetal.,2016].Another interestingdirectionistoconnectthelanguagelearningtaskpresentedinthispaperwithother heterogeneoustaskssuchasnavigation. Acknowledgements WethankXiaochenLian,ZhuoyuanChen,YiYangandQingSunfortheirdiscussionsandcomments. References M.Andrychowicz,M.Denil,S.G.Colmenarejo,M.W.Hoffman,D.Pfau,T.Schaul,andN.deFreitas.Learning tolearnbygradientdescentbygradientdescent.In NIPS ,2016. S.Antol,A.Agrawal,J.Lu,M.Mitchell,D.Batra,C.L.Zitnick,andD.Parikh.VQA:VisualQuestion Answering.In ICCV ,2015. D.Bahdanau,P.Brakel,K.Xu,A.Goyal,R.Lowe,J.Pineau,A.C.Courville,andY.Bengio.Anactor-critic algorithmforsequenceprediction.In ICLR ,2017. A.Das,S.Kottur,,J.M.Moura,S.Lee,andD.Batra.Learningcooperativevisualdialogagentswithdeep reinforcementlearning. CoRR ,abs/1703.06585,2017. J.C.Duchi,E.Hazan,andY.Singer.Adaptivesubgradientmethodsforonlinelearningandstochastic optimization. JournalofMachineLearningResearch ,12:2121Œ2159,2011. J.N.Foerster,Y.M.Assael,N.deFreitas,andS.Whiteson.Learningtocommunicatewithdeepmulti-agent reinforcementlearning.In NIPS ,2016. J.He,J.Chen,X.He,J.Gao,L.Li,L.Deng,andM.Ostendorf.Deepreinforcementlearningwithanatural languageactionspace.In ACL ,2016. P.K.Kuhl.Earlylanguageacquisition:crackingthespeechcode. NatRevNeurosci ,5(2):831Œ843,2004. A.Lazaridou,A.Peysakhovich,andM.Baroni.Multi-agentcooperationandtheemergenceof(natural)language. In ICLR ,2017. J.Li,W.Monroe,A.Ritter,D.Jurafsky,M.Galley,andJ.Gao.Deepreinforcementlearningfordialogue generation.In EMNLP ,2016. J.Li,A.H.Miller,S.Chopra,M.Ranzato,andJ.Weston.Learningthroughdialogueinteractions.In ICLR , 2017a. J.Li,W.Monroe,T.Shi,A.Ritter,andD.Jurafsky.Adversariallearningforneuraldialoguegeneration. CoRR , abs/1701.06547,2017b. J.Mao,W.Xu,Y.Yang,J.Wang,Z.Huang,andA.Yuille.Deepcaptioningwithmultimodalrecurrentneural networks(m-RNN). ICLR ,2015. V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wierstra,andM.Riedmiller.PlayingAtari withdeepreinforcementlearning.In NIPSDeepLearningWorkshop .2013. I.MordatchandP.Abbeel.Emergenceofgroundedcompositionallanguageinmulti-agentpopulations. CoRR , abs/1703.04908,2017. A.I.PetursdottirandJ.R.Mellor.Reinforcementcontingenciesinlanguageacquisition. PolicyInsightsfrom theBehavioralandBrainSciences ,4(1):25Œ32,2016. M.Ranzato,S.Chopra,M.Auli,andW.Zaremba.Sequenceleveltrainingwithrecurrentneuralnetworks.In ICLR ,2016. I.V.Serban,A.Sordoni,Y.Bengio,A.C.Courville,andJ.Pineau.Buildingend-to-enddialoguesystemsusing generativehierarchicalneuralnetworkmodels.In AAAI ,2016. B.F.Skinner. VerbalBehavior .CopleyPublishingGroup,1957. B.C.Stadie,P.Abbeel,andI.Sutskever.Third-personimitationlearning.In ICLR ,2017. F.Strub,H.deVries,J.Mary,B.Piot,A.C.Courville,andO.Pietquin.End-to-endoptimizationofgoal-driven andvisuallygroundeddialoguesystems.In IJCAI ,2017. S.Sukhbaatar,A.Szlam,andR.Fergus.Learningmultiagentcommunicationwithbackpropagation.In NIPS , 2016. I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequencelearningwithneuralnetworks.In NIPS ,2014. R.S.SuttonandA.G.Barto. ReinforcementLearning:AnIntroduction .MITPress,1998. O.VinyalsandQ.V.Le.Aneuralconversationalmodel. CoRR ,abs/1506.05869,2015. 9 O.Vinyals,A.Toshev,S.Bengio,andD.Erhan.Showandtell:Aneuralimagecaptiongenerator.In CVPR , 2015. T.Wen,M.Gasic,N.Mrksic,P.Su,D.Vandyke,andS.J.Young.SemanticallyconditionedLSTM-based naturallanguagegenerationforspokendialoguesystems.In EMNLP ,2015. J.Weston.Dialog-basedlanguagelearning.In NIPS ,2016. C.D.Yang. KnowledgeandLearninginNaturalLanguage .OxfordUniversityPressUK,2003. H.Yu,J.Wang,Z.Huang,Y.Yang,andW.Xu.Videoparagraphcaptioningusinghierarchicalrecurrentneural networks.In CVPR ,2016. L.Yu,W.Zhang,J.Wang,andY.Yu.SeqGAN:Sequencegenerativeadversarialnetswithpolicygradient.In AAAI ,2017. 10  
371 TransactionsoftheAssociationforComputationalLinguistics,vol.4,pp.371Œ383,2016.ActionEditor:HolgerSchwenk. Submissionbatch:1/2016;Revisionbatch:4/2016;Published7/2016. c  2016AssociationforComputationalLinguistics.DistributedunderaCC-BY4.0license. 372 373 374 375 376 377 378 379 380 381 382 383 384  
PublishedasaconferencepaperatICLR2017 E XPLORING S PARSITYIN R ECURRENT N EURAL N ETWORKS SharanNarang,ErichElsen  ,GregDiamos&ShubhoSengupta y BaiduResearch f sharan,gdiamos g @baidu.com A BSTRACT RecurrentNeuralNetworks(RNN)arewidelyusedtosolveavarietyofproblems andasthequantityofdataandtheamountofavailablecomputehaveincreased, sohavemodelsizes.Thenumberofparametersinrecentstate-of-the-artnetworks makesthemhardtodeploy,especiallyonmobilephonesandembeddeddevices. Thechallengeisduetoboththesizeofthemodelandthetimeittakestoeval- uateit.InordertodeploytheseRNNsef,weproposeatechniqueto reducetheparametersofanetworkbypruningweightsduringtheinitialtraining ofthenetwork.Attheendoftraining,theparametersofthenetworkaresparse whileaccuracyisstillclosetotheoriginaldenseneuralnetwork.Thenetwork sizeisreducedby8  andthetimerequiredtotrainthemodelremainsconstant. Additionally,wecanprunealargerdensenetworktoachievebetterthanbase- lineperformancewhilestillreducingthetotalnumberofparameters. PruningRNNsreducesthesizeofthemodelandcanalsohelpachieve inferencetimespeed-upusingsparsematrixmultiply.Benchmarksshowthatus- ingourtechniquemodelsizecanbereducedby90%andspeed-upisaround2  to7  . 1I NTRODUCTION Recentadvancesinmultiplesuchasspeechrecognition(Graves&Jaitly,2014;Amodeietal., 2015),languagemodeling(J ´ ozefowiczetal.,2016)andmachinetranslation(Wuetal.,2016)canbe atleastpartiallyattributedtolargertrainingdatasets,largermodelsandmorecomputethatallows largermodelstobetrainedonlargerdatasets. Forexample,thedeepneuralnetworkusedforacousticmodelinginHannunetal.(2014)had11 millionparameterswhichgrewtoapproximately67millionforbidirectionalRNNsandfurtherto 116millionforthelatestforwardonlyGRUmodelsinAmodeietal.(2015).Andinlanguagemod- elingthesizeofthenon-embeddingparameters(mostlyintherecurrentlayers)haveexplodedeven asvariouswaysofhandengineeringsparsityintotheembeddingshavebeenexploredinJ ´ ozefowicz etal.(2016)andChenetal.(2015a). Theselargemodelsfacetwochallengesindeployment.Mobilephonesandembedded deviceshavelimitedmemoryandstorageandinsomecasesnetworkbandwidthisalsoaconcern.In addition,theevaluationofthesemodelsrequiresaamountofcomputation.Evenincases whenthenetworkscanbeevaluatedfastenough,itwillstillhaveaimpactonbatterylife inmobiledevices(Hanetal.,2015). InferenceperformanceofRNNsisdominatedbythememorybandwidthofthehardware,sincemost oftheworkissimplyreadingintheparametersateverytimestep.Movingfromadensecalculation toasparseonecomeswithapenalty,butifthesparsityfactorislargeenough,thenthesmaller amountofdatarequiredbythesparseroutinesbecomesawin.Furthermore,thissuggeststhatifthe parametersizescanbereducedtoincacheorotherveryfastmemory,thenlargespeedupscould berealized,resultinginasuper-linearincreaseinperformance.  NowatGoogleBraineriche@google.com y NowatFacebookAIResearchssengupta@fb.com 1 arXiv:1704.05119v2  [cs.LG]  6 Nov 2017PublishedasaconferencepaperatICLR2017 ThemorepowerfulserverclassGPUsusedindatacenterscangenerallyperforminferencequickly enoughtoserveoneuser,butinthedatacenterperformanceperdollarisveryimportant.Techniques thatallowmodelstobeevaluatedfasterenablemoreuserstobeservedperGPUincreasingthe effectiveperformanceperdollar. Weproposeamethodtoreducethenumberofweightsinrecurrentneuralnetworks.Whilethe networkistrainingweprogressivelysetmoreandmoreweightstozerousingamonotonically increasingthreshold.Bycontrollingtheshapeofthefunctionthatmapsiterationcounttothreshold value,wecancontrolhowsparsetheweightmatricesbecome.Weprunealltheweightsof arecurrentlayer;otherlayertypeswithfewerparametersarenotpruned.Separate thresholdfunctionscanbeusedforeachlayer,althoughinpracticeweuseonethresholdfunction perlayertype.Withthisapproach,wecanachievesparsityof90%withasmalllossinaccuracy.We showthistechniqueworkswithGatedRecurrentUnits(GRU)(Choetal.,2014)aswellasvanilla RNNs. Inadditiontotheoflessstorageandfasterinference,thistechniquecanalsoimprovethe accuracyoveradensebaseline.Bystartingwithalargerdensematrixthanthebaselineandthen pruningitdown,wecanachieveequalorbetteraccuracycomparedtothebaselinebutwithamuch smallernumberofparameters. Thisapproachcanbeimplementedeasilyincurrenttrainingframeworksandisagnostictothe optimizationalgorithm.Furthermore,trainingtimedoesnotincreaseunlikepreviousapproaches suchasinHanetal.(2015).Stateoftheartresultsinspeechrecognitiongenerallyrequiredaysto weeksoftrainingtime,soafurther3-4  increaseintrainingtimeisundesirable. 2R ELATED W ORK Therehavebeenseveralproposalstoreducethememoryfootprintofweightsandactivationsin neuralnetworks.Onemethodistouseaedpointrepresentationtoquantizeweightstosigned bytesandactivationstounsignedbytes(Vanhouckeetal.,2011).Anothertechniquethathasbeen triedinthepastistolearnalowrankfactorizationoftheweightmatrices.Onemethodistocarefully constructoneofthefactorsandlearntheother(Deniletal.,2013).Inspiredbythistechnique,a lowrankapproximationfortheconvolutionlayersachievestwicethespeedwhilestayingwithin1% oftheoriginalmodelintermsofaccuracy(Dentonetal.,2014).Theconvolutionlayercanalsobe approximatedbyasmallersetofbasis(Jaderbergetal.,2014).Bydoingthistheyachievea 2.5xspeedupwithnolossinaccuracy.Quantizationtechniqueslikek-meansclusteringofweights canalsoreducethestoragesizeofthemodelsbyfocusingonlyonthefullyconnectedlayers(Gong etal.,2014).Ahashfunctioncanalsoreducememoryfootprintbytyingtogetherweightsthatfall inthesamehashbucket(Chenetal.,2015b).Thisreducesthemodelsizebyafactorof8. Yetanotherapproachtoreducecomputeandnetworksizeisthroughnetworkpruning.Onemethod istouseseveralbiastechniquestodecayweights(Hanson&Pratt,1989).Yetanotherapproach istousethediagonaltermsofaHessianmatrixtoconstructasaliencythresholdandusedthisto dropweightsthatfallbelowagivensaliencythreshold(LeCunetal.,1989).Inthistechnique,once aweighthasbeensetto0,thenetworkisretrainedwiththeseweightsfrozenat0.OptimalBrain SurgeonisanotherworkinthesameveinthatprunesweightsusingtheinverseofaHessianmatrix withtheadditionaladvantageofnore-trainingafterpruning(Hassibietal.,1993). BothpruningandquantizationtechniquescanbecombinedtogetimpressivegainsonAlexNet trainedontheImageNetdataset(Hanetal.,2015).Inthiscase,pruning,quantizationandsubsequent Huffmanencodingresultsina35xreductioninmodelsizewithoutaffectingaccuracy.Therehas alsobeensomerecentworktoshrinkmodelsizeforrecurrentandLSTMnetworksusedinautomatic speechrecognition(ASR)(Luetal.,2016).ByusingahybridstrategyofusingToeplitzmatrices forthebottomlayerandsharedlow-rankfactorsonthetoplayers,theywereabletoreducethe parametersofaLSTMby75%whileincurringa0.3%increaseinworderrorrate(WER). Ourmethodisapruningtechniquethatiscomputationallyefforlargerecurrentnetworks thathavebecomethenormforautomaticspeechrecognition.Unlikethemethodsthatneedto approximateaHessian(LeCunetal.,1989;Hassibietal.,1993)ourmethodusesasimpleheuristic tochoosethethresholdusedtodropweights.Yetanotheradvantage,whencomparedtomethods thatneedre-training(Hanetal.,2015),isthatourpruningtechniqueispartoftrainingandneeds 2 PublishedasaconferencepaperatICLR2017 Table1:Hyper-Parametersusedfordeterminingthreshold(  ) HYPER-PARAMDESCRIPTIONHEURISTICVALUES start itr IterationtostartpruningStartofsecondepoch ramp itr Iterationtoincreasetherateof pruning Startof25%oftotalepochs end itr Iterationtostoppruningmorepa- rameters Startof50%oftotalepochs start slope (  ) InitialslopetoprunetheweightsSeeequation1 ramp slope ( ˚ ) Rampslopetochangetherateof pruning 1 : 5  to 2  freq Numberofiterationsafterwhich  isupdated 100 noadditionalre-training.Eventhoughourtechniquerequiresjudiciouschoiceofpruninghyper- parameters,wefeelthatitiseasierthanchoosingthestructureofmatricestoguidethe forrecurrentnetworks(Luetal.,2016).Anotherapproachforpruningfeedforwardneuralnetworks forspeechrecognitionisusingsimplethresholdtopruneallweights(Yuetal.,2012)ataparticular epoch.However,wethatgradualpruningproducesbetterresultsthanhardpruning. 3I MPLEMENTATION Ourpruningapproachinvolvesmaintainingasetofmasks,amonotonicallyincreasingthreshold andasetofhyperparametersthatareusedtodeterminethethreshold.Duringmodelinitialization, wecreateasetofbinarymasks,oneforeachweightinthenetworkthatareallinitiallysettoone. Aftereveryoptimizerupdatestep,eachweightismultipliedwithitscorrespondingmask.Atregular intervals,themasksareupdatedbysettingallparametersthatarelowerthanthethresholdtozero. Thethresholdiscomputedusinghyper-parametersshowninTable1.Thehyper-parameterscontrol theduration,rateandfrequencyofpruningtheparametersforeachlayer.Weuseadifferentset ofhyper-parametersforeachlayertyperesultinginadifferentthresholdforeachlayertype.The thresholdisupdatedatregularintervalsusingthehyper-parametersaccordingtoAlgorithm1.We don'tmodifythegradientsintheback-propagationstep.Itispossiblefortheupdatesofapruned weighttobelargerthanthethresholdofthatlayer.Inthiscase,theweightwillbeinvolvedinthe forwardpassagain. Weprovideheuristicstohelpdetermine start itr , ramp itr and end itr intable1.Afterpickingthese hyperparametersandassumingthat ramp slope ( ˚ )is1.5  start slope (  ),wecalculate(  )using equation1.  = 2  q  freq 2  ( ramp itr  start itr )+3  ( end itr  ramp itr ) (1) Inordertodetermine q inequation1,weuseanexistingweightarrayfromapreviouslytrained model.Theweightsaresortedusingabsolutevaluesandwepicktheweightcorrespondingtothe 90thpercentileas q .Thisallowsustopickreasonablevaluesforthehyper-parametersrequiredfor pruning.Avalidationsetcanbeusedtotunetheseparameters. Weonlyprunetheweightsoftherecurrentandlinearlayersbutnotthebiasesorbatchnormpa- rameterssincetheyaremuchfewerinnumbercomparedtotheweights.Fortherecurrentlayers, wepruneboththeinputweightmatrixandtherecurrentweightmatrix.Similarly,wepruneallthe weightsingatedrecurrentunitsincludingthoseoftheresetandupdategates. 3 PublishedasaconferencepaperatICLR2017 Algorithm1 PruningAlgorithm current itr =0 while training do forall parameters do param =( param and mask ) if current itr > start itr and current itr < end itr then if ( current itr mod freq ) ==0 then if current itr < ramp itr then  =   ( current itr  start itr +1) = freq else  =(   ( ramp itr  start itr +1)+ ˚  ( current itr  ramp itr +1)) = freq endif mask = abs ( param ) < endif endif endfor current itr +=1 endwhile 4E XPERIMENTS Werunallourexperimentsonatrainingsetof2100hoursofEnglishspeechdataandavalidation setof3.5hoursofmulti-speakerdata.Thisisasmallsubsetofthedatasetsthatweusetotrainour state-of-the-artautomaticspeechrecognitionmodels.WetrainthemodelsusingNesterovSGDfor 20epochs.Besidesthehyper-parametersfordeterminingthethreshold,allotherhyper-parameters remainunchangedbetweenthedenseandsparsetrainingruns.Wethatourpruningapproach workswellforvanillabidirectionalrecurrentlayersandforwardonlygatedrecurrentunits. 4.1B IDIRECTIONAL RNN S WeusetheDeepSpeech2modelfortheseexperiments.AsshowninTable2,thismodelhas2 convolutionlayers,followedby7bidirectionalrecurrentlayersandaCTCcostlayer.Eachrecurrent linearlayerhas1760hiddenunits,creatinganetworkofapproximately67millionparameters.For theseexperiments,weprunethelinearlayersthatfeedintotherecurrentlayers,theforwardand backwardrecurrentlayersandfullyconnectedlayerbeforetheCTClayer.Theseexperimentsuse clippedunits(ReLU) ˙ ( x )=min(max( x; 0) ; 20) astheactivationfunction. Inthesparserun,thepruningbeginsshortlyaftertheepochandcontinuesuntilthe10 th epoch. Wechosethesehyper-parameterssothatthemodelhasanoverallsparsityof88%attheendof pruning,whichis8xsmallerthantheoriginaldensemodel.Thecharactererrorrate(CER)onthe devsetisabout20%worserelativetothedensemodelasshowninTable3. Anargumentagainstthissparsityresultmightbethatwearetakingadvantageofalargemodelthat ovourrelativelysmalldataset.Inordertotestthishypothesis,wetrainadensemodelwith704 hiddenunitsineachlayer,thathasapproximatelythesamenumberofparametersasthesparse model.Table3showsthatthismodelperformsworsethanthesparsemodels.Thussparsemodelis abetterapproachtoreduceparametersthanusingadensemodelwithfewerhiddenunits. Inordertorecoverthelossinaccuracy,wetrainsparsemodelswithlargerrecurrentlayerswith 2560and3072hiddenunits.Figure1ashowsthetraininganddevcurvesforthesesparsemodels comparedtothedensebaselinemodel.Theseexperimentsusethesamehyper-parameters(except forsmallchangesinthepruninghyper-parameters)andthesamedatasetasthebaselinemodel. AsweseeinTable3,themodelwith2560hiddenunitsachievesa0.75%relativeimprovement comparedtothedensebaselinemodel,whilethemodelwith3072hiddenunitshasa3.95%im- provement.Thedense2560modelalsoimprovestheCERby11.85%relativetothedensebaseline model.Thesparse2560modelisabout12%worsethanthecorrespondingdensemodel.Boththese largemodelsareprunedtoachieveasparsityofaround92%.Thesesparselargermodelshave fewerparametersthanthebaselinedensemodel. 4 PublishedasaconferencepaperatICLR2017 Table2:DeepSpeech2architecturewith1760hiddenunits LAYERIDTYPE#PARAMS layer02DConvolution19616 layer12DConvolution239168 layer2BidirectionalRecurrentLinear8507840 layer3BidirectionalRecurrentLinear9296320 layer4BidirectionalRecurrentLinear9296320 layer5BidirectionalRecurrentLinear9296320 layer6BidirectionalRecurrentLinear9296320 layer7BidirectionalRecurrentLinear9296320 layer8BidirectionalRecurrentLinear9296320 layer9FullyConnected3101120 layer10CTCCost95054 WealsocompareourgradualpruningapproachtothehardpruningapproachproposedinYuetal. (2012).Intheirapproach,allparametersbelowacertainthresholdareprunedatparticularepoch. Table4showstheresultsofpruningtheRNNdensebaselinemodelatdifferentepochstoachieve parametercountrangingfrom8millionto11million.Thenetworkistrainedforthesame numberofepochsasthegradualpruningexperiments.Thesehardthresholdresultsarecompared withtheRNNSparse1760modelinTable3.Forapproximatelysamenumberofparameters,gradual pruningis7%to9%betterthanhardpruning. Weconcludethatpruningmodelstoachievesparsityofaround90%reducestherelativeaccuracy ofthemodelby10%to20%.However,foragivenperformancerequirement,itisbettertoprunea largermodelthantouseasmallerdensemodel.Graduallypruningamodelproducesbetterresults thanhardpruning. Table3:GRU&bidirectionalRNNmodelresults MODEL#UNITSCER#PARAMSRELATIVEPERF RNNDenseBaseline176010.6767million0.0% RNNDenseSmall70414.5011.6million-35.89% RNNDenseMedium25609.43141million11.85% RNNSparse1760176012.888.3million-20.71% RNNSparseMedium2560 10.59 11.1million0.75% RNNSparseBig3072 10.25 16.7million3.95% GRUDense25609.55115million0.0% GRUSparse256010.8713million-13.82% GRUSparseMedium3568 9.76 17.8million-2.20% Table4:RNNdensebaselinemodelwithhardpruning #UNITSPRUNEDEPOCHCER#PARAMSRELATIVEPERF 1760513.828million-29.52% 1760713.2711million-24.37% 17601013.418.4million-25.68% 17601213.638million-27.74% 17601526.339.2million-146.77% 5 PublishedasaconferencepaperatICLR2017 (a) (b) Figure1:Traininganddevcurvesforbaseline(dense)andsparsetraining.Figure1aincludes traininganddevcurvesformodelswithlargerrecurrentlayerswith2560and3072hiddenunits comparedtothe1760densebaseline.Figure1bplotsthetraininganddevcurvesforGRUmodels (sparseanddense)with2560parameters. Table5:Gatedrecurrentunitsmodel LAYERIDTYPE#PARAMS layer02DConvolution19616 layer12DConvolution239168 layer2GatedRecurrentLinear29752320 layer3GatedRecurrentLinear39336960 layer4GatedRecurrentLinear39336960 layer5RowConvolution107520 layer6FullyConnected6558720 layer7CTCCost74269 4.2G ATED R ECURRENT U NITS WealsoexperimentedwithGRUmodelsshowninTable5,thathave2560hiddenunitsintheGRU layerandatotalof115millionparameters.Fortheseexperiments,weprunealllayersexceptthe convolutionlayerssincetheyhaverelativelyfewerparameters. Figure1bcomparesthetraininganddevcurvesofasparseGRUmodeladenseGRUmodel.The sparseGRUmodelhasa13.8%dropintheaccuracyrelativetothedensemodel.Asshownin Table3,thesparsemodelhasanoverallsparsityof88.6%with13millionparameters.Similar totheRNNmodels,wetrainasparseGRUmodelwith3568hiddenunits.Thedatasetandthe hyperparametersarenotchangedfromthepreviousGRUexperiments.Thismodelhasanoverall sparsityof91.82%with17.8millionparameters.AsshowninTable3,themodelwith3568hidden unitsisonly2.2%worsethanthebaselinedenseGRUmodel.Weexpecttomatchtheperformance oftheGRUdensenetworkbyslightlyloweringthesparsityofthisnetworkorbyincreasingthe hiddenunitsforthelayers. Inaddition,weexperimentedwithpruningonlytheGRUlayersandkeepingalltheparametersin fullyconnectedlayers.Theaccuracyfortheseexperimentsisaround7%worsethanthebaseline densemodel.However,thismodelonlyachieves50%compressionduetothesizeofthefully connectedlayers. 6 PublishedasaconferencepaperatICLR2017 Table6:GEMMtimesforrecurrentlayerswithdifferentsparsity LAYERSIZESPARSITYLAYERTYPETIME(  sec)SPEEDUP 17600%RNN561 176095%RNN202.8 256095%RNN291.93 307295%RNN481.16 25600%GRU3131 256095%GRU466.80 356895%GRU893.5 5P ERFORMANCE 5.1C OMPUTETIME Thesuccessofdeeplearninginrecentyearshavebeendrivenbylargemodelstrainedonlarge datasets.Howeverthisalsoincreasestheinferencetimeafterthemodelshavebeendeployed.We canmitigatethiseffectbyusingsparselayers. AGeneralMatrix-MatrixMultiply(GEMM)isthemostcomputeintensiveoperationinevaluatinga neuralnetworkmodel.Table6comparestimesforGEMMforrecurrentlayerswithdifferentnumber ofhiddenunitsthatare95%sparse.TheperformancebenchmarkwasrunusingNVIDIA'sCUDNN andcuSPARSElibrariesonaTitanXMaxwellGPUandcompiledusingCUDA7.5.Allexperiments arerunonaminibatchof1andinthiscase,theoperationisknownasasparsematrix-vectorproduct (SpMV).Wecanachievespeed-upsrangingfrom3xto1.15xdependingonthesizeoftherecurrent layer.Similarly,fortheGRUmodels,thespeed-upsrangefrom7xto3.5x.However,wenotice thatcuSPARSEperformanceissubstantiallylowerthantheapproximately20xspeedupthatwe wouldexpectbycomparingthebandwidthrequirementsofthe95%sparseanddensenetworks. StateoftheartSpMVroutinescanachieveclosetodevicememorybandwidthforawidearrayof matrixshapesandsparsitypatterns(seeBaxter(2016)andLiuetal.(2013)).Thismeansthatthe performanceshouldimprovebythefactorthatparametercountsarereduced.Additionally,we thatthecuSPARSEperformancedegradeswithlargerbatchsizes.Itshouldbepossibleforabetter implementationtofurtherexploitthereuseoftheweightmatrixprovidedbylargebatch sizes. 5.2C OMPRESSION Pruningallowsustoreducethememoryfootprintofamodelwhichallowsthemtobedeployed onphonesandotherembeddeddevices.TheDeepSpeech2modelcanbecompressedfrom268 MBtoaround32MB(1760hiddenunits)or64MB(3072hiddenunits).TheGRUmodelcanbe compressedfrom460MBto50MB.Theseprunedmodelscanbefurtherquantizeddownto orothersmallerdatatypestofurtherreducethememoryrequirementswithoutimpactingaccuracy. 6D ISCUSSION 6.1P RUNING C HARACTERISTICS Figure2ashowsthesparsityofalltherecurrentlayerswiththesamehyper-parametersusedtoprune thelayers.Thelayersareorderedsuchthatlayer1isclosesttoinputandlayer14istherecur- rentlayerbeforethecostlayer.Weseethattheinitiallayersareprunedmoreaggressivelycompared tothelayers.Wealsoperformedexperimentswherethehyperparametersaredifferentforthe recurrentlayersresultinginequalsparsityforallthelayers.However,wegethigherCERforthese experiments.Weconcludethattogetgoodaccuracy,itisimportanttoprunethelayersslightly lessthantheinitialones. 7 PublishedasaconferencepaperatICLR2017 (a) (b) Figure2:Pruningcharacteristics.Figure2aplotssparsityofrecurrentlayersinthenetworkwith thesamehyper-parametersusedforpruning.Figure2bplotsthepruningscheduleofasinglelayer duringatrainingrun. InFigure2b,weplotthepruningscheduleofa95%sparserecurrentlayerofthebidirectionalmodel trainedfor20epochs(55000iterations).Webeginpruningthenetworkatthestartofthesecond epochat2700iterations.Westoppruningalayerafter10epochs(halfthetotalepochs)arecomplete at27000iterations.Weseethatnearly25000weightsareprunedbefore5epochsarecompleteat around15000iterations.Inourexperiments,we'venoticedthatpruningschedulesthatareaconvex curvetendtooutperformscheduleswithalinearslope. 6.2P ERSISTENT K ERNELS PersistentRecurrentNeuralNetworks(Diamosetal.,2016)isatechniquethatincreasesthecompu- tationalintensityofevaluatinganRNNbycachingtheweightsinon-chipmemorysuchascaches, blockRAM,orregisteracrossmultipletimesteps.Ahighdegreeofsparsityallows largePersistentRNNstobestoredinon-chipmemory.Whenalltheweightsarestoredin aNVIDIAP100GPUcansupportavanillaRNNsizeofabout2600hiddenunits.Withthesame datatype,at 90% sparsity,and 99% sparsity,aP100cansupportRNNswithabout8000,and24000 hiddenunitsrespectively.Weexpectthesekernelstobebandwidthlimitedoutofthememorythat isusedtostoretheparameters.Thisoffersthepotentialofa146xspeedupcomparedtotheTitanX GPUiftheentireRNNlayercanbestoredinregistersratherthantheGPUDRAMofaTitanX. Additionally,sparsematrixmultiplicationinvolvesschedulingandloadbalancingphasestodivide theworkupevenlyoverthousandsofthreadsandtoroutecorrespondingweightsandactivations toindividualthreads.SincethesparsitypatternsforRNNsareedovermanytimestepsthese schedulingandloadbalancingoperationscanbefactoredoutsideoftheloop,performedonce,and reusedmanytimes. 7C ONCLUSIONAND F UTURE W ORK WehavedemonstratedthatbypruningtheweightsofRNNsduringtrainingwecansparsemod- elsthataremoreaccuratethandensemodelswhilereducingmodelsize.Thesesparse modelsareespeciallysuitedfordeploymentonmobiledevicesandonback-endserverfarmsdue totheirsmallsizeandincreasedcomputationalefy.Evenwithexistingsub-optimalsparse matrix-vectorlibrarieswerealizespeed-upswiththesemodels.Thistechniqueisorthogonalto quantizationtechniqueswhichwouldallowforevenfurtherreductionsinmodelsizeandcorre- spondingincreaseinperformance. Wewishtoinvestigatewhetherthesetechniquescangeneralizetolanguagemodelingtasksandif theycaneffectivelyreducethesizeofembeddinglayers.Wealsowishtocomparethesparsity generatedbyourpruningtechniquetothatobtainedbyL1regularization. 8 PublishedasaconferencepaperatICLR2017 Weareinvestigatingtrainingtechniquesthatdon'trequiremaintainingdensematricesforasig- portionofthecalculation.Furtherworkremainstoimplementoptimalsmallbatchsparse matrix-densevectorroutineforGPUsandARMprocessorsthatwouldhelpindeployment. A CKNOWLEDGMENTS WewouldliketothankBryanCatanzaroforhelpfuldiscussionsrelatedtothiswork. R EFERENCES DarioAmodei,RishitaAnubhai,EricBattenberg,CarlCase,JaredCasper,BryanCatanzaro,Jing- dongChen,MikeChrzanowski,AdamCoates,GregDiamos,etal.Deepspeech2:End-to-end speechrecognitioninenglishandmandarin. arXivpreprintarXiv:1512.02595 ,2015. SeanBaxter.Moderngpu,2016.URL https://nvlabs.github.io/moderngpu/ segreduce.html . WelinChen,DavidGrangier,andMichaelAuli.Strategiesfortraininglargevocabularyneural languagemodels. CoRR ,abs/1512.04906,2015a.URL http://arxiv.org/abs/1512. 04906 . WenlinChen,JamesT.Wilson,StephenTyree,KilianQ.Weinberger,andYixinChen.Compressing neuralnetworkswiththehashingtrick. CoRR ,abs/1504.04788,2015b.URL http://arxiv. org/abs/1504.04788 . KyunghyunCho,BartVanMerri ¨ enboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,Hol- gerSchwenk,andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoder forstatisticalmachinetranslation. arXivpreprintarXiv:1406.1078 ,2014. MishaDenil,BabakShakibi,LaurentDinh,Marc'AurelioRanzato,andNandodeFreitas.Predicting parametersindeeplearning. CoRR ,abs/1306.0543,2013.URL http://arxiv.org/abs/ 1306.0543 . EmilyDenton,WojciechZaremba,JoanBruna,YannLeCun,andRobFergus.Exploitinglinear structurewithinconvolutionalnetworksforefevaluation. CoRR ,abs/1404.0736,2014. URL http://arxiv.org/abs/1404.0736 . GregDiamos,ShubhoSengupta,BryanCatanzaro,MikeChrzanowski,AdamCoates,ErichElsen, JesseEngel,AwniHannun,andSanjeevSatheesh.Persistentrnns:Stashingrecurrentweightson- chip.In ProceedingsofThe33rdInternationalConferenceonMachineLearning ,pp.2024Œ2033, 2016. YunchaoGong,LiuLiu,MingYang,andLubomirD.Bourdev.Compressingdeepconvolutional networksusingvectorquantization. CoRR ,abs/1412.6115,2014.URL http://arxiv.org/ abs/1412.6115 . AlexGravesandNavdeepJaitly.Towardsend-to-endspeechrecognitionwithrecurrentneural networks.In ICML ,volume14,pp.1764Œ1772,2014. SongHan,HuiziMao,andWilliamJDally.Deepcompression:Compressingdeepneuralnetwork withpruning,trainedquantizationandhuffmancoding. CoRR,abs/1510.00149 ,2,2015. AwniHannun,CarlCase,JaredCasper,BryanCatanzaro,GregDiamos,ErichElsen,RyanPrenger, SanjeevSatheesh,ShubhoSengupta,AdamCoates,etal.Deepspeech:Scalingupend-to-end speechrecognition. arXivpreprintarXiv:1412.5567 ,2014. StephenJos ´ eHansonandLorienPratt.Advancesinneuralinformationprocessingsystems1.chap- terComparingBiasesforMinimalNetworkConstructionwithBack-propagation,pp.177Œ185. MorganKaufmannPublishersInc.,SanFrancisco,CA,USA,1989.ISBN1-558-60015-9.URL http://dl.acm.org/citation.cfm?id=89851.89872 . 9 PublishedasaconferencepaperatICLR2017 BabakHassibi,DavidGStork,andGregoryJWolff.Optimalbrainsurgeonandgeneralnetwork pruning.In NeuralNetworks,1993.,IEEEInternationalConferenceon ,pp.293Œ299.IEEE, 1993. MaxJaderberg,AndreaVedaldi,andAndrewZisserman.Speedingupconvolutionalneuralnetworks withlowrankexpansions. CoRR ,abs/1405.3866,2014.URL http://arxiv.org/abs/ 1405.3866 . RafalJ ´ ozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.Exploringthe limitsoflanguagemodeling. CoRR ,abs/1602.02410,2016.URL http://arxiv.org/abs/ 1602.02410 . YannLeCun,JohnSDenker,SaraASolla,RichardEHoward,andLawrenceDJackel.Optimal braindamage.In NIPs ,volume2,pp.598Œ605,1989. XingLiu,MikhailSmelyanskiy,EdmondChow,andPradeepDubey.Efsparsematrix-vector multiplicationonx86-basedmany-coreprocessors.In Proceedingsofthe27thInternationalACM ConferenceonInternationalConferenceonSupercomputing ,ICS'13,pp.273Œ282,NewYork, NY,USA,2013.ACM.ISBN978-1-4503-2130-3.doi:10.1145/2464996.2465013.URL http: //doi.acm.org/10.1145/2464996.2465013 . ZhiyunLu,VikasSindhwani,andTaraN.Sainath.Learningcompactrecurrentneuralnetworks. CoRR ,abs/1604.02594,2016.URL http://arxiv.org/abs/1604.02594 . VincentVanhoucke,AndrewSenior,andMarkZ.Mao.Improvingthespeedofneuralnetworkson cpus.In DeepLearningandUnsupervisedFeatureLearningWorkshop,NIPS2011 ,2011. YonghuiWu,MikeSchuster,ZhifengChen,QuocV.Le,MohammadNorouzi,WolfgangMacherey, MaximKrikun,YuanCao,QinGao,KlausMacherey,JeffKlingner,ApurvaShah,MelvinJohn- son,XiaobingLiu,LukaszKaiser,StephanGouws,YoshikiyoKato,TakuKudo,HidetoKazawa, KeithStevens,GeorgeKurian,NishantPatil,WeiWang,CliffYoung,JasonSmith,JasonRiesa, AlexRudnick,OriolVinyals,GregCorrado,MacduffHughes,andJeffreyDean.Google'sneural machinetranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation. CoRR , abs/1609.08144,2016.URL http://arxiv.org/abs/1609.08144 . DongYu,FrankSeide,GangLi,andLiDeng.Exploitingsparsenessindeepneuralnetworks forlargevocabularyspeechrecognition.In 2012IEEEInternationalConferenceonAcoustics, SpeechandSignalProcessing(ICASSP) ,pp.4409Œ4412.IEEE,2012. 10  
